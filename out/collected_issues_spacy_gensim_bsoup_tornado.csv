search_keywords,issue_link,issue_title,issue_body,issue_score,issue_views,answer_1,answer_2,answer_3
SpaCy unexpected behavior,https://stackoverflow.com/questions/71093471,Unexpected behavior of SpaCy matcher with negation,"Somehow I have trouble understanding the negation in SpaCy matchers. I tried this code:
import spacy
from spacy.matcher import Matcher
import json



nlp = spacy.load('en_core_web_sm')
    #from spacy.tokenizer import Tokenizer
matcher = Matcher(nlp.vocab)

Sentence = ""The cat is black""

negative_sentence = ""The cat is not black""

test_pattern = '''
[
  [
    {
      ""TEXT"": ""cat""
    },
    {
      ""LEMMA"": ""be""
    },
    {
      ""LOWER"": ""not"",
      ""OP"": ""!""
      
    },
    {
      ""LOWER"": ""black""
    }
  ]
]
''' 

db = json.loads(test_pattern)

matcher.add(""TEST_PATTERNS"", db)


'''*********************Validate matcher on positive sentence******************'''
doc = nlp(Sentence, matcher)

matches = matcher(doc)

if matches != []:
    print('Positive sentence identified')
else:
    print('Nothing found for positive sentence')

'''*********************Validate matcher on negative sentence******************'''
doc = nlp(negative_sentence, matcher)
matches = matcher(doc)

if matches != []:
    print('Negative sentence identified')
else:
    print('Nothing found for negative sentence')
    


The result is:

Nothing found for positive sentence
Nothing found for negative sentence

I would expect that the sentence ""The cat is black"" would be a match. Furthermore, when I replace the ! with any other sign (""*"", ""?"", or ""+"") it works as expected:
import spacy
from spacy.matcher import Matcher
import json



nlp = spacy.load('en_core_web_sm')
    #from spacy.tokenizer import Tokenizer
matcher = Matcher(nlp.vocab)

Sentence = ""The cat is black""

negative_sentence = ""The cat is not black""

test_pattern = '''
[
  [
    {
      ""TEXT"": ""cat""
    },
    {
      ""LEMMA"": ""be""
    },
    {
      ""LOWER"": ""not"",
      ""OP"": ""?""
      
    },
    {
      ""LOWER"": ""black""
    }
  ]
]
''' 

db = json.loads(test_pattern)

matcher.add(""TEST_PATTERNS"", db)


'''*********************Validate matcher on positive sentence******************'''
doc = nlp(Sentence, matcher)

matches = matcher(doc)

if matches != []:
    print('Positive sentence identified')
else:
    print('Nothing found for positive sentence')

'''*********************Validate matcher on negative sentence******************'''
doc = nlp(negative_sentence, matcher)
matches = matcher(doc)

if matches != []:
    print('Negative sentence identified')
else:
    print('Nothing found for negative sentence')


Result:

Positive sentence identified
Negative sentence identified

How can I use the negation and only identify ""The cat is black"" and not ""The cat is not black"".
The reason why like to of the ""OP"" is because there might also other words between ""is"" and ""black"" (e.g., ""The cat is kind and black"" and not ""The cat is not kind and black"" ).
Any help on understanding negation with SpaCy matchers is highly appreciated.
",2,366,"Each dictionary in your match pattern corresponds to a token by default. With the ! operator it still corresponds to one token, just in a negative sense. With the * operator it corresponds to zero or more tokens, with + it's one or more tokens.
Looking at your original pattern, these are your tokens:

text: cat
lemma: be
text: not, op: !
lower: cat

Given the sentence ""The cat is black"", the match process works like this:

""the"" matches nothing so we skip it.
""cat"" matches your first token.
""is"" matches your second token.
""black"" matches your third token because it is not ""not""
The sentence ends so there is no ""cat"" token, so the match fails.

When debugging patterns it's helpful to step through them like above.
For the other ops... * and ? work because ""not"" matches zero times. I would not expect + to work in the positive case.
The way you are trying to avoid matching negated things is kind of tricky. I would recommend you match all sentences with the relevant words first, ignoring negation, and then check if there is negation using the dependency parse.
",,
SpaCy unexpected output,https://stackoverflow.com/questions/67252812,Can&#39;t evaluate custom ner in spacy 3.0 using CLI,"I'm trying to load a custom pre-trained model with custom pipeline from disk as a pipeline in spacy 3.0:
The code of the factory is like this:
@CustomEng.factory(""ner-crf"")
def create_my_component(nlp, name):
    crf_extractor = CRFExtractor().from_disk(""path-to-model"")
    return CRFEntityExtractor(nlp, crf_extractor=crf_extractor)

Then I added to 'ner-crf' to my Language class like this:
    nlp = spacy.blank('custom-eng')
    nlp.add_pipe('ner-crf')
    nlp.to_disk('../model')

There's a thing I think may be relevant: When I use to_disk in order to save the nlp object there is no ner-crf package (the pipeline I just added)  in the saved object.
Then I run this CLI command to evaluate the NER pipeline:
python -m spacy evaluate ../model/ ../corpus/dev.spacy --output ../model/metrics.json --gpu-id 0 --code ../../../spacy_utils/custom-eng/__init__.py

But I get this error :
Traceback (most recent call last):
  File ""/home/marzi/anaconda3/envs/spacy-tutorial/lib/python3.8/runpy.py"", line 194, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File ""/home/marzi/anaconda3/envs/spacy-tutorial/lib/python3.8/runpy.py"", line 87, in _run_code
    exec(code, run_globals)
  File ""/home/marzi/anaconda3/envs/spacy-tutorial/lib/python3.8/site-packages/spacy/__main__.py"", line 4, in &lt;module&gt;
    setup_cli()
  File ""/home/marzi/anaconda3/envs/spacy-tutorial/lib/python3.8/site-packages/spacy/cli/_util.py"", line 69, in setup_cli
    command(prog_name=COMMAND)
  File ""/home/marzi/anaconda3/envs/spacy-tutorial/lib/python3.8/site-packages/click/core.py"", line 829, in __call__
    return self.main(*args, **kwargs)
  File ""/home/marzi/anaconda3/envs/spacy-tutorial/lib/python3.8/site-packages/click/core.py"", line 782, in main
    rv = self.invoke(ctx)
  File ""/home/marzi/anaconda3/envs/spacy-tutorial/lib/python3.8/site-packages/click/core.py"", line 1259, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File ""/home/marzi/anaconda3/envs/spacy-tutorial/lib/python3.8/site-packages/click/core.py"", line 1066, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File ""/home/marzi/anaconda3/envs/spacy-tutorial/lib/python3.8/site-packages/click/core.py"", line 610, in invoke
    return callback(*args, **kwargs)
  File ""/home/marzi/anaconda3/envs/spacy-tutorial/lib/python3.8/site-packages/typer/main.py"", line 497, in wrapper
    return callback(**use_params)  # type: ignore
  File ""/home/marzi/anaconda3/envs/spacy-tutorial/lib/python3.8/site-packages/spacy/cli/evaluate.py"", line 42, in evaluate_cli
    evaluate(
  File ""/home/marzi/anaconda3/envs/spacy-tutorial/lib/python3.8/site-packages/spacy/cli/evaluate.py"", line 75, in evaluate
    nlp = util.load_model(model)
  File ""/home/marzi/anaconda3/envs/spacy-tutorial/lib/python3.8/site-packages/spacy/util.py"", line 326, in load_model
    return load_model_from_path(Path(name), **kwargs)
  File ""/home/marzi/anaconda3/envs/spacy-tutorial/lib/python3.8/site-packages/spacy/util.py"", line 392, in load_model_from_path
    return nlp.from_disk(model_path, exclude=exclude)
  File ""/home/marzi/anaconda3/envs/spacy-tutorial/lib/python3.8/site-packages/spacy/language.py"", line 1883, in from_disk
    util.from_disk(path, deserializers, exclude)
  File ""/home/marzi/anaconda3/envs/spacy-tutorial/lib/python3.8/site-packages/spacy/util.py"", line 1176, in from_disk
    reader(path / key)
  File ""/home/marzi/anaconda3/envs/spacy-tutorial/lib/python3.8/site-packages/spacy/language.py"", line 1877, in &lt;lambda&gt;
    deserializers[name] = lambda p, proc=proc: proc.from_disk(
TypeError: from_disk() got an unexpected keyword argument 'exclude'


The custom NER classes that I used belong to spacy-crfsuite library which works fine in spacy 2 but they have no sample code for Spacy 3 yet so I'm trying to make it work in spacy 3.0 myself.
",1,464,"From spaCy v3.0 onwards, pipeline components are expected to support an exclude keyword on their to_disk method. You can add the exclude keyword to your function, give it a default, and simply not use its value in the function body, and this error should be resolved.
For completeness, here's the migration guide for the transition from v2 to v3, which may include some additional interesting pointers for you: https://spacy.io/usage/v3#migrating
",,
SpaCy unexpected result,https://stackoverflow.com/questions/63693463,SpaCy lemmatizer removes capitalization,"I would like to lemmatize some textual data in Hungarian language and encountered a strange feature in spaCy. The token.lemma_ function works well in terms of lemmatization, however, it returns some of the sentences without first letter capitalization. This is quite annoying, as my next function, unnest_stences (R) requires first capital letters in order to identify and break the text down into individual sentences. 
First I thought the problem was that I used the latest version of spaCy since I had gotten a warning that

UserWarning: [W031] Model  'hu_core_ud_lg' (0.3.1) requires spaCy v2.1
and is incompatible with the current spaCy  version (2.3.2). This may
lead to unexpected results or runtime errors. To resolve this,
download a newer compatible model or retrain your custom model with
the current spaCy  version.

So I went ahead and installed spacy 2.1, but the problem still persists. 
The source of my data are some email messages I cannot share here, but here is a small, artificial example:
# pip install -U spacy==2.1 # takes  9 mins
# pip install hu_core_ud_lg # takes 50 mins

import spacy
from spacy.lemmatizer import Lemmatizer
import hu_core_ud_lg
import pandas as pd
nlp = hu_core_ud_lg.load()

a = ""Tisztelt levélíró!""
b = ""Köszönettel vettük megkeresését.""
df = pd.DataFrame({'text':[a, b]})

output_lemma = []

for i in df.text:
    mondat = """"
    doc = nlp(i)    
    for token in doc:
        mondat = mondat + "" "" + token.lemma_
    output_lemma.append(mondat)

output_lemma

which yields
[' tisztelt levélíró !', ' köszönet vesz megkeresés .']

but I would expect
[' Tisztelt levélíró !', ' Köszönet vesz megkeresés .']

When I pass my original data to the function, it returns some sentences with upercase first letters, others with lowercase letters. For some strange reason I couldn't reproduce that pattern above, but I guess the main point is visible. The function does not work as expected.
Any ideas how I could fix this?
I'm using Jupyter Notebook, Python 2.7, Win 7 and a Toshiba laptop (Portégé Z830-10R i3-2367M).
",1,1141,"Lowercasing is the expected behavior of spaCy's lemmatizer for non-proper-noun tokens.
One workaround is to check if each token is titlecased, and convert to original casing after lemmatizing (only applies to the first character).
import spacy

nlp = spacy.load('en_core_web_sm')

text = 'This is a test sentence.'
doc = nlp(text)
newtext = ' '.join([tok.lemma_.title() if tok.is_title else tok.lemma_ for tok in doc])
print(newtext)
# This be a test sentence .

",,
SpaCy unexpected result,https://stackoverflow.com/questions/51514545,Spacy - English language model outruns german language model on german text?,"Is it by design that the english language model performs better on german salution entities than the german model?

# pip install spacy
# python -m spacy download en
# python -m spacy download de

nlp = spacy.load('en')
# Uncomment line below to get less good results
# nlp = spacy.load('de')

# Process text
text = (u""Das Auto kauft Herr Müller oder Frau Meier, Frank Muster"")
doc = nlp(text)

# Find named entities
for entity in doc.ents:
    print(entity.text, entity.label_)


expected result if using nlp = spacy.load('en'). All three PERSON is returned

Das Auto ORG
Herr Müller PERSON
Frau Meier PERSON
Frank Muster PERSON


unexpected result if using nlp = spacy.load('de'). Only one of three PERSON is returned

Frank Muster PERSON




Info about spaCy


spaCy version:** 2.0.12
Platform:** Linux-4.17.2-1-ARCH-x86_64-with-arch-Arch-Linux
Python version:** 3.6.5
Models:** en, de

",0,635,"It's not by design, but it's certainly possible that this is a side-effect of the training data and the statistical predictions. The English model is trained on a larger NER corpus with more entity types, while the German model uses NER data based on Wikipedia. 

In Wikipedia text, full names like ""Frank Muster"" are quite common, whereas things like ""Herr Muster"" are typically avoided. This might explain why the model only labels the full name as a person and not the others. The example sentence also makes it easy for the English model to guess correctly – in English,  capitalization is a much stronger indicator of a named entity than it is in German. This might explain why the model consistently labels all capitalised multi-word spans as entities.

In any case, this is a good example of how subtle language-specific or stylistic conventions end up influencing a model's predictions. It also shows why you almost always want to fine-tune a model with more examples specific to your data. I do think that the German model will likely perform better on German texts overall, but if references like ""Herr Müller"" are common in your texts, you probably want to update the model with more examples of them in different contexts.
",,
SpaCy strange behavior,https://stackoverflow.com/questions/66433304,spacy- why nlp() works for single string while nlp.pipe() works fine for a list of strings?,"I recently ran into a strange behavior while using spacy,
which is when I process string,
in case the string is a single string object,
I have to use nlp(string),
while I have to use nlp.pipe(a list) for a list made of strings elements.
The example is as below.
string='this is a string to be process by nlp'

doc =['this','is','a','string','list','to','be','processed','by','spacy']

stringprocess= list(nlp(string))

listprocess = list(nlp.pipe(doc))

listprocess

stringprocess

Why is this?
I assume this must be something to do with nlp.pipe() behavior which is generator.
What is the reason?
Thank you.
",3,612,"Spacy does this because generators are more efficient. Since generators are consumed only once they are more memory efficient than a list.
According to their documentation instead of processing texts one-by-one and applying nlp pipeline it processes texts in batches.
Furthermore, you can configure batch size in nlp.pipe to optimize performance according to your system

Process the texts as a stream using nlp.pipe and buffer them in
batches, instead of one-by-one. This is usually much more efficient.

If your goal is to process large streams of data using nlp.pipe it would be much more efficient to write a streamer/generator to produce results as you need them from database/filesystem than loading everything in memory and then processing them one by one.
spacy pipe
",,
SpaCy strange behavior,https://stackoverflow.com/questions/62057053,Dockerized Python Scripts Having Issues Accessing Files Stored to /tmp,"I want to apologize in advance that I don't have specific code examples to share (though I have included my docker-compose file below that may or may not be helpful). I am having a strange issue I can't seem to trace and I am not 100% sure what to share.

I have a django + celery setup running in Docker (based off of cookiecutter-django). Everything seems to be working great at first. I have extensively tested and used this setup outside of Docker, and the Dockerized Celery tasks generally behave as I'd expect (i.e., as they did when they were not Dockerized).

Here's where things get weird, though. For various reasons, I need to load some data files and create temp files at runtime that I can't just put in my docker file. One example is Using NamedTemporaryFile. Another is installing the data files from Spacy. In both cases, my scripts are storing data to /tmp (I know the easy answer here is to put them in my docker file, but I can't predict which files I need ahead of time, sadly). When my celery task worker tries to access data files it supposedly created, downloaded, and/or stored to /tmp, I keep getting file not found errors. Weird thing is I don't get any errors in my logs that file creation or downloads failed...

Yet, when I /bin/bash into my celeryworker container and cd to the /tmp directory, sure enough, no files are there... If, using the python console in my container, I run the same code, I have no issues and the files do show up in /tmp. I'm not getting a consistent error message and the behavior is manifesting itself differently in different scripts, so it's hard to give you a specific error message / stack trace. The common element appears to stem from some issue that arises when my celery task tries to dynamically store data in /tmp and then access it immediately after that. I'm a Docker newb and am at a loss as to what to do next. I suspect this may be a permissioning issue, but I've tried chmodding /tmp to 777 and it didn't fix anything. I also thought it might be a volume issue, but, if that's the case, I wasn't sure why I can make everything work if I use bash inside my container. 

Anyone have any advice here? Spent a couple of days trying to trace the source of the issue at this point and am at a dead end. My docker file and configs are pretty much plain vanilla copies of Django Cookiecutter defaults.

version: '3'

volumes:
  local_postgres_data: {}
  local_postgres_data_backups: {}

services:
  django: &amp;django
    build:
      context: .
      dockerfile: ./compose/local/django/Dockerfile
    image: gremlin_gplv3_local_django
    container_name: django
    depends_on:
      - postgres
      - tika
      - redis
    volumes:
      - .:/app
    env_file:
      - ./.envs/.local/.django
      - ./.envs/.local/.postgres
    ports:
      - ""8000:8000""
    command: /start

  postgres:
    build:
      context: .
      dockerfile: ./compose/production/postgres/Dockerfile
    image: gremlin_gplv3_production_postgres
    container_name: postgres
    volumes:
      - local_postgres_data:/var/lib/postgresql/data
      - local_postgres_data_backups:/backups
    env_file:
      - ./.envs/.local/.postgres

  redis:
    image: redis:5.0
    container_name: redis

  celeryworker:
    &lt;&lt;: *django
    image: gremlin_gplv3_local_celeryworker
    container_name: celeryworker
    depends_on:
      - redis
      - postgres

    ports: []
    command: /start-celeryworker

  celerybeat:
    &lt;&lt;: *django
    image: gremlin_gplv3_local_celerybeat
    container_name: celerybeat
    depends_on:
      - redis
      - postgres

    ports: []
    command: /start-celerybeat

  flower:
    &lt;&lt;: *django
    image: gremlin_gplv3_local_flower
    container_name: flower
    ports:
      - ""5555:5555""
    command: /start-flower

  tika:
    image: lexpredict/tika-server
    command: /start-tika

",2,802,"I figured this out... well, mostly. The problem is Spacy (and other, similar libraries and tools that download data files) put them into local directories on their file system and possibly create symlinks to them). In a docker container, however, these files and symlinks are not persistent, however, unless the parent directory is in a docker volume.
What I ended up doing is creating docker volumes for the folder that Spacy (or whatever library) uses to store data files / libraries. In my case, Spacy is always called from via Celery which has its own docker image in my docker-compose stack, so I needed to attach volumes for each of my Spacy data directories to my celeryworker like so:
version: '3'

volumes:
  local_postgres_data: {}
  local_postgres_data_backups: {}
  worker_usr: {}
  worker_root: {}
  worker_tmp: {}

services:
  
  [...]

  celeryworker:
    &lt;&lt;: *django
    image: local_django:latest
    container_name: celeryworker
    depends_on:
      - redis
      - postgres
    volumes:
      - worker_usr:/usr
      - worker_tmp:/tmp
      - worker_root:/root
      - .:/app

    ports: []
    command: /start-celeryworker

All of this said, I've noticed there are situations where, at first pass, the installation of a data file (like a Spacy model) in my worker container throws an error that the data file is still not accessible, yet, when this happens (and it's not all the time), I can just run the install again and 99% of the time, this appears to fix the issue. I have not had time to try to troubleshoot that. Perhaps someone else can figure that part out.
",,
SpaCy strange behavior,https://stackoverflow.com/questions/46222462,Rest Template unable to parse json rest api response properly,"I am trying to extract Named Entities from text using Spacy's NER for German text. I have exposed the service as a REST POST request which takes source text as input and returns a dictionary(Map) of list of named entities (person, location, organization). These services are exposed using Flask Restplus hosted on a linux server. 

Consider for a sample text, I get following response using POST request at REST API exposed via Swagger UI:

{
  ""ner_locations"": [
    ""Deutschland"",
    ""Niederlanden""
  ],
  ""ner_organizations"": [
    ""Miele &amp; Cie. KG"",
    ""Bayer CropScience AG""
  ],
  ""ner_persons"": [
    ""Sebastian Krause"",
    ""Alex Schröder""
  ]
}


When I use Spring's RestTemplate to POST request at the API hosted at Linux server from Spring boot application (on Windows OS in Eclipse). The json parsing is done correctly. I have added following line for using UTF-8 encoding.

restTemplate.getMessageConverters().add(0, new StringHttpMessageConverter(Charset.forName(""UTF-8"")));


But When I deploy this spring boot application on linux machine and POST request to API for NER tagging, the ner_persons are not parsed correctly. While remotely debugging, I get following response

{
  ""ner_locations"": [
    ""Deutschland"",
    ""Niederlanden""
  ],
  ""ner_organizations"": [
    ""Miele &amp; Cie. KG"",
    ""Bayer CropScience AG""
  ],
  ""ner_persons"": [
    ""Sebastian "",
    ""Krause"",
    ""Alex "",
    ""Schröder""
  ]
}


I am not able to understand why this strange behavior occurs in case of persons but not organizations.
",1,541,"Being new to python, it took me 2 days of debugging to understand the real problem and to find a workaround fix.

The reason was that the names (e.g., ""Sebastian Krause"") were separated by \xa0 i.e., non-breaking space character (e.g., ""Sebastian\xa0Krause"") instead of a whitespace. So Spacy was failing to detect them as a single NamedEntity.

Browsing through SO, I found following solution from here:

import unicodedata 
norm_text = unicodedata.normalize(""NFKD"", source_text)


This also normalizes other unicode characters such as \u2013,\u2026, etc. 
",,
SpaCy strange behavior,https://stackoverflow.com/questions/57825664,spacy similarity bigger than 1,"The spaCy similarity works strange sometimes.
If we compare the completely equal texts, we got a score of 1.0.
but the texts are almost equal we can get a score &gt; 1.
This behavior could harm our code.
Why we got this &gt; 1.0 score and can we predict it?

def calc_score(text_source, text_target):
    return nlp(text_source).similarity(nlp(text_target))

# nlp = spacy.load('en_core_web_md')
calc_score('software development', 'Software development')
# 1.0000000155153665

",0,517,"From https://spacy.io/usage/vectors-similarity:


  Identical tokens are obviously 100% similar to each other (just not
  always exactly 1.0, because of vector math and floating point
  imprecisions).


Just use np.clip as per https://stackoverflow.com/a/13232356/447599
",,
SpaCy strange output,https://stackoverflow.com/questions/63366054,Strange character when I translate a pdf file to text using pdfminer,"I translated a pdf file using pdfminer and I realize that in several situations I found a strange non-ascii 'ﬁ' replacing 'fi'.
An easy way to correct this problem seems to be
 content=re.sub('ﬁ','fi',content)

However, I only could correct the problem because I noticed it and It is worth mentioning that it is very difficult to note it. I only note because I was writing a report in latex about a mistake my code was doing due to an incorrect classification that Spacy was providing to the 'fortified' (with this character). In this moment, I realize that the dvi file (output of the latex laguage) was failing. When I checked it I realized that these two characters 'fi' were replaced by something else.
This seems to be probably a kind of pdf font problem.
Is there a list of problems like this that I can predict and automatically solve before any nlp activity? Or maybe a way to use Spacy to check if a given word is unknown (I believe that this word 'fortified' with the strange replacement was unknown for spacy)? Or yet to look for non-ascii characters in the translated text?
Which of these solutions work?
",2,543,"In the end, I have now replaced automatically all ligatures:
        if(isinstance(content, str)):
            content=re.sub(r'\uA732', 'AA', content)
            content=re.sub(r'\uA733', 'aa', content)
    
            content=re.sub(r'\u00C6', 'AE', content)
            content=re.sub(r'\u00E6', 'ae', content)
    
            content=re.sub(r'\uA734', 'AO', content)
            content=re.sub(r'\uA735', 'ao', content)
    
            content=re.sub(r'\uA736', 'AU', content)
            content=re.sub(r'\uA737', 'au', content)
            
            content=re.sub(r'\uA738', 'AV', content)
            content=re.sub(r'\uA739', 'av', content)
    
            content=re.sub(r'\uA73A', 'AV', content)
            content=re.sub(r'\uA73B', 'av', content)
    
            content=re.sub(r'\uA73C', 'AY', content)
            content=re.sub(r'\uA73D', 'ay', content)
            
            content=re.sub(r'\u1F670', 'et', content)        
    
            content=re.sub(r'\uFB00', 'ff', content)
            content=re.sub(r'\uFB03', 'ffi', content)
            content=re.sub(r'\uFB04', 'ffl', content)
            content=re.sub(r'\uFB01', 'fi', content)
            content=re.sub(r'\uFB02', 'fl', content)
    
            content=re.sub(r'\u01F6', 'Hv', content)
            content=re.sub(r'\u0195', 'hv', content)
    
            content=re.sub(r'\u2114', 'lb', content)
            
            content=re.sub(r'\u1EFA', 'lL', content)
            content=re.sub(r'\u1EFB', 'll', content)
    
            content=re.sub(r'\u0152', 'OE', content)
            content=re.sub(r'\u0153', 'oe', content)
    
            content=re.sub(r'\uA74E', 'OO', content)
            content=re.sub(r'\uA74F', 'oo', content)
            
            content=re.sub(r'\uFB06', 'st', content)
            
            content=re.sub(r'\uFB05', 'ft', content)        
            
            content=re.sub(r'\uA728', 'TZ', content)
            content=re.sub(r'\uA729', 'tz', content)
            
            content=re.sub(r'\u1D6B', 'ue', content)
            content=re.sub(r'\uAB63', 'uo', content)        
            
             content=re.sub(r'\uA760', 'VY', content)
             content=re.sub(r'\uA761', 'vy', content)        

",,
SpaCy strange output,https://stackoverflow.com/questions/49703099,Unable to create a custom entity type/label using Matcher in Spacy 2,"I am trying to create a custom entity label called FRUIT using the rule-based Matcher (i.e. adding on_match rules), following the spaCy guide. I'm using spaCy 2.0.11, so I believe the steps to do so have changed compared to spaCy 1.X

Example: doc = nlp('Tom wants to eat some apples at the United Nations')
Expected text and entity outputs:

Tom PERSON
apples FRUIT
the United Nations ORG


However, I seem to get the following error: [E084] Error assigning label ID 7429577500961755728 to span: not in StringStore. I have included my code below. When I change nlp.vocab.strings['FRUIT'] to nlp.vocab.strings['EVENT'], strangely it works but apples would be assigned the entity label EVENT. Anyone else encountering this issue? 

doc = nlp('Tom wants to eat some apples at the United Nations')

FRUIT = nlp.vocab.strings['FRUIT']

def add_ent(matcher, doc, i, matches):
    # Get the current match and create tuple of entity label, start and end.
    # Append entity to the doc's entity. (Don't overwrite doc.ents!)
    match_id, start, end = matches[i]    
    doc.ents += ((FRUIT, start, end),)

matcher = Matcher(nlp.vocab)
pattern = [{'LOWER': 'apples'}]
matcher.add('AddApple', add_ent, pattern)

matches = matcher(doc)

for ent in doc.ents:
    print(ent.text, ent.label_)

",0,833,"Oh okay, I think I found a solution. The label has to be added to nlp.vocab.strings if it is not there:

nlp.vocab.strings.add('FRUIT') 

",,
SpaCy strange output,https://stackoverflow.com/questions/60656880,spaCy BERT dictionary,"I am trying to access spaCy BERT dictionary, but I receive strange output from the model. For instance for en_core_web_lg model I can extract ~1.3 million tokens like this

nlp = spacy.load(""en_core_web_lg"") 
tokens = [t for t in nlp.vocab]


When I do the same for en_trf_bertbaseuncased_lg model I only get 478 tokens, 

nlp = spacy.load(""en_trf_bertbaseuncased_lg"") 
tokens = [t for t in nlp.vocab]


while there should be ~30k tokens according to BERT paper. Is there a way I can access them via nlp.vocab or via custom component attributes?
",0,337,"I had a similar issue with standard models like ""en"". Interestingly enough the length is exactly the same.

import spacy
nlp = spacy.load(""en"")
len([t for t in nlp.vocab])
478


Turned out that models like ""en"" does not contain the real vectors (see https://github.com/explosion/spaCy/issues/1520 ). So I guess this is also the case for Bert. 
",,
SpaCy strange result,https://stackoverflow.com/questions/50752266,Spacy - Tokenize quoted string,"I am using spacy 2.0 and using a quoted string as input.  

Example string

""The quoted text 'AA XX' should be tokenized""


and expecting to extract 

[The, quoted, text, 'AA XX', should, be, tokenized]


I however get some strange results while experimenting.  Noun chunks and ents looses one of the quote.  

import spacy
nlp = spacy.load('en')
s = ""The quoted text 'AA XX' should be tokenized""
doc = nlp(s)
print([t for t in doc])
print([t for t in doc.noun_chunks])
print([t for t in doc.ents])


Result

[The, quoted, text, ', AA, XX, ', should, be, tokenized]
[The quoted text 'AA XX]
[AA XX']


What is the best way to address what I need
",7,4824,"While you could modify the tokenizer and add your own custom prefix, suffix and infix rules that exclude quotes, I'm not sure this is the best solution here.

For your use case, it might make more sense to add a component to your pipeline that merges (certain) quoted strings into one token before the tagger, parser and entity recognizer are called. To accomplish this, you can use the rule-based Matcher and find combinations of tokens surrounded by '. The following pattern looks for one or more alphanumeric characters:

pattern = [{'ORTH': ""'""}, {'IS_ALPHA': True, 'OP': '+'}, {'ORTH': ""'""}]


Here's a visual example of the pattern in the interactive matcher demo. To do the merging, you can then set up the Matcher, add the pattern and write a function that takes a Doc object, extracts the matched spans and merges them into one token by calling their .merge method.

import spacy
from spacy.matcher import Matcher

nlp = spacy.load('en')
matcher = Matcher(nlp.vocab)
matcher.add('QUOTED', None, [{'ORTH': ""'""}, {'IS_ALPHA': True, 'OP': '+'}, {'ORTH': ""'""}])

def quote_merger(doc):
    # this will be called on the Doc object in the pipeline
    matched_spans = []
    matches = matcher(doc)
    for match_id, start, end in matches:
        span = doc[start:end]
        matched_spans.append(span)
    for span in matched_spans:  # merge into one token after collecting all matches
        span.merge()
    return doc

nlp.add_pipe(quote_merger, first=True)  # add it right after the tokenizer
doc = nlp(""The quoted text 'AA XX' should be tokenized"")
print([token.text for token in doc])
# ['The', 'quoted', 'text', ""'AA XX'"", 'should', 'be', 'tokenized']


For a more elegant solution, you can also refactor the component as a reusable class that sets up the matcher in its __init__ method (see the docs for examples).

If you add the component first in the pipeline, all other components like the tagger, parser and entity recognizer will only get to see  the retokenized Doc. That's also why you might want to write more specific patterns that only merge certain quoted strings you care about. In your example, the new token boundaries improve the predictions – but I can also think of many other cases where they don't, especially if the quoted string is longer and contains a significant part of the sentence.
",,
SpaCy strange result,https://stackoverflow.com/questions/50078741,strange similarity result in spacy,"I am playing around with the similarity function in Spacy and observed something that I dont understand:

import spacy
nlp = spacy.load('en_core_web_sm')  
doc1 = nlp(""Honda Civic Toyota"")
doc2 = nlp(""Honda Civic Toyota car Christian God"")

for token in doc1:
    print (token.text, doc1[0].similarity(token))
for token in doc2:
    print (token.text, doc2[0].similarity(token))


Output:

Honda 1.0
Civic 0.6631208
Toyota 0.4700994
Honda 1.0
Civic 0.6806056
Toyota 0.54713947
car 0.22469836
Christian 0.5016042
God 0.4778438


The word Honda is being compared to all the other words in doc1 and doc2 and it can be observed that when Honda is compared with Civic and Toyota, the similarity is different in doc1 and doc2.
My understanding is that the similarity is computed from the consine similarity of the Glove vector of the words, which is loaded from 'en_core_web_sm'. If that is the case, shouldn't the similarity between the same pair of words the same regardless of its context?

I think clearly I am misunderstanding something, would appreciate it if someone could clarify on it.
",3,1448,"As mentioned in Word Vectors and Semantic  Similarity Doc:


  Similarity is determined by comparing word vectors or ""word embeddings"", multi-dimensional meaning representations of a word.To make them compact and fast, spaCy's small models (all packages that
  end in sm) don't ship with word vectors, and only include
  context-sensitive tensors. This means you can still use the
  similarity() methods to compare documents, spans and tokens – but the
  result won't be as good, and individual tokens won't have any vectors
  assigned.So in order to use real word vectors, you need to download a larger model


so small models don't use word vectors for similarity.
",,
SpaCy strange result,https://stackoverflow.com/questions/75173490,How can I check similarity in meaning and not just having same words between two texts with spacy,"I'm trying to compare two different texts—one coming from a Curriculum Vitae (CV) and the other from a job announcement.
After cleaning the texts, I'm trying to compare them to detect if a job announcement is more linked to a specific CV.
I am trying to do this using similarity matching in spaCy via the following code:
similarity = pdf_text.similarity(final_text_from_annonce)

This works well, but I'm getting strange results from two different CVs for the same job announcement. Specifically, I get the same similarity score (~0.6), however, one should clearly be higher than the other.
I checked on spaCy website and I found this very important sentence:

Vector averaging means that the vector of multiple tokens is insensitive to the order of the words. Two documents expressing the same meaning with dissimilar wording will return a lower similarity score than two documents that happen to contain the same words while expressing different meanings.

So, what do I need to use or code to make spaCy compare my two texts based on their meaning instead of the occurrence of words?
I am expecting a parameter for the similarity function of spaCy, or another function that will compare my both texts and calculate a similarity score based on the meaning of the texts and not if the same words are used.
",2,840,"The spaCy library by default will use the average of the word embeddings of words in a sentence to determine semantic similarity. This can be thought of as a naive sentence embedding approach. Such an approach could work, but if you were to use it is recommended that you first filter non-meaningful words (e.g. common words) to prevent them from undesirably influencing the final sentence embeddings.
The alternative (and more reliable) solution is to use a different pipeline within spaCy that has been designed to use sentence embeddings created specifically with a dedicated sentence encoder (e.g. the Universal Sentence Encoder (USE) [1] by Cer et al.). Martino Mensio created a package called spacy-universal-sentence-encoder that makes use of this model. Install it via the following command in your command prompt:
pip install spacy-universal-sentence-encoder

Then you can compute the semantic similarity between sentences as follows:
import spacy_universal_sentence_encoder

# Load one of the models: ['en_use_md', 'en_use_lg', 'xx_use_md', 'xx_use_lg']
nlp = spacy_universal_sentence_encoder.load_model('en_use_lg')

# Create two documents
doc_1 = nlp('Hi there, how are you?')
doc_2 = nlp('Hello there, how are you doing today?')

# Use the similarity method to compare the full documents (i.e. sentences)
print(doc_1.similarity(doc_2))  # Output: 0.9356049733134972
# Or make the comparison using a predefined span of the second document 
print(doc_1.similarity(doc_2[0:7])) # Output: 0.9739387861159459

As a side note, when you run the nlp = spacy_universal_sentence_encoder.load_model('en_use_lg') command for the first time, you may have to do so with administrator rights to allow TensorFlow to create the models folder in C:\Program Files\Python310\Lib\site-packages\spacy_universal_sentence_encoder and download the appropriate model. If you don't, it is possible that there will be a PermissionDeniedError and the code will not run.
References
[1] Cer, D., Yang, Y., Kong, S.Y., Hua, N., Limtiaco, N., John, R.S., Constant, N., Guajardo-Cespedes, M., Yuan, S., Tar, C. and Sung, Y.H., 2018. Universal sentence encoder. arXiv preprint arXiv:1803.11175.
",,
SpaCy strange result,https://stackoverflow.com/questions/62735456,Understanding and using Coreference resolution Stanford NLP tool (in Python 3.7),"I am trying to understand the Coreference NLP Stanford tools.
This is my code and it is working:
import os
os.environ[""CORENLP_HOME""] = ""/home/daniel/StanfordCoreNLP/stanford-corenlp-4.0.0""

from stanza.server import CoreNLPClient

text = 'When he came from Brazil, Daniel was fortiﬁed with letters from Conan but otherwise did not know a soul except Herbert. Yet this giant man from the Northeast, who had never worn an overcoat or experienced a change of seasons, did not seem surprised by his past.'

with CoreNLPClient(annotators=['tokenize','ssplit','pos','lemma','ner', 'parse', 'depparse','coref'],
               properties={'annotators': 'coref', 'coref.algorithm' : 'neural'},timeout=30000, memory='16G') as client:

    ann = client.annotate(text)

chains = ann.corefChain
chain_dict=dict()
for index_chain,chain in enumerate(chains):
    chain_dict[index_chain]={}
    chain_dict[index_chain]['ref']=''
    chain_dict[index_chain]['mentions']=[{'mentionID':mention.mentionID,
                                          'mentionType':mention.mentionType,
                                          'number':mention.number,
                                          'gender':mention.gender,
                                          'animacy':mention.animacy,
                                          'beginIndex':mention.beginIndex,
                                          'endIndex':mention.endIndex,
                                          'headIndex':mention.headIndex,
                                          'sentenceIndex':mention.sentenceIndex,
                                          'position':mention.position,
                                          'ref':'',
                                          } for mention in chain.mention ]


for k,v in chain_dict.items():
    print('key',k)
    mentions=v['mentions']
    for mention in mentions:
        words_list = ann.sentence[mention['sentenceIndex']].token[mention['beginIndex']:mention['endIndex']]
        mention['ref']=' '.join(t.word for t in words_list)
        print(mention['ref'])
    

I tried three algorithms:

statistical (as in the code above). Results:


he
this giant man from the Northeast , who had never worn an overcoat or experienced a change of seasons
Daniel
his



neural


this giant man from the Northeast , who had never worn an overcoat or experienced a change of seasons ,
his



deterministic (I got the error below)
 &gt; Starting server with command: java -Xmx16G -cp
 &gt; /home/daniel/StanfordCoreNLP/stanford-corenlp-4.0.0/*
 &gt; edu.stanford.nlp.pipeline.StanfordCoreNLPServer -port 9000 -timeout
 &gt; 30000 -threads 5 -maxCharLength 100000 -quiet True -serverProperties
 &gt; corenlp_server-9fedd1e9dfb14c9e.props -preload
 &gt; tokenize,ssplit,pos,lemma,ner,parse,depparse,coref Traceback (most
 &gt; recent call last):
 &gt; 
 &gt;   File ""&lt;ipython-input-58-0f665f07fd4d&gt;"", line 1, in &lt;module&gt;
 &gt;     runfile('/home/daniel/Documentos/Working Papers/Leader traits/Code/20200704 - Modeling
 &gt; Organizing/understanding_coreference.py',
 &gt; wdir='/home/daniel/Documentos/Working Papers/Leader
 &gt; traits/Code/20200704 - Modeling Organizing')
 &gt; 
 &gt;   File
 &gt; ""/home/daniel/anaconda3/lib/python3.7/site-packages/spyder_kernels/customize/spydercustomize.py"",
 &gt; line 827, in runfile
 &gt;     execfile(filename, namespace)
 &gt; 
 &gt;   File
 &gt; ""/home/daniel/anaconda3/lib/python3.7/site-packages/spyder_kernels/customize/spydercustomize.py"",
 &gt; line 110, in execfile
 &gt;     exec(compile(f.read(), filename, 'exec'), namespace)
 &gt; 
 &gt;   File ""/home/daniel/Documentos/Working Papers/Leader
 &gt; traits/Code/20200704 - Modeling
 &gt; Organizing/understanding_coreference.py"", line 21, in &lt;module&gt;
 &gt;     ann = client.annotate(text)
 &gt; 
 &gt;   File
 &gt; ""/home/daniel/anaconda3/lib/python3.7/site-packages/stanza/server/client.py"",
 &gt; line 470, in annotate
 &gt;     r = self._request(text.encode('utf-8'), request_properties, **kwargs)
 &gt; 
 &gt;   File
 &gt; ""/home/daniel/anaconda3/lib/python3.7/site-packages/stanza/server/client.py"",
 &gt; line 404, in _request
 &gt;     raise AnnotationException(r.text)
 &gt; 
 &gt; AnnotationException: java.lang.RuntimeException:
 &gt; java.lang.IllegalArgumentException: No enum constant
 &gt; edu.stanford.nlp.coref.CorefProperties.CorefAlgorithmType.DETERMINISTIC



Questions:

Why am I getting this error with the deterministic?

Any piece of code using the NLP Stanford in Python seems to be much slower than the codes related with Spacy or NLTK. I know that there is no coreference in these other libraries. But for instance when I use import nltk.parse.stanford import StanfordDependencyParser for dependence parse it is much faster then this StanfordNLP library. Is there any way to acelerate this CoreNLPClient in Python?

I will use this library to work with long texts. Is it better to work with smaller pieces with the entire text? Long texts can cause wrong results for coreference resolution (I have found very strange results for this coreference library when I am using long texts)? Is there an optimal size?

Results:


The results from the statistical algorithm seems to be better. I expected that the best result would come from the neural algorithm. Do you agree with me? There are 4 valid mention in the statistical algorithm while only 2 when I am using the neural algorithm.
Am I missing something?
",2,1456,"
You may find the list of supported algorithms in Java documentation: link

You might want to start the server and then just use it, something like
# Here's the slowest part—models are being loaded
client = CoreNLPClient(...)

ann = client.annotate(text)

...

client.stop()



But I cannot give you any clue regarding 3 and 4.
",,
SpaCy strange result,https://stackoverflow.com/questions/56451239,"When processing IMDB data prepared by myself with a Keras RNN, accuracy never exceeds 0.5","A very strange thing is happening. I fetched the IMDB corpus from Kaggle, kept only the 50,000 positive and negative texts, counted word frequencies, sorted words according to their decreasing frequency, replaced in the texts the 10,000 most frequent words by their rank (plus 3 units), inserted a 1 at all sentence begins and replaced all words outside the 10,000 most frequent ones by number 2. In this I followed exactly the instructions given in the documentation of Keras imdb class.

I then ran a RNN with an Embedding layer, a SimpleRNN layer, a Dense layer. The result I get is an accuracy always around 0.5, no matter how hard I try. I then replace my code by imdb.load_data(num_words=10000) and get an accuracy of 0.86 already at the third epoch. How is this possible? Why such an extreme difference? What have I done wrong?

Here is the code I used:

import re, os, time, pickle
word=re.compile(r'^[A-Za-z]+$')
spacyline=re.compile(r'^([0-9]+) ([^ ]+) ([^ ]+) ([^ ]+) ([0-9]+) ([A-Za-z]+)')

DICT=dict()

inp=open(""imdb_master_lemma.txt"")
for ligne in inp:
    if (ligne[0:9]==""DEBUT DOC""):
        if (ligne[-4:-1]==""neg""):
            classe=-1
        elif (ligne[-4:-1]==""pos""):
            classe=1
    elif (ligne[0:9]==""FIN DOCUM""):
        a=1
    else:
        res=spacyline.findall(ligne)
        if res:
            lemma=str(res[0][3])
            if (word.match(lemma)):
                if (lemma in DICT.keys()):
                    DICT[lemma] += 1
                else:
                    DICT[lemma]=1
inp.close()

SORTED_WORDS=sorted(DICT.keys(), key=lambda x:DICT[x], reverse=True)
THOUSAND=SORTED_WORDS[:9997]
ORDRE=dict()
c=0
for w in THOUSAND:
    ORDRE[w]=c
    c+=1
CORPUS=[]
CLASSES=[]

inp=open(""imdb_master_lemma.txt"")
for ligne in inp:
    if (ligne[0:9]==""DEBUT DOC""):
        if (ligne[-4:-1]==""neg""):
            classe=0
        elif (ligne[-4:-1]==""pos""):
            classe=1
        a=[]
    if (ligne[0:9]==""DEBUT PHR""):
        a.append(1)
    elif (ligne[0:9]==""FIN DOCUM""):
        CORPUS.append(a)
        CLASSES.append(classe)
    else:
        res=spacyline.findall(ligne)
        if res:
            lemma=str(res[0][3])
            if lemma in ORDRE:
                a.append(ORDRE[lemma]+3)
            elif (word.match(lemma)):
                a.append(2)
inp.close()

from sklearn.utils import shuffle
CORPUS, CLASSES=shuffle(CORPUS, CLASSES)

out=open(""keras1000.pickle"",""wb"")
pickle.dump((CORPUS,CLASSES,ORDRE),out)
out.close()


The file imdb_master_lemma.txt contains the IMDB texts processed by Spacy, and I keep only the lemma (which is already in lowercase, so this is more or less what is used in Keras imdb only it should work even better since there are no plurals and verbs are lemmatized). Once the pickle file stored, I recall it and use it as follows:

picklefile=open(""keras1000.pickle"",""rb"")
(CORPUS,CLASSES,ORDRE)=pickle.load(picklefile)
picklefile.close()

import numpy as np
def vectorize_sequences(sequences, dimension=10000):
    results = np.zeros((len(sequences), dimension))
    for i, sequence in enumerate(sequences):
        results[i, sequence] = 1.
    return results
x_train = np.array(vectorize_sequences(CORPUS[:25000]),dtype=object)
x_test = np.array(vectorize_sequences(CORPUS[25000:]),dtype=object)
train_labels = np.array(CLASSES[:25000])
test_labels = np.array(CLASSES[25000:])

from keras import models
from keras import layers
from keras.models import Sequential
from keras.layers import Flatten, Dense, Embedding, SimpleRNN, LSTM, Bidirectional
from keras.preprocessing import sequence

input_train = sequence.pad_sequences(x_train, maxlen=500)
input_test = sequence.pad_sequences(x_test, maxlen=500)
print('input_train shape:', input_train.shape)
print('input_test shape:', input_test.shape)

model = Sequential()
model.add(Embedding(10000, 32))
model.add(SimpleRNN(32))
model.add(Dense(1, activation='sigmoid'))
model.compile(optimizer='rmsprop',
              loss='binary_crossentropy',
              metrics=['acc'])

history = model.fit(input_train,
                    train_labels,
                    epochs=10,
                    batch_size=128,
                    validation_split=0.2)
results = model.evaluate(input_test, test_labels)

print(results)


The result is utterly disappointing, an accuracy around 0.5. When I replace the 14 first lines by

from keras.datasets import imdb
(x_train, train_labels), (x_test, test_labels) = imdb.load_data(num_words=10000)


then everything works as described in Chollet's book and I get immediately a very high accuracy.

Can anyone tell me what I am doing wrong?

PS. Here is a small sample of the data, to illustrate the preparation process: The two first sentences of the first IMDB document, as processed by spaCy, are

DEBUT DOCUMENT neg
DEBUT PHRASE
0 Once RB once 1 advmod
1 again RB again 5 advmod
2 Mr. NNP mr. 3 compound
3 Costner NNP costner 5 nsubj
4 has VBZ have 5 aux
5 dragged VBN drag 5 ROOT
6 out RP out 5 prt
7 a DT a 8 det
8 movie NN movie 5 dobj
9 for IN for 5 prep
10 far RB far 11 advmod
11 longer JJR long 9 pcomp
12 than IN than 11 prep
13 necessary JJ necessary 12 amod
14 . . . 5 punct
FIN PHRASE
DEBUT PHRASE
15 Aside RB aside 16 advmod
16 from IN from 33 prep
17 the DT the 21 det
18 terrific JJ terrific 19 amod
19 sea NN sea 21 compound
20 rescue NN rescue 21 compound
21 sequences NNS sequence 16 pobj
22 , , , 21 punct
23 of IN of 26 prep
24 which WDT which 23 pobj
25 there EX there 26 expl
26 are VBP be 21 relcl
27 very RB very 28 advmod
28 few JJ few 26 acomp
29 I PRP -PRON- 33 nsubj
30 just RB just 33 advmod
31 did VBD do 33 aux
32 not RB not 33 neg
33 care VB care 33 ROOT
34 about IN about 33 prep
35 any DT any 34 pobj
36 of IN of 35 prep
37 the DT the 38 det
38 characters NNS character 36 pobj
39 . . . 33 punct
FIN PHRASE


This becomes:

[1, 258, 155, 5920, 13, 979, 38, 6, 14, 17, 207, 165, 68, 1526, 1, 1044, 33, 3, 1212, 1380, 1396, 382, 7, 58, 34, 4, 51, 150, 37, 19, 12, 338, 39, 91, 7, 3, 46,


etc. As you can see, the 1 shows the sentence begin, 258 is once, 155 is again, I have missed mr. because it contains a period (but this can hardly be the reason my system is failing), 5920 is costner (apparently Kevin Costner's name appears so often, that it is included in the 10,000 most frequent words), 13 is have, 979 is drag, 38 is out, 6 is the article a, 14 is the word movie, and so on. These ranks are all very reasonable, I think, so I don't see what may have went wrong.
",0,733,"I think the problem is that you are one-hot encoding your input data (x_trainand x_test) in your vectorize_sequences function. If you skip that step, your model should work as with the Keras example data.

The reason is that your input layer model.add(Embedding(10000, 32))expects the actual indices of each word in a sequence. So something as you show in your example:

In [1] : print(x_train[0])
Out[1] : [1, 258, 155, 5920, 13, 979, 38, 6, 14, 17, 207, ...]


The embedding layer will then map those indices to the corresponding word vectors and stack them together in the right order before feeding them to your RNN.

When you one-hot encode the sequences not only do you lose the ordering in your text, but you also get a vector with dimension 10000, that Keras probably chops out when you define your maxlen for padding.

input_train = sequence.pad_sequences(x_train, maxlen=500)


That is not to say that one-hot encoding is not a valid approach. It just does not fit your architecture and would be more suitable for a simple feedforward network.

All that said, I have not tested your code and cannot say yet whether that is all that needs fixing. Let me know if that helps or if you need other insights.

Update

I just downloaded your data and ran your code only changing the input vectors. As mentioned above, you just need to feed the network with the indices you got from your preprocessing step.

Simply replace

def vectorize_sequences(sequences, dimension=10000):
    results = np.zeros((len(sequences), dimension))
    for i, sequence in enumerate(sequences):
        results[i, sequence] = 1.
    return results
x_train = np.array(vectorize_sequences(CORPUS[:25000]),dtype=object)
x_test = np.array(vectorize_sequences(CORPUS[25000:]),dtype=object)


with

x_train = CORPUS[:25000]
x_test = CORPUS[25000:]


and your code should work nicely. I quickly got to 95% accuracy in 5 epochs.
",,
SpaCy inconsistent output,https://stackoverflow.com/questions/61118213,spacy lemmatizing inconsistency with lemma_lookup table,"There seems to be an inconsistency when iterating over a spacy document and lemmatizing the tokens compared to looking up the lemma of the word in the Vocab lemma_lookup table.

nlp = spacy.load(""en_core_web_lg"")
doc = nlp(""I'm running faster"")
for tok in doc: 
  print(tok.lemma_)


This prints out ""faster"" as lemma for the token ""faster"" instead of ""fast"". However the token does exist in the lemma_lookup table.

nlp.vocab.lookups.get_table(""lemma_lookup"")[""faster""]


which outputs ""fast""

Am I doing something wrong? Or is there another reason why these two are different? Maybe my definitions are not correct and I'm comparing apples with oranges?

I'm using the following versions on Ubuntu Linux:
spacy==2.2.4
spacy-lookups-data==0.1.0
",2,1254,"With a model like en_core_web_lg that includes a tagger and rules for a rule-based lemmatizer, it provides the rule-based lemmas rather than the lookup lemmas when POS tags are available to use with the rules. The lookup lemmas aren't great overall and are only used as a backup if the model/pipeline doesn't have enough information to provide the rule-based lemmas.

With faster, the POS tag is ADV, which is left as-is by the rules. If it had been tagged as ADJ, the lemma would be fast with the current rules.

The lemmatizer tries to provide the best lemmas it can without requiring the user to manage any settings, but it's also not very configurable right now (v2.2). If you want to run the tagger but have lookup lemmas, you'll have to replace the lemmas after running the tagger.
","aab wrote, that:

The lookup lemmas aren't great overall and are only used as a backup
if the model/pipeline doesn't have enough information to provide the
rule-based lemmas.

This is also how I understood it from the spaCy code, but since I wanted to add my own dictionaries to improve the lemmatization of the pretrained models, I decided to try out the following, which worked:
#load model
nlp = spacy.load('es_core_news_lg')
#define dictionary, where key = lemma, value = token to be lemmatized - not case-sensitive
corr_es = {
    ""decir"":[""dixo"", ""decia"", ""Dixo"", ""Decia""],
    ""ir"":[""iba"", ""Iba""],
    ""pacerer"":[""parecia"", ""Parecia""],
    ""poder"":[""podia"", ""Podia""],
    ""ser"":[""fuesse"", ""Fuesse""],
    ""haber"":[""habia"", ""havia"", ""Habia"", ""Havia""],
    ""ahora"" : [""aora"", ""Aora""],
    ""estar"" : [""estàn"", ""Estàn""],
    ""lujo"" : [""luxo"",""luxar"", ""Luxo"",""Luxar""],
    ""razón"" : [""razon"", ""razòn"", ""Razon"", ""Razòn""],
    ""caballero"" : [""cavallero"", ""Cavallero""],
    ""mujer"" : [""muger"", ""mugeres"", ""Muger"", ""Mugeres""],
    ""vez"" : [""vèz"", ""Vèz""],
    ""jamás"" : [""jamas"", ""Jamas""],
    ""demás"" : [""demas"", ""demàs"", ""Demas"", ""Demàs""],
    ""cuidar"" : [""cuydado"", ""Cuydado""],
    ""posible"" : [""possible"", ""Possible""],
    ""comedia"":[""comediar"", ""Comedias""],
    ""poeta"":[""poetas"", ""Poetas""],
    ""mano"":[""manir"", ""Manir""],
    ""barba"":[""barbar"", ""Barbar""],
    ""idea"":[""ideo"", ""Ideo""]
}
#replace lemma with key in lookup table
for key, value in corr_es.items():
    for token in value:
        correct = key
        wrong = token
        nlp.vocab.lookups.get_table(""lemma_lookup"")[token] = key
#process the text
nlp(text) 

Hopefully this could help.
",
SpaCy inconsistent output,https://stackoverflow.com/questions/55746174,"Why is the currency symbol sometimes, but not always included in spacy MONEY entities?","In the sentence ""I saved $6 hundred."" spacy NER recognizes ""$6 hundred"" as MONEY entity.
So far, so good. 

But in the sentence ""I saved $600."" it recognizes ""600"" as MONEY - without the dollar sign!
This seems inconsistent. Am I missing something?  

Here's the code to reproduce:

import en_core_web_sm
nlp = en_core_web_sm.load()

def print_entities(txt):
    print(""Entities for input: '{}'"".format(txt))
    doc = nlp(txt)
    for entity in doc.ents:
        print('\t', entity.text, entity.label_)


print_entities(""I saved $6 hundred."")
print_entities(""I saved $600."")


This gives the output:

Entities for input: 'I saved $6 hundred.'
     $6 hundred MONEY
Entities for input: 'I saved $600.'
     600 MONEY


I'm doing this in a fresh python 3.6.8 virtual environment with spacy version 2.1.3 and en_core_web_sm version 2.1.0
",0,738,"Another possible work-around until a more satisfying solution is developed: 

def money_merger(doc):
    with doc.retokenize() as retokenizer:
        for money in [e for e in doc.ents if e.label_ == 'MONEY']:
            if doc[money.start - 1].is_currency:
                retokenizer.merge(doc[
                money.start-1:money.end])
return doc

nlp.add_pipe(money_merger, after='ner')

","I confirm that the issue (1) is still here, (2) doesn't depend on the currency symbol, (3) only occurs when the entity is just a symbol + number (i.e., no literal expression), and (4) the symbols are not included in the span (you can list it, it doesn't appear) or the entity.start_char and entity.end_char.

A quick-and-dirty hack consists in testing whether the text starts AND ends up with a figure, in which case you get the currency symbol before or after. For example, if source is the initial document and entity the money entity:

def complete_text(source, entity):
    t=entity.text
    try:
        float( t[0] + t[len(t)-1] )
    except:
        return t
    if entity[0].nbor(-1).is_currency:
        return entity[0].nbor(-1).text + t
    else:
        return t + entity[0].nbor(1).text


Hope there will be some neater suggestions!
",
SpaCy inconsistent issue,https://stackoverflow.com/questions/67100601,Python 3.7 spaCy Help Needed- Environment Inconsistency Issue?,"I am facing an issue when trying to call spaCy into my Jupyter notebook. When I run
import spacy I get the below:

I have used spaCy before many times with no issue, but I noticed this problem began after I was trying to also install from neuralcoref import Coref and am not sure if that has caused this.
When I go into the terminal and run conda list spacy it looks like spaCy is available:

I do not really understand what the errors are suggesting, but I tried to reinstall murmurhash using conda install -c anaconda murmurhash after which I got this. This is just a screenshot of the first few but there are MANY packages that are allegedly causing the inconsistency:

Following the list of packages causing inconsistencies, I get this:

For reference, I am using MacOS and python 3.7. How can I fix this?
",1,1194,"spacy&gt;=3.0 and neuralcoref are currently not compatible - the Cython API of spaCy's v3 has changed too much. This might be causing conflicts in your environment?
",,
Gensim unexpected output,https://stackoverflow.com/questions/62936489,Siamese LSTM for document similarity using keras giving input error,"I have written this code to use the Siamese method to calculate the similarity of two documents. I want to embed my vectorize layer (embedding is performed using Google News Dataset) of two separate documents using vectorization approach and then feed it to LSTM and output of LSTM goes into Cosine function to measure the similarity of two documents.
#importing libraries
from __future__ import print_function
import gensim
import numpy as np
import nltk
from nltk.tokenize import sent_tokenize, word_tokenize
import csv
import re
import pandas as pd
from pandas import DataFrame
import pandas as pd
nltk.download('punkt')

from tensorflow import keras
from keras.preprocessing import sequence
from keras.models import Sequential
from keras.layers import Dense, Dropout, Embedding, LSTM, Bidirectional, TimeDistributed
from tensorflow.keras import layers

#Loading pre=trained word2vec model

from gensim.models.keyedvectors import KeyedVectors

# You need to dowload google pre-trained model using below link
# https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit
#Change the path according to your directory

model_path = 'D:\GoogleNews_vectors_negative300\GoogleNews_vectors_negative300.bin'   
w2v_model = KeyedVectors.load_word2vec_format(model_path, binary=True)

#Setting Parameters for model

class DocSim(object):
    def __init__(self, w2v_model , stopwords=[]):
        self.w2v_model = w2v_model
        self.stopwords = stopwords
        
    def vectorize(self, doc):
        """"""Identify the vector values for each word in the given document""""""
        doc = doc.lower()
        words = [w for w in doc.split("" "") if w not in self.stopwords]
        word_vecs = []
        for word in words:
            try:
                vec = self.w2v_model[word]
                word_vecs.append(vec)
            except KeyError:
                # Ignore, if the word doesn't exist in the vocabulary
                pass

        # Assuming that document vector is the mean of all the word vectors

        vector = np.mean(word_vecs, axis=0)
        return vector
        
        
    def Siamese_cosine_sim(self, vectorA, vectorB):
        model = Sequential()
        model.add(LSTM(20, return_sequences=True),input_shape=[vectorA,vectorB])
        model.compile(loss='binary_crossentropy', optimizer='adam')
        outputs = layers.Dense(1, activation=""sigmoid"")(left_doc,right_doc)

        """"""Find the cosine similarity distance between two vectors.""""""
        csim = np.dot(left_doc, right_doc) / (np.linalg.norm(left_doc) * np.linalg.norm(right_doc))
        if np.isnan(np.sum(csim)):
            return 0
        return csim
 

    def calculate_similarity(self, withdigits_source_rules, withdigits_target_rules=[], threshold=0.8):
            """"""Calculates &amp; returns similarity scores between given source rules &amp; all
            the target rules""""""
            if isinstance(withdigits_target_rules, str):
                withdigits_target_rules = [withdigits_target_rules]


            source_vec = self.vectorize(withdigits_source_rules)
            results = []

            for rule in withdigits_target_rules:
                target_vec = self.vectorize(rule)
                sim_score = self.Siamese_cosine_sim (source_vec, target_vec)
                if sim_score &gt; threshold:
                    results.append({
                        'Siamese Sim Score':sim_score,
                        'Target Rule':rule
                    })


                # Sort results by score in desc order
                results.sort(key=lambda k : k['Siamese Sim Score'] , reverse=True)

            return results

ds = DocSim(w2v_model)

#Two documents data
withdigits_source_rules =set([""2.1 Separation of trains"",""2.3.1.2 Level crossing obstruction"",""2.2.1.1 Safety is compromised if a train proceeds without a movement autority"",""Principle: The method of signalling must maintain a space interval between trains that is safe."",""2.1.1 Context""])

#Calculate the similarity score between a source rule &amp; a target rule.


if isinstance(withdigits_source_rules, str):
    withdigits_source_rules = [withdigits_source_rules]
   

# This will return one target rules text with a similarity score

for rule in withdigits_source_rules:
    sim_scores= ds.calculate_similarity(rule, withdigits_target_rules)

    

    
    # Printing the output in text file
    
    print(""Source rule: {} \n\nSimilarity with Target Rule is \n\n {}\n"".format(rule, sim_scores) , file=open(""output.txt"", ""a""))
    print(""\n"")
    
    
    # Printing output in Jupyter
    
    print(""Source rule: {} \n\nSimilarity with Target Rule is \n\n {}\n"".format(rule, sim_scores) )
    print(""\n"")


I am getting following error if someone can help me to solve this issue along with LSTM input function?
TypeError: add() got an unexpected keyword argument 'input_shape'

",0,503,"Refer to the documentation here for adding layers to a Sequential model. The add method only accepts one parameter - layer. If the passed argument is not a layer instance, it raises TypeError, which is precisely the error it threw. I guess, you wanted to pass the input_shape parameter to the LSTM layer (line after creating Sequential model). Just move it inside the LSTM layer and it should work fine.
",,
Gensim unexpected result,https://stackoverflow.com/questions/42376652,Gensim LDA alpha-parameter,"I tried the three default-options for alpha in gensim's lda implementation and now wonder about the result:
The sum of topic-probabilities over all documents is smaller than the number of documents in the corpus (see below). For example alpha = 'symmetric' yields about 9357 as sum of topic-probabilities, however, the number of topics is 9459. Could one tell me the reason for this unexpected result?

alpha = symmetric
nr_of_docs = 9459
sum_of_topic_probs = 9357.12285605

alpha = asymmetric
nr_of_docs = 9459
sum_of_topic_probs = 9375.29253851

alpha = auto
nr_of_docs = 9459
sum_of_topic_probs = 9396.40123459

",1,6035,"I tried to replicate your problem but in my case (using a very small corpus), I could not find any difference between the three sums. 
I will still share the paths I tried in the case anybody else wants to replicate the problem ;-)

I use some small example from gensim's website and train the three different LDA models:

from gensim import corpora, models
texts = [['human', 'interface', 'computer'],
         ['survey', 'user', 'computer', 'system', 'response', 'time'],
         ['eps', 'user', 'interface', 'system'],
         ['system', 'human', 'system', 'eps'],
         ['user', 'response', 'time'],
         ['trees'],
         ['graph', 'trees'],
         ['graph', 'minors', 'trees'],
         ['graph', 'minors', 'survey']]

dictionary = corpora.Dictionary(texts)

corpus = [dictionary.doc2bow(text) for text in texts]

lda_sym = models.ldamodel.LdaModel(corpus=corpus, id2word=dictionary, num_topics=10, update_every=1,
                                      chunksize =100000, passes=1, alpha='symmetric')
lda_asym = models.ldamodel.LdaModel(corpus=corpus, id2word=dictionary, num_topics=10, update_every=1,
                                      chunksize =100000, passes=1, alpha='asymmetric')
lda_auto = models.ldamodel.LdaModel(corpus=corpus, id2word=dictionary, num_topics=10, update_every=1,
                                      chunksize =100000, passes=1, alpha='auto')


Now I sum over the topic probabilities for all documents (9 documents in total)

counts = {}
for model in [lda_sym, lda_asym, lda_auto]:
    s = 0
    for doc_n in range(len(corpus)):
        s += pd.DataFrame(lda_sym[corpus[doc_n]])[1].sum()
        if s &lt; 1:
            print('Sum smaller than 1 for')
            print(model, doc_n)
    counts[model] = s


And indeed the sums are always 9:

counts = {&lt;gensim.models.ldamodel.LdaModel at 0x7ff3cd1f3908&gt;: 9.0,
          &lt;gensim.models.ldamodel.LdaModel at 0x7ff3cd1f3048&gt;: 9.0,
          &lt;gensim.models.ldamodel.LdaModel at 0x7ff3cd1f3b70&gt;: 9.0}


Of course that's not a representative example since it's so small. So if you could, maybe provide some more details about your corpus. 

In general I would assume that this should always be the case. My first intuition was that maybe empty documents would change the sum, but that is also not the case, since empty documents just yield a topic distribution identical to alpha (which makes sense):

pd.DataFrame(lda_asym[[]])[1]


returns 

0    0.203498
1    0.154607
2    0.124657
3    0.104428
4    0.089848
5    0.078840
6    0.070235
7    0.063324
8    0.057651
9    0.052911


which is identical to

lda_asym.alpha

array([ 0.20349777,  0.1546068 ,  0.12465746,  0.10442834,  0.08984802,
    0.0788403 ,  0.07023542,  0.06332404,  0.057651  ,  0.05291085])


which also sums to 1.

From a theoretical point of view, choosing different alphas will yield to completely different LDA models. 

Alpha is the hyper parameter for the Dirichlet prior. The Dirichlet prior is the distribution from which we draw theta. And theta becomes the parameter that decides what shape the topic distribution is. So essentially, alpha influences how we draw topic distributions. That is why choosing different alphas will also give you slightly different results for

lda.show_topics()


But I do not see why the sum over document probabilities should differ from 1 for any LDA model or any kind of document.
","I think the problem is as default setting, the minimum_probability is set to 0.01 not 0.00. 

You can check out the LDA model code here:

Therefore if you are training your model with the default setting, it might not return a sum of 1.00 when adding up the prob across topics for a specific document.

Since the minimum_probability is passed in here, you can always change it by something like this to reset it:

your_lda_model_name.minimum_probability = 0.0
",
Gensim unexpected issue,https://stackoverflow.com/questions/51791964,DeprecationWarning in Gensim `most_similar`?,"While implementating Word2Vec in Python 3.7, I am facing an unexpected scenario related to depreciation. My question is what exactly is the depreciation warning with respect to 'most_similar' in word2vec gensim python?
Currently, I am getting the following issue.
DeprecationWarning: Call to deprecated most_similar (Method will be removed in 4.0.0, use self.wv.most_similar() instead).
model.most_similar('hamlet')
FutureWarning: Conversion of the second argument of issubdtype from int to np.signedinteger is deprecated. In future, it will be treated as np.int32 == np.dtype(int).type.
if np.issubdtype(vec.dtype, np.int):
Please help to curb this issue? Any help is appreciated.
The code what, I have tried is as follows.
import re
from gensim.models import Word2Vec
from nltk.corpus import gutenberg

sentences = list(gutenberg.sents('shakespeare-hamlet.txt'))   
print('Type of corpus: ', type(sentences))
print('Length of corpus: ', len(sentences))

for i in range(len(sentences)):
    sentences[i] = [word.lower() for word in sentences[i] if re.match('^[a-zA-Z]+', word)]
print(sentences[0])    # title, author, and year
print(sentences[1])
print(sentences[10])
model = Word2Vec(sentences=sentences, size = 100, sg = 1, window = 3, min_count = 1, iter = 10, workers = 4)
model.init_sims(replace = True)
model.save('word2vec_model')
model = Word2Vec.load('word2vec_model')
model.most_similar('hamlet')

",4,9863,"It's a warning  which that it's about to become obsolete and non-functional.


  Usually things are deprecated for a few versions giving anyone using them enough time to move to the new method before they are removed.


They've moved most_similar to wv

So most_simliar() should look something like:

model.wv.most_similar('hamlet')


src ref

Hope this helps

Edit : using wv.most_similar()

import re
from gensim.models import Word2Vec
from nltk.corpus import gutenberg

sentences = list(gutenberg.sents('shakespeare-hamlet.txt'))   
print('Type of corpus: ', type(sentences))
print('Length of corpus: ', len(sentences))

for i in range(len(sentences)):
    sentences[i] = [word.lower() for word in sentences[i] if re.match('^[a-zA-Z]+', word)]
print(sentences[0])    # title, author, and year
print(sentences[1])
print(sentences[10])
model = Word2Vec(sentences=sentences, size = 100, sg = 1, window = 3, min_count = 1, iter = 10, workers = 4)
model.init_sims(replace = True)
model.save('word2vec_model')
model = Word2Vec.load('word2vec_model')
similarities = model.wv.most_similar('hamlet')
for word , score in similarities:
    print(word , score)

","A deprecation warning is a warning to indicate the use of things that may or may not exist in future versions of Python, often replaced by other things. (tells what they are)

It appears that the errors originate inside of Word2Vec, and not your code. Removing these errors would entail going into that library and changing its code.

Try doing what it tells you to do.

Change your model.most_similar('hamlet') to model.wv.most_similar('hamlet')

I am unfamiliar with this package, so adjust to how it would work for your use. 
","After the update to 4.0.0 version, the function model.most_similar() will be removed. So what you can do is to modify the function to model.wv.most_similar(). The same goes for the function model.similarity(). You have to change it to model.wv.similarity(). 
"
Gensim strange behavior,https://stackoverflow.com/questions/46447882,Weights of CNN model go to really small values and after NaN,"I am not able to understand the reason why the weights of following model are going smaller and smaller until NaN during training. 

The model is the following:

def initialize_embedding_matrix(embedding_matrix):
    embedding_layer = Embedding(
        input_dim=embedding_matrix.shape[0],
        output_dim=embedding_matrix.shape[1],
        weights=[embedding_matrix],
        trainable=True)
    return embedding_layer

def get_divisor(x):
    return K.sqrt(K.sum(K.square(x), axis=-1))


def similarity(a, b):
    numerator = K.sum(a * b, axis=-1)
    denominator = get_divisor(a) * get_divisor(b)
    denominator = K.maximum(denominator, K.epsilon())
    return numerator / denominator


def max_margin_loss(positive, negative):
    loss_matrix = K.maximum(0.0, 1.0 + negative - Reshape((1,))(positive))
    loss = K.sum(loss_matrix, axis=-1, keepdims=True)
    return loss


def warp_loss(X):
    z, positive_entity, negatives_entities = X
    positiveSim = Lambda(lambda x: similarity(x[0], x[1]), output_shape=(1,), name=""positive_sim"")([z, positive_entity])
    z_reshaped = Reshape((1, z.shape[1].value))(z)
    negativeSim = Lambda(lambda x: similarity(x[0], x[1]), output_shape=(negatives_titles.shape[1].value, 1,), name=""negative_sim"")([z_reshaped, negatives_entities])
    loss = Lambda(lambda x: max_margin_loss(x[0], x[1]), output_shape=(1,), name=""max_margin"")([positiveSim, negativeSim])
    return loss

def mean_loss(y_true, y_pred):
    return K.mean(y_pred - 0 * y_true)

def build_nn_model():
    wl, tl = load_vector_lookups()
    embedded_layer_1 = initialize_embedding_matrix(wl)
    embedded_layer_2 = initialize_embedding_matrix(tl)

    sequence_input_1 = Input(shape=(_NUMBER_OF_LENGTH,), dtype='int32',name=""text"")
    sequence_input_positive = Input(shape=(1,), dtype='int32', name=""positive"")
    sequence_input_negatives = Input(shape=(10,), dtype='int32', name=""negatives"")

    embedded_sequences_1 = embedded_layer_1(sequence_input_1)
    embedded_sequences_positive = Reshape((tl.shape[1],))(embedded_layer_2(sequence_input_positive))
    embedded_sequences_negatives = embedded_layer_2(sequence_input_negatives)

    conv_step1 = Convolution1D(
        filters=1000,
        kernel_size=5,
        activation=""tanh"",
        name=""conv_layer_mp"",
        padding=""valid"")(embedded_sequences_1)

    conv_step2 = GlobalMaxPooling1D(name=""max_pool_mp"")(conv_step1)
    conv_step3 = Activation(""tanh"")(conv_step2)
    conv_step4 = Dropout(0.2, name=""dropout_mp"")(conv_step3)
    z = Dense(wl.shape[1], name=""predicted_vec"")(conv_step4) # activation=""linear""

    loss = warp_loss([z, embedded_sequences_positive, embedded_sequences_negatives])
    model = Model(
        inputs=[sequence_input_1, sequence_input_positive, sequence_input_negatives],
        outputs=[loss]
        )
    model.compile(loss=mean_loss, optimizer=Adam())
    return model

model = build_nn_model()
x, y_real, y_fake = load_x_y()
    X_train = {
    'text': x_train,
    'positive': y_real_train,
    'negatives': y_fake_train
}

model.fit(x=X_train,  y=np.ones(len(x_train)), batch_size=10, shuffle=True, validation_split=0.1, epochs=10)


To describe the model a bit: 


I have two pre-trained embeddings(wl,tl) and I initialize the Keras embeddings with these values.
There are 3 inputs. The sequence_input_1 has integers as input (indexes of words. ex. [42, 32 .., 4]). On them sequence.pad_sequences(X, maxlen=_NUMBER_OF_LENGTH) is used to have fixed length. sequence_input_positive which is an integer of the positive output and sequence_input_negatives which are N random negative outputs (10 in the code above) for each example.
max_margin_loss measures the difference between the cosinus_similarity(positive_example, sequence_input_1) andcosinus_similarity(negative_example[i], sequence_input_1) and the Adam optimizer is used to minimize loss.


While training this model even with only 20 data points the weights in the Convolution1D and Dense goes to NaN. If I add more data points the embedding weights go to NaN too. I can observe that as the model runs the weights are going smaller and smaller until they go to NaN. Something noticable also is that the loss does not go to NaN. When weights reach NaN, the loss goes to zero.

I am unable to find what is going wrong. 

This is what I tried until now:


I have seen that people are using stochastic gradient descent when hinge loss is used. Using SGD optimizer didn't change something in the behavior here.
changed the number of batch size. No change in behavior.
checked input data not to have nan values.
normalized the input matrix (pre-trained data) for embedding with np.linalg.norm
transform  pre-trained matrix from float64 to float32


Do you see anything strange in the architecture of the model? If not: I am unable to find a way to debug the architecture in order to understand why weights are going smaller and smaller till reach NaN. Is there some steps people are using when they notice this kind of behaviour?

Edit:

By using trainable=False in the Embeddings this behaviour of nan weights is NOT observed, and the training seems to have smooth results. However I want the embeddings to be trainable. So why this behavior when the embeddings are trainable??

Edit2:

Using trainable=True and by uniformly randomly initializing the weights embeddings_initializer='uniform' the training is smooth. So the reason happening is my word embeddings. I have checked my pre-trained word embeddings and there are no NaN values. I have also normalized them in case this was causing it but no lack. Cant think anything else why these specific weights are giving this behaviour.

Edit3:

It seems that what causing this was that a lot of rows from one of the Embeddings trained in gensim where all zeros. ex.

[0.2, 0.1, .. 0.3],
[0.0, 0.0, .. 0.0],
[0.0, 0.0, .. 0.0],
[0.0, 0.0, .. 0.0],
[0.2, 0.1, .. 0.1]


It was not so easy to find it as the dimension of the embeddings where really big.

I am leaving this question open in case someone comes up with something similar or wants to answer the question  asked above: ""Is there some steps people are using when they notice this kind of behaviour?""
",1,697,"By your edits, it got a little easier to find the problem. 

Those zeros passed unchanged to the warp_loss function. 
The part that went through the convolution remained unchanged at first, because any filters multiplied by zero result in zero, and the default bias initializer is also 'zeros'. The same idea applies to the dense (filters * 0 = 0 and bias initializer = 'zeros')

That reached this line: return numerator / denominator and caused an error (division by zero)

It's a common practice I've seen in many codes to add K.epsilon() to avoid this:

return numerator / (denominator + K.epsilon())

",,
Gensim strange result,https://stackoverflow.com/questions/46731926,Gensim: quality of word2vec model seems not to correlate with num of iterations in training,"I'm playing with gensim's wordvec and try to build a model using the terms from a large medical thesaurus as sentences. There are about 1 million terms (most of the multiword terms which I treat as sentences) and the hope is, that if word2vec sees terms like ""breast cancer"" and ""breast tumor"" etc. it will be able to conclude that ""cancer"" and ""tumor"" are somewhat similar. 

I run experiments in which I track how similar terms like that are when using different numbers of iterations but it seems that the results don't correlate. I'd expect that when considering word pairs like (wound, lesion), (thorax, lung), (cancer, tumor) etc, when going from 5 to 100 iterations there'd be a tendency (even if small) that the one word in the pair is ""more similar"" to the the other as the number of iterations grows. But no, results appear pretty random or even getting worse.  

Specifically: I loop with 1,5,10,20,50,100 iterations and train a w2v model and then for my word pairs above check the rank of the 2nd word in the list (say ""lung"") of similar words (as returned by w2v) for the first word (say ""thorax""), then sum up and build the average. And the average rank is growing (!) not decreasing, meaning as training proceeds, the vectors for ""lung"" and ""thorax"" move further and further away from each other. 

I didn't expect gensim to detect the clean synonyms and also perhaps 'only' 1 million terms (sentences) is not enough, but still I am puzzled by this effect. 

Does anyone have a suspicion? 

====================================================

Added after comments and feedback came in: 

Thanks for the detailed feedback, gojomo. I had checked many of these issues before: 


yes, the thesaurus terms (""sentences"") come in the right format, e.g. ['breast', 'cancer'] 
yes, of the ~1mio terms more than 850.000 are multiword. Clear that 1-word terms won't provide any context. But there should be ample evidence from the multiword terms
the examples I gave ('clinic', 'cancer', 'lung', ...) occur in many hundreds of terms, often many thousands. This is what I find odd: That not even for words this frequent really good similar words are suggested. 
you ask for the code: Here it is https://www.dropbox.com/s/fo3fazl6frj99ut/w2vexperiment.py?dl=0 It expects to be called (python3) with the name of the model and then the SKOS-XML files of a large thesaurus like Snomed

python w2vexperiment.py snomed-w2v.model SKOS/*.skos
I the code you see that I create a new model with each new experiment (with a different number of iterations) So there should be no effect that one run pollutes the other (wrong learning rate etc...) 
I have set min_count to 10 


Still: the models don't get better but often worse as number of iterations grows. And even the better ones (5 or 10 iterations) give me strange results for my test words... 
",0,1019,"I suspect there's something wrong with your corpus prep, or training – usually word2vec can rank such similarities well.  

Are you supplying the terms alone (eg ['breast, 'tumor'] or ['prophylaxis'] as very tiny sentences), or the terms plus definitions/synonyms as somewhat longer sentences?  

The latter would be better. 

If the former, then 1-word 'sentences' achieve nothing: there's no neighboring 'context' for word2vec to learn anything, and they're essentially skipped. 

And mere 2-word sentences might get some effect, but don't necessarily provide the kind of diverse contexts helpful for training to induce the useful vector arrangements. 

Also if it's 1-million 'sentences' of just 1-4 words each, it's kind of a small dataset, and individual words might not be appearing often enough, in sufficiently slightly-varied contexts, for them to get good vectors. You should check the words/tokens of interest, in the model.wv.vocab dict, for a count value that indicates there were enough examples to induce a good vector - ideally 10+ occurrences each (and more is better). 

So: more data, and more diverse usages from the relevant domain, are always a good idea. A thesaurus with synonyms in each 'sentence', that are many words (5 to dozens), might be enough.

You don't show your code or training-parameters, but people tweaking the defaults, or following outdated online examples, can often sabotage the algorithm's effectiveness. 

For example, it's distressingly common to see people who call train() multiple times, in their own iteration loop, to mismanage the learning-rate alpha such that some iterations run with a negative alpha – meaning every backpropagation serves to drive the context-vectors towards lower target-word predictiveness, the exact opposite of what should be happening. (It's best to either supply the corpus &amp; iter on Word2Vec initialization, or call train() just once. Only advanced tinkerers should need to call train() multiple times.)

Similarly, while naive intuition is often ""keeping more words/info must be better"", and thus people lower min_count to 1 or 0, such low-frequency words can't get good vectors with just 1 (or a few) occurences, but since they are very numerous (in total), they can interfere with other words' meaningful training. (The surviving, more-frequent words get better vectors when low-frequency words are discarded.) 

Good luck!
",,
Gensim inconsistent output,https://stackoverflow.com/questions/50352777,Shape ValueError in LSTM network using Tensorflow,"I want to train a LSTM model with Tensorflow. I have a text data as input and I get doc2vec of each paragraph of the text and pass it to the lstm layers but I get ValueError because of inconsistency of shape rank.
I've searched through Stackoverflow for similar questions and some tutorials, but I couldn't solve this error. Do you have any idea what should I do?
Here is the error:


  Traceback (most recent call last):
    File ""writeRNN.py"", line 97, in 
      outputs, final_state = tf.nn.dynamic_rnn(cell, embed, initial_state=initial_state)
    File ""myven/lib/python3.5/site-packages/tensorflow/python/ops/rnn.py"", line 627, in dynamic_rnn
      dtype=dtype)
    File ""myven/lib/python3.5/site-packages/tensorflow/python/ops/rnn.py"", line 690, in _dynamic_rnn_loop
      for input_ in flat_input)
    File ""myven/lib/python3.5/site-packages/tensorflow/python/ops/rnn.py"", line 690, in 
      for input_ in flat_input)
    File ""myven/lib/python3.5/site-packages/tensorflow/python/framework/tensor_shape.py"", line 761, in with_rank_at_least
      raise ValueError(""Shape %s must have rank at least %d"" % (self, rank))
  ValueError: Shape (?, ?) must have rank at least 3


And below is the code:

lstm_size = 128
lstm_layers = 1
batch_size = 50
learning_rate = 0.001

# Create the graph object
graph = tf.Graph()
# Add nodes to the graph
with graph.as_default():
    inputs_ = tf.placeholder(tf.int32, [None, None], name='inputs')
    labels_ = tf.placeholder(tf.int32, [None, None], name='labels')
    keep_prob = tf.placeholder(tf.float32, name='keep_prob')
with graph.as_default():
    lstm = tf.contrib.rnn.BasicLSTMCell(lstm_size)
    drop = tf.contrib.rnn.DropoutWrapper(lstm, output_keep_prob=keep_prob)
    cell = tf.contrib.rnn.MultiRNNCell([drop] * lstm_layers)
    initial_state = cell.zero_state(batch_size, tf.float32)

with graph.as_default():
    outputs, final_state = tf.nn.dynamic_rnn(cell, inputs_, initial_state=initial_state)


with graph.as_default():
    predictions = tf.contrib.layers.fully_connected(outputs[:, -1], 1, activation_fn=tf.sigmoid)
    cost = tf.losses.mean_squared_error(labels_, predictions)
    optimizer = tf.train.AdamOptimizer(learning_rate).minimize(cost)


with graph.as_default():
    correct_pred = tf.equal(tf.cast(tf.round(predictions), tf.int32), labels_)
    accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))
with tf.Session(graph=graph) as sess:
    sess.run(tf.global_variables_initializer())
    for e in range(epochs):
        state = sess.run(initial_state)
        for ii, (x, y) in enumerate(get_batches(train_x, train_y, batch_size), 1):
            feed = {inputs_: x, labels_: y[:, None], keep_prob: 0.5, initial_state: state}
            loss, state, _ = sess.run([cost, final_state, optimizer], feed_dict=feed)       


I got error on outputs, final_state = tf.nn.dynamic_rnn(cell, inputs_, initial_state=initial_state) Line as the error I described.
doc2vec model is trained on gensim and converts each sentence into a vector with 100 value.
I tried to change inputs_ shape and labels_ shape but also I get same error!
I really don't know what should I do?!

I really thank if you could answer my question.
",0,394,"If you look at the documentation for the dynamic_rnn function it says that the inputs should have three dimensions. The first one is for the batch size, the second is for the sequence length, and the third is the data embedding size. Your input only has two dimensions, which is why there is an error.
",,
Gensim inconsistent result,https://stackoverflow.com/questions/36263594,gensim word2vec giving inconsistent results,"i am using gensim in ubuntu. version is 0.12.4. my word2vec model is not consistent. every time i build the model based on the same exact sentences and same parameter it still have different presentations of the words.

here is the code (that i stole from the initial post)

&gt;&gt;&gt; from nltk.corpus import brown
&gt;&gt;&gt; from gensim.models import Word2Vec
&gt;&gt;&gt; sentences = brown.sents()[:100]
&gt;&gt;&gt; model = Word2Vec(sentences, size=10, window=5, min_count=5, workers=4)
&gt;&gt;&gt; model[sentences[0][0]]
array([ 0.04913874,  0.04574081, -0.07402877, -0.03270053,  0.06598952,
        0.04157289,  0.05075986,  0.01770534, -0.03796235,  0.04594197], dtype=float32)
&gt;&gt;&gt; model = Word2Vec(sentences, size=10, window=5, min_count=5, workers=4)
&gt;&gt;&gt; model[sentences[0][0]]
array([ 0.04907205,  0.04569579, -0.07379777, -0.03273782,  0.06579078,
        0.04167712,  0.05083019,  0.01780009, -0.0378389 ,  0.04578455], dtype=float32)
&gt;&gt;&gt; model = Word2Vec(sentences, size=10, window=5, min_count=5, workers=4)
&gt;&gt;&gt; model[sentences[0][0]]
array([ 0.04906179,  0.04569826, -0.07382379, -0.03274316,  0.06583244,
        0.04166647,  0.0508585 ,  0.01777468, -0.03784611,  0.04578935], dtype=float32)


I have also tried to set seed to some fixed int but this didnt seem to help. i also tried to reinstall gensim which also didnt help.

Any idea how to stabilize my model??
",1,1060,"Try to set PYTHONHASHSEED environment variable  as stated here
https://github.com/gojomo/gensim/blob/develop/gensim/models/doc2vec.py#L566
",,
Gensim inconsistent result,https://stackoverflow.com/questions/62543491,Inconsistent results when training gensim model with gensim.downloader vs manual loading,"I am trying to understand what is going wrong in the following example.
To train on the 'text8' dataset as described in the docs, one only has to do the following:
import gensim.downloader as api
from gensim.models import Word2Vec

dataset = api.load('text8')
model = Word2Vec(dataset)

doing this gives very good embedding vectors, as verified by evaluating on a word-similarity task.
However, when loading the same textfile which is used above manually, as in
text_path = '~/gensim-data/text8/text'
text = []
with open(text_path) as file:
    for line in file:
        text.extend(line.split())
text = [text]

model = Word2Vec(test)

The model still says it's training for the same number of epochs as above (5), but training is much faster, and the resulting vectors have a very, very bad performance on the similarity task.
What is happening here? I suppose it could have to do with the number of 'sentences', but the text8 file seems to have only a single line, so does gensim.downloader split the text8 file into sentences? If yes, of which length?
",0,277,"In your second example, you've created a training dataset with just a single text with the entire contents of the file. That's about 1.1 million word tokens, in a single list.
Word2Vec (&amp; other related algorithms) in gensim have an internal implementation limitation, in their optimized paths, of 10,000 tokens per text item. All additional tokens are ignored.
So, in your 2nd case, 99% of your data is being discarded. Training may seem instant, but very little actual training will have occurred. (Word-vectors for words that only appear past the 1st 10,000 tokens won't have been trained at all, having only their initial randomly-set values.) If you enable logging at the INFO level, you'll see more details about each step of the process, and discrepancies like this may be easier to identify.
Yes, the api.load() variant takes extra steps to break the single-line-file into 10,000-token chunks. I believe it's using the LineSentence utility class for this purpose, whose source can be examined here:
https://github.com/RaRe-Technologies/gensim/blob/e859c11f6f57bf3c883a718a9ab7067ac0c2d4cf/gensim/models/word2vec.py#L1209
However, I recommend avoiding the api.load() functionality entirely. It doesn't just download data; it also downloads a shim of additional outside-of-version-control Python code for prepping that data for extra operations. Such code is harder to browse &amp; less well-reviewed than official gensim release code as packaged for PyPI/etc, which also presents a security risk. Each load target (by name like 'text8') might do something different, leaving you with a different object type as the return value.
It's much better for understanding to directly download precisely the data files you need, to known local paths, and do the IO/prep yourself, from those paths, so you know what steps have been applied, and the only code you're running is the officially versioned &amp; released code.
",,
Gensim inconsistent issue,https://stackoverflow.com/questions/69786001,How to know that the token ids in a gensim pre-trained word2vec will match the ids of a tokenizer&#39;s vocabulary,"I am building a pytorch BiLSTM that utilizes pre-trained gensim word2vec. I first used a nn.Embedding layer that was trained with the model from scratch but, i decided to use a pre-trained word2vec embeddings to improve accuracy.
My model architecture follows a simple BiLSTM architecture, where the first layer is the embedding layer followed by a BiLSTM layer(s), and lastly two feed forward layers.
import torch
import gensim

import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F

word2vec = gensim.models.Word2Vec.load('path_to_word2vec/wikipedia_cbow_100')
weights = torch.FloatTensor(word2vec.wv.vectors)

class BiLSTM_model(torch.nn.Module) :
    def __init__(self, max_features, embedding_dim, hidden_dim, num_layers, lstm_dropout) :
        # max_features is the vocabulary size (num of tokens/words).
        super().__init__()
        # self.embeddings = nn.Embedding(max_features, embedding_dim, padding_idx=0)
        self.embeddings = nn.Embedding.from_pretrained(weights)
        self.lstm = nn.LSTM(word2vec.wv.vector_size,
                            hidden_dim,
                            batch_first=True,
                            bidirectional=True,
                            num_layers = num_layers,
                            dropout=lstm_dropout)
        self.relu=nn.ReLU()
        self.fc1 = nn.Linear(hidden_dim * 2, 64)
        self.dropout = nn.Dropout(0.2)
        self.fc2 = nn.Linear(64, config['num_classes'])

    def forward(self, input):
        embeddings_out = self.embeddings(input)
        lstm_out, (hidden, cell) = self.lstm(embeddings_out)
        hidden = torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1)
        rel = self.relu(hidden)
        dense1 = self.fc1(rel)
        drop = self.dropout(dense1)
        final_out = self.fc2(drop)

        return final_out

i use a keras tokenizer to tokenize the text and obtain the token ids.
from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences

## Tokenize the sentences
tokenizer = Tokenizer(num_words=config['max_features'])
tokenizer.fit_on_texts(list(train_X))
train_X = tokenizer.texts_to_sequences(train_X)
test_X = tokenizer.texts_to_sequences(test_X)

finally i use a standard training loop with an optimizer and a loss function. The code runs fine but there are no performance gains from using the pre-trained embeddings.
I suspect that it has to do with token ids not matching between the keras.preprocessing.text tokenizer and the gensim pre-trained embeddings for the words. My question is, how do i confirm (or deny) this inconsistency and ,if it is the case, how do i handle the issue?
Note: i am using a custom word2vec embeddings for the Arabic language. You can find the embeddings here.
",1,577,"After looking into jhso's comment. It seems that the solution for this problem is to use word2vec.wv.index2word which will return the vocabulary (words) as a list sorted in an order which reflects a word's embedding.
for example, the following code:
pretrained_embedding = gensim.models.Word2Vec.load('path/to/embedding')
word_vectors= pretrained_embedding.wv
for i in range (0,3):
  print(f""{i}: '{word_vectors.index2word[i]}'"")

will print:
0: 'this'
1: 'is'
2: 'an'
3: 'example'

where this token will have the id 0 and so on.
You then use word2vec.wv.index2word as input to the keras.preprocessing.text.Tokenizer object's .fit_on_texts() method as following:
vocabulary = pretrained_embeddings.index2word
tokenizer = Tokenizer(num_words=config['max_features'])
tokenizer.fit_on_texts(vocabulary)

this should preserve the token ids between the gensim word2vec model and the keras tokenizer.
",,
BeautifulSoup unexpected behavior,https://stackoverflow.com/questions/39582787,Unable to find all links with BeautifulSoup to extract links from a website (Link identification),"I’m using this code found here ( retrieve links from web page using python and BeautifulSoup) to extract all links from a website using.

import httplib2
from BeautifulSoup import BeautifulSoup, SoupStrainer

http = httplib2.Http()
status, response = http.request('http://www.bestwestern.com.au')

for link in BeautifulSoup(response, parseOnlyThese=SoupStrainer('a')):
    if link.has_attr('href'):
        print link['href']


I’m using this site http://www.bestwestern.com.au  as test.
Unfortunately, I notice that the code is not extracting some links for example this one http://www.bestwestern.com.au/about-us/careers/ . I don’t know why. 
In the code of the page this is what I found. 

&lt;li&gt;&lt;a href=""http://www.bestwestern.com.au/about-us/careers/""&gt;Careers&lt;/a&gt;&lt;/li&gt;


I think the extractor should normally identify it. 
On the BeautifulSoup documentation I can read:  “The most common type of unexpected behavior is that you can’t find a tag that you know is in the document. You saw it going in, but find_all() returns [] or find() returns None. This is another common problem with Python’s built-in HTML parser, which sometimes skips tags it doesn’t understand. Again, the solution is to install lxml or html5lib.” 
So I installed html5lib. But I still have the same behavior. 

Thank you for your help
",0,2897,"Ok so this is a old question but I stumbled upon it in my search and it seems like it should be relatively simple to accomplish. I did switch from httplib2 to requests.

import requests
from bs4 import BeautifulSoup, SoupStrainer
baseurl = 'http://www.bestwestern.com.au'

SEEN_URLS = []
def get_links(url):
    response = requests.get(url)
    for link in BeautifulSoup(response.content, 'html.parser', parse_only=SoupStrainer('a', href=True)):
        print(link['href'])
        SEEN_URLS.append(link['href'])
        if baseurl in link['href'] and link['href'] not in SEEN_URLS:
            get_links(link['href'])

if __name__ == '__main__':
    get_links(baseurl)

","One problem is - you are using BeautifulSoup version 3 which is not being maintained anymore. You need to upgrade to BeautifulSoup version 4:

pip install beautifulsoup4


Another problem is that there is no ""careers"" link on the main page, but there is one on the ""sitemap"" page - request it and parse with the default html.parser  parser - you'll see ""careers"" link printed among others:

import requests
from bs4 import BeautifulSoup, SoupStrainer

response = requests.get('http://www.bestwestern.com.au/sitemap/')

for link in BeautifulSoup(response.content, ""html.parser"", parse_only=SoupStrainer('a', href=True)):
    print(link['href'])


Note how I've moved the ""has to have href"" rule to the soup strainer.
",
BeautifulSoup unexpected output,https://stackoverflow.com/questions/53090032,Beautiful Soup is adding whitespaces between every character,"I'm running Beautiful Soup on some HTML documents. All I want is to extract the text without any tags.

from bs4 import BeautifulSoup

def aex_extract_text(html):
    soup = BeautifulSoup(html, 'lxml')
    text = soup.get_text()
    return text


My input is somewhat pathological in the number of redundant tags that it's got. But it is automatically generated HTML, so there shouldn't be any missing angles or anything of that nature. It's some Drupal system that creates the HTML. On almost all documents it runs fine, but on about 1/50 I'm getting the first few lines good and then whitespace between every character, and the tags aren't being removed. 

Anyone know what causes this sort of breakdown? I can't see anything obvious. The fact that it's breaking out in the middle of a paragraph is confusing. I don't need perfect preservation of the text, so if I need to pull out a few characters I don't mind.

Example input:

[...]

&lt;/FONT&gt;&lt;/FONT&gt;&lt;/P&gt;  &lt;P LANG=en-US CLASS=western ALIGN=JUSTIFY STYLE=margin-bottom: 0cm; line-height: 150%&gt;  &lt;BR&gt;  &lt;/P&gt;  &lt;P LANG=en-US 
CLASS=western ALIGN=JUSTIFY STYLE=margin-bottom: 0cm; line-height: 150%&gt;  &lt;FONT FACE=Century Gothic, serif&gt;&lt;FONT SIZE=3&gt;The 2&lt;/FONT&gt;&lt;/FONT&gt;&lt;SUP&gt;&lt;FONT 
FACE=Century Gothic, serif&gt;&lt;FONT SIZE=3&gt;nd&lt;/FONT&gt;&lt;/FONT&gt;&lt;/SUP&gt;&lt;FONT FACE=Century Gothic, serif&gt;&lt;FONT SIZE=3&gt;  Respondent was declared the duly 
elected Member of Parliament for the  Chilanga Constituency.&lt;/FONT&gt;&lt;/FONT&gt;&lt;/P&gt;  &lt;P LANG=en-US CLASS=western ALIGN=JUSTIFY STYLE=margin-bottom: 
0cm; line-height: 150%&gt;  &lt;BR&gt;  &lt;/P&gt;  &lt;P LANG=en-US CLASS=western ALIGN=JUSTIFY STYLE=margin-bottom: 0cm; line-height: 150%&gt;  &lt;FONT FACE=Century 
Gothic, serif&gt;&lt;FONT SIZE=3&gt;The result was  unsuccessfully challenged, in the High Court, to have it declared  null and void on a number of allegations 
not relevant to this appeal  as it will be seen later in this judgment.&lt;/FONT&gt;&lt;/FONT&gt;&lt;/P&gt;  &lt;P LANG=en-US CLASS=western ALIGN=JUSTIFY STYLE=margin-
bottom: 0cm; line-height: 150%&gt;  &lt;BR&gt;  &lt;/P&gt;  &lt;P LANG=en-US CLASS=western ALIGN=JUSTIFY STYLE=margin-bottom: 0cm; line-height: 150%&gt;  &lt;FONT FACE=Century 
Gothic, serif&gt;&lt;FONT SIZE=3&gt;It is also not in  dispute that the learned trial judge informed the parties of his  intention to call some witnesses 
after the defence had closed its  case. Five witnesses, in total, were called by the court and they  were designated as court witnesses.&lt;/FONT&gt;&lt;/FONT&gt;&lt;/P&gt;  &lt;P LANG=en-US CLASS=western ALIGN=JUSTIFY STYLE=margin-bottom: 
0cm; line-height: 150%&gt;  &lt;BR&gt;  &lt;/P&gt;  &lt;P LANG=en-US CLASS=western ALIGN=JUSTIFY STYLE=margin-bottom: 0cm; line-height: 150%&gt;  &lt;FONT FACE=Century Gothic, serif&gt;&lt;FONT SIZE=3&gt;This appeal is  premised on one ground of appeal 
which is attacking the learned trial  judge&amp;rsquo;s jurisdiction to call witnesses on his own motion.


[...]

Example output:

[...]

The 2nd  Respondent was declared the duly elected Member of Parliament for the  Chilanga Constituency.     The result was  unsuccessfully challenged, in the High Court, to have it declared  null and void on a number of allegations not relevant to this appeal  as it will be seen later in this judgment.     It is also not in  dispute that the learned trial judge informed the parties of his  intention to call some witnesses       a   f   t   e   r       t   h   e       d   e   f   e   n   c   e       h   a   d       c   l   o   s   e   d       i   t   s           c   a   s   e   .       F   i   v   e       w   i   t   n   e   s   s   e   s   ,       i   n       t   o   t   a   l   ,       w   e   r   e       c   a   l   l   e   d       b   y       t   h   e       c   o   u   r   t       a   n   d       t   h   e   y           w   e   r   e       d   e   s   i   g   n   a   t   e   d       a   s       c   o   u   r   t       w   i   t   n   e   s   s   e   s   .   /   F   O   N   T   &gt;   /   F   O   N   T   &gt;   /   P   &gt;           P       L   A   N   G   =   e   n   -   U   S       C   L   A   S   S   =   w   e   s   t   e   r   n       A   L   I   G   N   =   J   U   S   T   I   F   Y       S   T   Y   L   E   =   m   a   r   g   i   n   -   b   o   t   t   o   m   :       0   c   m   ;       l   i   n   e   -   h   e   i   g   h   t   :       1   5   0   %   &gt;           B   R   &gt;           /   P   &gt;           P       L   A   N   G   =   e   n   -   U   S       C   L   A   S   S   =   w   e   s   t   e   r   n       A   L   I   G   N   =   J   U   S   T   I   F   Y       S   T   Y   L   E   =   m   a   r   g   i   n   -   b   o   t   t   o   m   :   

[...]

UPDATE:

here's the full input up to where it goes wrong:

&lt;!DOCTYPE HTML PUBLIC -//W3C//DTD HTML 4.0 Transitional//EN&gt;  &lt;HTML&gt;  &lt;HEAD&gt;    &lt;META HTTP-EQUIV=CONTENT-TYPE CONTENT=text/html; charset=windows-1252&gt;      &lt;TITLE&gt;&lt;/TITLE&gt;     &lt;META NAME=GENERATOR CONTENT=OpenOffice.org 3.3  (Win32)&gt;   &lt;META NAME=AUTHOR CONTENT=user&gt;     &lt;META NAME=CREATED CONTENT=20081007;9280000&gt;    &lt;META NAME=CHANGED CONTENT=20081007;9280000&gt;    &lt;STYLE TYPE=text/css&gt;   &lt;!--        @page { size: 21.59cm 27.94cm; margin-left: 2.54cm; margin-right: 2.54cm; margin-top: 1.27cm; margin-bottom: 2.54cm }       P { margin-bottom: 0.21cm; line-height: 115%; text-align: left }        P.western { font-family: Calibri, serif; font-size: 11pt; so-language: en-US }          P.cjk { font-family: Arial Unicode MS, sans-serif; so-language: en-US }         P.ctl { font-family: Calibri, serif; font-size: 11pt }      --&gt;     &lt;/STYLE&gt;  &lt;/HEAD&gt;  &lt;BODY LANG=en-ZA DIR=LTR STYLE=border: none; padding: 0cm&gt;  &lt;DIV TYPE=HEADER&gt;    &lt;P LANG=en-US ALIGN=CENTER STYLE=margin-bottom: 0cm; line-height: 0.18cm&gt;    &lt;SPAN STYLE=font-style: normal&gt;&lt;SPAN STYLE=font-weight: normal&gt;J   &lt;SDFIELD TYPE=PAGE SUBTYPE=RANDOM FORMAT=ARABIC&gt;26&lt;/SDFIELD&gt;&lt;/SPAN&gt;&lt;/SPAN&gt;&lt;/P&gt;      &lt;P LANG=en-US STYLE=margin-bottom: 1.17cm; line-height: 0.18cm&gt;&lt;BR&gt;     &lt;/P&gt;  &lt;/DIV&gt;  &lt;P LANG=en-US CLASS=western ALIGN=RIGHT STYLE=margin-bottom: 0.35cm; line-height: 150%&gt;                                &lt;FONT FACE=Calibri, serif&gt;&lt;FONT SIZE=2 STYLE=font-size: 11pt&gt;&lt;SPAN LANG=en-US&gt;&lt;SPAN STYLE=font-style: normal&gt;&lt;SPAN STYLE=font-weight: normal&gt;              (706)&lt;/SPAN&gt;&lt;/SPAN&gt;&lt;/SPAN&gt;&lt;/FONT&gt;&lt;/FONT&gt;&lt;/P&gt;  &lt;P LANG=en-US CLASS=western STYLE=margin-bottom: 0cm; line-height: 150%&gt;  &lt;BR&gt;  &lt;/P&gt;  &lt;P LANG=en-US CLASS=western STYLE=margin-bottom: 0cm; line-height: 150%&gt;  &lt;FONT FACE=Century Gothic, serif&gt;&lt;FONT SIZE=3&gt;&lt;B&gt;IN THE SUPREME  COURT OF ZAMBIA                           SCZ APPEAL NO.182 OF 2007  &lt;/B&gt;&lt;/FONT&gt;&lt;/FONT&gt;&lt;/P&gt;  &lt;P LANG=en-US CLASS=western STYLE=margin-bottom: 0.35cm&gt;&lt;A NAME=DDE_LINK&gt;&lt;/A&gt;  &lt;FONT FACE=Century Gothic, serif&gt;&lt;FONT SIZE=3&gt;&lt;B&gt;HOLDEN AT LUSAKA                                                       SCZ Judgment No.  31 OF 2008&lt;/B&gt;&lt;/FONT&gt;&lt;/FONT&gt;&lt;/P&gt;  &lt;P LANG=en-US CLASS=western STYLE=margin-bottom: 0.35cm&gt;&lt;FONT FACE=Century Gothic, serif&gt;&lt;FONT SIZE=3&gt;&lt;B&gt;(CIVIL  JURISDICTION)&lt;/B&gt;&lt;/FONT&gt;&lt;/FONT&gt;&lt;/P&gt;  &lt;P LANG=en-US CLASS=western STYLE=margin-bottom: 0cm; line-height: 150%&gt;  &lt;BR&gt;  &lt;/P&gt;  &lt;P LANG=en-US CLASS=western STYLE=margin-bottom: 0cm; line-height: 150%&gt;  &lt;FONT FACE=Century Gothic, serif&gt;&lt;FONT SIZE=3&gt;&lt;B&gt;IN THE MATTER OF:             THE ELECTORAL ACT NO.12 OF 2006 SECTION 93 OF THE &lt;/B&gt;&lt;/FONT&gt;&lt;/FONT&gt;  &lt;/P&gt;  &lt;P LANG=en-US CLASS=western STYLE=margin-left: 3.81cm; text-indent: 1.27cm; margin-bottom: 0cm; line-height: 150%&gt;  &lt;FONT FACE=Century Gothic, serif&gt;&lt;FONT SIZE=3&gt;&lt;B&gt;LAWS OF ZAMBIA&lt;/B&gt;&lt;/FONT&gt;&lt;/FONT&gt;&lt;/P&gt;  &lt;P LANG=en-US CLASS=western STYLE=margin-left: 5.08cm; text-indent: -5.08cm; margin-bottom: 0cm; line-height: 150%&gt;  &lt;FONT FACE=Century Gothic, serif&gt;&lt;FONT SIZE=3&gt;&lt;B&gt;IN THE MATTER OF:       PARLIAMENTARY ELECTION FOR CHILANGA CONSTITUENCY HELD ON THE 28&lt;/B&gt;&lt;/FONT&gt;&lt;/FONT&gt;&lt;SUP&gt;&lt;FONT FACE=Century Gothic, serif&gt;&lt;FONT SIZE=3&gt;&lt;B&gt;TH&lt;/B&gt;&lt;/FONT&gt;&lt;/FONT&gt;&lt;/SUP&gt;&lt;FONT FACE=Century Gothic, serif&gt;&lt;FONT SIZE=3&gt;&lt;B&gt;  SEPTEMBER, 2006&lt;/B&gt;&lt;/FONT&gt;&lt;/FONT&gt;&lt;/P&gt;  &lt;P LANG=en-US CLASS=western STYLE=margin-bottom: 0cm; line-height: 150%&gt;  &lt;FONT FACE=Century Gothic, serif&gt;&lt;FONT SIZE=3&gt;&lt;B&gt;IN THE MATTER OF:        AN ELECTION PETITION &lt;/B&gt;&lt;/FONT&gt;&lt;/FONT&gt;  &lt;/P&gt;  &lt;P LANG=en-US CLASS=western STYLE=margin-bottom: 0cm; line-height: 150%&gt;  &lt;BR&gt;  &lt;/P&gt;  &lt;P LANG=en-US CLASS=western STYLE=margin-bottom: 0cm; line-height: 150%&gt;  &lt;FONT FACE=Century Gothic, serif&gt;&lt;FONT SIZE=3&gt;&lt;B&gt;BETWEEN:&lt;/B&gt;&lt;/FONT&gt;&lt;/FONT&gt;&lt;/P&gt;  &lt;P LANG=en-US CLASS=western STYLE=margin-bottom: 0.35cm&gt;&lt;FONT FACE=Century Gothic, serif&gt;&lt;FONT SIZE=3&gt;&lt;B&gt;       PRISCILLA  MWENYA KAMANGA         -      APPELLANT&lt;/B&gt;&lt;/FONT&gt;&lt;/FONT&gt;&lt;/P&gt;  &lt;P LANG=en-US CLASS=western STYLE=margin-bottom: 0.35cm&gt;&lt;FONT FACE=Century Gothic, serif&gt;&lt;FONT SIZE=3&gt;&lt;B&gt;         AND&lt;/B&gt;&lt;/FONT&gt;&lt;/FONT&gt;&lt;/P&gt;  &lt;P LANG=en-US CLASS=western STYLE=margin-bottom: 0.35cm&gt;&lt;FONT FACE=Century Gothic, serif&gt;&lt;FONT SIZE=3&gt;&lt;B&gt;        THE  ATTORNEY-GENERAL             -              1&lt;/B&gt;&lt;/FONT&gt;&lt;/FONT&gt;&lt;SUP&gt;&lt;FONT FACE=Century Gothic, serif&gt;&lt;FONT SIZE=3&gt;&lt;B&gt;ST&lt;/B&gt;&lt;/FONT&gt;&lt;/FONT&gt;&lt;/SUP&gt;&lt;FONT FACE=Century Gothic, serif&gt;&lt;FONT SIZE=3&gt;&lt;B&gt;  RESPONDENT&lt;/B&gt;&lt;/FONT&gt;&lt;/FONT&gt;&lt;/P&gt;  &lt;P LANG=en-US CLASS=western STYLE=margin-bottom: 0cm; line-height: 150%&gt;  &lt;BR&gt;  &lt;/P&gt;  &lt;P LANG=en-US CLASS=western STYLE=margin-bottom: 0cm; line-height: 150%&gt;  &lt;FONT FACE=Century Gothic, serif&gt;&lt;FONT SIZE=3&gt;&lt;B&gt;      HON. NG&amp;rsquo;ANDU  PETER MAGANDE     -      2&lt;/B&gt;&lt;/FONT&gt;&lt;/FONT&gt;&lt;SUP&gt;&lt;FONT FACE=Century Gothic, serif&gt;&lt;FONT SIZE=3&gt;&lt;B&gt;ND&lt;/B&gt;&lt;/FONT&gt;&lt;/FONT&gt;&lt;/SUP&gt;&lt;FONT FACE=Century Gothic, serif&gt;&lt;FONT SIZE=3&gt;&lt;B&gt;  RESPONDENT&lt;/B&gt;&lt;/FONT&gt;&lt;/FONT&gt;&lt;/P&gt;  &lt;P LANG=en-US CLASS=western STYLE=margin-bottom: 0cm; line-height: 150%&gt;  &lt;BR&gt;  &lt;/P&gt;  &lt;P LANG=en-US CLASS=western STYLE=margin-bottom: 0cm; line-height: 150%&gt;  &lt;FONT FACE=Century Gothic, serif&gt;&lt;FONT SIZE=3&gt;&lt;B&gt;CORAM :  Sakala,  CJ.,Chirwa, Mumba, Chitengi and Mushabati; JJS.&lt;/B&gt;&lt;/FONT&gt;&lt;/FONT&gt;&lt;/P&gt;  &lt;P LANG=en-US CLASS=western STYLE=margin-bottom: 0.35cm&gt;&lt;FONT FACE=Century Gothic, serif&gt;&lt;FONT SIZE=3&gt;&lt;B&gt;On  25&lt;/B&gt;&lt;/FONT&gt;&lt;/FONT&gt;&lt;SUP&gt;&lt;FONT FACE=Century Gothic, serif&gt;&lt;FONT SIZE=3&gt;&lt;B&gt;TH&lt;/B&gt;&lt;/FONT&gt;&lt;/FONT&gt;&lt;/SUP&gt;&lt;FONT FACE=Century Gothic, serif&gt;&lt;FONT SIZE=3&gt;&lt;B&gt;  June, 2006 and 19&lt;/B&gt;&lt;/FONT&gt;&lt;/FONT&gt;&lt;SUP&gt;&lt;FONT FACE=Century Gothic, serif&gt;&lt;FONT SIZE=3&gt;&lt;B&gt;th&lt;/B&gt;&lt;/FONT&gt;&lt;/FONT&gt;&lt;/SUP&gt;&lt;FONT FACE=Century Gothic, serif&gt;&lt;FONT SIZE=3&gt;&lt;B&gt;  August, 2008&lt;/B&gt;&lt;/FONT&gt;&lt;/FONT&gt;&lt;/P&gt;  &lt;P LANG=en-US CLASS=western STYLE=margin-bottom: 0.35cm&gt;&lt;FONT FACE=Century Gothic, serif&gt;&lt;FONT SIZE=3&gt;&lt;B&gt;For  the Appellants :  Mr.B.C. Mutale, SC. and Mr L. Kalaluka of Ellis and  Company&lt;/B&gt;&lt;/FONT&gt;&lt;/FONT&gt;&lt;/P&gt;  &lt;P LANG=en-US CLASS=western STYLE=margin-bottom: 0cm; border-top: none; border-bottom: 1.50pt solid #00000a; border-left: none; border-right: none; padding-top: 0cm; padding-bottom: 0.04cm; padding-left: 0cm; padding-right: 0cm; line-height: 150%&gt;  &lt;FONT FACE=Century Gothic, serif&gt;&lt;FONT SIZE=3&gt;&lt;B&gt;For the 1&lt;/B&gt;&lt;/FONT&gt;&lt;/FONT&gt;&lt;SUP&gt;&lt;FONT FACE=Century Gothic, serif&gt;&lt;FONT SIZE=3&gt;&lt;B&gt;st&lt;/B&gt;&lt;/FONT&gt;&lt;/FONT&gt;&lt;/SUP&gt;&lt;FONT FACE=Century Gothic, serif&gt;&lt;FONT SIZE=3&gt;&lt;B&gt;  Respondent:  Mr M. Mukwasa &amp;ndash; State Advocate&lt;/B&gt;&lt;/FONT&gt;&lt;/FONT&gt;&lt;/P&gt;  &lt;P LANG=en-US CLASS=western STYLE=margin-bottom: 0.35cm&gt;&lt;FONT FACE=Century Gothic, serif&gt;&lt;FONT SIZE=3&gt;&lt;B&gt;For  the 2&lt;/B&gt;&lt;/FONT&gt;&lt;/FONT&gt;&lt;SUP&gt;&lt;FONT FACE=Century Gothic, serif&gt;&lt;FONT SIZE=3&gt;&lt;B&gt;nd&lt;/B&gt;&lt;/FONT&gt;&lt;/FONT&gt;&lt;/SUP&gt;&lt;FONT FACE=Century Gothic, serif&gt;&lt;FONT SIZE=3&gt;&lt;B&gt;  Respondent: Mr S.C. Malama, SC. of Jaques and Partners&lt;/B&gt;&lt;/FONT&gt;&lt;/FONT&gt;&lt;/P&gt;  &lt;P LANG=en-US CLASS=western STYLE=margin-bottom: 0.35cm&gt;&lt;FONT FACE=Century Gothic, serif&gt;&lt;FONT SIZE=3&gt;______________________________________________________________________________&lt;/FONT&gt;&lt;/FONT&gt;&lt;/P&gt;  &lt;P LANG=en-US CLASS=western ALIGN=CENTER STYLE=margin-bottom: 0cm; border-top: none; border-bottom: 1.50pt solid #00000a; border-left: none; border-right: none; padding-top: 0cm; padding-bottom: 0.04cm; padding-left: 0cm; padding-right: 0cm; line-height: 150%&gt;  &lt;FONT FACE=Century Gothic, serif&gt;&lt;FONT SIZE=4&gt;&lt;B&gt;JUDGMENT&lt;/B&gt;&lt;/FONT&gt;&lt;/FONT&gt;&lt;/P&gt;  &lt;P LANG=en-US CLASS=western STYLE=margin-bottom: 0cm; line-height: 150%&gt;  &lt;BR&gt;  &lt;/P&gt;  &lt;P LANG=en-US CLASS=western ALIGN=JUSTIFY STYLE=margin-bottom: 0cm; line-height: 150%&gt;  &lt;FONT FACE=Century Gothic, serif&gt;&lt;FONT SIZE=3&gt;&lt;B&gt;Mushabati, JS.,  delivered the judgment of the Court.&lt;/B&gt;&lt;/FONT&gt;&lt;/FONT&gt;&lt;/P&gt;  &lt;P LANG=en-US CLASS=western ALIGN=JUSTIFY STYLE=margin-bottom: 0cm; line-height: 150%&gt;  &lt;BR&gt;  &lt;/P&gt;  &lt;P LANG=en-US CLASS=western STYLE=margin-bottom: 0.35cm&gt;&lt;BR&gt;&lt;BR&gt;  &lt;/P&gt;  &lt;P LANG=en-US CLASS=western ALIGN=RIGHT STYLE=margin-bottom: 0cm; line-height: 150%&gt;  &lt;FONT FACE=Century Gothic, serif&gt;&lt;FONT SIZE=3&gt;&lt;B&gt;(707)&lt;/B&gt;&lt;/FONT&gt;&lt;/FONT&gt;&lt;/P&gt;  &lt;P LANG=en-US CLASS=western ALIGN=JUSTIFY STYLE=margin-bottom: 0cm; line-height: 150%&gt;  &lt;FONT FACE=Century Gothic, serif&gt;&lt;FONT SIZE=3&gt;&lt;B&gt;Cases referred to:&lt;/B&gt;&lt;/FONT&gt;&lt;/FONT&gt;&lt;/P&gt;  &lt;OL&gt;      &lt;LI&gt;&lt;P LANG=en-US ALIGN=JUSTIFY STYLE=margin-bottom: 0cm; line-height: 150%&gt;     &lt;FONT FACE=Calibri, serif&gt;&lt;FONT SIZE=2 STYLE=font-size: 11pt&gt;&lt;SPAN LANG=en-US&gt;&lt;SPAN STYLE=font-style: normal&gt;&lt;SPAN STYLE=font-weight: normal&gt;Double    Mwale Vs The People [1984] Z.R. 76&lt;/SPAN&gt;&lt;/SPAN&gt;&lt;/SPAN&gt;&lt;/FONT&gt;&lt;/FONT&gt;&lt;/P&gt;  &lt;/OL&gt;  &lt;P LANG=en-US STYLE=margin-left: 1.27cm; margin-bottom: 0.35cm&gt;&lt;FONT FACE=Century Gothic, serif&gt;&lt;B&gt;Simwanza  Vs The People [1985] Z.R.15&lt;/B&gt;&lt;/FONT&gt;&lt;/P&gt;  &lt;P LANG=en-US STYLE=margin-left: 1.27cm; margin-bottom: 0.35cm&gt;&lt;FONT FACE=Century Gothic, serif&gt;&lt;B&gt;Jones  Vs National Coal Board [1957] 2. ALL  E.R. 155&lt;/B&gt;&lt;/FONT&gt;&lt;/P&gt;  &lt;P LANG=en-US STYLE=margin-left: 1.27cm; margin-bottom: 0.35cm&gt;&lt;FONT FACE=Century Gothic, serif&gt;&lt;B&gt;Re  Enock and Zaretzky, Bock and Co. [1910] 1 K.B.327&lt;/B&gt;&lt;/FONT&gt;&lt;/P&gt;  &lt;P LANG=en-US STYLE=margin-left: 1.27cm; margin-bottom: 0.35cm&gt;&lt;FONT FACE=Century Gothic, serif&gt;&lt;B&gt;Porter  Vs Magill [2002] 1 ALL E.R. 465&lt;/B&gt;&lt;/FONT&gt;&lt;/P&gt;  &lt;P LANG=en-US STYLE=margin-left: 1.27cm; margin-bottom: 0.35cm&gt;&lt;FONT FACE=Century Gothic, serif&gt;&lt;B&gt;Fallow  Vs Calvert [1960] 2.Q.B. 201&lt;/B&gt;&lt;/FONT&gt;&lt;/P&gt;  &lt;P LANG=en-US STYLE=margin-left: 1.27cm; margin-bottom: 0.35cm&gt;&lt;FONT FACE=Century Gothic, serif&gt;&lt;B&gt;Water  Welles Ltd Vs Jackson [1984] Z.R. 98&lt;/B&gt;&lt;/FONT&gt;&lt;/P&gt;  &lt;P LANG=en-US STYLE=margin-left: 1.27cm; margin-bottom: 0.35cm&gt;&lt;FONT FACE=Century Gothic, serif&gt;&lt;B&gt;Zambia  Telecommunications Ltd Vs Celtel (Z) Ltd SCZ No. 90 of 2006  (unreported)&lt;/B&gt;&lt;/FONT&gt;&lt;/P&gt;  &lt;P LANG=en-US STYLE=margin-left: 1.27cm; margin-bottom: 0.35cm&gt;&lt;FONT FACE=Century Gothic, serif&gt;&lt;B&gt;Mazoka  and others Vs Mwanawasa and others [2005] Z.R. 138&lt;/B&gt;&lt;/FONT&gt;&lt;/P&gt;  &lt;P LANG=en-US STYLE=margin-left: 1.27cm; margin-bottom: 0.35cm&gt;&lt;FONT FACE=Century Gothic, serif&gt;&lt;B&gt;Thomas  Mumba and others Vs The People SCZ Appeal No. 92-95 (unreported) &lt;/B&gt;&lt;/FONT&gt;  &lt;/P&gt;  &lt;P LANG=en-US STYLE=margin-left: 1.27cm; margin-bottom: 0.35cm&gt;&lt;FONT FACE=Century Gothic, serif&gt;&lt;B&gt;Lewanika  and others Vs Chiluba [1998] Z.R. 79&lt;/B&gt;&lt;/FONT&gt;&lt;/P&gt;  &lt;P LANG=en-US STYLE=margin-left: 1.27cm; margin-bottom: 0.35cm&gt;&lt;FONT FACE=Century Gothic, serif&gt;&lt;B&gt;Mabenga  Vs Wina [2003] Z.R. 110&lt;/B&gt;&lt;/FONT&gt;&lt;/P&gt;  &lt;P LANG=en-US ALIGN=JUSTIFY STYLE=margin-left: 1.27cm; margin-bottom: 0cm; line-height: 150%&gt;  &lt;BR&gt;  &lt;/P&gt;  &lt;P LANG=en-US CLASS=western ALIGN=JUSTIFY STYLE=margin-bottom: 0cm; line-height: 150%&gt;  &lt;FONT FACE=Century Gothic, serif&gt;&lt;FONT SIZE=3&gt;&lt;B&gt;Legislation  referred to:&lt;/B&gt;&lt;/FONT&gt;&lt;/FONT&gt;&lt;/P&gt;  &lt;P LANG=en-US CLASS=western STYLE=margin-bottom: 0.35cm&gt;&lt;FONT FACE=Century Gothic, serif&gt;&lt;B&gt;Criminal  Procedure Code, Cap. 87 &amp;ndash; S.149&lt;/B&gt;&lt;/FONT&gt;&lt;/P&gt;  &lt;P LANG=en-US CLASS=western STYLE=margin-bottom: 0.35cm&gt;&lt;FONT FACE=Century Gothic, serif&gt;&lt;B&gt;Electoral  Act, No.12 of 2006 &amp;ndash; SS.102(3) and 103(1)(a) and (b)&lt;/B&gt;&lt;/FONT&gt;&lt;/P&gt;  &lt;P LANG=en-US CLASS=western ALIGN=JUSTIFY STYLE=margin-bottom: 0cm; line-height: 150%&gt;  &lt;BR&gt;  &lt;/P&gt;  &lt;P LANG=en-US CLASS=western ALIGN=JUSTIFY STYLE=margin-bottom: 0cm; line-height: 150%&gt;  &lt;FONT FACE=Century Gothic, serif&gt;&lt;FONT SIZE=3&gt;&lt;B&gt;Other works  referred to:&lt;/B&gt;&lt;/FONT&gt;&lt;/FONT&gt;&lt;/P&gt;  &lt;P LANG=en-US CLASS=western STYLE=margin-bottom: 0.35cm&gt;&lt;FONT FACE=Century Gothic, serif&gt;&lt;B&gt;Halsburys  Laws of England 4&lt;/B&gt;&lt;/FONT&gt;&lt;SUP&gt;&lt;FONT FACE=Century Gothic, serif&gt;&lt;B&gt;th&lt;/B&gt;&lt;/FONT&gt;&lt;/SUP&gt;&lt;FONT FACE=Century Gothic, serif&gt;&lt;B&gt;  Edition Vol. 17 Page 195 Paragraph 281&lt;/B&gt;&lt;/FONT&gt;&lt;/P&gt;  &lt;P LANG=en-US CLASS=western ALIGN=JUSTIFY STYLE=margin-bottom: 0cm; line-height: 150%&gt;  &lt;BR&gt;  &lt;/P&gt;  &lt;P LANG=en-US CLASS=western ALIGN=JUSTIFY STYLE=margin-bottom: 0cm; line-height: 150%&gt;  &lt;FONT FACE=Century Gothic, serif&gt;&lt;FONT SIZE=3&gt;This is an appeal  against the High Court judgment of 30&lt;/FONT&gt;&lt;/FONT&gt;&lt;SUP&gt;&lt;FONT FACE=Century Gothic, serif&gt;&lt;FONT SIZE=3&gt;th&lt;/FONT&gt;&lt;/FONT&gt;&lt;/SUP&gt;&lt;FONT FACE=Century Gothic, serif&gt;&lt;FONT SIZE=3&gt;  July, 2007 dismissing the appellant&amp;rsquo;s petition against the  election of the 2&lt;/FONT&gt;&lt;/FONT&gt;&lt;SUP&gt;&lt;FONT FACE=Century Gothic, serif&gt;&lt;FONT SIZE=3&gt;nd&lt;/FONT&gt;&lt;/FONT&gt;&lt;/SUP&gt;&lt;FONT FACE=Century Gothic, serif&gt;&lt;FONT SIZE=3&gt;  Respondent as a Member of Parliament for the Chilanga Parliamentary  Constituency during the Presidential and Parliamentary General  Elections held on 28&lt;/FONT&gt;&lt;/FONT&gt;&lt;SUP&gt;&lt;FONT FACE=Century Gothic, serif&gt;&lt;FONT SIZE=3&gt;th&lt;/FONT&gt;&lt;/FONT&gt;&lt;/SUP&gt;&lt;FONT FACE=Century Gothic, serif&gt;&lt;FONT SIZE=3&gt;  September, 2006, seeking to nullify the 2&lt;/FONT&gt;&lt;/FONT&gt;&lt;SUP&gt;&lt;FONT FACE=Century Gothic, serif&gt;&lt;FONT SIZE=3&gt;nd&lt;/FONT&gt;&lt;/FONT&gt;&lt;/SUP&gt;&lt;FONT FACE=Century Gothic, serif&gt;&lt;FONT SIZE=3&gt;  Respondent&amp;rsquo;s election on a number of malpractices as pleaded in  the petition.&lt;/FONT&gt;&lt;/FONT&gt;&lt;/P&gt;  &lt;P LANG=en-US CLASS=western ALIGN=JUSTIFY STYLE=margin-bottom: 0cm; line-height: 150%&gt;  &lt;BR&gt;  &lt;/P&gt;  &lt;P LANG=en-US CLASS=western ALIGN=RIGHT STYLE=margin-bottom: 0cm; line-height: 150%&gt;  &lt;BR&gt;  &lt;/P&gt;  &lt;P LANG=en-US CLASS=western ALIGN=RIGHT STYLE=margin-bottom: 0cm; line-height: 150%&gt;  &lt;FONT FACE=Century Gothic, serif&gt;&lt;FONT SIZE=3&gt;&lt;B&gt;(708)&lt;/B&gt;&lt;/FONT&gt;&lt;/FONT&gt;&lt;/P&gt;  &lt;P LANG=en-US CLASS=western ALIGN=JUSTIFY STYLE=margin-bottom: 0cm; line-height: 150%&gt;  &lt;BR&gt;  &lt;/P&gt;  &lt;P LANG=en-US CLASS=western ALIGN=JUSTIFY STYLE=margin-bottom: 0cm; line-height: 150%&gt;  &lt;FONT FACE=Century Gothic, serif&gt;&lt;FONT SIZE=3&gt;We wish to state that  in the court below, the petition was prosecuted by two Petitioners  but the 2&lt;/FONT&gt;&lt;/FONT&gt;&lt;SUP&gt;&lt;FONT FACE=Century Gothic, serif&gt;&lt;FONT SIZE=3&gt;nd&lt;/FONT&gt;&lt;/FONT&gt;&lt;/SUP&gt;&lt;FONT FACE=Century Gothic, serif&gt;&lt;FONT SIZE=3&gt;  Petitioner, Capt. Cosmas Moono, is not a party to this appeal.  However, for clarity&amp;rsquo;s sake we shall simply refer to the  appellant as the Petitioner and the 1&lt;/FONT&gt;&lt;/FONT&gt;&lt;SUP&gt;&lt;FONT FACE=Century Gothic, serif&gt;&lt;FONT SIZE=3&gt;st&lt;/FONT&gt;&lt;/FONT&gt;&lt;/SUP&gt;&lt;FONT FACE=Century Gothic, serif&gt;&lt;FONT SIZE=3&gt;  and 2&lt;/FONT&gt;&lt;/FONT&gt;&lt;SUP&gt;&lt;FONT FACE=Century Gothic, serif&gt;&lt;FONT SIZE=3&gt;nd&lt;/FONT&gt;&lt;/FONT&gt;&lt;/SUP&gt;&lt;FONT FACE=Century Gothic, serif&gt;&lt;FONT SIZE=3&gt;  Respondents as the 1&lt;/FONT&gt;&lt;/FONT&gt;&lt;SUP&gt;&lt;FONT FACE=Century Gothic, serif&gt;&lt;FONT SIZE=3&gt;st&lt;/FONT&gt;&lt;/FONT&gt;&lt;/SUP&gt;&lt;FONT FACE=Century Gothic, serif&gt;&lt;FONT SIZE=3&gt;  and 2&lt;/FONT&gt;&lt;/FONT&gt;&lt;SUP&gt;&lt;FONT FACE=Century Gothic, serif&gt;&lt;FONT SIZE=3&gt;nd&lt;/FONT&gt;&lt;/FONT&gt;&lt;/SUP&gt;&lt;FONT FACE=Century Gothic, serif&gt;&lt;FONT SIZE=3&gt;  Respondents respectively, the titles they held in the court below.&lt;/FONT&gt;&lt;/FONT&gt;&lt;/P&gt;  &lt;P LANG=en-US CLASS=western ALIGN=JUSTIFY STYLE=margin-bottom: 0cm; line-height: 150%&gt;  &lt;BR&gt;  &lt;/P&gt;  &lt;P LANG=en-US CLASS=western ALIGN=JUSTIFY STYLE=margin-bottom: 0cm; line-height: 150%&gt;  &lt;FONT FACE=Century Gothic, serif&gt;&lt;FONT SIZE=3&gt;The undisputed facts  of this case are that both the Petitioner and the 2&lt;/FONT&gt;&lt;/FONT&gt;&lt;SUP&gt;&lt;FONT FACE=Century Gothic, serif&gt;&lt;FONT SIZE=3&gt;nd&lt;/FONT&gt;&lt;/FONT&gt;&lt;/SUP&gt;&lt;FONT FACE=Century Gothic, serif&gt;&lt;FONT SIZE=3&gt;  Respondent contested the Parliamentary General Elections held on 28&lt;/FONT&gt;&lt;/FONT&gt;&lt;SUP&gt;&lt;FONT FACE=Century Gothic, serif&gt;&lt;FONT SIZE=3&gt;th&lt;/FONT&gt;&lt;/FONT&gt;&lt;/SUP&gt;&lt;FONT FACE=Century Gothic, serif&gt;&lt;FONT SIZE=3&gt;  September, 2006 in the Chilanga Constituency. The Petitioner stood on  the Patriotic Front (PF) ticket; while the 2&lt;/FONT&gt;&lt;/FONT&gt;&lt;SUP&gt;&lt;FONT FACE=Century Gothic, serif&gt;&lt;FONT SIZE=3&gt;nd&lt;/FONT&gt;&lt;/FONT&gt;&lt;/SUP&gt;&lt;FONT FACE=Century Gothic, serif&gt;&lt;FONT SIZE=3&gt;  Respondent stood on the ticket of the Movement for Multiparty  Democracy (MMD).


UPDATE:

I've voted to close my own question. The answer (which works) is here: 
BeautifulSoup return unexpected extra spaces
",4,895,"Look - the input document is invalid HTML - because it has three close tags at beginning:

&lt;/FONT&gt;&lt;/FONT&gt;&lt;/P&gt;


may be this is the reason. May be also parse and change &lt;BR&gt; to &lt;BR/&gt;
",,
BeautifulSoup unexpected output,https://stackoverflow.com/questions/38246620,XML parser removes processing instruction close delimiter&#39;s question mark,"I've encountered the following unexpected behaviour with the xml parser used by Python 3.4's BeautifulSoup 4. When parsing an xml stylesheet, the question mark in the closing delimiter disappears:

Input:

BeautifulSoup('&lt;?xml version=""1.0"" encoding=""UTF-8""?&gt;&lt;?xml-stylesheet href=""myStyleSheet.xsl"" type=""text/xsl""?&gt;','xml')


Output:

&lt;?xml version=""1.0"" encoding=""utf-8""?&gt;
&lt;?xml-stylesheet href=""myStyleSheet.xsl"" type=""text/xsl""&gt;


Notice how type=""text/xsl""?&gt; changed to type=""text/xsl""&gt;.

Also, the example found here breaks in the same way.

Is this intentional? Should I report a bug? Is there any way to prevent this other than inserting a question mark in the correct place afterwards?
",2,302,"From the changelog for version 4.5.0 of BeautifulSoup released 7/19/2016:

""* Corrected handling of XML processing instructions. [bug=1504393]""
",,
BeautifulSoup unexpected output,https://stackoverflow.com/questions/50560526,Extract subclass from class using beautifulsoup,"I'm working with the following HTML snippet from a page on Goodreads using Python 3.6.3:

&lt;div class=""quoteText""&gt;
      “Don't cry because it's over, smile because it happened.”
  &lt;br/&gt;  ―
    &lt;a class=""authorOrTitle"" href=""/author/show/61105.Dr_Seuss""&gt;Dr. Seuss&lt;/a&gt;
&lt;/div&gt;, &lt;div class=""quoteText""&gt;


I used BeautifulSoup to scrape the HTML and isolate just the ""quoteText"" class seen in the snippet above. Now, I want to save the quote and author name as separate strings. I was able to get the author name using

(quote_tag.find(class_=""quoteText"")).text


I'm not sure how to do the same for the quote. I'm guessing I need a way to remove the  subclass from my output and tried using the extract method.

quote.extract(class_=""authorOrTitle"")


but I got an error saying extract got an unexpected keyword argument 'class_'
Is there any other way to do what I'm trying to do?

This is my first time posting on here so I apologize if the post doesn't meet particular specificity/formatting/other standards.
",1,4250,"
  PageElement.extract() removes a tag or string from the tree. It
  returns the tag or string that was extracted


from bs4 import BeautifulSoup
a='''&lt;div class=""quoteText""&gt;
      “Don't cry because it's over, smile because it happened.”
  &lt;br/&gt;  -
    &lt;a class=""authorOrTitle"" href=""/author/show/61105.Dr_Seuss""&gt;Dr. Seuss&lt;/a&gt;
&lt;/div&gt;, &lt;div class=""quoteText""&gt;'''
s=BeautifulSoup(a,'lxml')
s.find(class_=""authorOrTitle"").extract()
print(s.text)

",,
BeautifulSoup unexpected output,https://stackoverflow.com/questions/42419382,wagtail comparing revisions produces : render() got an unexpected keyword argument &#39;context&#39;,"On version 1.9, trying out the comparing revisions of a page new feature locally, when :

    render() got an unexpected keyword argument 'context'
    Request Method: GET
    Request URL:      http://localhost:8000/admin/pages/48/revisions/compare/488...489/
    Django Version: 1.10.5
    Exception Type: TypeError
    Exception Value:  
    render() got an unexpected keyword argument 'context'
    Exception Location: /usr/local/lib/python2.7/site-   packages/wagtail/wagtailcore/blocks/base.py in render, line 442
    Python Executable:  /usr/local/bin/python2
    Python Version: 2.7.13
    Python Path:  
    ['/code',
    '/usr/local/bin',
    '/usr/local/lib/python27.zip',
    '/usr/local/lib/python2.7',
    '/usr/local/lib/python2.7/plat-linux2',
    '/usr/local/lib/python2.7/lib-tk',
    '/usr/local/lib/python2.7/lib-old',
    '/usr/local/lib/python2.7/lib-dynload',
    '/usr/local/lib/python2.7/site-packages']
    Server time:  Thu, 23 Feb 2017 14:51:34 +0000
    Error during template rendering

    In template /usr/local/lib/python2.7/site-    packages/wagtail/wagtailadmin/templates/wagtailadmin/pages/revisions/comp    are.html, error at line 37
    render() got an unexpected keyword argument 'context'
    27              &lt;/thead&gt;
    28  
    29              &lt;tbody&gt;
    30                  {% for comp in comparison %}
    31                      &lt;tr&gt;
    32                          &lt;td class=""title"" valign=""top""&gt;
    33                              &lt;h2&gt;{{ comp.field_label }}:&lt;/h2&gt;
    34                          &lt;/td&gt;
    35                          &lt;td class=""comparison{% if not  comp.is_field %} no-padding{% endif %}""&gt;
    36                              {% if comp.is_field %}
    37                                  {{ comp.htmldiff }}
    38                              {% elif comp.is_child_relation %}
    39                                  {% for child_comp in  comp.get_child_comparisons %}
    40                                      &lt;div class=""comparison__child-object {% if child_comp.is_addition %}addition{% elif child_comp.is_deletion %}deletion{% endif %}""&gt;
    41                                          {% with child_comp.get_position_change as move %}
    42                                              {% if move %}
    43                                              &lt;div class=""help- block help-info""&gt;
    44                                                  &lt;p&gt;
    45                                                      {% if move &gt;   0 %}
    46                                                          {%  blocktrans count counter=move %}
    47                                                              Moved   down 1 place.
    Traceback Switch to copy-and-paste view

    /usr/local/lib/python2.7/site-packages/django/core/handlers/exception.py in inner
            response = get_response(request) ...
    ▶ Local vars
    /usr/local/lib/python2.7/site-packages/django/core/handlers/base.py   in _legacy_get_response
            response = self._get_response(request) ...
    ▶ Local vars
    /usr/local/lib/python2.7/site-packages/django/core/handlers/base.py in _get_response
                response = self.process_exception_by_middleware(e,   request) ...
    ▶ Local vars
    /usr/local/lib/python2.7/site-packages/django/core/handlers/base.py  in _get_response
                response = wrapped_callback(request, *callback_args, **callback_kwargs) ...
    ▶ Local vars
    /usr/local/lib/python2.7/site- packages/django/views/decorators/cache.py in _cache_controlled
            response = viewfunc(request, *args, **kw) ...
    ▶ Local vars
    /usr/local/lib/python2.7/site- packages/wagtail/wagtailadmin/decorators.py in decorated_view
            return view_func(request, *args, **kwargs) ...
    ▶ Local vars
    /usr/local/lib/python2.7/site-  packages/wagtail/wagtailadmin/views/pages.py in revisions_compare
        'comparison': comparison, ...
    ▶ Local vars
    /usr/local/lib/python2.7/site-packages/django/shortcuts.py in render
    content = loader.render_to_string(template_name, context, request,   using=using) ...
    ▶ Local vars
/usr/local/lib/python2.7/site-packages/django/template/loader.py in  render_to_string
    return template.render(context, request) ...
    ▶ Local vars
    /usr/local/lib/python2.7/site- packages/django/template/backends/django.py in render
            return self.template.render(context) ...
    ▶ Local vars
    /usr/local/lib/python2.7/site-packages/django/template/base.py in render
                    return self._render(context) ...
    ▶ Local vars
    /usr/local/lib/python2.7/site-packages/django/template/base.py in    _render
        return self.nodelist.render(context) ...
    ▶ Local vars
    /usr/local/lib/python2.7/site-packages/django/template/base.py in render
                bit = node.render_annotated(context) ...
    ▶ Local vars
/usr/local/lib/python2.7/site-packages/django/template/base.py in render_annotated
            return self.render(context) ...
    ▶ Local vars
    /usr/local/lib/python2.7/site-packages/django/template/loader_tags.py in render
         return compiled_parent._render(context) ...
    ▶ Local vars
    /usr/local/lib/python2.7/site-packages/django/template/base.py in _render
        return self.nodelist.render(context) ...
    ▶ Local vars
    /usr/local/lib/python2.7/site-packages/django/template/base.py in  render
                bit = node.render_annotated(context) ...
    ▶ Local vars
    /usr/local/lib/python2.7/site-packages/django/template/base.py in render_annotated
            return self.render(context) ...
    ▶ Local vars
    /usr/local/lib/python2.7/site-packages/django/template/loader_tags.py in render
        return compiled_parent._render(context) ...
    ▶ Local vars
    /usr/local/lib/python2.7/site-packages/django/template/base.py in _render
        return self.nodelist.render(context) ...
    ▶ Local vars
/usr/local/lib/python2.7/site-packages/django/template/base.py in render
                bit = node.render_annotated(context) ...
    ▶ Local vars
    /usr/local/lib/python2.7/site-packages/django/template/base.py in  render_annotated
            return self.render(context) ...
    ▶ Local vars
    /usr/local/lib/python2.7/site-packages/django/template/loader_tags.py in render
        return compiled_parent._render(context) ...
    ▶ Local vars
    /usr/local/lib/python2.7/site-packages/django/template/base.py in _render
           return self.nodelist.render(context) ...
    ▶ Local vars
    /usr/local/lib/python2.7/site-packages/django/template/base.py in render
                bit = node.render_annotated(context) ...
    ▶ Local vars
    /usr/local/lib/python2.7/site-packages/django/template/base.py in render_annotated
            return self.render(context) ...
    ▶ Local vars
    /usr/local/lib/python2.7/site-packages/django/template/loader_tags.py in render
                 result = block.nodelist.render(context) ...
    ▶ Local vars
    /usr/local/lib/python2.7/site-packages/django/template/base.py in render
                bit = node.render_annotated(context) ...
    ▶ Local vars
    /usr/local/lib/python2.7/site-packages/django/template/base.py in   render_annotated
            return self.render(context) ...
    ▶ Local vars
    /usr/local/lib/python2.7/site-packages/django/template/loader_tags.py in render
                result = block.nodelist.render(context) ...
    ▶ Local vars
    /usr/local/lib/python2.7/site-packages/django/template/base.py in render
                bit = node.render_annotated(context) ...
    ▶ Local vars
    /usr/local/lib/python2.7/site-packages/django/template/base.py in render_annotated
            return self.render(context) ...
    ▶ Local vars
    /usr/local/lib/python2.7/site-packages/django/template/defaulttags.py in render
                    nodelist.append(node.render_annotated(context)) ...
    ▶ Local vars
    /usr/local/lib/python2.7/site-packages/django/template/base.py in  render_annotated
            return self.render(context) ...
    ▶ Local vars
    /usr/local/lib/python2.7/site-packages/django/template/defaulttags.py  in render
                return nodelist.render(context) ...
    ▶ Local vars
    /usr/local/lib/python2.7/site-packages/django/template/base.py in render
                bit = node.render_annotated(context) ...
    ▶ Local vars
    /usr/local/lib/python2.7/site-packages/django/template/base.py in render_annotated
            return self.render(context) ...
    ▶ Local vars
    /usr/local/lib/python2.7/site-packages/django/template/base.py in render
            output = self.filter_expression.resolve(context) ...
    ▶ Local vars
    /usr/local/lib/python2.7/site-packages/django/template/base.py in resolve
                obj = self.var.resolve(context) ...
    ▶ Local vars
    /usr/local/lib/python2.7/site-packages/django/template/base.py in resolve
            value = self._resolve_lookup(context) ...
    ▶ Local vars
    /usr/local/lib/python2.7/site-packages/django/template/base.py in _resolve_lookup
                            current = current() ...
    ▶ Local vars
    /usr/local/lib/python2.7/site-packages/wagtail/wagtailadmin/compare.py in htmldiff
            BeautifulSoup(force_text(self.val_a), 'html5lib').getText(),   ...
   ▶ Local vars
   /usr/local/lib/python2.7/site-packages/django/utils/encoding.py in    force_text
                s = six.text_type(s) ...
   ▶ Local vars
   /usr/local/lib/python2.7/site-packages/wagtail/wagtailcore/blocks/stream_block.py in __str__
        return self.__html__() ...
   ▶ Local vars
   /usr/local/lib/python2.7/site-packages/wagtail/wagtailcore/blocks/stream_block.py in __html__
        return self.stream_block.render(self) ...
    ▶ Local vars
    /usr/local/lib/python2.7/site- packages/wagtail/wagtailcore/blocks/base.py in render
            return self.render_basic(value, context=context) ...
    ▶ Local vars
    /usr/local/lib/python2.7/site-  packages/wagtail/wagtailcore/blocks/stream_block.py in render_basic
                for child in value ...
    ▶ Local vars
/usr/local/lib/python2.7/site-packages/wagtail/wagtailcore/blocks/base.py in render
        return self.block.render(self.value, context=context) 

",0,581,"It looks like you have a custom StreamField block that defines its own render method. This method was updated in Wagtail 1.6 to accept a context keyword argument, and so you need to update it as described here:

http://docs.wagtail.io/en/stable/releases/1.6.html#render-and-render-basic-methods-on-streamfield-blocks-now-accept-a-context-keyword-argument

The old signature (without the context keyword argument) would have continued to work in Wagtail 1.6 and 1.7, but output a deprecation warning; support for it was dropped completely in Wagtail 1.8.

StreamField blocks now allow custom get_template methods for overriding templates in instances from Wagtail 2.0. 

http://docs.wagtail.io/en/stable/releases/2.0.html#other-features
",,
BeautifulSoup unexpected result,https://stackoverflow.com/questions/20205455,How to correctly parse UTF-8 encoded HTML to Unicode strings with BeautifulSoup?,"I'm running a Python program which fetches a UTF-8-encoded web page, and I extract some text from the HTML using BeautifulSoup.

However, when I write this text to a file (or print it on the console), it gets written in an unexpected encoding.

Sample program:

import urllib2
from BeautifulSoup import BeautifulSoup

# Fetch URL
url = 'http://www.voxnow.de/'
request = urllib2.Request(url)
request.add_header('Accept-Encoding', 'utf-8')

# Response has UTF-8 charset header,
# and HTML body which is UTF-8 encoded
response = urllib2.urlopen(request)

# Parse with BeautifulSoup
soup = BeautifulSoup(response)

# Print title attribute of a &lt;div&gt; which uses umlauts (e.g. können)
print repr(soup.find('div', id='navbutton_account')['title'])


Running this gives the result:

# u'Hier k\u0102\u015bnnen Sie sich kostenlos registrieren und / oder einloggen!'


But I would expect a Python Unicode string to render ö in the word können as \xf6:

# u'Hier k\xf6bnnen Sie sich kostenlos registrieren und / oder einloggen!'


I've tried passing the 'fromEncoding' parameter to BeautifulSoup, and trying to read() and decode() the response object, but it either makes no difference, or throws an error.

With the command curl www.voxnow.de | hexdump -C, I can see that the web page is indeed UTF-8 encoded (i.e. it contains 0xc3 0xb6) for the ö character:

      20 74 69 74 6c 65 3d 22  48 69 65 72 20 6b c3 b6  | title=""Hier k..|
      6e 6e 65 6e 20 53 69 65  20 73 69 63 68 20 6b 6f  |nnen Sie sich ko|
      73 74 65 6e 6c 6f 73 20  72 65 67 69 73 74 72 69  |stenlos registri|


I'm beyond the limit of my Python abilities, so I'm at a loss as to how to debug this further. Any advice?
",35,120993,"As justhalf points out above, my question here is essentially a duplicate of this question.

The HTML content reported itself as UTF-8 encoded and, for the most part it was, except for one or two rogue invalid UTF-8 characters.

This apparently confuses BeautifulSoup about which encoding is in use, and when trying to first decode as UTF-8 when passing the content to BeautifulSoup like 
this:

soup = BeautifulSoup(response.read().decode('utf-8'))


I would get the error:

UnicodeDecodeError: 'utf8' codec can't decode bytes in position 186812-186813: 
                    invalid continuation byte


Looking more closely at the output, there was an instance of the character Ü which was wrongly encoded as the invalid byte sequence 0xe3 0x9c, rather than the correct 0xc3 0x9c.

As the currently highest-rated answer on that question suggests, the invalid UTF-8 characters can be removed while parsing, so that only valid data is passed to BeautifulSoup:

soup = BeautifulSoup(response.read().decode('utf-8', 'ignore'))

","Encoding the result to utf-8 seems to work for me:

print (soup.find('div', id='navbutton_account')['title']).encode('utf-8')


It yields:

Hier kÃ¶nnen Sie sich kostenlos registrieren und / oder einloggen!

",
BeautifulSoup unexpected result,https://stackoverflow.com/questions/59373948,Scraping JSON served by script and converting to dataframe,"I am trying to obtain the details from this job post (script json to df) but am unable to make progress. 

import requests 
from bs4 import BeautifulSoup 
import pandas as pd
from pandas.io.json import json_normalize
import time
import re
import json
from pandas.compat import StringIO


URLS=['https://www.iimjobs.com/j/specialist-manager-operations-student-services-elearning-education-management-organization-4-6-yrs-774235.html?ref=cl','https://www.iimjobs.com/j/specialist-manager-operations-student-services-elearning-education-management-organization-4-6-yrs-774235.html?ref=cl']


i=0
for URL in URLS:
#     time.sleep(5)   
    r = requests.get(URL) 
    soup = BeautifulSoup(r.content, 'html5lib') 
    # print(soup.prettify()) 
    table=soup.find(""script"" , type='application/ld+json').text
    data = json.loads(json.dumps(table))
    if i == 0:
        df = pd.read_json(data)  
    if i != 0:
        dfnew=pd.read_json(data)
        df=df.append(dfnew)     
    i=i+1
df.to_csv('jobs.csv', index=False)
print(df)


Can someone help me with this please? the detailed error is as follows:

ValueError                                Traceback (most recent call last)
&lt;ipython-input-9-5f9199aa28fd&gt; in &lt;module&gt;
     30     data = json.loads(json.dumps(table))
     31     if i == 0:
---&gt; 32         df = pd.read_json(data)
     33     if i != 0:
     34         print(URL)

C:\ProgramData\Anaconda3\lib\site-packages\pandas\io\json\json.py in read_json(path_or_buf, orient, typ, dtype, convert_axes, convert_dates, keep_default_dates, numpy, precise_float, date_unit, encoding, lines, chunksize, compression)
    425         return json_reader
    426 
--&gt; 427     result = json_reader.read()
    428     if should_close:
    429         try:

C:\ProgramData\Anaconda3\lib\site-packages\pandas\io\json\json.py in read(self)
    535             )
    536         else:
--&gt; 537             obj = self._get_object_parser(self.data)
    538         self.close()
    539         return obj

C:\ProgramData\Anaconda3\lib\site-packages\pandas\io\json\json.py in _get_object_parser(self, json)
    554         obj = None
    555         if typ == 'frame':
--&gt; 556             obj = FrameParser(json, **kwargs).parse()
    557 
    558         if typ == 'series' or obj is None:

C:\ProgramData\Anaconda3\lib\site-packages\pandas\io\json\json.py in parse(self)
    650 
    651         else:
--&gt; 652             self._parse_no_numpy()
    653 
    654         if self.obj is None:

C:\ProgramData\Anaconda3\lib\site-packages\pandas\io\json\json.py in _parse_no_numpy(self)
    869         if orient == ""columns"":
    870             self.obj = DataFrame(
--&gt; 871                 loads(json, precise_float=self.precise_float), dtype=None)
    872         elif orient == ""split"":
    873             decoded = {str(k): v for k, v in compat.iteritems(

ValueError: Unexpected character found when decoding object value


I am essentially trying to scrape job details like title, description, skills, jobLocation etc.However the JSON served for this particular URL seems to fail and I have not been able to figure this one out yet
",1,263,"The issue is that some of the values in the script json contains double quotes for html (ie: class=""""). So it's treating that as the end and start of a new string, and does not have a comma, or a key:value that is valid.

So if you take care of that, it should work:

import requests 
from bs4 import BeautifulSoup 
import pandas as pd
from pandas.io.json import json_normalize
import time
import re
import json
from pandas.compat import StringIO


URLS=['https://www.iimjobs.com/j/specialist-manager-operations-student-services-elearning-education-management-organization-4-6-yrs-774235.html?ref=cl','https://www.iimjobs.com/j/specialist-manager-operations-student-services-elearning-education-management-organization-4-6-yrs-774235.html?ref=cl']


i=0
for URL in URLS:
#     time.sleep(5)   
    r = requests.get(URL) 
    soup = BeautifulSoup(r.content, 'html5lib') 
    # print(soup.prettify()) 
    table=soup.find(""script"" , type='application/ld+json').text
    data = json.loads(json.dumps(table))
    if i == 0:
        try:
            df = pd.read_json(data)  
        except:
            data = data.replace('=""""', '=')
            df = pd.read_json(data)  
    if i != 0:
        try:
            dfnew=pd.read_json(data)
            df=df.append(dfnew) 

        except:
            data = data.replace('=""""', '=')
            dfnew=pd.read_json(data)
            df=df.append(dfnew) 

    i=i+1
df.to_csv('jobs.csv', index=False)
print(df)

",,
BeautifulSoup unexpected result,https://stackoverflow.com/questions/1697774,Matching tags in BeautifulSoup,"I'm trying to count the number of tags in the 'soup' from a beautifulsoup result. I'd like to use a regular expression but am having trouble.
The code Ive tried is as follows:

reg_exp_tag = re.compile(""&lt;[^&gt;*&gt;"")
tags = re.findall(reg_exp_tag, soup(cast as a string))


but re will not allow reg_exp_tag, giving an unexpected end of regular expression error.

Any help would be much appreciated!

Thanks
",1,550,"If you've already parsed the HTML with BeautifulSoup, why parse it again?  Try this:

num_tags = len(soup.findAll())

","Shouldn't that be ""&lt;[^&gt;]*&gt;"" instead of ""&lt;[^&gt;*&gt;""?

(the class needs to be closed with a ])
",
BeautifulSoup unexpected result,https://stackoverflow.com/questions/71253524,HTML parsing not working as expected using BeautifulSoup,"I'm using Python 3 and the BeautifulSoup module, version 4.9.3. I'm trying to use this package to practice parsing some simple HTML.
The string I have is the following:
text = '''&lt;li&gt;&lt;p&gt;Some text&lt;/p&gt;is put here&lt;/li&gt;&lt;li&gt;&lt;p&gt;And other text is put here&lt;/p&gt;&lt;/li&gt;'''
I use BeautifulSoup as follows:
x = BeautifulSoup(text, ""html.parser"")
I then experiment with Beautiful Soup's functionality with the following script:
for li in x.find_all('li'):
    print(li)
    print(li.string)
    print(li.next_element)
    print(li.next_element)
    print(li.next_element.string)
    print(""\n"")

The results (at least for the first iteration) are unexpected:
&lt;li&gt;&lt;p&gt;Some text&lt;/p&gt;is put here&lt;/li&gt;
None
&lt;p&gt;Some text&lt;/p&gt;
Some text


&lt;li&gt;&lt;p&gt;And other text is here&lt;/p&gt;&lt;/li&gt;
And other text is here
&lt;p&gt;And other text is here&lt;/p&gt;
And other text is here

Why is the string attribute of the first li tag None, whereas the string attribute of the inner p tag is not None?
Similarly, if I do:
x.find_all('li', string=re.compile('text'))
I only get one result (the 2nd tag).
But if I do:
for li in x.find_all('li'):
    print(li.find_all(string=re.compile('text')))

I get 2 results (both tags).
",0,347,"Paraphrasing the doc:


If a tag has only one child, and that child is a NavigableString, the child is made available as .string.
If a tag’s only child is another tag, and that tag has a .string, then the parent tag is considered to have the same .string as its child.
If a tag contains more than one thing, then it’s not clear what .string should refer to, so .string is defined to be None.


Let's apply these rules to your question:

Why is the string attribute of the first li tag None, whereas the string attribute of the inner p tag is not None?

The inner p tag satisfies rule #1; it has exactly one child, and that child is a NavigableString, so .string returns that child.
The first li satisfies rule #3; it has more than one child, so .string would be ambiguous.

Considering your second question, let's consult the doc for the string= argument to .find_all()

With string you can search for strings instead of tags. ... Although string is for finding strings, you can combine it with arguments that find tags: Beautiful Soup will find all tags whose .string matches your value for string.

Your first example:
x.find_all('li', string=re.compile('text'))
# [&lt;li&gt;&lt;p&gt;And other text is put here&lt;/p&gt;&lt;/li&gt;]

That searches for all of the li tags whose .string matches the regular expression. But we have already seen that the first li's .string is None, so it doesn't match.
Your second example:
for li in x.find_all('li'):
    print(li.find_all(string=re.compile('text')))
# ['Some text']
# ['And other text is put here']

This searches for all of the strings contained anywhere in each of the li trees. For the first tree, li.p.string exists and matches, even if li.string doesn't.
",,
BeautifulSoup unexpected result,https://stackoverflow.com/questions/62520919,Web scraping data from Transfermarkt,"I wrote a web-scraping procedure to scrape data from Transfermarkt.de
First, I get the data from the 20 biggest transfer from the last 10 years
headers = {'User-Agent': 
           'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/47.0.2526.106 Safari/537.36'}

df_consolidado = pd.DataFrame()
df = {}
temporadas = list(range(2009,2020))

#Crio os lista para armazenar as informações
Jogadores = []
Valores_Transf = []
Href = []

for t in temporadas:
    print(t)
    for p in range(1,5):
        #Carrega a pagina p do temporada t
        page = ""https://www.transfermarkt.de/transfers/transferrekorde/statistik/top/saison_id/"" + str(t) + ""/land_id//ausrichtung//spielerposition_id//altersklasse/u23/leihe//w_s//plus/0/galerie/0/page/"" + str(p)        
        print(page)
        pageTree = requests.get(page, headers=headers)
        pageSoup = BeautifulSoup(pageTree.content, 'html.parser')

        #Pega os dados das transferências
        jogador = pageSoup.find_all(""a"", {""class"": ""spielprofil_tooltip""})        
        valor_transf = pageSoup.find_all(""td"", {""class"": ""rechts hauptlink""})

        #Introduzo as informações nas listas
        for i in range(0,25):
            Jogadores.append(jogador[i].text)
            Valores_Transf.append(float(valor_transf[i].text.replace('Mio.', '').replace('€', '').replace(',', '.').replace('Leihgebühr:', '').replace('Leih-Ende', '0')))
            Href.append(jogador[i]['href'])

    df[t] = pd.DataFrame({""Temporada"":int(t),""Jogador"":Jogadores,""Valor Transferência"":Valores_Transf, ""Ref"":Href})

Then I combine all those dfs:
#Combinar os vários dfs gerados
df = pd.concat([df[2009], df[2010],df[2011], df[2012], df[2013], df[2014],df[2015], df[2016], df[2017], df[2018], df[2019]])

But at the last and most important step, I'm finding some troubles. Through the following code, I tried to get more detailed info:
headers = {'User-Agent': 
           'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/47.0.2526.106 Safari/537.36'}

Altura = []
Idade_Atual = []
Idade_Transf = []
Maior_Valor = []
Data_Max_Valor = []


for index, row in df.iterrows():
    page = ""https://www.transfermarkt.de"" + row['Ref']
    pageTree = requests.get(page, headers=headers, timeout=1000)    
    pageSoup = BeautifulSoup(pageTree.content, 'html.parser')

    #Carrego o objeto contendo os dados
    dados_agrupados = None
    dados_agrupados = pageSoup.find_all(""table"", {""class"": ""auflistung""})
    
    dados = []
    tabela = []
    print(page)
    for d in dados_agrupados:
        dados.extend(d.find_all(""td""))
        tabela.extend(d.find_all(""th""))

        #Verifico a estrutura da table para copiar os dados
        for t in range(len(tabela)):            
            if tabela[t].text == ""Height:"":                
                if dados[t].text != ""N/A"":
                    Altura.append(float(dados[t].text.rstrip(' m').replace(',', '.')))
                else:
                    Altura.append(0)
                
            if tabela[t].text == ""Age:"":
                Idade_Atual.append(int(dados[t].text))
                Idade_Transf.append(int((row['Temporada']-2020) + int(dados[t].text)))
            
            if tabela[t].text == ""Foot:"":
                Pe_Dominante.append(dados[t].text)
                
    
    #Carrego o objeto contendo o maior valor de mercado do jogador
    dados_agrupados_val = None
    dados_agrupados_val = pageSoup.find_all(""div"", {""class"": ""right-td""})    

    Data_Max_Valor.append(int(dados_agrupados_val[2].text.replace(' ', '')[-5:-1]))
    if ""k"" in str(dados_agrupados_val[2].text.replace('Mio.', '').replace('€', '').replace(',', '.').replace(' ', '')[:-12]):
        Maior_Valor.append(float(dados_agrupados_val[2].text.replace('Mio.', '').replace('Â', '').replace('â', '').replace('¬', '').replace('k', '').replace('€', '').replace(',', '.').replace(' ', '')[:-12])/1000)
    else:
        Maior_Valor.append(float(dados_agrupados_val[2].text.replace('Mio.', '').replace('Â', '').replace('â', '').replace('¬', '').replace('€', '').replace(',', '.').replace(' ', '')[:-13]))
    
df[""Altura""] = Altura
df[""Idade_Atual""] = Idade_Atual
df[""Idade_Transf""] = Idade_Transf           
df[""Max_Valor""] = Maior_Valor
df[""Data_Max_Valor""] = Data_Max_Valor

#Idade calculado quando o máximo valor de mercado foi atingido
df[""Idade_Max_Valor""] = df[""Data_Max_Valor""] - (df[""Temporada""]-df[""Idade_Transf""])

df

But I ended up with the following error:
---------------------------------------------------------------------------
SysCallError                              Traceback (most recent call last)
~\Anaconda3\lib\site-packages\urllib3\contrib\pyopenssl.py in wrap_socket(self, sock, server_side, do_handshake_on_connect, suppress_ragged_eofs, server_hostname)
    455             try:
--&gt; 456                 cnx.do_handshake()
    457             except OpenSSL.SSL.WantReadError:

~\Anaconda3\lib\site-packages\OpenSSL\SSL.py in do_handshake(self)
   1914         result = _lib.SSL_do_handshake(self._ssl)
-&gt; 1915         self._raise_ssl_error(self._ssl, result)
   1916 

~\Anaconda3\lib\site-packages\OpenSSL\SSL.py in _raise_ssl_error(self, ssl, result)
   1638                     if errno != 0:
-&gt; 1639                         raise SysCallError(errno, errorcode.get(errno))
   1640                 raise SysCallError(-1, ""Unexpected EOF"")

SysCallError: (10054, 'WSAECONNRESET')

During handling of the above exception, another exception occurred:

SSLError                                  Traceback (most recent call last)
~\Anaconda3\lib\site-packages\urllib3\connectionpool.py in urlopen(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)
    599                                                   body=body, headers=headers,
--&gt; 600                                                   chunked=chunked)
    601 

~\Anaconda3\lib\site-packages\urllib3\connectionpool.py in _make_request(self, conn, method, url, timeout, chunked, **httplib_request_kw)
    342         try:
--&gt; 343             self._validate_conn(conn)
    344         except (SocketTimeout, BaseSSLError) as e:

~\Anaconda3\lib\site-packages\urllib3\connectionpool.py in _validate_conn(self, conn)
    838         if not getattr(conn, 'sock', None):  # AppEngine might not have  `.sock`
--&gt; 839             conn.connect()
    840 

~\Anaconda3\lib\site-packages\urllib3\connection.py in connect(self)
    343             server_hostname=server_hostname,
--&gt; 344             ssl_context=context)
    345 

~\Anaconda3\lib\site-packages\urllib3\util\ssl_.py in ssl_wrap_socket(sock, keyfile, certfile, cert_reqs, ca_certs, server_hostname, ssl_version, ciphers, ssl_context, ca_cert_dir)
    346         if HAS_SNI and server_hostname is not None:
--&gt; 347             return context.wrap_socket(sock, server_hostname=server_hostname)
    348 

~\Anaconda3\lib\site-packages\urllib3\contrib\pyopenssl.py in wrap_socket(self, sock, server_side, do_handshake_on_connect, suppress_ragged_eofs, server_hostname)
    461             except OpenSSL.SSL.Error as e:
--&gt; 462                 raise ssl.SSLError('bad handshake: %r' % e)
    463             break

SSLError: (""bad handshake: SysCallError(10054, 'WSAECONNRESET')"",)

During handling of the above exception, another exception occurred:

MaxRetryError                             Traceback (most recent call last)
~\Anaconda3\lib\site-packages\requests\adapters.py in send(self, request, stream, timeout, verify, cert, proxies)
    448                     retries=self.max_retries,
--&gt; 449                     timeout=timeout
    450                 )

~\Anaconda3\lib\site-packages\urllib3\connectionpool.py in urlopen(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)
    637             retries = retries.increment(method, url, error=e, _pool=self,
--&gt; 638                                         _stacktrace=sys.exc_info()[2])
    639             retries.sleep()

~\Anaconda3\lib\site-packages\urllib3\util\retry.py in increment(self, method, url, response, error, _pool, _stacktrace)
    398         if new_retry.is_exhausted():
--&gt; 399             raise MaxRetryError(_pool, url, error or ResponseError(cause))
    400 

MaxRetryError: HTTPSConnectionPool(host='www.transfermarkt.de', port=443): Max retries exceeded with url: /bojan-krkic/profil/spieler/44675 (Caused by SSLError(SSLError(""bad handshake: SysCallError(10054, 'WSAECONNRESET')"")))

During handling of the above exception, another exception occurred:

SSLError                                  Traceback (most recent call last)
&lt;ipython-input-5-7f98723c208e&gt; in &lt;module&gt;
     11 for index, row in df.iterrows():
     12     page = ""https://www.transfermarkt.de"" + row['Ref']
---&gt; 13     pageTree = requests.get(page, headers=headers, timeout=1000)
     14     pageSoup = BeautifulSoup(pageTree.content, 'html.parser')
     15 

~\Anaconda3\lib\site-packages\requests\api.py in get(url, params, **kwargs)
     73 
     74     kwargs.setdefault('allow_redirects', True)
---&gt; 75     return request('get', url, params=params, **kwargs)
     76 
     77 

~\Anaconda3\lib\site-packages\requests\api.py in request(method, url, **kwargs)
     58     # cases, and look like a memory leak in others.
     59     with sessions.Session() as session:
---&gt; 60         return session.request(method=method, url=url, **kwargs)
     61 
     62 

~\Anaconda3\lib\site-packages\requests\sessions.py in request(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)
    531         }
    532         send_kwargs.update(settings)
--&gt; 533         resp = self.send(prep, **send_kwargs)
    534 
    535         return resp

~\Anaconda3\lib\site-packages\requests\sessions.py in send(self, request, **kwargs)
    644 
    645         # Send the request
--&gt; 646         r = adapter.send(request, **kwargs)
    647 
    648         # Total elapsed time of the request (approximately)

~\Anaconda3\lib\site-packages\requests\adapters.py in send(self, request, stream, timeout, verify, cert, proxies)
    512             if isinstance(e.reason, _SSLError):
    513                 # This branch is for urllib3 v1.22 and later.
--&gt; 514                 raise SSLError(e, request=request)
    515 
    516             raise ConnectionError(e, request=request)

SSLError: HTTPSConnectionPool(host='www.transfermarkt.de', port=443): Max retries exceeded with url: /bojan-krkic/profil/spieler/44675 (Caused by SSLError(SSLError(""bad handshake: SysCallError(10054, 'WSAECONNRESET')"")))

Does anyone understand what's happening?
",0,1110,"The problem may be because if the pyOpenSSL version you use, try updating it with the help of this link.
","It looks like this might be a problem on the server side, not with your code. The SysCallError code you're getting is a Windows Sockets error code. Citing from the Microsoft Docs:

WSAECONNRESET
10054
Connection reset by peer.
An existing connection was forcibly closed by the remote host. This normally results if the peer application on the remote host is suddenly stopped, the host is rebooted, the host or remote network interface is disabled, or the remote host uses a hard close (see setsockopt for more information on the SO_LINGER option on the remote socket). This error may also result if a connection was broken due to keep-alive activity detecting a failure while one or more operations are in progress. Operations that were in progress fail with WSAENETRESET. Subsequent operations fail with WSAECONNRESET.

It might be that this was a temporary issue. I was able to run your script and send requests for quite a while without issues. If this keeps happening when sending requests to that server, consider catching these exceptions using something like this:
try:
    pageTree = requests.get(page, headers=headers, timeout=1000)
except requests.exceptions.SSLError as e:
    print(f'request to {page} failed: {e}')
    # or retry the request until it succeeds

",
BeautifulSoup unexpected result,https://stackoverflow.com/questions/50508759,find all links in html parsed beautiful soup,"I am using beautifulsoup with python. In scrapping pages links are not enclosed in &lt;a href&gt; tags. 

I want to get all links starting with http/https using soup operation. I have tried some regex given here but they are giving unexpected results for me. 
so i thought if anything is possible using soup?

Example responses from which i want to get links:

&lt;html&gt;\n&lt;head&gt;\n&lt;/head&gt;\n&lt;link href=""https://fonts.googleapis.com/css?family=Open+Sans:600"" rel=""stylesheet""/&gt;\n&lt;style&gt;\n    html, body {\n    height: 100%;\n    width: 100%;\n    }\n\n    body {\n    background: #F5F6F8;\n    font-size: 16px;\n    font-family: \'Open Sans\', sans-serif;\n    color: #2C3E51;\n    }\n    .main {\n    display: flex;\n    align-items: center;\n    justify-content: center;\n    height: 100vh;\n    }\n    .main &gt; div &gt; div,\n    .main &gt; div &gt; span {\n    text-align: center;\n    }\n    .main span {\n    display: block;\n    padding: 80px 0 170px;\n    font-size: 3rem;\n    }\n    .main .app img {\n    width: 400px;\n    }\n  &lt;/style&gt;\n&lt;script type=""text/javascript""&gt;\n      var fallback_url = ""null"";\n      var store_link = ""itms-apps://itunes.apple.com/GB/app/id1032680895?ls=1&amp;mt=8"";\n      var web_store_link = ""https://itunes.apple.com/GB/app/id1032680895?mt=8"";\n      var loc = window.location;\n      function redirect_to_web_store(loc) {\n        loc.href = web_store_link;\n      }\n      function redirect(loc) {\n        loc.href = store_link;\n        if (fallback_url.startsWith(""http"")) {\n          setTimeout(function() {\n            loc.href = fallback_url;\n          },5000);\n        }\n      }\n  &lt;/script&gt;\n&lt;body onload=""redirect(loc)""&gt;\n&lt;div class=""main""&gt;\n&lt;div class=""workarea""&gt;\n&lt;div class=""logo""&gt;\n&lt;img onclick=""redirect_to_web_store(loc)"" src=""https://cdnappicons.appsflyer.com/app|id1032680895.png"" style=""width:200px;height:200px;border-radius:20px;""/&gt;\n&lt;/div&gt;\n&lt;span&gt;BetBull: Sports Betting &amp;amp; Tips&lt;/span&gt;\n&lt;div class=""app""&gt;\n&lt;img onclick=""redirect_to_web_store(loc)"" src=""https://cdn.appsflyer.com/af-statics/images/rta/app_store_badge.png""/&gt;\n&lt;/div&gt;\n&lt;/div&gt;\n&lt;/div&gt;\n&lt;/body&gt;\n&lt;/html&gt;


Tried:

regex_pattern_to_find_all_links = r'(?:(?:https?|ftp):\/\/)?[\w/\-?=%.]+\.[\w/\-?=%.]+'
soup = BeautifulSoup(resp.read(), 'html.parser')
urls = re.findall(regex_pattern_to_find_all_links, str(soup))


Result:

['https://fonts.googleapis.com/css?family=Open', '//itunes.apple.com/GB/app/id1032680895?ls=1', 'https://itunes.apple.com/GB/app/id1032680895?mt=8', 'window.location', 'loc.href', 'loc.href', 'fallback_url.startsWith', 'loc.href', 'https://cdnappicons.appsflyer.com/app', 'id1032680895.png', 'https://cdn.appsflyer.com/af-statics/images/rta/app_store_badge.png']


As you can see above, I am not sure why regex is matching things which are not even urls.

What I have tried.
Most upvoted and accepted answer here is not able to detect links at all!!
I am not sure what i am doing wrong,
",0,1045,"The problem is with protocol that you made optional and engine isn't forced to match it if it is satisfied with the rest of patterns. Try this instead:

(?:(?:https?|ftp):\/\/|\bwww\.)[^\s""']+


Not bulletproof but much better. It matches strings starting with https? or ftp or those with no protocols but www.

See live demo here
",,
BeautifulSoup unexpected result,https://stackoverflow.com/questions/34370455,BeautifulSoup for extracting text,"I'm not familiar to HTML and trying to extract the main body of a HTML. Firstly I have to filter all elements of HTML but remain text merely.
I receive some unexpected results as below when using method get_text() of BeautifulSoup.

var suffix = device.type === ""pc"" ? "".pc"" : "".mobile"";requirejs.config({
paths: {
    ""F"": ""http://y0.ifengimg.com/base/origin/F-amd-1.2.0.min"",
    ""FM"":  ""http://y0.ifengimg.com/commonpage/1130/F-amd-mobile-1.1.0.min"",
    ""debug"": ""http://y0.ifengimg.com/commonpage/1130/F-amd-mobile-1.1.0.min"",


Of course the text is included, but I don't want the function or other elements of HTML. After checking the code of HTML, it seems that these kinds of functions or scripts are between 2 elements &lt;script&gt; and &lt;/script&gt;

I wonder whether I should use re module or BeautifulSoup can deal with my problem.

Have already done via method extract()...
But received another error. That looks like...
 &lt;img src***=""1""/&gt; 

Still it remains in soup.get_text(). Don't know why it, as a tag, isn't extracted. Surely I can remove it manually, but that seems not elegant for a programmer.
",0,352,"Hmm...looks like we can simply extract them (remove them from the BeautifulSoup Object, your HTML file):

&gt;&gt;&gt; soup = BeautifulSoup('&lt;p&gt;Hello&lt;/p&gt;&lt;script&gt;console.log(""A test!"")&lt;/script&gt;')
&gt;&gt;&gt; soup.get_text()
'Helloconsole.log(""A test!"")'

&gt;&gt;&gt; soup
&lt;p&gt;Hello&lt;/p&gt;&lt;script&gt;console.log(""A test!"")&lt;/script&gt;

&gt;&gt;&gt; soup.find('script')
&lt;script&gt;console.log(""A test!"")&lt;/script&gt;

&gt;&gt;&gt; soup.find('script').extract()
&lt;script&gt;console.log(""A test!"")&lt;/script&gt;

&gt;&gt;&gt; soup
&lt;p&gt;Hello&lt;/p&gt;

&gt;&gt;&gt; soup.get_text()
'Hello'
&gt;&gt;&gt; 


However if you have more script tags in your HTML file, use soup.find_all() instead like this:

for tag in soup.find_all('script'):
    tag.extract()

print(soup.get_text())

",,
BeautifulSoup unexpected result,https://stackoverflow.com/questions/26519253,Attempting to fetch SSLv3 site with urllib2 is raising a httplib.BadStatusLine exception,"I am attempting to read a site into BeautifulSoup and so far every attempt has failed at the point where I attempt to open a secure connection (I initially tried to approach this with Python 3, but as you can see that was also fraught with danger).  Here is my latest attempt involving urllib2 (I haven't found a urllib3 example or have had much success updating this code to urllib3):

import httplib, ssl, urllib2, socket
from bs4 import BeautifulSoup

class HTTPSConnectionV3(httplib.HTTPSConnection):
    def __init__(self, *args, **kwargs):
        httplib.HTTPSConnection.__init__(self, *args, **kwargs)
        def connect(self):
            sock = socket.create_connection((self.host, self.port), self.timeout)
            if self._tunnel_host:
                self.sock = sock
                self._tunnel()
                try:
                    self.sock = ssl.wrap_socket(sock, self.key_file, self.cert_file, ssl_version=ssl.PROTOCOL_SSLv3)
                except ssl.SSLError, e:
                    print(""Trying SSLv3."")
                    self.sock = ssl.wrap_socket(sock, self.key_file, self.cert_file, ssl_version=ssl.PROTOCOL_SSLv23)

class HTTPSHandlerV3(urllib2.HTTPSHandler):
    def https_open(self, req):
        return self.do_open(HTTPSConnectionV3, req)

# install opener
urllib2.install_opener(urllib2.build_opener(HTTPSHandlerV3()))

r = urllib2.urlopen('https://bw6.clpccd.cc.ca.us/clpccd/2014/02/sched_l.htm')

s= r.read()

soup = BeautifulSoup(s)
for t in soup.findAll('h2'):
    print(t)


When I run this code I get the following stack-trace:

Traceback (most recent call last):
  File ""test.py"", line 27, in &lt;module&gt;
    r = urllib2.urlopen('https://bw6.clpccd.cc.ca.us/clpccd/2014/02/sched_l.htm')
  File ""/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/urllib2.py"", line 127, in urlopen
    return _opener.open(url, data, timeout)
  File ""/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/urllib2.py"", line 404, in open
    response = self._open(req, data)
  File ""/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/urllib2.py"", line 422, in _open
    '_open', req)
  File ""/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/urllib2.py"", line 382, in _call_chain
    result = func(*args)
  File ""test.py"", line 22, in https_open
    return self.do_open(HTTPSConnectionV3, req)
  File ""/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/urllib2.py"", line 1187, in do_open
    r = h.getresponse(buffering=True)
  File ""/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/httplib.py"", line 1045, in getresponse
    response.begin()
  File ""/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/httplib.py"", line 409, in begin
    version, status, reason = self._read_status()
  File ""/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/httplib.py"", line 373, in _read_status
    raise BadStatusLine(line)
httplib.BadStatusLine: ''


To make matters trickier here is what I see when I cURL the URL:

$ curl -v https://bw6.clpccd.cc.ca.us/clpccd/2014/02/sched_l.htm
* Adding handle: conn: 0x7ff1f1804000
* Adding handle: send: 0
* Adding handle: recv: 0
* Curl_addHandleToPipeline: length: 1
* - Conn 0 (0x7ff1f1804000) send_pipe: 1, recv_pipe: 0
* About to connect() to bw6.clpccd.cc.ca.us port 443 (#0)
*   Trying 205.155.225.145...
* Connected to bw6.clpccd.cc.ca.us (205.155.225.145) port 443 (#0)
* Server aborted the SSL handshake
* Closing connection 0
curl: (35) Server aborted the SSL handshake


If I force SSLv3, I get the expected output:

$ curl -v -3 https://bw6.clpccd.cc.ca.us/clpccd/2014/02/sched_l.htm
* Adding handle: conn: 0x7ffa50804000
* Adding handle: send: 0
* Adding handle: recv: 0
* Curl_addHandleToPipeline: length: 1
* - Conn 0 (0x7ffa50804000) send_pipe: 1, recv_pipe: 0
* About to connect() to bw6.clpccd.cc.ca.us port 443 (#0)
*   Trying 205.155.225.145...
* Connected to bw6.clpccd.cc.ca.us (205.155.225.145) port 443 (#0)
* SSL 3.0 connection using SSL_RSA_WITH_RC4_128_SHA
* Server certificate: bw6.clpccd.cc.ca.us
* Server certificate: VeriSign Class 3 Secure Server CA - G3
* Server certificate: VeriSign Class 3 Public Primary Certification Authority - G5
* Server certificate: Class 3 Public Primary Certification Authority
&gt; GET /clpccd/2014/02/sched_l.htm HTTP/1.1
&gt; User-Agent: Mozilla/5.0 (compatible; MSIE 9.0; Windows NT 6.1; Trident/5.0)
&gt; Host: bw6.clpccd.cc.ca.us
&gt; Accept: */*
&gt; Referer: 
&gt; 
&lt; HTTP/1.1 200 OK
&lt; Date: Thu, 23 Oct 2014 00:00:11 GMT
* Server Oracle-Application-Server-10g/10.1.3.4.0 Oracle-HTTP-Server is not blacklisted
&lt; Server: Oracle-Application-Server-10g/10.1.3.4.0 Oracle-HTTP-Server
&lt; Last-Modified: Wed, 22 Oct 2014 20:05:42 GMT
&lt; ETag: ""422e-1e72-54480e16""
&lt; Accept-Ranges: bytes
&lt; Content-Length: 7794
&lt; Connection: close
&lt; Content-Type: text/html
&lt; 
&lt;html....&gt;

* Closing connection 0


In case my earlier attempt helps anyone here is what my approach looked like when I was using the Requests library with Python 3 (following their documentation for Example: Specific SSL Version¶)

import ssl
import requests
from requests.adapters import HTTPAdapter
from requests.packages.urllib3.poolmanager import PoolManager

from bs4 import BeautifulSoup

headers = {'User-agent': 'Mozilla/5.0 (Windows NT 5.2; rv:2.0.1) Gecko/20100101 Firefox/4.0.1'}

class Ssl3HttpAdapter(HTTPAdapter):
    """"""""Transport adapter"" that allows us to use SSLv3.""""""
    def init_poolmanager(self, connections, maxsize, block=True):
        self.poolmanager = PoolManager(num_pools=connections,
                                        maxsize=maxsize,
                                        block=block,
                                        ssl_version=ssl.PROTOCOL_SSLv3)

s = requests.session()
s.mount('https://bw6.clpccd.cc.ca.us', Ssl3HttpAdapter())

r = s.get('https://bw6.clpccd.cc.ca.us/clpccd/2014/02/sched_l.htm', headers=headers)

soup = BeautifulSoup(r.text)
for t in soup.findAll('h2'):
    print(t)


Which produced a similar (but more cryptic) stack-trace:

Traceback (most recent call last):
  File ""/usr/local/lib/python3.4/site-packages/requests/packages/urllib3/connectionpool.py"", line 331, in _make_request
    httplib_response = conn.getresponse(buffering=True)
TypeError: getresponse() got an unexpected keyword argument 'buffering'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/usr/local/lib/python3.4/site-packages/requests/packages/urllib3/connectionpool.py"", line 516, in urlopen
    body=body, headers=headers)
  File ""/usr/local/lib/python3.4/site-packages/requests/packages/urllib3/connectionpool.py"", line 333, in _make_request
    httplib_response = conn.getresponse()
  File ""/usr/local/Cellar/python3/3.4.2_1/Frameworks/Python.framework/Versions/3.4/lib/python3.4/http/client.py"", line 1172, in getresponse
    response.begin()
  File ""/usr/local/Cellar/python3/3.4.2_1/Frameworks/Python.framework/Versions/3.4/lib/python3.4/http/client.py"", line 351, in begin
    version, status, reason = self._read_status()
  File ""/usr/local/Cellar/python3/3.4.2_1/Frameworks/Python.framework/Versions/3.4/lib/python3.4/http/client.py"", line 321, in _read_status
    raise BadStatusLine(line)
http.client.BadStatusLine: ''

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/usr/local/lib/python3.4/site-packages/requests/adapters.py"", line 362, in send
    timeout=timeout
  File ""/usr/local/lib/python3.4/site-packages/requests/packages/urllib3/connectionpool.py"", line 559, in urlopen
    _pool=self, _stacktrace=stacktrace)
  File ""/usr/local/lib/python3.4/site-packages/requests/packages/urllib3/util/retry.py"", line 245, in increment
    raise six.reraise(type(error), error, _stacktrace)
  File ""/usr/local/lib/python3.4/site-packages/requests/packages/urllib3/packages/six.py"", line 309, in reraise
    raise value.with_traceback(tb)
  File ""/usr/local/lib/python3.4/site-packages/requests/packages/urllib3/connectionpool.py"", line 516, in urlopen
    body=body, headers=headers)
  File ""/usr/local/lib/python3.4/site-packages/requests/packages/urllib3/connectionpool.py"", line 333, in _make_request
    httplib_response = conn.getresponse()
  File ""/usr/local/Cellar/python3/3.4.2_1/Frameworks/Python.framework/Versions/3.4/lib/python3.4/http/client.py"", line 1172, in getresponse
    response.begin()
  File ""/usr/local/Cellar/python3/3.4.2_1/Frameworks/Python.framework/Versions/3.4/lib/python3.4/http/client.py"", line 351, in begin
    version, status, reason = self._read_status()
  File ""/usr/local/Cellar/python3/3.4.2_1/Frameworks/Python.framework/Versions/3.4/lib/python3.4/http/client.py"", line 321, in _read_status
    raise BadStatusLine(line)
requests.packages.urllib3.exceptions.ProtocolError: ('Connection aborted.', BadStatusLine(""''"",))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""./test.py"", line 23, in &lt;module&gt;
    r = s.get('https://bw6.clpccd.cc.ca.us/clpccd/2014/02/sched_l.htm', headers=headers)
  File ""/usr/local/lib/python3.4/site-packages/requests/sessions.py"", line 469, in get
    return self.request('GET', url, **kwargs)
  File ""/usr/local/lib/python3.4/site-packages/requests/sessions.py"", line 457, in request
    resp = self.send(prep, **send_kwargs)
  File ""/usr/local/lib/python3.4/site-packages/requests/sessions.py"", line 569, in send
    r = adapter.send(request, **kwargs)
  File ""/usr/local/lib/python3.4/site-packages/requests/adapters.py"", line 407, in send
    raise ConnectionError(err, request=request)
requests.exceptions.ConnectionError: ('Connection aborted.', BadStatusLine(""''"",))

",0,936,"In order to make progress I have written this code, though I am not satisfied that this is a Pythonic answer:

import os
import subprocess

from bs4 import BeautifulSoup
FNULL = open(os.devnull, 'w')

html = subprocess.Popen([""curl"", ""-3"", ""https://bw6.clpccd.cc.ca.us/clpccd/2014/02/sched_l.htm"", stdout=subprocess.PIPE, stderr=FNULL).communicate()[0]

soup = BeautifulSoup(html)

for t in soup.findAll('h2'):
    print(t.text)


Basically I'm just calling curl -3 &lt;url&gt; and capturing the output.
",,
BeautifulSoup unexpected issue,https://stackoverflow.com/questions/68641383,Headless chrome and html parser string,"I'm currently using selenium and BeautifulSoup to scrape a website but I'm running into two major issues, first of all, I can't get Chrome to launch in headless mode and it says there are multiple unexpected ends of inputs (photo of said errors). The other problem I have is that I keep getting an error on the line that contains ""html.parser"" saying that a 'str' is not a callable object. Any advice on these issues would be greatly appreciated thank you.
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
import urllib.request
import lxml
import html5lib
import time
from bs4 import BeautifulSoup

#config options
options = Options()
options.headless = True

# Set the URL you want to webscrape from
url = 'https://tokcount.com/?user=mrsam993'

# Connect to the URL
browser = webdriver.Chrome(options=options, executable_path='D:\chromedriver') #chrome_options=options
browser.get(url)

# Parse HTML and save to BeautifulSoup object
soup = BeautifulSoup(browser.page_source(), ""html.parser"")
browser.quit()

# for i in range(10):
links = soup.findAll('span', class_= 'odometer-value')
print(links)

",0,842,"In order to launch to launch chrome in headless mode,and parse the content to html with BeautifulSoup4, this is what you can do:
#Importing necessary packages
from selenium import webdriver 
from selenium.webdriver.chrome.service import Service as ChromeService 
from webdriver_manager.chrome import ChromeDriverManager 

url = 'https://tokcount.com/?user=mrsam993' 

options = webdriver.ChromeOptions()  
options.headless = True 

with webdriver.Chrome(service=ChromeService(ChromeDriverManager().install()), options=options) as driver: #modified 
    driver.get(url)
    
    print(""Page URL: "", driver.current_url)
    print(""Page title: "", driver.title)

    #Get the source page
    html = driver.page_source

ParsedContent = soup(html, 'html.parser')
ParsedContent
 

Make sure you have the following packages: Selenium, webdriver manager.
pip install selenium
pip install webdriver_manager

","As for the headless you need to call this way:
from selenium import webdriver

options = webdriver.ChromeOptions()
...

the page_source is not a method. So you need to remove the brackets:
browser.page_source

",
BeautifulSoup unexpected issue,https://stackoverflow.com/questions/70662022,"How to remove element tags from results, Web Scraping Articles with Python","I've recently been teaching myself python and instead of diving right into courses I decided to think of some script ideas I could research and work through myself. The first I decided to make after seeing something similar referenced in a video was a web scraper to grab articles from sites, such as the New York Times. (I'd like to preface the post by stating that I understand some sites might have varying TOS regarding this and I want to make it clear I'm only doing this to learn the aspects of code and do not have any other motive -- I also have an account to NYT and have not done this on websites where I do not possess an account)
I've gained a bit of an understanding of the python required to perform this as well as began utilizing some BeautifulSoup commands and some of it works well! I've found the specific elements that refer to parts of the article in F12 inspect and am able to successfully grab just the text from these parts.
When it comes to the body of the article, however, the elements are set up in such a way that I'm having troubling grabbing all of the text and not bringing some tags along with it.
Where I'm at so far:
from bs4 import BeautifulSoup
import requests

source = requests.get('https://www.nytimes.com/2022/01/08/us/teachers-unions-covid-schools.html').text

soup = BeautifulSoup(source, 'lxml')

print('----------')
print(f'TITLE: {soup.title.string}')
print('----------')
print(f'H1: {soup.h1.string}')
print(f'H2: {soup.h2.string}')
print(f'H3: {soup.h3.string}')
print('----------')
article_summary = soup.find('p', class_='css-w6ymp8 e1wiw3jv0').text
print(f'Article summary: {article_summary}')
print('----------')
image_summary = soup.find('span', class_='css-16f3y1r e13ogyst0').text
print(f'Image summary: {image_summary}')
print('----------')
authors = soup.find('p', class_= 'css-aknsld e1jsehar1')
author1 = authors.find('span', class_= 'css-1baulvz').text
author2 = authors.find('span', class_= 'css-1baulvz last-byline').text
print(f'Authors: {author1} and {author2}')
print('----------')

for item in soup.select('.StoryBodyCompanionColumn'):
    try:
        para = item.find_all('p')
        print(para)
    except Exception as e:
        print('f')

The output I get from this is:
----------
TITLE: Teachers’ Unions Push for Remote Schooling, Worrying  Democrats - The New York Times
----------
H1: As More Teachers’ Unions Push for Remote Schooling, Parents Worry. So Do Democrats.
H2: The Coronavirus Pandemic: Latest Updates
H3: None
----------
Article summary: Chicago teachers have voted to go remote. Other unions are agitating for change. For Democrats, who promised to keep schools open, the tensions are a distinctly unwelcome development.
----------
Image summary: Alex Brandenburg, an elementary school teacher, protested outside of the Oakland Unified School District headquarters on Friday as part of a sick out.
----------
Authors: Dana Goldstein and Noam Scheiber
----------
[&lt;p class=""css-axufdj evys1bk0""&gt;Few American cities have labor politics as fraught as Chicago’s, where the nation’s third-largest school system shut down this week after teachers’ union members refused to work in person, arguing that classrooms were unsafe amid the Omicron surge.&lt;/p&gt;, &lt;p class=""css-axufdj evys1bk0""&gt;But in a number of other places, the tenuous labor peace that has allowed most schools to operate normally this year is in danger of collapsing.&lt;/p&gt;, &lt;p class=""css-axufdj evys1bk0""&gt;While not yet threatening to walk off the job, unions are back at negotiating tables, pushing in some cases for a return to remote learning. They frequently cite understaffing because of illness, and shortages of rapid tests and medical-grade masks. Some teachers, in a rear-guard action, have staged sick outs.&lt;/p&gt;, &lt;p class=""css-axufdj evys1bk0""&gt;In Milwaukee, schools are remote until Jan. 18, because of staffing issues. But the teachers’ union president, Amy Mizialko, doubts that the situation will significantly improve&lt;span class=""css-8l6xbc evw5hdy0""&gt; &lt;/span&gt;and worries that the school board will resist extending online classes.&lt;/p&gt;]
[&lt;p class=""css-axufdj evys1bk0""&gt;“I anticipate it’ll be a fight,” Ms. Mizialko said.&lt;/p&gt;, &lt;p class=""css-axufdj evys1bk0""&gt;She credited the district for at least delaying in-person schooling to start the year but criticized Democratic officials for placing unrealistic pressure on teachers and schools.&lt;/p&gt;, &lt;p class=""css-axufdj evys1bk0""&gt;“I think that Joe Biden and Miguel Cardona and the newly elected mayor of New York City and Lori Lightfoot — they can all declare that schools will be open,” Ms. Mizialko added, referring to the U.S. education secretary and the mayor of Chicago. “But unless they have hundreds of thousands of people to step in for educators who are sick in this uncontrolled surge, they won’t be.”&lt;/p&gt;]
[&lt;p class=""css-axufdj evys1bk0""&gt;For many parents and teachers, the pandemic has become a slog of anxiety over the risk of infection, child care crises, the tedium of school-through-a-screen and, most of all, chronic instability.&lt;/p&gt;, &lt;p class=""css-axufdj evys1bk0""&gt;And for Democrats, the revival of tensions over remote schooling is a distinctly unwelcome development.&lt;/p&gt;]
[&lt;p class=""css-axufdj evys1bk0""&gt;Because they have close ties to the unions, Democrats are concerned that additional closures like those in Chicago could lead to a possible replay of the party’s recent loss in Virginia’s governor race. &lt;a class=""css-1g7m0tk"" href=""https://dfer.org/press/poll-confirms-education-motivating-issue-for-va-voters-in-2021-election-likely-to-be-major-factor-in-midterms/"" rel=""noopener noreferrer"" target=""_blank"" title=""""&gt;Polling&lt;/a&gt; showed that school disruptions were an important issue for swing voters who broke Republican — particularly suburban white women.&lt;/p&gt;, &lt;p class=""css-axufdj evys1bk0""&gt;“It’s a big deal in most state polling we do,” said Brian Stryker, a partner at the polling firm ALG Research &lt;a class=""css-1g7m0tk"" href=""https://thirdway.imgix.net/pdfs/override/Qualitative-Research-Findings-%E2%80%93-Virginia-Post-Election-Research.pdf"" rel=""noopener noreferrer"" target=""_blank"" title=""""&gt;whose work&lt;/a&gt; in Virginia indicated that school closures hurt Democrats.&lt;/p&gt;, &lt;p class=""css-axufdj evys1bk0""&gt;“Anyone who thinks this is a political problem that stops at the Chicago city line is kidding themselves,” added Mr. Stryker, whose firm polled for President Biden’s 2020 campaign. “This is going to resonate all across Illinois, across the country.”&lt;/p&gt;, &lt;p class=""css-axufdj evys1bk0""&gt;More than one million of the country’s 50 million public school students&lt;span class=""css-8l6xbc evw5hdy0""&gt; &lt;/span&gt;were affected by districtwide shutdowns in the first week of January, many of which were announced abruptly and triggered a wave of frustration among parents.&lt;/p&gt;, &lt;p class=""css-axufdj evys1bk0""&gt;“The kids are not the ones that are seriously ill by and large, but we know kids are the ones suffering from remote learning,” said Dan Kirk, whose son attends Walter Payton College Preparatory High School in Chicago, which was closed amid the district’s standoff this week.&lt;/p&gt;, &lt;p class=""css-axufdj evys1bk0""&gt;Several nonunion charter-school networks and districts temporarily transitioned to remote learning after the holidays. But as has been true throughout the pandemic, most of the temporary districtwide closures — including in Detroit, Cleveland, Milwaukee — are taking place in liberal-leaning areas with powerful unions and a more cautious approach to the coronavirus.&lt;/p&gt;]
[&lt;p class=""css-axufdj evys1bk0""&gt;The unions’ demands echo the ones they have made for nearly two years, despite all that has changed. There are now vaccines and &lt;a class=""css-1g7m0tk"" href=""https://www.cdc.gov/coronavirus/2019-ncov/science/science-briefs/transmission_k_12_schools.html#sars-cov-2"" rel=""noopener noreferrer"" target=""_blank"" title=""""&gt;the reassuring knowledge&lt;/a&gt; that in-school transmission of the virus has been limited.&lt;span class=""css-8l6xbc evw5hdy0""&gt; &lt;/span&gt;The Omicron variant, while highly contagious, appears to cause less severe illness than previous iterations of Covid-19. &lt;/p&gt;]
[&lt;p class=""css-axufdj evys1bk0""&gt;Most district leaders and many educators say it is imperative for schools to remain open. They cite a large body of research showing that closures harm children, &lt;a class=""css-1g7m0tk"" href=""https://www.nytimes.com/2021/07/28/us/covid-schools-at-home-learning-study.html"" title=""""&gt;academically&lt;/a&gt; and &lt;a class=""css-1g7m0tk"" href=""https://www.nytimes.com/2022/01/04/briefing/american-children-crisis-pandemic.html"" title=""""&gt;emotionally&lt;/a&gt;, and widen income and racial disparities. &lt;/p&gt;, &lt;p class=""css-axufdj evys1bk0""&gt;But some local union officials are far warier of packed classrooms. In Newark, schools began 2022 with an unexpected stretch of remote learning, set to end on Jan. 18. John Abeigon, the Newark Teachers Union president, said he was hopeful about the return to buildings but that he remained unsure if every school could operate safely.&lt;span class=""css-8l6xbc evw5hdy0""&gt; &lt;/span&gt;Student vaccination is far from universal, and most parents have not consented to their children taking regular virus tests.&lt;/p&gt;, &lt;p class=""css-axufdj evys1bk0""&gt;Mr. Abeigon said that if tests remain scarce, he might ask for remote learning at specific schools with low vaccination rates and high case counts. He agreed that online learning was a burden to working parents but argued that educators should not be sacrificed for the good of the economy.&lt;/p&gt;, &lt;p class=""css-axufdj evys1bk0""&gt;“I’d see the entire city of Newark unemployed before I allowed one single teacher’s aide to die needlessly,” he said.&lt;/p&gt;, &lt;p class=""css-axufdj evys1bk0""&gt;In Los Angeles, the district has worked closely with the union to keep classrooms open after one of the longest pandemic shutdowns in the country last school year. The vaccination rate for students 12 and older is about 90 percent, with a student vaccine mandate set to &lt;a class=""css-1g7m0tk"" href=""https://www.nytimes.com/2021/12/18/us/los-angeles-vaccine-mandate-delayed.html"" title=""""&gt;kick in this fall&lt;/a&gt;. All students and staff are tested for the virus weekly.&lt;/p&gt;]
[&lt;p class=""css-axufdj evys1bk0""&gt;Still, the president of the local union, Cecily Myart-Cruz, would not rule out pushing for a districtwide return to remote learning in the coming weeks. “You know, I want to be honest — I don’t know,” she said.&lt;/p&gt;, &lt;p class=""css-axufdj evys1bk0""&gt;The tensions are not limited to liberal&lt;span class=""css-8l6xbc evw5hdy0""&gt; &lt;/span&gt;states. In Kentucky, teachers’ unions and at least &lt;a class=""css-1g7m0tk"" href=""https://www.wdrb.com/in-depth/remote-learning-probable-at-some-point-for-jcps-as-covid-19-cases-surge-pollio-says/article_aae24c48-6e40-11ec-846b-5bdbd3d76870.html"" rel=""noopener noreferrer"" target=""_blank"" title=""""&gt;one large school district&lt;/a&gt; have said they need the flexibility to go remote amid escalating infection rates.&lt;/p&gt;, &lt;p class=""css-axufdj evys1bk0""&gt;But the Republican-controlled state legislature has granted no more than 10 days for such instruction districtwide, and unions there worry that may be inadequate. Jeni Ward Bolander, a leader of a statewide union, said that teachers may have to walk off the job.&lt;/p&gt;, &lt;p class=""css-axufdj evys1bk0""&gt;“Frustration is building on teachers,” Ms. Ward Bolander said. “I hate to say we’d walk out at that point, but it’s absolutely possible.”&lt;/p&gt;, &lt;p class=""css-axufdj evys1bk0""&gt;National teachers’ unions continue to call for classrooms to remain open, but local affiliates hold the most power in negotiations over whether individual districts will close schools.&lt;/p&gt;, &lt;p class=""css-axufdj evys1bk0""&gt;And over the last decade, some locals, including those in Los Angeles and Chicago, were taken over by activist leaders whose tactics can be more aggressive than those of national leaders like &lt;a class=""css-1g7m0tk"" href=""https://www.nytimes.com/2021/02/08/us/schools-reopening-teachers-unions.html"" title=""""&gt;Randi Weingarten&lt;/a&gt; of the American Federation of Teachers and &lt;a class=""css-1g7m0tk"" href=""https://www.nytimes.com/2021/12/12/us/politics/teachers-union-becky-pringle.html"" title=""""&gt;Becky Pringle&lt;/a&gt; of the National Education Association, both close allies of President Biden.&lt;/p&gt;]
[&lt;p class=""css-axufdj evys1bk0""&gt;Complicating matters, some local unions face internal pressure from their own members. &lt;a class=""css-1g7m0tk"" href=""https://sanfrancisco.cbslocal.com/2022/01/06/covid-oakland-unified-school-district-warns-potential-teacher-sickout/"" rel=""noopener noreferrer"" target=""_blank"" title=""""&gt;In the Bay Area&lt;/a&gt;, splinter groups of teachers in both Oakland and San Francisco have planned sick outs, and demanded N95 masks, more virus testing and other safety measures.&lt;/p&gt;, &lt;p class=""itemClass""&gt;&lt;strong&gt;The latest Covid data in the U.S.&lt;!-- --&gt; &lt;/strong&gt;&lt;span&gt;As the Omicron surge causes case counts to reach record highs and hospitalizations to surpass the height of the Delta wave, here’s &lt;a href=""https://www.nytimes.com/interactive/2022/01/07/us/covid-data-explained.html?action=click&amp;amp;pgtype=Article&amp;amp;state=default&amp;amp;module=styln-coronavirus&amp;amp;variant=show&amp;amp;region=MAIN_CONTENT_3&amp;amp;block=storyline_levelup_swipe_recirc""&gt;how to think about the data&lt;/a&gt; and what it’s beginning to show about &lt;a href=""https://www.nytimes.com/interactive/2022/01/09/us/omicron-cities-cases-hospitals.html?action=click&amp;amp;pgtype=Article&amp;amp;state=default&amp;amp;module=styln-coronavirus&amp;amp;variant=show&amp;amp;region=MAIN_CONTENT_3&amp;amp;block=storyline_levelup_swipe_recirc""&gt;Omicron’s potential toll across the county&lt;/a&gt;.&lt;/span&gt;&lt;/p&gt;, &lt;p class=""itemClass""&gt;&lt;strong&gt;Around the world.&lt;!-- --&gt; &lt;/strong&gt;&lt;span&gt;In Europe, &lt;a href=""https://www.nytimes.com/live/2022/01/10/world/omicron-covid-testing-vaccines/germany-braces-for-more-protests-as-vaccine-rules-tighten-across-europe?action=click&amp;amp;pgtype=Article&amp;amp;state=default&amp;amp;module=styln-coronavirus&amp;amp;variant=show&amp;amp;region=MAIN_CONTENT_3&amp;amp;block=storyline_levelup_swipe_recirc""&gt;Germany is bracing for major protests&lt;/a&gt; against restrictions after thousands took to the streets in France and Austria, and a tough new vaccine requirement came into force in Italy. In Uganda, &lt;a href=""https://www.nytimes.com/2022/01/10/world/africa/uganda-schools-reopen.html?action=click&amp;amp;pgtype=Article&amp;amp;state=default&amp;amp;module=styln-coronavirus&amp;amp;variant=show&amp;amp;region=MAIN_CONTENT_3&amp;amp;block=storyline_levelup_swipe_recirc""&gt;schools reopened&lt;/a&gt; after the longest pandemic-prompted shutdown in the world.&lt;/span&gt;&lt;/p&gt;, &lt;p class=""itemClass""&gt;&lt;strong&gt;Staying safe.&lt;!-- --&gt; &lt;/strong&gt;&lt;span&gt;Worried about spreading Covid? Keep yourself and others safe by following some basic guidance on &lt;a href=""https://www.nytimes.com/article/tests-covid-omicron-pcr-rapid.html?action=click&amp;amp;pgtype=Article&amp;amp;state=default&amp;amp;module=styln-coronavirus&amp;amp;variant=show&amp;amp;region=MAIN_CONTENT_3&amp;amp;block=storyline_levelup_swipe_recirc""&gt;when to test&lt;/a&gt; and &lt;a href=""https://www.nytimes.com/article/at-home-covid-tests-accuracy.html?action=click&amp;amp;pgtype=Article&amp;amp;state=default&amp;amp;module=styln-coronavirus&amp;amp;variant=show&amp;amp;region=MAIN_CONTENT_3&amp;amp;block=storyline_levelup_swipe_recirc""&gt;how to use at-home virus tests&lt;/a&gt; (if you can find them). Here is what to do &lt;a href=""https://www.nytimes.com/article/testing-positive-covid-omicron-variant.html?action=click&amp;amp;pgtype=Article&amp;amp;state=default&amp;amp;module=styln-coronavirus&amp;amp;variant=show&amp;amp;region=MAIN_CONTENT_3&amp;amp;block=storyline_levelup_swipe_recirc""&gt;if you test positive for the coronavirus&lt;/a&gt;.&lt;/span&gt;&lt;/p&gt;, &lt;p class=""css-axufdj evys1bk0""&gt;Rori Abernethy, a middle-school teacher in San Francisco, organized a sick out there on Thursday. She said the Chicago action had prompted some teachers to ask, “Why isn’t our union doing this?”&lt;/p&gt;, &lt;p class=""css-axufdj evys1bk0""&gt;In Chicago and San Francisco, working-class parents of color disproportionately send their children to the public schools, and they have often supported strict safety measures during the pandemic, including periods of remote learning. And in New York, the nation’s largest school district, schools are operating in person with increased virus testing, with limited dissent from teachers.&lt;/p&gt;, &lt;p class=""css-axufdj evys1bk0""&gt;But the politics become more complicated in suburbs, where union leaders may find themselves at odds with public officials at pains to&lt;span class=""css-8l6xbc evw5hdy0""&gt; &lt;/span&gt;preserve in-person schooling.&lt;/p&gt;, &lt;p class=""css-axufdj evys1bk0""&gt;In Fairfax County, Virginia’s largest district, the superintendent has &lt;a class=""css-1g7m0tk"" href=""https://www.fcps.edu/return-school/return-school-safety/navigating-january-2022-covid-surge"" rel=""noopener noreferrer"" target=""_blank"" title=""""&gt;a plan&lt;/a&gt; for switching individual schools to remote learning in the event of many absent teachers.&lt;/p&gt;, &lt;p class=""css-axufdj evys1bk0""&gt;Kimberly Adams, the president of the&lt;span class=""css-8l6xbc evw5hdy0""&gt; &lt;/span&gt;local education association, said her union may want stricter measures. And she said that districts should be planning for virus surges by distributing devices for potential short bursts of online schooling. &lt;/p&gt;, &lt;p class=""css-axufdj evys1bk0""&gt;But Dan Helmer, a Democratic state delegate whose swing district includes part of Fairfax County, said there was little support among his constituents for a return to online education.&lt;/p&gt;]
[&lt;p class=""css-axufdj evys1bk0""&gt;Deb Andraca, a Democratic state representative in Wisconsin whose district lies just north of Milwaukee, where schools went remote this past week, said that Republicans have targeted her seat and that she expected schools to be a line of attack.&lt;/p&gt;, &lt;p class=""css-axufdj evys1bk0""&gt;“Everyone I know wants schools to stay open,” she said. “But there’s a lot of talk about how teachers’ unions don’t want schools to stay open.”&lt;/p&gt;, &lt;p class=""css-axufdj evys1bk0""&gt;Jim Hobart, a partner at Public Opinion Strategies, a polling firm that counts several Republican senators and governors as clients, said the school closure issue created two advantages for G.O.P. candidates. It has helped narrow their margins among a demographic they’ve traditionally struggled with — white women between their mid-20s and mid-50s — and it has generally undermined Democrats’ claims to competence.&lt;/p&gt;, &lt;p class=""css-axufdj evys1bk0""&gt;“A lot of people — Biden, Mayor Lightfoot in Chicago — have said schools should be open,” Mr. Hobart said. “If they’re not able to prevent schools from choosing to close, that shows a weakness on their part.”&lt;/p&gt;, &lt;p class=""css-axufdj evys1bk0""&gt;Labor officials say that many of their critics are acting in bad faith, exploiting parents’ pandemic-related frustrations to advance longstanding political goals, like discrediting unions and expanding private-school vouchers.&lt;/p&gt;, &lt;p class=""css-axufdj evys1bk0""&gt;Thus far, neither the critiques nor the broader pandemic challenges appear to have significantly hampered unions’ public standing, even according to &lt;a class=""css-1g7m0tk"" href=""https://www.educationnext.org/hunger-for-stability-quells-appetite-for-change-results-2021-education-next-survey-public-opinion-poll/"" rel=""noopener noreferrer"" target=""_blank"" title=""""&gt;polls&lt;/a&gt; conducted by&lt;span class=""css-8l6xbc evw5hdy0""&gt; &lt;/span&gt;researchers skeptical of teachers’ unions.&lt;/p&gt;]
[&lt;p class=""css-axufdj evys1bk0""&gt;And if it turns out that Democratic candidates pay a political price for unions’ assertiveness, local labor officials do not consider it to be among their top concerns.&lt;/p&gt;, &lt;p class=""css-axufdj evys1bk0""&gt;If periods of remote learning this winter hurt the Democratic Party, “that’s a question for the consultants and the brain trusts to figure out,” said Mr. Abeigon, the Newark union president. “But that it’s the right thing to do? There’s no question in my mind.”&lt;/p&gt;, &lt;p class=""css-pncxxs etfikam0""&gt;Holly Secon&lt;!-- --&gt; contributed reporting from San Francisco.&lt;/p&gt;]

I feel so close to having it accomplished, but I've hit a roadblock so I'm hoping someone can tell me how to clean this up a bit.
If I were to do:
for item in soup.select('.StoryBodyCompanionColumn'):
    try:
        para = item.find('p').text
        print(para)
    except Exception as e:
        print('f')

If I use find('p').text or find('p').get_text instead of find_all('p'), it will either give me a failure and print out f or it will give me only the FIRST paragraph within a certain div.(I can upload the results that it spits out when I do the other options but there's a few more I could add and it would greatly lengthen this post even further) (This article has 10 or 11 different divs for the body, each with 2-4 paragraphs in each, and they ALL possess the same class_= tag. This is really where I've been running into an issue. I might be able to splice out the tags with some nifty coding that simply deletes the undesired tags from my results, but I'd rather have smoother code that actually works and I know I'm missing something. Any and all help is very much appreciated as I continue learning!
(Apologies for length of post or if it's an inadequate question -- I'm knew to the site &amp; programming overall so I'm at the stage where I don't know what I don't know yet, so if there is a link that answers my question instead of you taking time to respond that would be fantastic as well, thanks!)
",0,1086,"Select the paragraphs more specific, while adding p to your css selector, than item is the paragraph and you can simply call .text or if there is something to strip -&gt; .text.strip() or .get_text(strip=True):
for item in soup.select('.StoryBodyCompanionColumn p'):
try:
    para = item.text
    print(para)
except Exception as e:
    print('f')


Just select the section tag that holds all p tags and call get_text() to get all human readable text:
soup.select_one('section[name=""articleBody""]').get_text(strip=True)

or iterate over the divs and join() the texts to a single string:
' '.join([item.get_text(strip=True) for item in soup.select('.StoryBodyCompanionColumn p')])

Example
from bs4 import BeautifulSoup
import requests

source = requests.get('https://www.nytimes.com/2022/01/08/us/teachers-unions-covid-schools.html').text

soup = BeautifulSoup(source, 'lxml')

soup.select_one('section[name=""articleBody""]').get_text(strip=True)

",,
BeautifulSoup strange behavior,https://stackoverflow.com/questions/13961831,Why is BeautifulSoup unable to correctly read/parse this RSS (XML) document?,"YCombinator is nice enough to provide an RSS feed and a big RSS feed containing the top items on HackerNews. I am trying to write a python script to access the RSS feed document and then parse out certain pieces of information using BeautifulSoup. However, I am getting some strange behavior when BeautifulSoup tries to get the content of each of the items.

Here are a few sample lines of the RSS feed:

&lt;rss version=""2.0""&gt;
&lt;channel&gt;
&lt;title&gt;Hacker News&lt;/title&gt;&lt;link&gt;http://news.ycombinator.com/&lt;/link&gt;&lt;description&gt;Links for the intellectually curious, ranked by readers.&lt;/description&gt;
&lt;item&gt;
    &lt;title&gt;EFF Patent Project Gets Half-Million-Dollar Boost from Mark Cuban and &amp;#39;Notch&amp;#39;&lt;/title&gt;
    &lt;link&gt;https://www.eff.org/press/releases/eff-patent-project-gets-half-million-dollar-boost-mark-cuban-and-notch&lt;/link&gt;
    &lt;comments&gt;http://news.ycombinator.com/item?id=4944322&lt;/comments&gt;
    &lt;description&gt;&lt;![CDATA[&lt;a href=""http://news.ycombinator.com/item?id=4944322""&gt;Comments&lt;/a&gt;]]&gt;&lt;/description&gt;
&lt;/item&gt;
&lt;item&gt;
    &lt;title&gt;Two Billion Pixel Photo of Mount Everest (can you find the climbers?)&lt;/title&gt;
    &lt;link&gt;https://s3.amazonaws.com/Gigapans/EBC_Pumori_050112_8bit_FLAT/EBC_Pumori_050112_8bit_FLAT.html&lt;/link&gt;
    &lt;comments&gt;http://news.ycombinator.com/item?id=4943361&lt;/comments&gt;
    &lt;description&gt;&lt;![CDATA[&lt;a href=""http://news.ycombinator.com/item?id=4943361""&gt;Comments&lt;/a&gt;]]&gt;&lt;/description&gt;
&lt;/item&gt;
...
&lt;/channel&gt;
&lt;/rss&gt;


Here is the code I have written (in python) to access this feed and print out the title, link, and comments for each item:

import sys
import requests
from bs4 import BeautifulSoup

request = requests.get('http://news.ycombinator.com/rss')
soup = BeautifulSoup(request.text)
items = soup.find_all('item')
for item in items:
    title = item.find('title').text
    link = item.find('link').text
    comments = item.find('comments').text
    print title + ' - ' + link + ' - ' + comments


However, this script is giving output that looks like this:

EFF Patent Project Gets Half-Million-Dollar Boost from Mark Cuban and &amp;#39;Notch&amp;#39; -  - http://news.ycombinator.com/item?id=4944322
Two Billion Pixel Photo of Mount Everest (can you find the climbers?) -  - http://news.ycombinator.com/item?id=4943361
...


As you can see, the middle item, link, is somehow being omitted. That is, the resulting value of link is somehow an empty string. So why is that?

As I dig into what is in soup, I realize that it is somehow choking when it parses the XML. This can be seen by looking at at the first item in items:

&gt;&gt;&gt; print items[0]
&lt;item&gt;&lt;title&gt;EFF Patent Project Gets Half-Million-Dollar Boost from Mark Cuban and &amp;#39;Notch&amp;#39;&lt;/title&gt;&lt;/link&gt;https://www.eff.org/press/releases/eff-patent-project-gets-half-million-dollar-boost-mark-cuban-and-notch&lt;comments&gt;http://news.ycombinator.com/item?id=4944322&lt;/comments&gt;&lt;description&gt;...&lt;/description&gt;&lt;/item&gt;


You'll notice that something screwy is happening with just the link tag. It just gets the close tag and then the text for that tag after it. This is some very strange behavior especially in contrast to title and comments being parsed without a problem.

This seems to be a problem with BeautifulSoup because what is actually read in by requests doesn't have any problems with it. I don't think it is limited to BeautifulSoup though because I tried using xml.etree.ElementTree API as well and the same problem arose (is BeautifulSoup built on this API?).

Does anyone know why this would be happening or how I can still use BeautifulSoup without getting this error?

Note: I was finally able to get what I wanted with xml.dom.minidom, but this doesn't seem like a highly recommended library. I would like to continue using BeautifulSoup if possible.

Update: I am on a Mac with OSX 10.8 using Python 2.7.2 and BS4 4.1.3.

Update 2: I have lxml and it was installed with pip. It is version 3.0.2. As far as libxml, I checked in /usr/lib and the one that shows up is libxml2.2.dylib. Not sure when or how that was installed.
",9,5134,"Wow, great question.  This strikes me as a bug in BeautifulSoup.  The reason that you can't access the link using soup.find_all('item').link is that when you first load the html into BeautifulSoup to begin with, it does something odd to the HTML:

&gt;&gt;&gt; from bs4 import BeautifulSoup as BS
&gt;&gt;&gt; BS(html)
&lt;html&gt;&lt;body&gt;&lt;rss version=""2.0""&gt;
&lt;channel&gt;
&lt;title&gt;Hacker News&lt;/title&gt;&lt;link/&gt;http://news.ycombinator.com/&lt;description&gt;Links
for the intellectually curious, ranked by readers.&lt;/description&gt;
&lt;item&gt;
&lt;title&gt;EFF Patent Project Gets Half-Million-Dollar Boost from Mark Cuban and 'No
tch'&lt;/title&gt;
&lt;link/&gt;https://www.eff.org/press/releases/eff-patent-project-gets-half-million-d
ollar-boost-mark-cuban-and-notch
    &lt;comments&gt;http://news.ycombinator.com/item?id=4944322&lt;/comments&gt;
&lt;description&gt;Comments]]&amp;gt;&lt;/description&gt;
&lt;/item&gt;
&lt;item&gt;
&lt;title&gt;Two Billion Pixel Photo of Mount Everest (can you find the climbers?)&lt;/ti
tle&gt;
&lt;link/&gt;https://s3.amazonaws.com/Gigapans/EBC_Pumori_050112_8bit_FLAT/EBC_Pumori_
050112_8bit_FLAT.html
    &lt;comments&gt;http://news.ycombinator.com/item?id=4943361&lt;/comments&gt;
&lt;description&gt;Comments]]&amp;gt;&lt;/description&gt;
&lt;/item&gt;
...
&lt;/channel&gt;
&lt;/rss&gt;&lt;/body&gt;&lt;/html&gt;


Look carefully--it has actually changed the first &lt;link&gt; tag to &lt;link/&gt; and then removed the &lt;/link&gt; tag.  I'm not sure why it would do this, but without fixing the problem in the BeautifulSoup.BeautifulSoup class initialization, you're not going to be able to use it for now.

Update:

I think your best (albeit hack-y) bet for now is to use the following for link:

&gt;&gt;&gt; soup.find('item').link.next_sibling
u'http://news.ycombinator.com/'

","@Yan Hudon is right .I have solved the problem with soup = BeautifulSoup(request.text, 'xml')
","Actually, the problem seems to be related with the parser you are using.  By default, a HTML one is used.  Try using soup = BeautifulSoup(request.text, 'xml') after installing the lxml module.

It will then use a XML parser instead of a HTML one and it should be all ok.

See http://www.crummy.com/software/BeautifulSoup/bs4/doc/#installing-a-parser for more info
"
BeautifulSoup strange behavior,https://stackoverflow.com/questions/32605694,BeautifulSoup4 find_all() behaves strange after extract() or decompose(),"I have observed a behavior that I found strange when using BeautifulSoup4. 
I have the following XML (filename: fake_product.xml):

&lt;product acronym=""ACRO1""&gt;
&lt;formats&gt;
    &lt;format id=""format1""&gt;
    &lt;/format&gt;
    &lt;format id=""format2""&gt;
    &lt;/format&gt;
    &lt;format id=""format3""&gt;
    &lt;/format&gt;
    &lt;format id=""format4""&gt;
    &lt;/format&gt;
    &lt;format id=""format5""&gt;
    &lt;/format&gt;
    &lt;format id=""format6""&gt;
    &lt;/format&gt;
&lt;/formats&gt;
&lt;/product&gt;


This TestCase fails:

import unittest
from bs4 import BeautifulSoup


class Test(unittest.TestCase):

    def setUp(self):
        with open('fake_product.xml') as f:
            self.soup = BeautifulSoup(f, 'xml')

    def test_product_removal(self):
        output = len(self.soup.find_all('format'))
        expected = 6
        self.assertEqual(output, expected)

        format_to_delete = self.soup.find(id='format2')
        format_to_delete.extract()
        #self.soup = BeautifulSoup(self.soup.prettify(), 'xml')
        output = len(self.soup.find_all('format'))
        expected -= 1
        self.assertEqual(output, expected)


The reason is that the find_all() cannot find all the formats anymore. If I do e.g. print self.soup.prettify() everything looks fine to me.
If I uncomment the commented line in the TestCase and create a new BeautifulSoup object after the extract(), the find_all() seems to work fine again and the TestCase succeeds. 

Can somebody explain this behavior to me?
",3,265,"This is a bug introduced in 4.4.0, see the BeautifulSoup 4 project bug tracker:


  In some situations, it seems calling extract() does not correctly adjust the next_sibling attribute of the previous element. This leaves the extracted element in the descendant generator. When later calling find(...) or find_all(...), the search then terminates at the extracted element, causing results to be missed.


This bug is related as well and contains a potential fix:


  Lines 265, 267, 274, 277 need != changing to is not
  
  Line 290 needs == changing to is


I can confirm that it fixes your specific test.

If you are not comfortable with editing your BeautifulSoup source code, then the work-around is to rebuild the tree as you did, or to downgrade to 4.3.2 until such time that a fix comes out.
",,
BeautifulSoup strange behavior,https://stackoverflow.com/questions/13376417,Strange behaviour with BeautifulSoup and converting HTML entities,"I have a strange problem with converting special characters from HTML. I have a Django project where text is stored HTML-encoded in a MySQL database. This is necessary, because I don't want to lose any formatting of the text.

In a preliminary step I must do operational things on the text like calculating positions, so I need to convert it first and clear it from all HTML-Tags. This is done by BeautifulSoup:

convertedText = str(BeautifulSoup(text.text, convertEntities=BeautifulSoup.HTML_ENTITIES))
convertedText = ''.join(BeautifulSoup(convertedText).findAll(text=True))


By working on my Django-default test-server everything works fine, but when I run it on my production server there are strange behaviors when converting special characters.

An example:

Test server

MySQL-Query gives me: &lt;p&gt;bassverst&amp;auml;rker&lt;/p&gt;

is correctly converted to: bassverstärker

Production server

MySQL-Query gives me: &lt;p&gt;bassverst&amp;auml;rker&lt;/p&gt;

This is is wrongly converted to: bassverst\ucc44rker

Somehow the &amp;auml; is converted into \ucc44 and this results in a wrong character.

My configuration:

Test server:


Django build-in solution (python manage.py runserver)
BeautifulSoup 3.2.1
Python 2.6.5
Ubuntu 2.6.32-43-generic


Production server:


Cherokee 1.2.101
BeautifulSoup 3.2.1
python 2.7.3
Ubuntu 3.2.0-32-generic


Because I don't know at which level the error occurs, I would like to ask if anybody can help me with this. Many thanks in advance.
",1,699,"I found a way to fix this. I didn't know that BeautifulSoup has the builtin method getText(). When converting HTML through:

convertedText = BeautifulSoup(text.text, convertEntities=BeautifulSoup.HTML_ENTITIES).getText()


eveything works fine on both servers. Although this works, it would be interesting to know why both servers are behaving differently when working with the example in the question.

However, thanks to all.
",,
BeautifulSoup strange behavior,https://stackoverflow.com/questions/49338402,BeautifulSoup find_all() Doesn&#39;t Find All Requested Elements,"I am seeing some strange behavior with BeautifulSoup as demonstrated in the example below.

import re
from bs4 import BeautifulSoup
html = """"""&lt;p style='color: red;'&gt;This has a &lt;b&gt;color&lt;/b&gt; of red. Because it likes the color red&lt;/p&gt;
&lt;p class='blue'&gt;This paragraph has a color of blue.&lt;/p&gt;
&lt;p&gt;This paragraph does not have a color.&lt;/p&gt;""""""
soup = BeautifulSoup(html, 'html.parser')
pattern = re.compile('color', flags=re.UNICODE+re.IGNORECASE)
paras = soup.find_all('p', string=pattern)
print(len(paras)) # expected to find 3 paragraphs with word ""color"" in it
  2
print(paras[0].prettify())
  &lt;p class=""blue""&gt;
    This paragraph as a color of blue.
  &lt;/p&gt;

print(paras[1].prettify())
  &lt;p&gt;
    This paragraph does not have a color.
  &lt;/p&gt;


As you can see for some reason the first paragraph of &lt;p style='color: red;'&gt;This has a &lt;b&gt;color&lt;/b&gt; of red. Because it likes the color red&lt;/p&gt; is not being picked up by find_all(...) and I cannot figure out why not.
",1,1083,"The string property expects the tag to contain only text and not tags. If you try printing .string for the first p tag, it'll return None, since, it has tags in it.
Or, to explain it better, the documentation says:

If a tag has only one child, and that child is a NavigableString, the child is made available as .string
If a tag contains more than one thing, then it’s not clear what .string should refer to, so .string is defined to be None

The way to overcome this, is to use a lambda function.
html = """"""&lt;p style='color: red;'&gt;This has a &lt;b&gt;color&lt;/b&gt; of red. Because it likes the color red&lt;/p&gt;
&lt;p class='blue'&gt;This paragraph has a color of blue.&lt;/p&gt;
&lt;p&gt;This paragraph does not have a color.&lt;/p&gt;""""""
soup = BeautifulSoup(html, 'html.parser')

first_p = soup.find('p')
print(first_p)
# &lt;p style=""color: red;""&gt;This has a &lt;b&gt;color&lt;/b&gt; of red. Because it likes the color red&lt;/p&gt;
print(first_p.string)
# None
print(first_p.text)
# This has a color of red. Because it likes the color red

paras = soup.find_all(lambda tag: tag.name == 'p' and 'color' in tag.text.lower())
print(paras)
# [&lt;p style=""color: red;""&gt;This has a &lt;b&gt;color&lt;/b&gt; of red. Because it likes the color red&lt;/p&gt;, &lt;p class=""blue""&gt;This paragraph has a color of blue.&lt;/p&gt;, &lt;p&gt;This paragraph does not have a color.&lt;/p&gt;]

","I haven't actually figured out why specifying the string (or text for older versions of BeautifulSoup) parameter of find_all(...) doesn't give me what I want but, the following does give me a generalized solution.

pattern = re.compile('color', flags=re.UNICODE+re.IGNORECASE)
desired_tags = [tag for tag in soup.find_all('p') if pattern.search(tag.text) is not None]

","If you want to grap the 'p' you can just do:

import re
from bs4 import BeautifulSoup
html = """"""&lt;p style='color: red;'&gt;This has a &lt;b&gt;color&lt;/b&gt; of red. Because it likes the color red&lt;/p&gt;
&lt;p class='blue'&gt;This paragraph has a color of blue.&lt;/p&gt;
&lt;p&gt;This paragraph does not have a color.&lt;/p&gt;""""""
soup = BeautifulSoup(html, 'html.parser')

paras = soup.find_all('p')
for p in paras:
  print (p.get_text())

"
BeautifulSoup strange behavior,https://stackoverflow.com/questions/17137870,Problems with BeautifulSoup and lxml parser,"I noticed a strange behavior when scraping some webpages using BeautifulSoup 4.1.0 and the lxml parser. The built-in html.parser didn't work for the webpage I was trying to scrape and I decided to use a lxml parser. 

The result of the print on my Eclipse console looks good for a fraction of a second and then, it automatically switches to an incomplete, useless and not-so-good-looking output with spaces between all the characters:

                           ! - -   S w i t c h   - - &amp;gt;                

                     / d i v &amp;gt; 


The doc-type of the page is:

&lt;!DOCTYPE html PUBLIC ""-//W3C//DTD XHTML 1.0 Transitional//EN"" ""http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd""&gt;


I was wondering if any of you guys encountered a similar problem and how to fix it. Thanks in advance.
",0,420,"To help people with this problem I wrote ""Extra spaces in documents parsed by Beautiful Soup: the definitive guide"". Basically, the problem is caused by a bug in lxml's HTML parser, triggered by HTML documents that include a  tag that defines the charset as other than UTF-8.

Please let me know if the suggestions in that document don't work for you.
","I used:

newsoup = str(soup).replace("" "", """")


and managed to pull out the info I needed with lxml. Using the html5lib parser also solve the problem and gave me a clean print.
",
BeautifulSoup strange output,https://stackoverflow.com/questions/20906416,BeautifulSoup soup.prettify() gives strange output,"I'm trying to parse a web site and I'm going to use it later in my Django project. To do that, I'm using urllib2 and BeautifulSoup4. However, I couldn't get what I want. The output of BeautifulSoup object is weird. I tried different pages, it worked (output is normal). I thought it is because of the page. Then, when my friend tried to do the same thing, he got normal output. I couldn't manage to figure out problem. 

This is the website I'm going to parse.

This is an example of the weird output after the command ""soup.prettify()"":

t   d       B   G   C   O   L   O   R   =   ""   #   9   9   0   4   0   4   ""       w   i   d   t   h   =   ""   3   ""   &amp;gt;   i   m   g       S   R   C   =   ""   1   p   .   g   i   f   ""       A   L   T       B   O   R   D   E   R   =   ""   0   ""       h   e   i   g   h   t   =   ""   1   ""       w   i   d   t   h   =   ""   3   ""   &amp;gt;   /   t   d   &amp;gt;   \n           /   t   r   &amp;gt;   \n           t   r   &amp;gt;   \n                   t   d       c   o   l   s   p   a   n   =   ""   3   ""       B   G   C   O   L   O   R   =   ""   #   9   9   0   4   0   4   ""       w   i   d   t   h   =   ""   6   0   0   ""       h   e   i   g   h   t   =   ""   3   ""   &amp;gt;   i   m   g       s   r   c   =   ""   1   p   .   g   i   f   ""       w   i   d   t   h   =   ""   6   0   0   ""   \n                   h   e   i   g   h   t   =   ""   1   ""   &amp;gt;   /   t   d   &amp;gt;   \n           /   t   r   &amp;gt;   \n   /   t   a   b   l   e   &amp;gt;   \n   /   c   e   n   t   e   r   &amp;gt;   /   d   i   v   &amp;gt;   \n   \n   p   &amp;gt;   &amp;amp;n   b   s   p   ;   &amp;amp;n   b   s   p   ;   &amp;amp;n   b   s   p   ;   &amp;amp;n   b   s   p   ;   /   p   &amp;gt;   \n   /   b   o   d   y   &amp;gt;   \n   /   h   t   m   l   &amp;gt;\n  &lt;/p&gt;\n &lt;/body&gt;\n&lt;/html&gt;'

",5,44308,"Here is a minimal example that does work for me, including the snippet of html that you have a problem with. It's hard to tell without your code, but my guess is you did something like ' '.join(A.split()) somewhere.

import urllib2, bs4

url = ""http://kafemud.bilkent.edu.tr/monu_tr.html""
req = urllib2.urlopen(url)
raw = req.read()
soup = bs4.BeautifulSoup(raw)

print soup.prettify().encode('utf-8')


Giving:

....
&lt;td bgcolor=""#990404"" width=""3""&gt;
       &lt;img alt="""" border=""0"" src=""1p.gif"" width=""3""/&gt;
      &lt;/td&gt;
      &lt;td bgcolor=""#FFFFFF"" valign=""TOP""&gt;
       &lt;div align=""left""&gt;
        &lt;table align=""left"" border=""0"" cellpadding=""10"" cellspacing=""0"" valign=""TOP"" width=""594""&gt;
         &lt;tr&gt;
          &lt;td align=""left"" valign=""top""&gt;
           &lt;table align=""left"" border=""0"" cellpadding=""0"" cellspacing=""0"" class=""icerik"" width=""574""&gt;
....

","This looks like you have your XML coming in with an encoding that beautifulsoup isn't expecting. My guess is that your XML is in UTF-16 and beautifulsoup is reading it as UTF-8. Python offers the .encode and .decode functions for switching between different encodings. Something like 

myXmlStr.encode(""utf-16"").decode(""utf-8"")


Would probably solve your problem if the issue is your incoming XML encoding. I'm new to beautiful soup myself, but a quick google suggests that if the  problem is the encoding of the output, prettify accepts an encoding parameter:

soup.prettify(""utf-16"")


Without more information I can't give you a clearer answer - but hopefully this points you in a helpful direction.
","Possibly you and your friend use different parsers. BeautifulSoup will use the parser it considers ""best"", and thus prefer lxml for speed reasons (if installed).  If using recent versions of Python (and the last version of the included parser), there are cases which are handled better by BeautifulSoup(text, 'html.parser'); this is the case e.g. when there are unmasked &lt; characters (instead of &amp;lt;) in text content.
"
BeautifulSoup strange output,https://stackoverflow.com/questions/39077890,Unable to crawl some href in a webpage using python and beautifulsoup,"I am currently crawling a web page using Python 3.4 and bs4 in order to collect the match results played by Serbia in Rio2016. So the url here contains links to all the match results she played, for example this.

Then I found that the link is located in the html source like this:

&lt;a href=""/en/volleyball/women/7168-serbia-italy/post"" ng-href=""/en/volleyball/women/7168-serbia-italy/post""&gt;
    &lt;span class=""score ng-binding""&gt;3 - 0&lt;/span&gt;
&lt;/a&gt;


But after several trials, this href=""/en/volleyball/women/7168-serbia-italy/post"" never show up. Then I tried to run the following code to get all the href from the url:

from bs4 import BeautifulSoup
import requests

Countryr = requests.get('http://rio2016.fivb.com/en/volleyball/women/teams/srb-serbia#wcbody_0_wcgridpadgridpad1_1_wcmenucontent_3_Schedule')
countrySoup = BeautifulSoup(Countryr.text)

for link in countrySoup.find_all('a'):
    print(link.get('href'))


Then a strange thing happened. The href=""/en/volleyball/women/7168-serbia-italy/post"" is not included in the output at all.

I found that this href is located in one of the tab pages href=""#scheduldedOver""  in side this url, and it is controlled by the following HTML code:

&lt;nav class=""tabnav""&gt;
    &lt;a href=""#schedulded"" ng-class=""{selected: chosenStatus == 'Pre' }"" ng-click=""setStatus('Pre')"" ng-href=""#schedulded""&gt;Scheduled&lt;/a&gt;
    &lt;a href=""#scheduldedLive"" ng-class=""{selected: chosenStatus == 'Live' }"" ng-click=""setStatus('Live')"" ng-href=""#scheduldedLive""&gt;Live&lt;/a&gt;
    &lt;a href=""#scheduldedOver"" class=""selected"" ng-class=""{selected: chosenStatus == 'Over' }"" ng-click=""setStatus('Over')"" ng-href=""#scheduldedOver""&gt;Complete&lt;/a&gt;
&lt;/nav&gt;


Then how should I get the href using BeautifulSoup inside a tab page?
",3,1405,"Thanks for all your help now I can get the correct url. It is a good learning experience for me. Thanks a lot :)

from bs4 import BeautifulSoup
import requests

Countryr = requests.get('http://rio2016.fivb.com/en/volleyball/women/teams/srb-serbia#wcbody_0_wcgridpadgridpad1_1_wcmenucontent_3_Schedule')
countrySoup = BeautifulSoup(Countryr.text)

for link in countrySoup.find_all('div', {'id': 'AngularPanel'}):
    linkUrl = link.get('data-serviceteammatches')

json = requests.get('http://rio2016.fivb.com' + linkUrl).json()

for item in json:
    print(item.get('Url'))


output:

/en/volleyball/women/7168-serbia-italy/post
/en/volleyball/women/7172-serbia-puerto rico/post
/en/volleyball/women/7177-usa-serbia/post
/en/volleyball/women/7181-china-serbia/post
/en/volleyball/women/7187-serbia-netherlands/post
/en/volleyball/women/7195-russia-serbia/post
/en/volleyball/women/7198-serbia-usa/post
/en/volleyball/women/7200-china-serbia/post

","The data is created dynamically, if you look at the actual source you can see Angularjs templating.

You can still get all the info in json format by mimicking an ajax call, in the source yuuuuou can also see a div like:

&lt;div id=""AngularPanel"" class=""main-wrapper"" ng-app=""fivb""
data-servicematchcenterbar=""/en/api/volley/matches/341/en/user/lives""
data-serviceteammatches=""/en/api/volley/matches/WOG2016/en/user/team/3017""
data-servicelabels=""/en/api/labels/Volley/en"" 
data-servicelive=""/en/api/volley/matches/WOG2016/en/user/live/""&gt;


Using the data-servicematchcenterbar href will give you all the info:

from bs4 import BeautifulSoup
import requests
from urlparse import urljoin

r = requests.get('http://rio2016.fivb.com/en/volleyball/women/teams/srb-serbia#wcbody_0_wcgridpadgridpad1_1_wcmenucontent_3_Schedule')
soup = BeautifulSoup(r.content)

base = ""http://rio2016.fivb.com/""

json = requests.get(urljoin(base, soup.select_one(""#AngularPanel"")[""data-serviceteammatches""])).json()


In  json you will see output like:

{""Id"": 7168, ""MatchNumber"": ""006"", ""TournamentCode"": ""WOG2016"", ""TournamentName"": ""Women's Olympic Games 2016"",
        ""TournamentGroupName"": """", ""Gender"": """", ""LocalDateTime"": ""2016-08-06T22:35:00"",
        ""UtcDateTime"": ""2016-08-07T01:35:00+00:00"", ""CalculatedMatchDate"": ""2016-08-07T03:35:00+02:00"",
        ""CalculatedMatchDateType"": ""user"", ""LocalDateTimeText"": ""August 06 2016"",
        ""Pool"": {""Code"": ""B"", ""Name"": ""Pool B"", ""Url"": ""/en/volleyball/women/results and ranking/round1#anchorB""},
        ""Round"": 68,
        ""Location"": {""Arena"": ""Maracanãzinho"", ""City"": ""Maracanãzinho"", ""CityUrl"": """", ""Country"": ""Brazil""},
        ""TeamA"": {""Code"": ""SRB"", ""Name"": ""Serbia"", ""Url"": ""/en/volleyball/women/teams/srb-serbia"",
                  ""FlagUrl"": ""/~/media/flags/flag_SRB.png?h=60&amp;w=60""},
        ""TeamB"": {""Code"": ""ITA"", ""Name"": ""Italy"", ""Url"": ""/en/volleyball/women/teams/ita-italy"",
                  ""FlagUrl"": ""/~/media/flags/flag_ITA.png?h=60&amp;w=60""},
        ""Url"": ""/en/volleyball/women/7168-serbia-italy/post"", ""TicketUrl"": """", ""Status"": ""Over"", ""MatchPointsA"": 3,
        ""MatchPointsB"": 0, ""Sets"": [{""Number"": 1, ""PointsA"": 27, ""PointsB"": 25, ""Hours"": 0, ""Minutes"": ""28""},
                                    {""Number"": 2, ""PointsA"": 25, ""PointsB"": 20, ""Hours"": 0, ""Minutes"": ""25""},
                                    {""Number"": 3, ""PointsA"": 25, ""PointsB"": 23, ""Hours"": 0, ""Minutes"": ""27""}],
        ""PoolRoundName"": ""Preliminary Round"", ""DayInfo"": ""Weekend Day"",
        ""WeekInfo"": {""Number"": 31, ""Start"": 7, ""End"": 13}, ""LiveStreamUri"": """"},


You can parse whatever you need from those.
",
BeautifulSoup strange output,https://stackoverflow.com/questions/73010716,BeautifulSoup find_all returns only first 50 tags,"I am trying to parse html data from this website with BeautifulSoup, but strangely enough, it returns only the first 50 tags that it finds.
When I search the html code through Google DevTools I get 115 matches for the class name that I am looking for.
url='https://wolt.com/az/aze/baku/restaurant/mcdonalds-nnrimanov-ms'
html=urllib.request.urlopen(url).read()
soup = BeautifulSoup(html,'html.parser')

modules=soup.find_all('div',attrs={'class':'MenuItem-module__content___mNrbB'})
print(len(modules))

Output:
&gt;&gt;&gt; 50

I have tried parsing other pages on this website, and still get only 50 results back.
I have also used the answer from Beautiful Soup findAll doesn't find them all to tweak my code.
Any help is appreciated!
",2,706,"The data is loaded from external URL in Json format. To get all 115 items you can use next example:
import json
import requests

url = ""https://restaurant-api.wolt.com/v4/venues/slug/mcdonalds-nnrimanov-ms/menu?unit_prices=true&amp;show_weighted_items=true""

data = requests.get(url).json()

# ucomment this to print all data:
# print(json.dumps(data, indent=4))

for i, item in enumerate(data[""items""], 1):
    print(""{:&lt;4} {}"".format(i, item[""name""]))

Prints:
...

106  Latte (200 ml)
107  Milk Chocolate (300 ml)
108  Milk Chocolate (200 ml)
109  Ketchup
110  Mayonnaise
111  Barbecue Sauce
112  Sweet Sauce
113  Mustard Sauce
114  1000 Island Sauce
115  Sweet Chilli

","You can't use BeautifulSoup for this task. You don't even need to grab the content of this page.
The link is https://wolt.com/en/aze/baku/restaurant/mcdonalds-nnrimanov-ms and the real link is https://restaurant-api.wolt.com/v4/venues/slug/mcdonalds-nnrimanov-ms/menu?unit_prices=true&amp;show_weighted_items=true
So in fact you can use this piece of code for grabbing the json file.
import urllib
import json
url='https://wolt.com/az/aze/baku/restaurant/mcdonalds-nnrimanov-ms'
link='https://restaurant-api.wolt.com/v4/venues/slug/'+url.replace('/', ' ').strip().split(' ')[-1]+'/menu?unit_prices=true&amp;show_weighted_items=true'
jsn=urllib.request.urlopen(link).read()
dic=json.loads(jsn)
print(dic['items'])
'''
Useful properties:
'alcohol_percentage': percentage of alcohol
'baseprice': price with no discount
'description' and 'name': as it says
'no_contact_delivery_allowed': something special in COVID times
'times': available times(a list)
each element in 'times': 'available_days_of_week': a list, as it means
'''

","When you first access the url, the page will load the first 50 elements with class MenuItem-module__content___mNrbB by default. The rest of them, along with some other content is being loaded dynamically by some scripts which execute in page. Requests is not able to execute Javascript (and BeautifulSoup is just the html parser).
You can test this by disabling Javascript in browser, and reloading the page: you will only find 50 elements in Dev Tools, corresponding to class MenuItem-module__content___mNrbB.
If you want to get all 119 elements corresponding to that class, you will need to inspect the network calls made by javascript - you can see those calls in Network tab, Dev tools, and you can try and scrape those endpoints (this was already clarified in another response).
I hope this is addressing your concerns as to why BeautifulSoup can only find 50 elements, and you understand that particular url/page behaviour now.
"
BeautifulSoup strange output,https://stackoverflow.com/questions/10402454,BeautifulSoup - Blank line in the output &#39;cause a strange useless &lt;tr&gt;&lt;/tr&gt;,"Little problem for you :-)

I am using BeautifulSoup to parse the content of a table in a HTML page. The problem is that between every line (CSV/EXCEL) of my output file, it pulls a blank line... 
This is an exemple of the HTML Table (which is very big)

&lt;tr&gt;&lt;td class=""normaltext"" valign=""TOP""&gt;Tesco - United Kingdom&amp;nbsp;&amp;nbsp;&lt;/td&gt;
&lt;td class=""normaltext""  valign=""TOP""&gt;CO&lt;/td&gt;
&lt;td class=""normaltext""  valign=""TOP""&gt;Unknown&amp;nbsp;&amp;nbsp;&lt;/td&gt;
&lt;td class=""normaltext""  align=""center"" valign=""top""&gt;lol&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td colspan=""5""&gt;&lt;hr&gt;&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td class=""normaltext"" valign=""TOP""&gt;Tesco - United Kingdom&amp;nbsp;&amp;nbsp;&lt;/td&gt;
&lt;td class=""normaltext""  valign=""TOP""&gt;CO&lt;/td&gt;
&lt;td class=""normaltext""  valign=""TOP""&gt;Unknown&amp;nbsp;&amp;nbsp;&lt;/td&gt;
&lt;td class=""normaltext""  align=""center"" valign=""top""&gt;lol&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td colspan=""5""&gt;&lt;hr&gt;&lt;/td&gt;&lt;/tr&gt;


Every &lt; tr&gt; you have this : &lt; tr&gt;&lt; td colspan=""5""&gt;&lt; hr&gt;&lt; /td&gt;&lt; /tr&gt; So It put a blank line in my CSV/Excel Sheet. I want to pull in the Excel Sheet all the information but without a blank line between every line ... 

Here is the script I use : 

rows = tableau[3].findAll('tr')
for tr in rows:
    cols = tr.findAll('td', attrs={'class' : 'normaltext'})
    y = 0
    x = x + 1
    for td in cols:
        texte_bu = td.text
        texte_bu = texte_bu.encode('utf-8')
        texte_bu = texte_bu.strip()
        ws.write(x,y,td.text)
        y = y + 1


BIG THANKS to the one who can give me the tip to get rib of this * blank useless line between every line of my output file :)
",2,1637,"The solution: when you find an empty row, then skip the loop and read in the next row. This avoids your writing an empty line to the workbook. :)

This is a working simulation. I have added a cosmetic adjustment in order to also avoid the top empty row being sent out. Hope this rids you of empty-line peskiness :)

from BeautifulSoup import BeautifulSoup
import xlwt

text = '''&lt;table&gt;&lt;tr&gt;&lt;td class=""normaltext"" valign=""TOP""&gt;Tesco - United Kingdom&amp;nbsp;&amp;nbsp;&lt;/td&gt;
&lt;td class=""normaltext""  valign=""TOP""&gt;CO&lt;/td&gt;
&lt;td class=""normaltext""  valign=""TOP""&gt;Unknown&amp;nbsp;&amp;nbsp;&lt;/td&gt;
&lt;td class=""normaltext""  align=""center"" valign=""top""&gt;BULATS&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td colspan=""5""&gt;&lt;hr&gt;&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td class=""normaltext"" valign=""TOP""&gt;Tesco - United Kingdom&amp;nbsp;&amp;nbsp;&lt;/td&gt;
&lt;td class=""normaltext""  valign=""TOP""&gt;CO&lt;/td&gt;
&lt;td class=""normaltext""  valign=""TOP""&gt;Unknown&amp;nbsp;&amp;nbsp;&lt;/td&gt;
&lt;td class=""normaltext""  align=""center"" valign=""top""&gt;BULATS&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td colspan=""5""&gt;&lt;hr&gt;&lt;/td&gt;&lt;/tr&gt;&lt;table&gt;'''

wb = xlwt.Workbook()
ws = wb.add_sheet('a test sheet')

soup = BeautifulSoup(text)
table = soup.find('table')
rows = table.findAll('tr')
x = 0
for tr in rows:
    cols = tr.findAll('td', attrs={'class' : 'normaltext'})
    if not cols: 
        # when we hit an empty row, we should not print anything to the workbook
        continue
    y = 0
    for td in cols:
        texte_bu = td.text
        texte_bu = texte_bu.encode('utf-8')
        texte_bu = texte_bu.strip()
        ws.write(x, y, td.text)
        print(x, y, td.text)
        y = y + 1
    # update the row pointer AFTER a row has been printed
    # this avoids the blank row at the top of your table
    x = x + 1

wb.save('example.xls')

",,
BeautifulSoup strange output,https://stackoverflow.com/questions/6488706,How to translate/convert unicode escaped &lt; and &gt; in a read HTML doc?,"When I read some (but not all) HTML files in python using a urllib2 opener, on some files I'm getting text filled with lots of backslashes and the unicode 003c strings.  I'm sending this text into BeautifulSoup and am having trouble finding what I'm looking for with findAll(), and I'm now thinking it's due to all these unicode strings.

What's going on with this, and how do I get rid of it?

Approaches like soup.prettify() have no effect.

Here's some example code (this comes from a Facebook profile)

\\u003cdiv class=\\""pas status fcg\\""&gt;Loading...\\u003c\\/div&gt;
\\u003c\\/div&gt;\\u003cdiv class=\\""uiTypeaheadView fbChatBuddyListTypeaheadView dark hidden_elem\\"" id=\\""u971289_14\\""&gt;\\u003c\\/div&gt;
\\u003c\\/div&gt;\\u003c\\/div&gt;\\u003cdiv class=\\""fbNubFlyoutFooter\\""&gt;
\\u003cdiv class=\\""uiTypeahead uiClearableTypeahead fbChatTypeahead\\"" id=\\""u971289_15\\""&gt;
\\u003cdiv class=\\""wrap\\""&gt;\\u003clabel class=\\""clear uiCloseButton\\"" for=\\""u971291_21\\""&gt;


This same HTML page looks fine and normal in a 'view source' window.

EDIT: Here's the code that's producing that text.  What's strange is that I don't get this kind of output from other HTML pages.  Note that I've replaced the username and password with USERNAME and PASSWORD for here.  You could try this on your own FB profile if you replace those two.

fbusername = ""USERNAME@gmail.com""
fbpassword = ""PASSWORD""
cookiefile = ""facebook.cookies""

cj = cookielib.MozillaCookieJar(cookiefile)
if os.access(cookiefile, os.F_OK):
    cf.load()

opener = urllib2.build_opener(
    urllib2.HTTPRedirectHandler(),
    urllib2.HTTPHandler(debuglevel=0),
    urllib2.HTTPSHandler(debuglevel=0),
    urllib2.HTTPCookieProcessor(cj)
)

opener.addheaders = [('User-agent','Mozilla/5.0 (Macintosh; U; Intel Mac OS X 10_6_7; en-us) AppleWebKit/533.21.1 (KHTML, like Gecko) Version/5.0.5 Safari/533.21.1'),('Referer','http://www.facebook.com/')]

def facebooklogin():
    logindata = urllib.urlencode({
        'email' : fbusername,
        'pass' : fbpassword,
    })

    response = opener.open(""https://login.facebook.com/login.php"",logindata)
    return ''.join(response.readlines())


print ""Logging in to Facebook...\n""
facebooklogin()
facebooklogin()
print ""Successful.\n""

fetchURL = 'http://www.facebook.com/USERNAME?ref=profile&amp;v=info'

f = opener.open(fetchURL)
fba = f.read()
f.close()
soup = BeautifulSoup(fba)
print soup

",2,1575,"The u"""""" construct is for Python 2. You omit the u for Python 3.

&gt;&gt;&gt; a=u""""""\\u003cdiv class=\\""pas status fcg\\""&gt;Loading...\\u003c\\/div&gt;
... \\u003c\\/div&gt;\\u003cdiv class=\\""uiTypeaheadView fbChatBuddyListTypeaheadView dark hidden_elem\\"" id=\\""u971289_14\\""&gt;\\u003c\\/div&gt;
... \\u003c\\/div&gt;\\u003c\\/div&gt;\\u003cdiv class=\\""fbNubFlyoutFooter\\""&gt;
... \\u003cdiv class=\\""uiTypeahead uiClearableTypeahead fbChatTypeahead\\"" id=\\""u971289_15\\""&gt;
... \\u003cdiv class=\\""wrap\\""&gt;\\u003clabel class=\\""clear uiCloseButton\\"" for=\\""u971291_21\\""&gt;
... """"""
&gt;&gt;&gt; print(a.decode('unicode_escape')).replace('\\/', '/')
&lt;div class=""pas status fcg""&gt;Loading...&lt;\/div&gt;
&lt;\/div&gt;&lt;div class=""uiTypeaheadView fbChatBuddyListTypeaheadView dark hidden_elem"" id=""u971289_14""&gt;&lt;\/div&gt;
&lt;\/div&gt;&lt;\/div&gt;&lt;div class=""fbNubFlyoutFooter""&gt;
&lt;div class=""uiTypeahead uiClearableTypeahead fbChatTypeahead"" id=""u971289_15""&gt;
&lt;div class=""wrap""&gt;&lt;label class=""clear uiCloseButton"" for=""u971291_21""&gt;


I hope this helps. If not, please improve the information you give in your question.

EDIT: suggested answer now changes \/ to / too.
",,
BeautifulSoup strange output,https://stackoverflow.com/questions/61062341,Scraping coronavirus-related data from a page which is dynamically loaded from a Tableau canvas (I think...),"I will be more than happy to find out this question is a duplicate, but if so - I can't find that Q&amp;A.

There is this mysterious page from the New York State Department of Health containing ""Fatalities by County and Age Group"". As the title implies, it contains two tables (""By County""/""By Age Group""). 

For some strange reason, the data on this page is super-secured. It can't be selected, the page can't be saved and it can't be printed. The data isn't on the page source. I also tried (and failed) to inspect xhr calls for the data.

Obviously, requests and beautifulsoup can't handle it. I tried the usual Selenium incantations (so, unless I'm told otherwise, I won't clutter this question with ""what I tried"" snippets). 

Desire output: the data from those two tables, in any conceivable format.

The only thing I can think of is to take a screenshot and try to ocr the image...

I don't know if it's Selenium, Tableau, the NYS Dep't of Health or just me, but it's time to call in the heavy artillery...
",1,952,"Let me explain for you the scenario:


Website is generating a session id behind that parameter X-Session-Id which is dynamically generated once you visit the main page page index. So i called it via GET request and I've picked it up from the headers response.
I've figured out an POST request which is automatically generated before you hit your desired url which is actually using the session id which we collected before. here is it https://covid19tracker.health.ny.gov/vizql/w/NYS-COVID19-Tracker/v/NYSDOHCOVID-19Tracker-Fatalities/clear/sessions/{session id}
Now we can call your target which is https://covid19tracker.health.ny.gov/views/NYS-COVID19-Tracker/NYSDOHCOVID-19Tracker-Fatalities?%3Aembed=yes&amp;%3Atoolbar=no&amp;%3Atabs=n. 
Now I noticed another XHR request to the back-end API. But before we do the call, We will parse the HTML content for picking up the time object which is responsible on generating the data freshly from the API so we will get an instant data (consider it like a live chat actually). in our case it's behind lastUpdatedAt inside the HTML
I noticed as well that we will need to pickup the recent X-Session-Id generated from our previous POST request.
Now we will make the call using our picked up session to https://covid19tracker.health.ny.gov/vizql/w/NYS-COVID19-Tracker/v/NYSDOHCOVID-19Tracker-Fatalities/bootstrapSession/sessions/{session}


Now we have received the full response. you can parse it or do whatever you want.

import requests
import re


data = {
    'worksheetPortSize': '{""w"":1536,""h"":1250}',
    'dashboardPortSize': '{""w"":1536,""h"":1250}',
    'clientDimension': '{""w"":1536,""h"":349}',
    'renderMapsClientSide': 'true',
    'isBrowserRendering': 'true',
    'browserRenderingThreshold': '100',
    'formatDataValueLocally': 'false',
    'clientNum': '',
    'navType': 'Reload',
    'navSrc': 'Top',
    'devicePixelRatio': '2.5',
    'clientRenderPixelLimit': '25000000',
    'allowAutogenWorksheetPhoneLayouts': 'true',
    'sheet_id': 'NYSDOH%20COVID-19%20Tracker%20-%20Fatalities',
    'showParams': '{""checkpoint"":false,""refresh"":false,""refreshUnmodified"":false}',
    'filterTileSize': '200',
    'locale': 'en_US',
    'language': 'en',
    'verboseMode': 'false',
    ':session_feature_flags': '{}',
    'keychain_version': '1'
}


def main(url):
    with requests.Session() as req:
        r = req.post(url)
        sid = r.headers.get(""X-Session-Id"")

        r = req.post(
            f""https://covid19tracker.health.ny.gov/vizql/w/NYS-COVID19-Tracker/v/NYSDOHCOVID-19Tracker-Fatalities/clear/sessions/{sid}"")

        r = req.get(
            ""https://covid19tracker.health.ny.gov/views/NYS-COVID19-Tracker/NYSDOHCOVID-19Tracker-Fatalities?%3Aembed=yes&amp;%3Atoolbar=no&amp;%3Atabs=n"")

        match = re.search(r""lastUpdatedAt.+?(\d+),"", r.text).group(1)

        time = '{""featureFlags"":""{\""MetricsAuthoringBeta\"":false}"",""isAuthoring"":false,""isOfflineMode"":false,""lastUpdatedAt"":xxx,""workbookId"":9}'.replace(
            'xxx', f""{match}"")

        data['stickySessionKey'] = time
        nid = r.headers.get(""X-Session-Id"")

        r = req.post(
            f""https://covid19tracker.health.ny.gov/vizql/w/NYS-COVID19-Tracker/v/NYSDOHCOVID-19Tracker-Fatalities/bootstrapSession/sessions/{nid}"", data=data)

        print(r.text)


main(""https://covid19tracker.health.ny.gov"")

","I've made a tableau scraper library to extract the data from Tableau worksheets
You can get all data in pandas dataframe for each worksheet with the following code:
from tableauscraper import TableauScraper as TS

url = ""https://covid19tracker.health.ny.gov/views/NYS-COVID19-Tracker/NYSDOHCOVID-19Tracker-Fatalities""

ts = TS()
ts.loads(url)
dashboard = ts.getWorkbook()

for t in dashboard.worksheets:
    # show worksheet name
    print(f""WORKSHEET NAME : {t.name}"")
    # show dataframe for this worksheet
    print(t.data)

Try this on repl.it
",
BeautifulSoup strange output,https://stackoverflow.com/questions/43148784,Local HTML File Scraping Urllib and BeautifulSoup,"I am very new to python and have been working from scratch on the following code for two weeks to scrape local files.  Probably nearly a hundred hours learning as much as I can about Python, versionality, importing packages such as lxml, bs4, requests, urllib, os, glob and more. 

I'm hopelessly stuck on the first part on getting 12,000 HTML files with strange names all in one directory to load and parse with BeautifulSoup.  I want to get all this data into a csv file or just to output so I can copy it to a file using clipboard.  

import bs4
from urllib.request import urlopen as uReq
from bs4 import BeautifulSoup as soup

#THIS LOCAL FILE WORKS PERFECTLY. I HAVE 12,000 HTML FILES IN THIS DIRECTORY TO PROCESS.  HOW?
#my_url = 'file://127.0.0.1/C:\\My Web Sites\\BioFachURLS\\www.organic-bio.com\\en\\company\\1-SUNRISE-FARMS.html'
my_url = 'http://www.organic-bio.com/en/company/23694-MARTHOMI-ALLERGY-FREE-FOODS-GMBH'

uClient = uReq(my_url)
page_html = uClient.read()
uClient.close()

# html parsing
page_soup = soup(page_html, ""html.parser"")

# grabs each field
contactname = page_soup.findAll(""td"", {""itemprop"": ""name""})
contactstreetaddress = page_soup.findAll(""td"", {""itemprop"": ""streetAddress""})
contactpostalcode = page_soup.findAll(""td"", {""itemprop"": ""postalCode""})
contactaddressregion = page_soup.findAll(""td"", {""itemprop"": ""addressRegion""})
contactaddresscountry = page_soup.findAll(""td"", {""itemprop"": ""addressCountry""})
contactfax = page_soup.findAll(""td"", {""itemprop"": ""faxNumber""})
contactemail = page_soup.findAll(""td"", {""itemprop"": ""email""})
contactphone = page_soup.findAll(""td"", {""itemprop"": ""telephone""})
contacturl = page_soup.findAll(""a"", {""itemprop"": ""url""})

#Outputs as text without tags
Company = contactname[0].text
Address = contactstreetaddress[0].text
Zip = contactpostalcode[0].text
Region = contactaddressregion[0].text
Country = contactaddresscountry[0].text
Fax = contactfax[0].text
Email = contactemail[0].text
Phone = contactphone[0].text
URL = contacturl[0].text

#Prints with comma delimiters

print(Company + ', ' + Address + ', ' + Zip + ', ' + Region + ', ' + Country + ', ' + Fax + ', ' + Email + ', ' + URL)

",1,9191,"I have worked with running through folders with bunches of file before, so I could suggest a little help.
We will start with for loop to files from folder
import os
from bs4 import BeautifulSoup as page_soup

phone = [] # A list to store all the phone
path = 'yourpath' # This is your folder name which stores all your html 
#be careful that you might need to put a full path such as C:\Users\Niche\Desktop\htmlfolder 
for filename in os.listdir(path): #Read files from your path

    #Here we are trying to find the full pathname
    for x in filename: #We will have A-H stored as path
        subpath = os.path.join(path, filename) 
        for filename in os.listdir(subpath):
        #Getting the full path of a particular html file
            fullpath = os.path.join(subpath, filename)
            #If we have html tag, then read it
            if fullpath.endswith('.html'): continue
            #Then we will run beautifulsoup to extract the contents
            soup = page_soup(open(fullpath), 'html.parser')
            #Then run your code
            # grabs each field
            contactname = page_soup.findAll(""td"", {""itemprop"": ""name""})
            contactstreetaddress = page_soup.findAll(""td"", {""itemprop"": ""streetAddress""})
            contactpostalcode = page_soup.findAll(""td"", {""itemprop"": ""postalCode""})
            contactaddressregion = page_soup.findAll(""td"", {""itemprop"": ""addressRegion""})
            contactaddresscountry = page_soup.findAll(""td"", {""itemprop"": ""addressCountry""})
            contactfax = page_soup.findAll(""td"", {""itemprop"": ""faxNumber""})
            contactemail = page_soup.findAll(""td"", {""itemprop"": ""email""})
            contactphone = page_soup.findAll(""td"", {""itemprop"": ""telephone""})
            contacturl = page_soup.findAll(""a"", {""itemprop"": ""url""})

            #Outputs as text without tags
            Company = contactname[0].text
            Address = contactstreetaddress[0].text
            Zip = contactpostalcode[0].text
            Region = contactaddressregion[0].text
            Country = contactaddresscountry[0].text
            Fax = contactfax[0].text
            Email = contactemail[0].text
            Phone = contactphone[0].text
            URL = contacturl[0].text
            #Here you might want to consider using dictionary or a list
            #For example append Phone to list call phone
            phone.append(Phone)

The code is a bit messy but it ran through all the possible folders (Even you have other folders inside your main folder) then try to find html tags, the open it.
I would suggest using dictionary with company as a key in which I presume that the company's name are different. A bunch of lists would also be great because your value will be sorted accordingly. I am not good with dictionary so I can't advise you more than this. I hope I answer your question.
P.S sorry for a messy code.
Edit : Fixing replace lxml with html.parser
",,
BeautifulSoup strange output,https://stackoverflow.com/questions/28447487,Problems Parsing NBA Boxscore Data with BeautifulSoup,"I am trying to parse player level NBA boxscore data from EPSN. The following is the initial portion of my attempt:

import numpy as np
import pandas as pd
import requests
from bs4 import BeautifulSoup
from datetime import datetime, date

request = requests.get('http://espn.go.com/nba/boxscore?gameId=400277722')
soup = BeautifulSoup(request.text,'html.parser')
table = soup.find_all('table')


It seems that BeautifulSoup is giving me a strange result. The last 'table' in the source code contains the player data and that is what I want to extract. Looking at the source code online shows that this table is closed at line 421, which is AFTER both teams' box scores. However, if we look at 'soup', there is an added line that closes the table BEFORE the Miami stats. This occurs at line 350 in the online source code.

The output from the parser 'html.parser' is:

Game 1: Tuesday, October 30thCeltics107FinalHeat120Recap »Boxscore »
Game 2: Sunday, January 27thHeat98Final2OTCeltics100Recap »Boxscore »
Game 3: Monday, March 18thHeat105FinalCeltics103Recap »Boxscore »
Game 4: Friday, April 12thCeltics101FinalHeat109Recap »Boxscore »

1 2 3 4 T

BOS 25 29 22 31107MIA 31 31 31 27120

Boston Celtics
STARTERS    
MIN
FGM-A
3PM-A
FTM-A
OREB
DREB
REB
AST
STL
BLK
TO
PF
+/-
PTS

Kevin Garnett, PF324-80-01-11111220254-49
Brandon Bass, PF286-110-03-4651110012-815
Paul Pierce, SF416-152-49-905552003-1723
Rajon Rondo, PG449-140-22-4077130044-1320
Courtney Lee, SG245-61-10-001110015-711
BENCH
MIN
FGM-A
3PM-A
FTM-A
OREB
DREB
REB
AST
STL
BLK
TO
PF
+/-
PTS

Jared Sullinger, PF81-20-00-001100001-32
Jeff Green, SF230-40-03-403301010-73
Jason Terry, SG252-70-34-400011033-108
Leandro Barbosa, SG166-83-31-201110001+416
Chris Wilcox, PFDNP COACH'S DECISION
Kris Joseph, SFDNP COACH'S DECISION
Jason Collins, CDNP COACH'S DECISION
Darko Milicic, CDNP COACH'S DECISIONTOTALS
FGM-A
3PM-A  
FTM-A
OREB


As you can see, it ends mid-table at 'OREB' and it never makes it to the Miami Heat section. The output using 'lxml' parser is:

Game 1: Tuesday, October 30thCeltics107FinalHeat120Recap »Boxscore »
Game 2: Sunday, January 27thHeat98Final2OTCeltics100Recap »Boxscore »
Game 3: Monday, March 18thHeat105FinalCeltics103Recap »Boxscore »
Game 4: Friday, April 12thCeltics101FinalHeat109Recap »Boxscore »

1 2 3 4T

BOS 25 29 22 31107MIA 31 31 31 27120


This doesn't include the box scores at all. The complete code I'm using (due to Daniel Rodriguez) looks something like:

import numpy as np
import pandas as pd
import requests
from bs4 import BeautifulSoup
from datetime import datetime, date

games = pd.read_csv('games_13.csv').set_index('id')
BASE_URL = 'http://espn.go.com/nba/boxscore?gameId={0}'

request = requests.get(BASE_URL.format(games.index[0]))
table = BeautifulSoup(request.text,'html.parser').find('table', class_='mod-data')
heads = table.find_all('thead')
headers = heads[0].find_all('tr')[1].find_all('th')[1:]
headers = [th.text for th in headers]
columns = ['id', 'team', 'player'] + headers

players = pd.DataFrame(columns=columns)

def get_players(players, team_name):
    array = np.zeros((len(players), len(headers)+1), dtype=object)
    array[:] = np.nan
    for i, player in enumerate(players):
        cols = player.find_all('td')
        array[i, 0] = cols[0].text.split(',')[0]
        for j in range(1, len(headers) + 1):
            if not cols[1].text.startswith('DNP'):
                array[i, j] = cols[j].text

    frame = pd.DataFrame(columns=columns)
    for x in array:
        line = np.concatenate(([index, team_name], x)).reshape(1,len(columns))
        new = pd.DataFrame(line, columns=frame.columns)
        frame = frame.append(new)
    return frame

for index, row in games.iterrows():
    print(index)
    request = requests.get(BASE_URL.format(index))
    table = BeautifulSoup(request.text, 'html.parser').find('table', class_='mod-data')
    heads = table.find_all('thead')
    bodies = table.find_all('tbody')

    team_1 = heads[0].th.text
    team_1_players = bodies[0].find_all('tr') + bodies[1].find_all('tr')
    team_1_players = get_players(team_1_players, team_1)
    players = players.append(team_1_players)

    team_2 = heads[3].th.text
    team_2_players = bodies[3].find_all('tr') + bodies[4].find_all('tr')
    team_2_players = get_players(team_2_players, team_2)
    players = players.append(team_2_players)

players = players.set_index('id')
print(players)
players.to_csv('players_13.csv')


A sample of the output I'd like is:

,id,team,player,MIN,FGM-A,3PM-A,FTM-A,OREB,DREB,REB,AST,STL,BLK,TO,PF,+/-,PTS
0,400277722,Boston Celtics,Brandon Bass,28,6-11,0-0,3-4,6,5,11,1,0,0,1,2,-8,15
0,400277722,Boston Celtics,Paul Pierce,41,6-15,2-4,9-9,0,5,5,5,2,0,0,3,-17,23
...
0,400277722,Miami Heat,Shane Battier,29,2-4,2-3,0-0,0,2,2,1,1,0,0,3,+12,6
0,400277722,Miami Heat,LeBron James,29,10-16,2-4,4-5,1,9,10,3,2,0,0,2,+12,26

",1,2031,"BeautifulSoup truncated part of the results for me as well, so I replaced soup.find_all option with re.findall

r = br.open('http://espn.go.com/nba/boxscore?gameId=400277722')
html = r.read()
soup = BeautifulSoup(html)

statnames = re.search('STARTERS&lt;/th&gt;.*?PTS&lt;/th&gt;',html, re.DOTALL).group()
th = re.findall('th.*&lt;/th', statnames) # each th tag contains a statname
names = ['Name', 'Team']
for t in th:
   t = re.sub('.*&gt;','',t)
   t = t.replace('&lt;/th','')
   names.append(t)
print names

celts = re.search('Boston Celtics.*?Total Team Turnovers',html,re.DOTALL).group()
heat = re.search('nba-small-mia floatleft.*?Total Team Turnovers',html,re.DOTALL).group()

players = str(soup).split('td nowrap')
for player in players[1:len(players)]:
   try:
       stats = [re.search('[A-Z]?[a-z]?[A-Z][a-z]{1,} [A-Z][a-z]{1,}',player).group()] 
   except:
       stats = [re.search('[A-Z]\.?[A-Z]?\.? [A-Z][a-z]{1,}',player).group()] # player name
       if stats[0] in celts:
          stats.append('Boston Celtics')
       elif stats[0] in heat:
          stats.append('Miami Heat')
   td = re.findall('td.*?/td', player) # each td tag contains a stat
   for t in td:
       t = re.findall('&gt;.*&lt;',t)
       t = re.sub('.*&gt;','',t[0])
       t = t.replace('&lt;','')
       if t!='' and t!='\xc2\xa0':
          stats.append(t)
    print stats


output =

['Name', 'Team', 'MIN', 'FGM-A', '3PM-A', 'FTM-A', 'OREB', 'DREB', 'REB', 'AST', 'STL', 'BLK', 'TO', 'PF', '+/-', 'PTS']
['Kevin Garnett', 'Boston Celtics', '32', '4-8', '0-0', '1-1', '1', '11', '12', '2', '0', '2', '5', '4', '-4', '9']
['Brandon Bass', 'Boston Celtics', '28', '6-11', '0-0', '3-4', '6', '5', '11', '1', '0', '0', '1', '2', '-8', '15']
['Paul Pierce', 'Boston Celtics', '41', '6-15', '2-4', '9-9', '0', '5', '5', '5', '2', '0', '0', '3', '-17', '23']
['Rajon Rondo', 'Boston Celtics', '44', '9-14', '0-2', '2-4', '0', '7', '7', '13', '0', '0', '4', '4', '-13', '20']
['Courtney Lee', 'Boston Celtics', '24', '5-6', '1-1', '0-0', '0', '1', '1', '1', '0', '0', '1', '5', '-7', '11']
['Jared Sullinger', 'Boston Celtics', '8', '1-2', '0-0', '0-0', '0', '1', '1', '0', '0', '0', '0', '1', '-3', '2']
['Jeff Green', 'Boston Celtics', '23', '0-4', '0-0', '3-4', '0', '3', '3', '0', '1', '0', '1', '0', '-7', '3']
['Jason Terry', 'Boston Celtics', '25', '2-7', '0-3', '4-4', '0', '0', '0', '1', '1', '0', '3', '3', '-10', '8']
['Leandro Barbosa', 'Boston Celtics', '16', '6-8', '3-3', '1-2', '0', '1', '1', '1', '0', '0', '0', '1', '+4', '16']
['Chris Wilcox', 'Boston Celtics', ""DNP COACH'S DECISION""]
['Kris Joseph', 'Boston Celtics', ""DNP COACH'S DECISION""]
['Jason Collins', 'Boston Celtics', ""DNP COACH'S DECISION""]
['Darko Milicic', 'Boston Celtics', ""DNP COACH'S DECISION""]
['Shane Battier', 'Miami Heat', '29', '2-4', '2-3', '0-0', '0', '2', '2', '1', '1', '0', '0', '3', '+12', '6']
['LeBron James', 'Miami Heat', '29', '10-16', '2-4', '4-5', '1', '9', '10', '3', '2', '0', '0', '2', '+12', '26']
['Chris Bosh', 'Miami Heat', '37', '8-15', '0-1', '3-4', '2', '8', '10', '1', '0', '3', '1', '3', '+15', '19']
['Mario Chalmers', 'Miami Heat', '36', '3-7', '0-1', '2-2', '0', '1', '1', '11', '3', '0', '1', '3', '+11', '8']
['Dwyane Wade', 'Miami Heat', '35', '10-22', '0-0', '9-11', '2', '1', '3', '4', '2', '1', '4', '3', '-6', '29']
['Udonis Haslem', 'Miami Heat', '11', '0-1', '0-0', '0-0', '0', '3', '3', '0', '0', '0', '1', '1', '-2', '0']
['Rashard Lewis', 'Miami Heat', '19', '4-5', '1-2', '1-2', '0', '5', '5', '1', '0', '1', '0', '1', '+1', '10']
['Norris Cole', 'Miami Heat', '6', '1-2', '1-2', '0-0', '0', '0', '0', '1', '0', '0', '1', '2', '+5', '3']
['Ray Allen', 'Miami Heat', '31', '5-7', '2-3', '7-8', '0', '2', '2', '2', '0', '0', '0', '1', '+9', '19']
['Mike Miller', 'Miami Heat', '7', '0-0', '0-0', '0-0', '0', '0', '0', '1', '0', '0', '0', '1', '+8', '0']
['Josh Harrellson', 'Miami Heat', ""DNP COACH'S DECISION""]
['James Jones', 'Miami Heat', ""DNP COACH'S DECISION""]
['Terrel Harris', 'Miami Heat', ""DNP COACH'S DECISION""]


To catch D.J. Augustine, the simplest (but not least concise) code is:

try:
    stats = [re.search('[A-Z]?[a-z]?[A-Z][a-z]{1,} [A-Z][a-z]{1,}',player).group()] 
except:
    stats = [re.search('[A-Z]\.?[A-Z]?\.? [A-Z][a-z]{1,}',player).group()]

","The code returns the correct data using the default parser which will probably lxml if you have it installed:

req = requests.get('http://espn.go.com/nba/boxscore?gameId=400277722')
soup = BeautifulSoup(req.content)
table = soup.find_all('table')
print(table)

....................
&lt;td nowrap="""" style=""text-align:left""&gt;&lt;a href=""http://espn.go.com/nba/player/_/id/2009/james-jones""&gt;James Jones&lt;/a&gt;, SF&lt;/td&gt;&lt;td colspan=""14"" style=""text-align:center""&gt;DNP COACH'S DECISION&lt;/td&gt;&lt;/tr&gt;&lt;tr align=""right"" class=""odd player-46-6490"" valign=""middle""&gt;
&lt;td nowrap="""" style=""text-align:left""&gt;&lt;a href=""http://espn.go.com/nba/player/_/id/6490/terrel-harris""&gt;Terrel Harris&lt;/a&gt;, SG&lt;/td&gt;&lt;td colspan=""14"" style=""text-align:center""&gt;DNP COACH'S DECISION&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;thead&gt;&lt;tr align=""right""&gt;&lt;th style=""text-align:left;""&gt;TOTALS&lt;/th&gt;&lt;th&gt;&lt;/th&gt;
&lt;th nowrap=""""&gt;FGM-A&lt;/th&gt;
&lt;th&gt;3PM-A&lt;/th&gt;
&lt;th&gt;FTM-A&lt;/th&gt;
&lt;th&gt;OREB
&lt;/th&gt;&lt;th&gt;DREB&lt;/th&gt;
&lt;th&gt;REB&lt;/th&gt;
&lt;th&gt;AST&lt;/th&gt;
&lt;th&gt;STL&lt;/th&gt;
&lt;th&gt;BLK&lt;/th&gt;
&lt;th&gt;TO&lt;/th&gt;
&lt;th&gt;PF&lt;/th&gt;
&lt;th&gt; &lt;/th&gt;
&lt;th&gt;PTS&lt;/th&gt;
&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr align=""right"" class=""even""&gt;&lt;td colspan=""2"" style=""text-align:left""&gt;&lt;/td&gt;&lt;td&gt;&lt;strong&gt;43-79&lt;/strong&gt;&lt;/td&gt;&lt;td&gt;&lt;strong&gt;8-16&lt;/strong&gt;&lt;/td&gt;&lt;td&gt;&lt;strong&gt;26-32&lt;/strong&gt;&lt;/td&gt;&lt;td&gt;&lt;strong&gt;5&lt;/strong&gt;&lt;/td&gt;&lt;td&gt;&lt;strong&gt;31&lt;/strong&gt;&lt;/td&gt;&lt;td&gt;&lt;strong&gt;36&lt;/strong&gt;&lt;/td&gt;&lt;td&gt;&lt;strong&gt;25&lt;/strong&gt;&lt;/td&gt;&lt;td&gt;&lt;strong&gt;8&lt;/strong&gt;&lt;/td&gt;&lt;td&gt;&lt;strong&gt;5&lt;/strong&gt;&lt;/td&gt;&lt;td&gt;&lt;strong&gt;8&lt;/strong&gt;&lt;/td&gt;&lt;td&gt;&lt;strong&gt;20&lt;/strong&gt;&lt;/td&gt;&lt;td&gt; &lt;/td&gt;&lt;td&gt;&lt;strong&gt;120&lt;/strong&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr align=""right"" class=""odd""&gt;&lt;td colspan=""2"" style=""text-align:left""&gt;&lt;strong&gt;&lt;/strong&gt;&lt;/td&gt;&lt;td&gt;&lt;strong&gt;54.4%&lt;/strong&gt;&lt;/td&gt;&lt;td&gt;&lt;strong&gt;50.0%&lt;/strong&gt;&lt;/td&gt;&lt;td&gt;&lt;strong&gt;81.3%&lt;/strong&gt;&lt;/td&gt;&lt;td colspan=""13""&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr bgcolor=""#ffffff""&gt;&lt;td align=""right"" colspan=""15"" style=""padding:10px;""&gt;&lt;div style=""float: right;""&gt;&lt;strong&gt;Fast break points:&lt;/strong&gt;   12&lt;br/&gt;&lt;strong&gt;Points in the paint:&lt;/strong&gt;   46&lt;br/&gt;&lt;strong&gt;Total Team Turnovers (Points off turnovers):&lt;/strong&gt;   8 (6)&lt;/div&gt;&lt;div style=""float: left;""&gt;+/- denotes team's net points while the player is on the court.&lt;/div&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;]


Using ""html.parser""  gave the the same truncated output as in your question but as you can see above without specifying it works fine.

It is working on both python 2.7 and 3.4 using  bs4 '4.3.2', my lxml version is 3.3.3.0.

If you have not got the latest bs4 you should update, you can use the diagnose method which will print out a report showing you how different parsers handle the document, and tell you if you’re missing a parser that Beautiful Soup could be using:

So with your html use the following to get a report:

from bs4.diagnose import diagnose
diagnose(request.text)


Using a regex to parse html has been well documented as not being a very good approach, a trivial change to the html and the regex can break. 
","Try using a different parser (lxml):

soup = BeautifulSoup(request.text,'lxml')
tables = soup.find_all('table')

for t in tables:
    print t.text


It will better detect the page structure
"
BeautifulSoup strange output,https://stackoverflow.com/questions/51003621,Parsing Arabic text in a local html file,"I am trying to extract some data from a local html file using python with BeautifulSoup, the file has some Arabic text data like titles. My problem is that when i try to print out this Arabic text, i get a strange string. An example of code with its output is provided along with the head section of the file, can anybody help me ?
",0,761,"Open the file with utf-8 encoding


  open(""body.htm"", encoding=""utf-8"")


or target the file to use utf-8

#!/usr/bin/env python
# -*- coding: utf-8 -*-

",,
BeautifulSoup strange output,https://stackoverflow.com/questions/6032457,Remove a bad tag completely with html5lib.sanitizer,"I'm trying to use html5lib.sanitizer to clean user-input as suggested in the docs

The problem is I want to remove bad tags completely and not just escape them (which seems like a bad idea anyway).

The workaround suggested in the patch here doesn't work as expected (it keeps inner content of a &lt;tag&gt;content&lt;/tag&gt;).

Specifically, I want to do something like this:

Input:

&lt;script&gt;bad_thing();&lt;/script&gt;
&lt;style&gt;* { background: #000; }&lt;/style&gt;
&lt;h1&gt;Hello world&lt;/h1&gt;
Lorem ipsum


Output:

&lt;h1&gt;Hello world&lt;/h1&gt;
Lorem ipsum


Any ideas on how to achieve it? I've tried BeautifulSoup, but it doesn't seem to work well, and lxml inserts &lt;p&gt;&lt;/p&gt; tags in very strange places (e.g. around src attrs). So far, html5lib seems to be the best thing for the purpose, if I could just get it to remove tags instead of escaping them.
",0,1045,"The challenge is to also strip unwanted nested tags. It isn't pretty but it's a step in the right direction:

from lxml.html import fromstring
from lxml import etree

html = '''
&lt;script&gt;bad_thing();&lt;/script&gt;
&lt;style&gt;* { background: #000; }&lt;/style&gt;
&lt;h1&gt;Hello world&lt;script&gt;bad_thing();&lt;/script&gt;&lt;/h1&gt;
Lorem ipsum
&lt;script&gt;bad_thing();&lt;/script&gt;
&lt;b&gt;Bold Text&lt;/b&gt;
'''

l = []
doc = fromstring(html)
for el in doc.xpath("".//h1|.//b""):
    i = etree.Element(el.tag)
    i.text, i.tail = el.text, el.tail
    l.append(etree.tostring(i))

print ''.join(l)


Which outputs:

&lt;h1&gt;Hello world&lt;/h1&gt;
Lorem ipsum
&lt;b&gt;Bold Text&lt;/b&gt;

",,
BeautifulSoup strange output,https://stackoverflow.com/questions/6837687,Python - Error Parsing HTML w/ BeautifulSoup,,-2,1176,,,
BeautifulSoup strange result,https://stackoverflow.com/questions/17859832,BeautifulSoup return unexpected extra spaces,"I am trying to grab some text from html documents with BeautifulSoup. In a very relavant case for me, it originates a strange and interesting result: after a certain point, the soup is full of extra spaces within the text (a space separates every letter from the following one). I tried to search the web in order to find a reason for that, but I met only some news about the opposite bug (no spaces at all).

Do you have some suggestion or hint on why it happens, and how to solve this problem?.

This is the very basic code that i created:

from bs4 import BeautifulSoup

import urllib2
html = urllib2.urlopen(""http://www.beppegrillo.it"")
prova = html.read()
soup = BeautifulSoup(prova)
print soup


And this is a line taken from the results, the line where this problem start to appear:


  value=\""Giuseppe labbate ogm? non vorremmo nuovi uccelli chiamati lontre\""&gt;&lt;input onmouseover=\""Tip('&lt;cen t e r   c l a s s = \ \ ' t i t l e _ v i d e o \ \ ' &gt; &lt; b &gt; G i u s e p p e   l a b b a t e   o g m ?   n o n   v o r r e m m o   n u o v i   u c c e l l i   c h i a m a t i   l o n t r e &lt; 

",17,3666,"I believe this is a bug with Lxml's HTML parser. 
Try:

from bs4 import BeautifulSoup

import urllib2
html = urllib2.urlopen (""http://www.beppegrillo.it"")
prova = html.read()
soup = BeautifulSoup(prova.replace('ISO-8859-1', 'utf-8'))
print soup


Which is a workaround for the problem. 
I believe the issue was fixed in lxml 3.0 alpha 2 and lxml 2.3.6, so it could be worth checking whether you need to upgrade to a newer version. 

If you want more info on the bug it was initially filed here:

https://bugs.launchpad.net/beautifulsoup/+bug/972466

Hope this helps,

Hayden
","You can specify the parser as html.parser:

soup = BeautifulSoup(prova, 'html.parser')


Also you can specify the html5 parser:

soup = BeautifulSoup(prova, 'html5')


Haven't installed the html5 parser yet? Install it from terminal:

sudo apt-get install python-html5lib


The xml parser may be used (soup = BeautifulSoup(prova, 'xml')) but you may see some differences in multi-valued attributes like class=""foo bar"".
","I encountered the same issue, it works after change the encoding
                with open(src, ""r"", encoding=""UTF-16"") as file:
                    html = file.read()
                    html = BeautifulSoup(html)

"
BeautifulSoup strange result,https://stackoverflow.com/questions/629999,"BeautifulSoup gives me unicode+html symbols, rather than straight up unicode. Is this a bug or misunderstanding?","I'm using BeautifulSoup to scrape a website. The website's page renders fine in my browser: 


  Oxfam International’s report entitled “Offside!
  http://www.coopamerica.org/programs/responsibleshopper/company.cfm?id=271


In particular, the single and double quotes look fine. They look html symbols rather than ascii, though strangely when I view source in FF3 they appear to be normal ascii.

Unfortunately, when I scrape I get something like this


  u'Oxfam International\xe2€™s report
  entitled \xe2€œOffside!


oops, I mean this:

u'Oxfam International\xe2€™s report entitled \xe2€œOffside!


The page's meta data indicates 'iso-88959-1' encoding. I've tried different encodings, played with unicode-&gt;ascii and html-&gt;ascii third party functions, and looked at the MS/iso-8859-1 discrepancy, but the fact of the matter is that ™ has nothing to do with a single quote, and I can't seem to turn the unicode+htmlsymbol combo into the right ascii or html symbol--in my limited knowledge, which is why I'm seeking help.

I'd be happy with an ascii double quote, "" or ""

The problem the following is that I'm concerned there are other funny symbols decoded incorrectly. 

\xe2€™


Below is some python to reproduce what I'm seeing, followed by the things I've tried.

import twill
from twill import get_browser
from twill.commands import go

from BeautifulSoup import BeautifulSoup as BSoup

url = 'http://www.coopamerica.org/programs/responsibleshopper/company.cfm?id=271'
twill.commands.go(url)
soup = BSoup(twill.commands.get_browser().get_html())
ps = soup.body(""p"")
p = ps[52]

&gt;&gt;&gt; p         
Traceback (most recent call last):
  File ""&lt;stdin&gt;"", line 1, in &lt;module&gt;
UnicodeEncodeError: 'ascii' codec can't encode character u'\xe2' in position 22: ordinal not in range(128)

&gt;&gt;&gt; p.string
u'Oxfam International\xe2€™s report entitled \xe2€œOffside!&lt;elided&gt;\r\n'


http://groups.google.com/group/comp.lang.python/browse_frm/thread/9b7bb3f621b4b8e4/3b00a890cf3a5e46?q=htmlentitydefs&amp;rnum=3&amp;hl=en#3b00a890cf3a5e46

http://www.fourmilab.ch/webtools/demoroniser/

http://www.crummy.com/software/BeautifulSoup/documentation.html

http://www.cs.tut.fi/~jkorpela/www/windows-chars.html

&gt;&gt;&gt; AsciiDammit.asciiDammit(p.decode())
u'&lt;p&gt;Oxfam International\xe2€™s report entitled \xe2€œOffside!

&gt;&gt;&gt; handle_html_entities(p.decode())
u'&lt;p&gt;Oxfam International\xe2\u20ac\u2122s report entitled \xe2\u20ac\u0153Offside! 

&gt;&gt;&gt; unicodedata.normalize('NFKC', p.decode()).encode('ascii','ignore')
'&lt;p&gt;Oxfam International€™s report entitled €œOffside!

&gt;&gt;&gt; htmlStripEscapes(p.string)
u'Oxfam International\xe2TMs report entitled \xe2Offside!


EDIT:

I've tried using a different BS parser:

import html5lib
bsoup_parser = html5lib.HTMLParser(tree=html5lib.treebuilders.getTreeBuilder(""beautifulsoup""))
soup = bsoup_parser.parse(twill.commands.get_browser().get_html())
ps = soup.body(""p"")
ps[55].decode()


which gives me this

u'&lt;p&gt;Oxfam International\xe2\u20ac\u2122s report entitled \xe2\u20ac\u0153Offside!


the best case decode seems to give me the same results:

unicodedata.normalize('NFKC', p.decode()).encode('ascii','ignore')
'&lt;p&gt;Oxfam InternationalTMs report entitled Offside! 


EDIT 2:

I am running Mac OS X 4 with FF 3.0.7 and Firebug

Python 2.5 (wow, can't believe I didn't state this from the beginning)
",5,8553,,,
BeautifulSoup strange result,https://stackoverflow.com/questions/56020937,Why isn&#39;t BeautifulSoup scraping the entire webpage?,"Premise: I am totally new to Python and web scraping. I am trying to scrape the data about the brands on this page: https://www.interbrand.com/best-brands/best-global-brands/2018/ranking/ , but BeautifulSoup extracts the html only up to a certain point. Nothing strange seems to be in the html there, as there are five almost equal tags before that one that BeautifulSoup extracts without any problem.

I already tried using three different parsers (the built-in one, lxml and html5lib), but I always get the same result.

Here is the code: 

import requests
page = requests.get(""https://www.interbrand.com/best-brands/best-global-brands/2018/ranking/"")
from bs4 import BeautifulSoup
soup = BeautifulSoup(page.content , 'html5lib')
print(soup.prettify())

",4,397,"Use Css selecor to get the output.

from bs4 import BeautifulSoup
import requests
page = requests.get(""https://www.interbrand.com/best-brands/best-global-brands/2018/ranking/"")
soup = BeautifulSoup(page.content , 'lxml')
Brand=[]
Country=[]
Region=[]
Sector=[]
for brnd in soup.select('div.brand-name'):
    Brand.append(brnd['title'])

for region in soup.select('div.brand-region'):
    Region.append(region['title'])

for county in soup.select('div.brand-country'):
    Country.append(county['title'])

for sector in soup.select('div.brand-sector'):
    Sector.append(sector['title'])

print(Brand)
print(Region)
print(Country)
print(Sector)


Output:

['Brand name: Apple', 'Brand name: Google', 'Brand name: Amazon', 'Brand name: Microsoft', 'Brand name: Coca-Cola', 'Brand name: Samsung', 'Brand name: Toyota', 'Brand name: Mercedes-Benz', 'Brand name: Facebook', ""Brand name: McDonald's"", 'Brand name: Intel', 'Brand name: IBM', 'Brand name: BMW', 'Brand name: Disney', 'Brand name: Cisco', 'Brand name: GE', 'Brand name: Nike', 'Brand name: Louis Vuitton', 'Brand name: Oracle', 'Brand name: Honda', 'Brand name: SAP', 'Brand name: Pepsi', 'Brand name: Chanel', 'Brand name: American Express', 'Brand name: Zara', 'Brand name: J.P. Morgan', 'Brand name: IKEA', 'Brand name: Gillette', 'Brand name: UPS', 'Brand name: H&amp;M', 'Brand name: Pampers', 'Brand name: Hermès', 'Brand name: Budweiser', 'Brand name: Accenture', 'Brand name: Ford', 'Brand name: Hyundai', 'Brand name: NESCAFÉ', 'Brand name: eBay', 'Brand name: Gucci', 'Brand name: Nissan', 'Brand name: Volkswagen', 'Brand name: Audi', 'Brand name: Philips', 'Brand name: Goldman Sachs', 'Brand name: Citi', 'Brand name: HSBC', 'Brand name: AXA', ""Brand name: L'Oréal"", 'Brand name: Allianz', 'Brand name: adidas', 'Brand name: Adobe', 'Brand name: Porsche', ""Brand name: Kellogg's"", 'Brand name: HP', 'Brand name: Canon', 'Brand name: Siemens', 'Brand name: Starbucks', 'Brand name: Danone', 'Brand name: Sony', 'Brand name: 3M', 'Brand name: Visa', 'Brand name: Nestlé', 'Brand name: Morgan Stanley', 'Brand name: Colgate', 'Brand name: Hewlett Packard Enterprise', 'Brand name: Netflix', 'Brand name: Cartier', 'Brand name: Huawei', 'Brand name: Banco Santander', 'Brand name: Mastercard', 'Brand name: Kia', 'Brand name: FedEx', 'Brand name: PayPal', 'Brand name: LEGO', 'Brand name: Salesforce.com', 'Brand name: Panasonic', 'Brand name: Johnson &amp; Johnson', 'Brand name: Land Rover', 'Brand name: DHL', 'Brand name: Ferrari', 'Brand name: Discovery', 'Brand name: Caterpillar', 'Brand name: Tiffany &amp; Co.', ""Brand name: Jack Daniel's"", 'Brand name: Corona', 'Brand name: KFC', 'Brand name: Heineken', 'Brand name: John Deere', 'Brand name: Shell', 'Brand name: MINI', 'Brand name: Dior', 'Brand name: Spotify', 'Brand name: Harley-Davidson', 'Brand name: Burberry', 'Brand name: Prada', 'Brand name: Sprite', 'Brand name: Johnnie Walker', 'Brand name: Hennessy', 'Brand name: Nintendo', 'Brand name: Subaru']
['Region: The Americas', 'Region: The Americas', 'Region: The Americas', 'Region: The Americas', 'Region: The Americas', 'Region: Asia Pacific', 'Region: Asia Pacific', 'Region: Europe &amp; Africa', 'Region: The Americas', 'Region: The Americas', 'Region: The Americas', 'Region: The Americas', 'Region: Europe &amp; Africa', 'Region: The Americas', 'Region: The Americas', 'Region: The Americas', 'Region: The Americas', 'Region: Europe &amp; Africa', 'Region: The Americas', 'Region: Asia Pacific', 'Region: Europe &amp; Africa', 'Region: The Americas', 'Region: Europe &amp; Africa', 'Region: The Americas', 'Region: Europe &amp; Africa', 'Region: The Americas', 'Region: Europe &amp; Africa', 'Region: The Americas', 'Region: The Americas', 'Region: Europe &amp; Africa', 'Region: The Americas', 'Region: Europe &amp; Africa', 'Region: The Americas', 'Region: The Americas', 'Region: The Americas', 'Region: Asia Pacific', 'Region: Europe &amp; Africa', 'Region: The Americas', 'Region: Europe &amp; Africa', 'Region: Asia Pacific', 'Region: Europe &amp; Africa', 'Region: Europe &amp; Africa', 'Region: Europe &amp; Africa', 'Region: The Americas', 'Region: The Americas', 'Region: Europe &amp; Africa', 'Region: Europe &amp; Africa', 'Region: Europe &amp; Africa', 'Region: Europe &amp; Africa', 'Region: Europe &amp; Africa', 'Region: The Americas', 'Region: Europe &amp; Africa', 'Region: The Americas', 'Region: The Americas', 'Region: Asia Pacific', 'Region: Europe &amp; Africa', 'Region: The Americas', 'Region: Europe &amp; Africa', 'Region: Asia Pacific', 'Region: The Americas', 'Region: The Americas', 'Region: Europe &amp; Africa', 'Region: The Americas', 'Region: The Americas', 'Region: The Americas', 'Region: The Americas', 'Region: Europe &amp; Africa', 'Region: Asia Pacific', 'Region: Europe &amp; Africa', 'Region: The Americas', 'Region: Asia Pacific', 'Region: The Americas', 'Region: The Americas', 'Region: Europe &amp; Africa', 'Region: The Americas', 'Region: Asia Pacific', 'Region: The Americas', 'Region: Europe &amp; Africa', 'Region: The Americas', 'Region: Europe &amp; Africa', 'Region: The Americas', 'Region: The Americas', 'Region: The Americas', 'Region: The Americas', 'Region: The Americas', 'Region: The Americas', 'Region: Europe &amp; Africa', 'Region: The Americas', 'Region: Europe &amp; Africa', 'Region: Europe &amp; Africa', 'Region: Europe &amp; Africa', 'Region: Europe &amp; Africa', 'Region: The Americas', 'Region: Europe &amp; Africa', 'Region: Europe &amp; Africa', 'Region: The Americas', 'Region: Europe &amp; Africa', 'Region: Europe &amp; Africa', 'Region: Asia Pacific', 'Region: Asia Pacific']
['Country: United States', 'Country: United States', 'Country: United States', 'Country: United States', 'Country: United States', 'Country: South Korea', 'Country: Japan', 'Country: Germany', 'Country: United States', 'Country: United States', 'Country: United States', 'Country: United States', 'Country: Germany', 'Country: United States', 'Country: United States', 'Country: United States', 'Country: United States', 'Country: France', 'Country: United States', 'Country: Japan', 'Country: Germany', 'Country: United States', 'Country: France', 'Country: United States', 'Country: Spain', 'Country: United States', 'Country: Sweden', 'Country: United States', 'Country: United States', 'Country: Sweden', 'Country: United States', 'Country: France', 'Country: United States', 'Country: United States', 'Country: United States', 'Country: South Korea', 'Country: Switzerland', 'Country: United States', 'Country: Italy', 'Country: Japan', 'Country: Germany', 'Country: Germany', 'Country: Netherlands', 'Country: United States', 'Country: United States', 'Country: United Kingdom', 'Country: France', 'Country: France', 'Country: Germany', 'Country: Germany', 'Country: United States', 'Country: Germany', 'Country: United States', 'Country: United States', 'Country: Japan', 'Country: Germany', 'Country: United States', 'Country: France', 'Country: Japan', 'Country: United States', 'Country: United States', 'Country: Switzerland', 'Country: United States', 'Country: United States', 'Country: United States', 'Country: United States', 'Country: France', 'Country: China', 'Country: Spain', 'Country: United States', 'Country: South Korea', 'Country: United States', 'Country: United States', 'Country: Denmark', 'Country: United States', 'Country: Japan', 'Country: United States', 'Country: United Kingdom', 'Country: United States', 'Country: Italy', 'Country: United States', 'Country: United States', 'Country: United States', 'Country: United States', 'Country: Mexico', 'Country: United States', 'Country: Netherlands', 'Country: United States', 'Country: Netherlands', 'Country: United Kingdom', 'Country: France', 'Country: Sweden', 'Country: United States', 'Country: United Kingdom', 'Country: Italy', 'Country: United States', 'Country: United Kingdom', 'Country: France', 'Country: Japan', 'Country: Japan']
['Sector: Technology', 'Sector: Technology', 'Sector: Retail', 'Sector: Technology', 'Sector: Beverages', 'Sector: Technology', 'Sector: Automotive', 'Sector: Automotive', 'Sector: Technology', 'Sector: Restaurants', 'Sector: Technology', 'Sector: Business Services', 'Sector: Automotive', 'Sector: Media', 'Sector: Technology', 'Sector: Diversified', 'Sector: Sporting Goods', 'Sector: Luxury', 'Sector: Technology', 'Sector: Automotive', 'Sector: Technology', 'Sector: Beverages', 'Sector: Luxury', 'Sector: Financial Services', 'Sector: Apparel', 'Sector: Financial Services', 'Sector: Retail', 'Sector: FMCG', 'Sector: Logistics', 'Sector: Apparel', 'Sector: FMCG', 'Sector: Luxury', 'Sector: Alcohol', 'Sector: Business Services', 'Sector: Automotive', 'Sector: Automotive', 'Sector: Beverages', 'Sector: Retail', 'Sector: Luxury', 'Sector: Automotive', 'Sector: Automotive', 'Sector: Automotive', 'Sector: Electronics', 'Sector: Financial Services', 'Sector: Financial Services', 'Sector: Financial Services', 'Sector: Financial Services', 'Sector: FMCG', 'Sector: Financial Services', 'Sector: Sporting Goods', 'Sector: Technology', 'Sector: Automotive', 'Sector: FMCG', 'Sector: Technology', 'Sector: Electronics', 'Sector: Diversified', 'Sector: Restaurants', 'Sector: FMCG', 'Sector: Electronics', 'Sector: Diversified', 'Sector: Financial Services', 'Sector: FMCG', 'Sector: Financial Services', 'Sector: FMCG', 'Sector: Technology', 'Sector: Media', 'Sector: Luxury', 'Sector: Technology', 'Sector: Financial Services', 'Sector: Financial Services', 'Sector: Automotive', 'Sector: Logistics', 'Sector: Financial Services', 'Sector: FMCG', 'Sector: Business Services', 'Sector: Electronics', 'Sector: FMCG', 'Sector: Automotive', 'Sector: Logistics', 'Sector: Automotive', 'Sector: Media', 'Sector: Diversified', 'Sector: Luxury', 'Sector: Alcohol', 'Sector: Alcohol', 'Sector: Restaurants', 'Sector: Alcohol', 'Sector: Diversified', 'Sector: Energy', 'Sector: Automotive', 'Sector: Luxury', 'Sector: Media', 'Sector: Automotive', 'Sector: Luxury', 'Sector: Luxury', 'Sector: Beverages', 'Sector: Alcohol', 'Sector: Alcohol', 'Sector: Electronics', 'Sector: Automotive']

",,
BeautifulSoup strange result,https://stackoverflow.com/questions/58276115,how to properly extract utf8 text (japanese symbols) from a webpage with BeautifulSoup4,"i downloaded webpages using wget. now i am trying to extract some data i need from those pages. the problem is with the Japanese words contained in this data. the English words extraction was perfect.
when i try to extract the Japanese words and use them in another app they appear gibberish. during testing diffrent methods there was one solution that fixed only half the japanese words.
what i tried: i tried
from_encoding=""utf-8"" 

which had no effect. also i tried multiple ways to extract the text from the html code like
section.get_text(strip=True) 
section.text.strip()

and others, also i tried to encode the generated text using URLencoding which did not work, also i tried using every code i could find on stackoverflow
one of the methods that strangely worked (but not completely) was saving the string in a dictionary then saving it into a JSON then calling the JSON from ANOTHER script. just using the dictionary, as it is, would not work. i have to use JSON as a middle man between two scripts. strange. (not all the words worked)
my question may seem like duplicates of anther question. but that other question is scraping from the internet. and what i am trying to do is extract from an offline source.
here is a simple script explaining the main problem
from bs4 import BeautifulSoup

page = BeautifulSoup(open(""page1.html""), 'html.parser', from_encoding=""utf-8"")
word = page.find('span', {'class' : ""radical-icon""})
wordtxt = word.get_text(strip=True)
  
#then save the word to a file
    
with open(""text.txt"", ""w"", encoding=""utf8"") as text_file:
    text_file.write(wordtxt)

when i open the file i get gibberish characters
here is the part of the html that BeautifulSoup searchs:
&lt;span class=""radical-icon"" lang=""ja""&gt;亠&lt;/span&gt;

the expected results is to get the symbols inside the text file. or to save them properly in anyway.
is there a better web scraper to use to properly get the utf8?
PS: sorry for bad english
",3,361,"i think i found an answer, just uninstall beautifulsoup4. i dont need it.

python has a builtin way to search for strings, i tried something like this:

import codecs
import re

with codecs.open(""page1.html"", 'r', 'utf-8') as myfile:
    for line in myfile:
        if line.find('&lt;span class=""radical-icon""') &gt; -1:
            result = re.search('&lt;span class=""radical-icon"" lang=""ja""&gt;(.*)&lt;/span&gt;', line)
            s = result.group(1)

with codecs.open(""text.txt"", 'w', 'utf-8') as textfile:
    textfile.write(s)



which is the over complicated and non-pythonic way of doing it. but what works works.
",,
BeautifulSoup strange result,https://stackoverflow.com/questions/44264658,Beautifulsoup 4 spans containg &#39;@&#39; return strange results,"I was able to get the needed list of spans using the following :

attrs = soup.find_all(""span"")


This returns a list of spans as Key and Value :

[
    &lt;span&gt;back camera resolution&lt;/span&gt;, 
    &lt;span class=""even""&gt;12 MP&lt;/span&gt;
]

[
    &lt;span&gt;front camera resolution&lt;/span&gt;, 
    &lt;span class=""even""&gt;16 MP&lt;/span&gt;
]

[
    &lt;span&gt;video resolution&lt;/span&gt;, 
    &lt;span class=""even""&gt;&lt;a class=""__cf_email__"" data-cfemail=""b98b888f89c9f98a89dfc9ca"" href=""/cdn-cgi/l/email-protection""&gt;[email protected]&lt;/a&gt;&lt;script data-cfhash=""f9e31"" type=""text/javascript""&gt;/* &lt;![CDATA[ */!function(t,e,r,n,c,a,p){try{t=document.currentScript||function(){for(t=document.getElementsByTagName('script'),e=t.length;e--;)if(t[e].getAttribute('data-cfhash'))return t[e]}();if(t&amp;&amp;(c=t.previousSibling)){p=t.parentNode;if(a=c.getAttribute('data-cfemail')){for(e='',r='0x'+a.substr(0,2)|0,n=2;a.length-n;n+=2)e+='%'+('0'+('0x'+a.substr(n,2)^r).toString(16)).slice(-2);p.replaceChild(document.createTextNode(decodeURIComponent(e)),c)}p.removeChild(t)}}catch(u){}}()/* ]]&gt; */&lt;/script&gt; - &lt;a class=""__cf_email__"" data-cfemail=""4677767e7636067576203635"" href=""/cdn-cgi/l/email-protection""&gt;[email protected]&lt;/a&gt;&lt;script data-cfhash=""f9e31"" type=""text/javascript""&gt;/* &lt;![CDATA[ */!function(t,e,r,n,c,a,p){try{t=document.currentScript||function(){for(t=document.getElementsByTagName('script'),e=t.length;e--;)if(t[e].getAttribute('data-cfhash'))return t[e]}();if(t&amp;&amp;(c=t.previousSibling)){p=t.parentNode;if(a=c.getAttribute('data-cfemail')){for(e='',r='0x'+a.substr(0,2)|0,n=2;a.length-n;n+=2)e+='%'+('0'+('0x'+a.substr(n,2)^r).toString(16)).slice(-2);p.replaceChild(document.createTextNode(decodeURIComponent(e)),c)}p.removeChild(t)}}catch(u){}}()/* ]]&gt; */&lt;/script&gt; - &lt;a class=""__cf_email__"" data-cfemail=""5067626010616260362023"" href=""/cdn-cgi/l/email-protection""&gt;[email protected]&lt;/a&gt;&lt;script data-cfhash=""f9e31"" type=""text/javascript""&gt;/* &lt;![CDATA[ */!function(t,e,r,n,c,a,p){try{t=document.currentScript||function(){for(t=document.getElementsByTagName('script'),e=t.length;e--;)if(t[e].getAttribute('data-cfhash'))return t[e]}();if(t&amp;&amp;(c=t.previousSibling)){p=t.parentNode;if(a=c.getAttribute('data-cfemail')){for(e='',r='0x'+a.substr(0,2)|0,n=2;a.length-n;n+=2)e+='%'+('0'+('0x'+a.substr(n,2)^r).toString(16)).slice(-2);p.replaceChild(document.createTextNode(decodeURIComponent(e)),c)}p.removeChild(t)}}catch(u){}}()/* ]]&gt; */&lt;/script&gt;
    &lt;/span&gt;
]


The original HTML for this is :



Why the 'video resolution' is converted like this ?
",3,1049,"The site is using the CloudFlare email protection feature, which appears to have replaced all strings with @ in them with obfuscated (XOR encryption) values to prevent scrapers from harvesting email addresses. Each replacement includes the JavaScript code to decode it. 

BeautifulSoup won't execute the Javascript, but your browser has executed it and replaced the &lt;a class=""__cf_email__""&gt; tags with the resulting decrypted data.

You can do the same with a small Python 3 function; all the JavaScript code does is 'decrypt' the (hex-encoded) value by using the first byte as the key in a simple XOR decryption routine:

def decode(cfemail):
    enc = bytes.fromhex(cfemail)
    return bytes([c ^ enc[0] for c in enc[1:]]).decode('utf8')

def deobfuscate_cf_email(soup):
    for encrypted_email in soup.select('a.__cf_email__'):
        decrypted = decode(encrypted_email['data-cfemail'])
        # remove the &lt;script&gt; tag from the tree
        script_tag = encrypted_email.find_next_sibling('script')
        script_tag.decompose()
        # replace the &lt;a class=""__cf_email__""&gt; tag with the decoded result
        encrypted_email.replace_with(decrypted)


To make the above work in Python 2, replace bytes with bytearray.

Demo:

&gt;&gt;&gt; from bs4 import BeautifulSoup
&gt;&gt;&gt; soup = BeautifulSoup('''
...     &lt;span&gt;video resolution&lt;/span&gt;,
...     &lt;span class=""even""&gt;&lt;a class=""__cf_email__"" data-cfemail=""b98b888f89c9f98a89dfc9ca"" href=""/cdn-cgi/l/email-protection""&gt;[email protected]&lt;/a&gt;&lt;script data-cfhash=""f9e31"" type=""text/javascript""&gt;/* &lt;![CDATA[ */!function(t,e,r,n,c,a,p){try{t=document.currentScript||function(){for(t=document.getElementsByTagName('script'),e=t.length;e--;)if(t[e].getAttribute('data-cfhash'))return t[e]}();if(t&amp;&amp;(c=t.previousSibling)){p=t.parentNode;if(a=c.getAttribute('data-cfemail')){for(e='',r='0x'+a.substr(0,2)|0,n=2;a.length-n;n+=2)e+='%'+('0'+('0x'+a.substr(n,2)^r).toString(16)).slice(-2);p.replaceChild(document.createTextNode(decodeURIComponent(e)),c)}p.removeChild(t)}}catch(u){}}()/* ]]&gt; */&lt;/script&gt; - &lt;a class=""__cf_email__"" data-cfemail=""4677767e7636067576203635"" href=""/cdn-cgi/l/email-protection""&gt;[email protected]&lt;/a&gt;&lt;script data-cfhash=""f9e31"" type=""text/javascript""&gt;/* &lt;![CDATA[ */!function(t,e,r,n,c,a,p){try{t=document.currentScript||function(){for(t=document.getElementsByTagName('script'),e=t.length;e--;)if(t[e].getAttribute('data-cfhash'))return t[e]}();if(t&amp;&amp;(c=t.previousSibling)){p=t.parentNode;if(a=c.getAttribute('data-cfemail')){for(e='',r='0x'+a.substr(0,2)|0,n=2;a.length-n;n+=2)e+='%'+('0'+('0x'+a.substr(n,2)^r).toString(16)).slice(-2);p.replaceChild(document.createTextNode(decodeURIComponent(e)),c)}p.removeChild(t)}}catch(u){}}()/* ]]&gt; */&lt;/script&gt; - &lt;a class=""__cf_email__"" data-cfemail=""5067626010616260362023"" href=""/cdn-cgi/l/email-protection""&gt;[email protected]&lt;/a&gt;&lt;script data-cfhash=""f9e31"" type=""text/javascript""&gt;/* &lt;![CDATA[ */!function(t,e,r,n,c,a,p){try{t=document.currentScript||function(){for(t=document.getElementsByTagName('script'),e=t.length;e--;)if(t[e].getAttribute('data-cfhash'))return t[e]}();if(t&amp;&amp;(c=t.previousSibling)){p=t.parentNode;if(a=c.getAttribute('data-cfemail')){for(e='',r='0x'+a.substr(0,2)|0,n=2;a.length-n;n+=2)e+='%'+('0'+('0x'+a.substr(n,2)^r).toString(16)).slice(-2);p.replaceChild(document.createTextNode(decodeURIComponent(e)),c)}p.removeChild(t)}}catch(u){}}()/* ]]&gt; */&lt;/script&gt;
...     &lt;/span&gt;
... ''')
&gt;&gt;&gt; deobfuscate_cf_email(soup)
&gt;&gt;&gt; soup
&lt;html&gt;&lt;body&gt;&lt;span&gt;video resolution&lt;/span&gt;,
    &lt;span class=""even""&gt;2160p@30fps - 1080p@30fps - 720@120fps
&lt;/span&gt;
&lt;/body&gt;&lt;/html&gt;

",,
BeautifulSoup strange result,https://stackoverflow.com/questions/9819141,Python BeautifulSoup - Different results with find and findAll,"I am trying to parse some text embedded in HTML using BeautifulSoup, using the ""text"" attribute.
Playing around with ""find"" and ""findAll"", I notice something strange and cannot get my head around why it behaves in this way...

import re
import BeautifulSoup

doc = ""&lt;html&gt;&lt;head&gt;&lt;title&gt;Page title&lt;/title&gt;&lt;title&gt;Author name&lt;/title&gt;&lt;/head&gt;""
soup = BeautifulSoup(doc)

# find test
test1 = soup1.find('title',text=re.compile(""Page"")) 
print test1 # Returns ""Page title""

# findAll test
test2 = soup1.findAll('title',text=re.compile(""Page"")) 
print test2 # Returns ""[&lt;title&gt;Page title&lt;/title&gt;, &lt;title&gt;Author name&lt;/title&gt;]""


In the second test, shouldn't the parser return the same results as in the first example?
The second test should be returning all 'title' tags whose text contains ""Page"", and yet it returns the second title tag too.

Is this expected or am I missing something?
",3,7509,"I got the results for BeautifulSoup version 4.7 in python3.7

&gt;&gt;&gt; soup.find('title', text=re.compile('Page'))
    &lt;title&gt;Page title&lt;/title&gt;
&gt;&gt;&gt; soup.findAll('title', text=re.compile('Page'))
   [&lt;title&gt;Page title&lt;/title&gt;]

",,
BeautifulSoup strange result,https://stackoverflow.com/questions/30788850,How should I show results of BeautifulSoup parsing in Django?,"I'm trying to scrape a web page using BeautifulSoup and Django. Here's my views.py which do this task:

def detail(request, article_id):
    article = get_object_or_404(Article, pk=article_id)
    html = urllib2.urlopen(""...url..."")
    soup = BeautifulSoup(html)
    title = soup.title

    return render(request, 'detail.html', {'article': article, 'title':title})


But when I use {{ title }} in django template files, it doesn't show anything. I've test it and it works in shell. I've added a line to this function:

print soup.title


and it prints it every time I reload the page, but it doesn't show up in templates.

The content also displayed very strange for some other commands like: find_all(""a"") or prettify method. Could anyone tell me how can I print the result of beautifulsoup correctly in django templates?
",1,1710,"what's the result that you print out?

have you try to use this?

soup.title.string


if you have to send html in to templates try:

{% autoescape off %}{{ title }}{% endautoescape %}

","Change the following -

title = soup.title


to - 

title = soup.title.text

",
BeautifulSoup strange result,https://stackoverflow.com/questions/9837713,using beautifulsoup 4 for xml causes strange behaviour (memory issues?),"I'm getting strange behaviour with this

&gt;&gt;&gt; from bs4 import BeautifulSoup

&gt;&gt;&gt; smallfile = 'small.xml'      #approx 600bytes
&gt;&gt;&gt; largerfile = 'larger.xml'    #approx 2300 bytes
&gt;&gt;&gt; len(BeautifulSoup(open(smallfile, 'r'), ['lxml', 'xml']))
1
&gt;&gt;&gt; len(BeautifulSoup(open(largerfile, 'r'), ['lxml', 'xml']))
0


Contents of small.xml:

&lt;?xml version=""1.0"" encoding=""us-ascii""?&gt;
&lt;Catalog&gt;
&lt;CMoverMissile id=""HunterSeekerMissile""&gt;
&lt;MotionPhases index=""1""&gt;
&lt;Driver value=""Guidance""/&gt;
&lt;Acceleration value=""3200""/&gt;
&lt;MaxSpeed value=""2.9531""/&gt;
&lt;Clearance value=""0.5""/&gt;
&lt;ClearanceLookahead value=""3""/&gt;
&lt;Outro value=""-4.5,-4.25""/&gt;
&lt;YawPitchRoll value=""MAX""/&gt;
&lt;/MotionPhases&gt;
&lt;MotionPhases index=""2""&gt;
&lt;Driver value=""Guidance""/&gt;
&lt;Acceleration value=""4""/&gt;
&lt;MaxSpeed value=""2.9531""/&gt;
&lt;Clearance value=""0.5""/&gt;
&lt;ClearanceLookahead value=""3""/&gt;
&lt;Outro value=""-2.25,-2""/&gt;
&lt;YawPitchRoll value=""MAX""/&gt;
&lt;/MotionPhases&gt;
&lt;/CMoverMissile&gt;
&lt;/Catalog&gt;


largerfile is simply the smaller file, but padded with spaces and newlines (inbetween the last two tags in case it's relevant). IE the structure and contents of the xml should be identical for both cases.

On rare occasions processing largerfile will actually yield a partial result where only a small portion of the xml has been parsed. I can't seem to reliably recreate the circumstances.

Since BeautifulSoup uses lxml, I tested to see if lxml could handle the files independently. lxml appeared to be able to parse both files.

&gt;&gt;&gt; from lxml import etree
&gt;&gt;&gt; tree = etree.parse(smallfile)
&gt;&gt;&gt; len(etree.tostring(tree))
547
&gt;&gt;&gt; tree = etree.parse(largerfile)
&gt;&gt;&gt; len(etree.tostring(tree))
2294


I'm using


netbook with 1gb ram
windows 7
lxml 2.3 (had some trouble installing this, I hope a dodgy installation isn't causing the problem)
beautiful soup 4.0.1
python 3.2 (I also have python 2.7x installed, but have been using 3.2 for this code)


What could be preventing the larger file from being processed properly? My current suspicion is some weird memory issue, since the file size seems to make a difference, perhaps in conjunction with some bug in how BeautifulSoup 4 interacts with lxml.

Edit:
to better illustrate...

&gt;&gt;&gt; smallsoup = BeautifulSoup(smallfile), ['lxml', 'xml'])
&gt;&gt;&gt; smallsoup
&lt;?xml version=""1.0"" encoding=""utf-8""?&gt;
&lt;Catalog&gt;
&lt;CMoverMissile id=""HunterSeekerMissile""&gt;
&lt;MotionPhases index=""1""&gt;
&lt;Driver value=""Guidance""/&gt;
&lt;Acceleration value=""3200""/&gt;
&lt;MaxSpeed value=""2.9531""/&gt;
&lt;Clearance value=""0.5""/&gt;
&lt;ClearanceLookahead value=""3""/&gt;
&lt;Outro value=""-4.5,-4.25""/&gt;
&lt;YawPitchRoll value=""MAX""/&gt;
&lt;/MotionPhases&gt;
&lt;MotionPhases index=""2""&gt;
&lt;Driver value=""Guidance""/&gt;
&lt;Acceleration value=""4""/&gt;
&lt;MaxSpeed value=""2.9531""/&gt;
&lt;Clearance value=""0.5""/&gt;
&lt;ClearanceLookahead value=""3""/&gt;
&lt;Outro value=""-2.25,-2""/&gt;
&lt;YawPitchRoll value=""MAX""/&gt;
&lt;/MotionPhases&gt;
&lt;/CMoverMissile&gt;
&lt;/Catalog&gt;
&gt;&gt;&gt; largersoup = BeautifulSoup(largerfile, ['lxml', 'xml'])
&gt;&gt;&gt; largersoup
&lt;?xml version=""1.0"" encoding=""utf-8""?&gt;

&gt;&gt;&gt;

&gt;&gt;&gt; repr(open(largefile, 'r').read())
'\'&lt;?xml version=""1.0"" encoding=""us-ascii""?&gt;\\n&lt;Catalog&gt;\\n&lt;CMoverMissile id=""HunterSeekerMissile""&gt;\\n&lt;MotionPhases index=""1""&gt;\\n&lt;Driver value=""Guidance""/&gt;\\n&lt;Acceleration value=""3200""/&gt;\\n&lt;MaxSpeed value=""2.9531""/&gt;\\n&lt;Clearance value=""0.5""/&gt;\\n&lt;ClearanceLookahead value=""3""/&gt;\\n&lt;Outro value=""-4.5,-4.25""/&gt;\\n&lt;YawPitchRoll value=""MAX""/&gt;\\n&lt;/MotionPhases&gt;\\n&lt;MotionPhases index=""2""&gt;\\n&lt;Driver value=""Guidance""/&gt;\\n&lt;Acceleration value=""4""/&gt;\\n&lt;MaxSpeed value=""2.9531""/&gt;\\n&lt;Clearance value=""0.5""/&gt;\\n&lt;ClearanceLookahead value=""3""/&gt;\\n&lt;Outro value=""-2.25,-2""/&gt;\\n&lt;YawPitchRoll value=""MAX""/&gt;\\n&lt;/MotionPhases&gt;\\n&lt;/CMoverMissile&gt;                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        &lt;/Catalog&gt;\''


note: there are many spaces (which probably won't show up in the browser) between  and \''
",1,2262,"len(soup) returns len(soup.contents) i.e., the number of immediate children (in this case a single child &lt;Catalog&gt;). 

BeautifulSoup fails to parse largerfile so len(soup) == 0
","It turns out the problem lies somewhere with BS4/LXML.
The author of BS4 (BeautifulSoup), recognises the problem (https://groups.google.com/group/beautifulsoup/browse_thread/thread/24a82209aca4c083):


  ""Apparently BS4+lxml won't parse an XML document that's longer than 
  about 550 bytes. I only tested it with small documents. The BS4 
  handler code is not even being called, which makes it hard to debug, 
  but it's not a guarantee the problem is on the lxml side.""


A slight tweak to J.F.Sebastian helpful code sample gives the size at which the code fails:

&gt;&gt;&gt; from bs4 import BeautifulSoup
&gt;&gt;&gt; from itertools import count
&gt;&gt;&gt; for n in count():
    s = ""&lt;a&gt;"" + "" "" * n + ""&lt;/a&gt;""
    nchildren = len(BeautifulSoup(s, 'xml'))
    if nchildren != 1: # broken
       print(len(s)) 
       break

1092


The code processes the xml as expected for a character count of less than or equal to
1091. XML of a string longer than or equal to 1092 usually fails.

UPDATE:
BeautifulSoup 4.0.2 has been released with a workaround: 


  ""This new version works around what appears to be a bug in lxml's 
  XMLParser.feed(), which was preventing BS from parsing XML documents 
  larger than about 512-1024 characters. ""

",
BeautifulSoup strange result,https://stackoverflow.com/questions/61094791,Decode base64 encoded urls using selenium,"I would like to scrape some images from a website. I checked the website and everything seemed pretty easy so I started with plain beautifulsoup. Then I noticed, that images are in strange format, probably base64 related, so I tried to decode it but nothing came out of it. I made a little research and I found suggestions to use selenium, because the image urls may be rendered via javascript. So I tried it with selenium with no success. 

I am trying to get the image url this way:

img = self.browser.execute_script(f""return document.querySelectorAll('picture &gt; img')[{num}]"").get_attribute('src')

There are 24 images on page so I iterate through them (via num). If I debug line by line, several urls render correctly, however, if I just let the code go with no breakpoints I get all urls like this: 

data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7.

I tried to base64 decode it, but it makes no sense to me. And it is also too short to be actual image. Correctly rendered urls show that the images are actually not gifs but jpgs.

I also tried to find the element by css selector (using both pure beautifulsoup and selenium) but the result was the same.

I found this discussion: How to extract img src from web page via lxml in beautifulsoup using python? but it did not help me either. I have not found any dynamic key (although there are similarities - there are multiple sizes of the pictures) and the base64 code is too short to be an actual image preview as mentioned above.

If I inspect element in browser I see correct url. Is there a way I can do the same using some beautiful soup or selenium (or other python framework for scraping)? What is the actual data encoded in base64?
",0,1204,"If you look at the source code of the website ,The images links you are trying to scrape exists in another tag noscript .
you can get them using requests and Beautifulsoup as follows :
import requests
from bs4 import BeautifulSoup as bs
url = 'https://eshop.nobilis.cz/aromaterapie/'
res = requests.get(url,headers={'User-Agent': 'Mozilla/5.0'})

soup = bs(res.content, 'html.parser')

images = soup.select('noscript img')
for img in images:
        img_link = img.get('src')
        img_alt  = img.get('alt')
        print(img_alt , '==&gt;' , img_link)

Output:
Obrázek kategorie Aromaterapie ==&gt; https://cdn.nobilis.cz/image/custom-w1920-h480-crop/content/aromaterapie_3840x960-bb98d24ff24a2c55.jpg
Keramický difuzér ==&gt; https://cdn.nobilis.cz/image/custom-w225-h250/data/persistent/products/4/6/33/keramicky-difuzer__S8Ru.jpg
Keramická destička ==&gt; https://cdn.nobilis.cz/image/custom-w225-h250/data/persistent/products/4/5/31/n1700-kopie__nQwF.jpg
Aroma difuzér ==&gt; https://cdn.nobilis.cz/image/custom-w225-h250/data/persistent/products/4/6/57/t0328-aroma-difuzer__JYKy.jpg
MINI difuzér ==&gt; https://cdn.nobilis.cz/image/custom-w225-h250/data/persistent/products/5/6/86/01-t0330-mini-difuzer__9RjF.jpg
Zen difuzér ==&gt; https://cdn.nobilis.cz/image/custom-w225-h250/data/persistent/products/5/3/20/t0329-zen-difuzer__IBcR.jpg
Náplně do MINI difuzéru 10 ks ==&gt; https://cdn.nobilis.cz/image/custom-w225-h250/data/persistent/products/4/5/7/t0331s__IqbM.jpg
Aromaterapie na cesty ==&gt; https://cdn.nobilis.cz/image/custom-w225-h250/data/persistent/products/4/5/55/s0103-aromaterapie-na-cesty__0hat.jpg
Keramická amforka ==&gt; https://cdn.nobilis.cz/image/custom-w225-h250/data/persistent/products/5/7/13/keramicka-amforka-kopie__bpFN.jpg
Prostorový difuzér éterických olejů ==&gt; https://cdn.nobilis.cz/image/custom-w225-h250/data/persistent/products/4/7/59/t0320__egh5.jpg
Směs éterických olejů Inspirace ==&gt; https://cdn.nobilis.cz/image/custom-w225-h250/data/persistent/products/7/0/57/e1081b-smes-eterickych-oleju-inspirace__YAb1.jpg
Směs éterických olejů Tantra ==&gt; https://cdn.nobilis.cz/image/custom-w225-h250/data/persistent/products/6/4/63/e2006b-smes-eterickych-oleju-tantra__KeIG.jpg
Éterický olej bio Citron ==&gt; https://cdn.nobilis.cz/image/custom-w225-h250/data/persistent/products/7/4/59/b0015b-bio-citron__KvPJ.jpg
Éterický olej Meduňka ==&gt; https://cdn.nobilis.cz/image/custom-w225-h250/data/persistent/products/6/6/94/e1027-medunka-1-ml__svsg.jpg
Éterický olej Bergamot ==&gt; https://cdn.nobilis.cz/image/custom-w225-h250/data/persistent/products/7/0/27/e0008b-etericky-olej-bergamot__gab2.jpg
Éterický olej Grapefruit ==&gt; https://cdn.nobilis.cz/image/custom-w225-h250/data/persistent/products/6/8/71/e0024b-etericky-olej-grapefruit__J85r.jpg
Éterický olej bio Rozmarýn ==&gt; https://cdn.nobilis.cz/image/custom-w225-h250/data/persistent/products/5/8/12/b0016b-bio-rozmaryn__POvK.jpg
Směs éterických olejů Druhý dech ==&gt; https://cdn.nobilis.cz/image/custom-w225-h250/data/persistent/products/7/3/27/e2002b-smes-eterickych-oleju-druhy-dech__dPzL.jpg
Éterický olej Šalvěj muškátová ==&gt; https://cdn.nobilis.cz/image/custom-w225-h250/data/persistent/products/5/8/93/e0045b-etericky-olej-salvej-muskatova__wAFx.jpg
Éterický olej Cypřiš ==&gt; https://cdn.nobilis.cz/image/custom-w225-h250/data/persistent/products/6/3/55/e0017b-etericky-olej-cypris__RxDS.jpg
Éterický olej Skořice, kůra ==&gt; https://cdn.nobilis.cz/image/custom-w225-h250/data/persistent/products/7/0/60/e0074b-etericky-olej-skorice-kura__tK0h.jpg
Éterický olej Geranium ==&gt; https://cdn.nobilis.cz/image/custom-w225-h250/data/persistent/products/6/2/71/e1057b-etericky-olej-geranium__dCRQ.jpg
Éterický olej Konopí ==&gt; https://cdn.nobilis.cz/image/custom-w225-h250/data/persistent/products/5/7/67/e0154h-konopi-1-ml__b2oW.jpg
Růže v jojobovém oleji ==&gt; https://cdn.nobilis.cz/image/custom-w225-h250/data/persistent/products/6/7/24/n1010c-ruze-v-jojobe-20-ml__jzLM.jpg
Éterický olej bio Tymián linalol ==&gt; https://cdn.nobilis.cz/image/custom-w225-h250/data/persistent/products/6/3/82/b0005a-bio-tymian-linalol__8IFa.jpg

",,
BeautifulSoup strange result,https://stackoverflow.com/questions/52593716,"BeautifulSoup can&#39;t get all source code of the page, only a little lines","from urllib.request import urlopen
from bs4 import BeautifulSoup

page_origin = urlopen(""https://stackoverflow.com"")
page_html = page_origin.read()
page_origin.close()
print(page_html)



  The result is the full html code of https://stackoverflow.com. It works fine. Because it's too long enough, I don't paste it on.


The problem is with BeautifulSoup. I add two lines' code to use BeautifulSoup to analyze the html. Strange things happened. It didn't work at all.

from urllib.request import urlopen
from bs4 import BeautifulSoup

page_origin = urlopen(""https://stackoverflow.com"")
page_html = page_origin.read()
page_origin.close()
# print(page_html)

page_soup = BeautifulSoup(page_html, features=""lxml"", from_encoding=""gbk"")
print(page_soup)


The result is very very simple.

&lt;!DOCTYPE html&gt;
&lt;html class=""html__responsive""&gt;
 &lt;head&gt;
  &lt;title&gt;
   Stack Overflow - Where Developers Learn, Share, &amp;amp; Build Careers
  &lt;/title&gt;
  &lt;link href=""https://cdn.sstatic.net/Sites/stackoverflow/img/favicon.ico?v=4f32ecc8f43d"" rel=""shortcut icon""/&gt;
  &lt;link href=""https://cdn.sstatic.net/Sites/stackoverflow/img/apple-touch-icon.png?v=c78bd457575a"" rel=""apple-touch-icon image_src""/&gt;
  &lt;link href=""/opensearch.xml"" rel=""search"" title=""Stack Overflow"" type=""application/opensearchdescription+xml""/&gt;
 &lt;/head&gt;
&lt;/html&gt;


It's not the full code of html and I can't analyze it at all.

Please help me, I debug it for too much time. Thanks.
",0,1650,"This gives the full source code for me:

import requests

from bs4 import BeautifulSoup

r = requests.get('https://stackoverflow.com/')

soup = BeautifulSoup(r.text, 'lxml')
print(soup)

",,
BeautifulSoup strange result,https://stackoverflow.com/questions/43502002,Python Beautifulsoup strange results,"I was trying to pull the heading of all the products on an amazon search.
It works, but the results are just giving me back the header and footer amazon links.

If I inspect the elements on amazon's source it lookslike the product titles are '' tags wrapped in anchors. However, this doesn't appear to be the case when trying to scrape the data.

import requests
import re
from bs4 import BeautifulSoup

def adverts_trade(max_pages):
    page = 1
    while page &lt;= max_pages:
        url = 'https://www.amazon.co.uk/s/ref=sr_pg_2?rh=n%3A560798%2Cn%3A560834%2Ck%3Acanon+lenses&amp;page=' + str(page) + '&amp;keywords=canon+lenses&amp;ie=UTF8'
        source_code = requests.get(url)
        plain_text = source_code.text
        soup = BeautifulSoup(plain_text, ""html.parser"")
        for link in soup.findAll('a'):
            #href = link.find('h2').get_text()
            print(link)
        page += 1

adverts_trade(10)

",0,214,"Amazon doesn't like you scraping data from them.  If you add this line to your code:

print(plain_text)


You'll see the following:

&gt;     &lt;!--
&gt;             To discuss automated access to Amazon data please contact api-services-support@amazon.com.
&gt;             For information about migrating to our APIs refer to our Marketplace APIs at
&gt; https://developer.amazonservices.co.uk/ref=rm_5_sv, or our Product
&gt; Advertising API at
&gt; https://affiliate-program.amazon.co.uk/gp/advertising/api/detail/main.html/ref=rm_5_ac
&gt; for advertising use cases.
&gt;     --&gt;


Don't expect for most sites to be able to simply use requests and bs4 to scrape data.   Either use their API or consider Selenium or some other scraping tool that can drive an actual browser.
","Are you trying to get the title from an  tag's title attribute? Or are you trying to get the title from the H2 child of a  tag? 

If you are going the first way then try print(link['title']) instead print the whole tag. In beautifulSoup you can access the attributes of a catched anchor as a normal dictionary.
",
BeautifulSoup strange result,https://stackoverflow.com/questions/10224287,Parse the content of a HTML table - But iframe is the source of problems,,-2,1589,,,
BeautifulSoup strange issue,https://stackoverflow.com/questions/47026542,Setting Mailchimp campaign content html not working,"I tried to update my campaign html content using mailchimp api:

/campaigns/{campaign_id}/content


You can find more information about this api here: https://developer.mailchimp.com/documentation/mailchimp/reference/campaigns/content/#

Before sending a campaign, I tried to get campaign content html, modified it and then set campaign content html using above api. I just simply use BeautifulSoup to append a new tag to content body:

content.body.append(BeautifulSoup('&lt;p&gt;Mailchimp is freaking shittttt&lt;/p&gt;'))


Then, some interesting things happen, the first campaign I created, it works fine, the tag added appears in my email. But, then the sub-sequence campaigns not working anymore, the tag added not appearing.

I observed something strange on my mailchimp campaign site, even though I set campaign html content, only Plain-Text Email gets changed (HTML Source still the old version) for both working and not working campaign.



Anyone got this issue before?
",1,2105,"I had a similar issue and I had to take a slightly different approach to solve it. According to this answer by Joel H., ""MailChimp doesn't allow updating the campaign's HTML content because the campaign type is based on a template. In order to update the HTML content, the campaign has to be set to custom HTML instead of a template.""

That solution didn't suit me but it led me to another solution: creating a template, creating editable content areas within that template, and then using the API to retrieve and edit the text in those content areas.

Here is an attempt at adapting my code to solve your problem. I'm using Python 3 and the mailchimp3 client.

default_footer_content = client.templates.default_content.all(template_id=TEMPLATE_ID)['sections']['SECTION_NAME']
new_footer_content = default_footer_content.replace(PLACEHOLDER, 'Mailchimp is freaking shittttt')
client.campaigns.content.update(campaign_id=CAMPAIGN_ID, data={'template': {'id': TEMPLATE_ID, 'sections': {'SECTION_NAME': new_footer_contennt}}})


Some pointers on the above code:


You can find TEMPLATE_ID with the API or simply by copying the numbers at the end of the URL when editing the template in the web interface
You define SECTION_NAME by placing 'mc:edit=""SECTION NAME""' in the appropriate place in the template
I've used .replace() rather than .append() so you will need to put PLACEHOLDER or similar at the appropriate place in the template


I hope that helps, happy to modify my answer if it needs more clarification. This is my first answer on Stack Overflow so constructive criticism appreciated :) 
",,
BeautifulSoup strange issue,https://stackoverflow.com/questions/26895813,PhantomJS does not navigate to the next page after clicking a link,"I am having a strange issue with PhantomJS or may be I am newbie. I am trying to login on NewEgg.com via Selenium by using PhantomJS. I am using Python for it. Issue is, when I use Firefox as a driver it works well but as soon as I set PhantomJS as a driver it does not go to next page hence give message:

Exception Message: u'{""errorMessage"":""Unable to find element with id \'UserName\'"",""request"":{""headers"":{""Accept"":""application/json"",""Accept-Encoding"":""identity"",""Connection"":""close"",""Content-Length"":""89"",""Content-Type"":""application/json;charset=UTF-8"",""Host"":""127.0.0.1:55372"",""User-Agent"":""Python-urllib/2.7""},""httpVersion"":""1.1"",""method"":""POST"",""post"":""{\\""using\\"": \\""id\\"", \\""sessionId\\"": \\""aaff4c40-6aaa-11e4-9cb1-7b8841e74090\\"", \\""value\\"": \\""UserName\\""}"",""url"":""/element"",""urlParsed"":{""anchor"":"""",""query"":"""",""file"":""element"",""directory"":""/"",""path"":""/element"",""relative"":""/element"",""port"":"""",""host"":"""",""password"":"""",""user"":"""",""userInfo"":"""",""authority"":"""",""protocol"":"""",""source"":""/element"",""queryKey"":{},""chunks"":[""element""]},""urlOriginal"":""/session/aaff4c40-6aaa-11e4-9cb1-7b8841e74090/element""}}' ; Screenshot: available via screen 


The reason I found after taking screenshot that phantom could not navigate the page and script got finished. How do I sort this out? Code Snippet I tried given below:

import requests
from bs4 import BeautifulSoup
from time import sleep
from selenium import webdriver
import datetime

my_username = ""user@mail.com""
my_password = ""password""

driver = webdriver.PhantomJS('/Setups/phantomjs-1.9.7-macosx/bin/phantomjs')
firefox_profile = webdriver.FirefoxProfile()
#firefox_profile.set_preference('permissions.default.stylesheet', 2)
firefox_profile.set_preference('permissions.default.image', 2)
firefox_profile.set_preference('dom.ipc.plugins.enabled.libflashplayer.so', 'false')
#driver = webdriver.Firefox(firefox_profile)
driver.set_window_size(1120, 550)
driver.get('http://newegg.com')
driver.find_element_by_link_text('Log in or Register').click()
driver.save_screenshot('screen.png')


I even put sleep but it is not making any difference.
",1,742,"I experienced this with PhantomJS when the content type of the second page is not correct. A normal browser would just interpret the content dynamically, but Phantom just dies, silently.
",,
BeautifulSoup strange issue,https://stackoverflow.com/questions/54638750,A website that I am trying to scrape is changing tags/IDs based on if it detects a crawler. Is there a way to avoid this?,"I am trying to write a basic web scraper that looks through a forum, goes into each post, then checks to see if the post has any github links, storing those links. I am doing this as a part of my research to see how people use and implement Smart Device routines. 

I'm fairly new to web scraping, and have been using BeautifulSoup, but I've run into a strange issue. First, my program:

from bs4 import BeautifulSoup
import requests
from user_agent import generate_user_agent

url = 'https://community.smartthings.com/c/projects-stories'

headers = {'User-Agent': generate_user_agent(device_type=""desktop"", os=('linux'))}
page_response = requests.get(url, timeout=5, headers=headers)

page = requests.get(url, timeout = 5)
#print(page.content)
if page.status_code == 200:
    print('URL: ', url, '\nRequest Successful!')
content = BeautifulSoup(page.content, 'html.parser')
print(content.prettify())

project_url = []
for i in content:
    project_url += content.find_all(""/div"", class_=""a href"")
print(project_url)


What I'm trying to do right now is simply collect all the url links to each individual post on the website. When I try to do this, it returns an empty list. After some experimentation in trying to pick out a specific url based on it's ID, I found that while the ID of each post does not seem to change every time the page is reloaded, it DOES change if the website detects that a scraper is being used. I believe this considering that when the contents of the webpage is printed to the console, at the end of the HTML data, there is a section that reads:

  &lt;!-- include_crawler_content? --&gt;
  &lt;/div&gt;
  &lt;footer class=""container""&gt;
   &lt;nav class=""crawler-nav"" itemscope="""" itemtype=""http://schema.org/SiteNavigationElement""&gt;
    &lt;a href=""/""&gt;
     Home
    &lt;/a&gt;
    &lt;a href=""/categories""&gt;
     Categories
    &lt;/a&gt;
    &lt;a href=""/guidelines""&gt;
     FAQ/Guidelines
    &lt;/a&gt;
    &lt;a href=""/tos""&gt;
     Terms of Service
    &lt;/a&gt;
    &lt;a href=""/privacy""&gt;
     Privacy Policy
    &lt;/a&gt;
   &lt;/nav&gt;


The website seems to detect the crawler and change the navigation based on that. I've tried generating a new user_agent to trick it, but I've had no luck.

Any ideas?
",0,1342,"You could potentially start by using 

content.findChildren('a')


and then go from there, sorting through the results for the links you want.
",,
BeautifulSoup strange issue,https://stackoverflow.com/questions/15174510,a strange issue when trying to analysis HTML with beautifulsoup,"i'm trying to write some python codes to gather music charts data from official websites, but i get in trouble when gathering billboard's data. i choose beautifulsoup to handle the HTML

my ENV:
python-2.7
beautifulsoup-3.2.0

first i analysis the HTML 

&gt;&gt;&gt; import BeautifulSoup, urllib2, re
&gt;&gt;&gt; html = urllib2.urlopen('http://www.billboard.com/charts/hot-100?page=1').read()
&gt;&gt;&gt; soup = BeautifulSoup.BeautifulSoup(html)


then i try to gather data what i want, e.g., the artist name

HTML:

&lt;div class=""listing chart_listing""&gt;

&lt;article id=""node-1491420"" class=""song_review no_category chart_albumTrack_detail no_divider""&gt;
  &lt;header&gt;
    &lt;span class=""chart_position position-down""&gt;11&lt;/span&gt;
            &lt;h1&gt;Ho Hey&lt;/h1&gt;
        &lt;p class=""chart_info""&gt;
      &lt;a href=""/artist/418560/lumineers""&gt;The Lumineers&lt;/a&gt;            &lt;br&gt;
      The Lumineers          &lt;/p&gt;


artist name is The Lumineers

&gt;&gt;&gt; print str(soup.find(""div"", {""class"" : re.compile(r'\bchart_listing')})\
... .find(""p"", {""class"":""chart_info""}).a.string)
Traceback (most recent call last):
  File ""&lt;stdin&gt;"", line 1, in &lt;module&gt;
AttributeError: 'NoneType' object has no attribute 'find'


NoneType! seems it can't grep the data what i want, maybe my rule is wrong, so i try to grep  some basic tag instead.

&gt;&gt;&gt; print str(soup.find(""div""))
None
&gt;&gt;&gt; print str(soup.find(""a""))
None
&gt;&gt;&gt; print str(soup.find(""title""))
&lt;title&gt;The Hot 100 : Page 2  | Billboard&lt;/title&gt;
&gt;&gt;&gt; print str(soup)
......entire HTML.....


i'm confusing, why can't it grep the basic tag like div, a? they indeed there. what's wrong with my codes? there is nothing wrong when i try to analysis other chart with these.
",0,224,"This seems to be a Beautifulsoup 3 issue. If you prettify() the output:

from BeautifulSoup import BeautifulSoup as soup3
import urllib2, re

html = urllib2.urlopen('http://www.billboard.com/charts/hot-100?page=1').read()
soup = soup3(html)
print soup.prettify()


you can see at the end of the output:

        &lt;script type=""text/javascript"" src=""//assets.pinterest.com/js/pinit.js""&gt;&lt;/script&gt;
&lt;/body&gt;
&lt;/html&gt;
  &lt;/script&gt;
 &lt;/head&gt;
&lt;/html&gt;


With two html end tags, it looks like BeautifulSoup3 is confused by the Javascript stuff in this data.

If you use:

from bs4 import BeautifulSoup as soup4
import urllib2, re

html = urllib2.urlopen('http://www.billboard.com/charts/hot-100?page=1').read()
soup = soup4(html)
print str(soup.find(""div"", {""class"" : re.compile(r'\bchart_listing')}).find(""p"", {""class"":""chart_info""}).a.string)


You get 'The Lumineers' as output.

If you cannot switch to bs4, I suggest you write out the html variable to a file out.txt, then change the script to read in in.txt and copy the output to the input and cutting away chunks.

from BeautifulSoup import BeautifulSoup as soup3
import re

html = open('in.txt').read()
soup = soup3(html)
print str(soup.find(""div"", {""class"" : re.compile(r'\bchart_listing')}).find(""p"", {""class"":""chart_info""}).a.string)


My first guess was to remove the &lt;head&gt; ... &lt;/head&gt; and that worked wonders.

After that you can solve that programmatically:

from BeautifulSoup import BeautifulSoup as soup3
import urllib2, re

htmlorg = urllib2.urlopen('http://www.billboard.com/charts/hot-100?page=1').read()
head_start = htmlorg.index('&lt;head')
head_end = htmlorg.rindex('&lt;/head&gt;')
head_end = htmlorg.index('&gt;', head_end)
html = htmlorg[:head_start] + htmlorg[head_end+1:]
soup = soup3(html)
print str(soup.find(""div"", {""class"" : re.compile(r'\bchart_listing')}).find(""p"", {""class"":""chart_info""}).a.string)

",,
BeautifulSoup inconsistent behavior,https://stackoverflow.com/questions/32644408,beautifulSoup inconsistent behavior,"I am completely puzzled by the behavior of the following HTML-scraping code that I wrote in two different environments and need help finding the root cause of this discrepancy.

import sys
import bs4
import md5
import logging
from urllib2 import urlopen
from platform import platform

# Log particulars of the environment
logging.warning(""OS platform is %s"" %platform())
logging.warning(""Python version is %s"" %sys.version)
logging.warning(""BeautifulSoup is at %s and its version is %s"" %(bs4.__file__, bs4.__version__))

# Open web-page and read HTML
url = 'http://www.ncbi.nlm.nih.gov/Traces/wgs/?val=JXIG&amp;size=all'
response = urlopen(url)
html = response.read()

# Calculate MD5 to ensure that the same string was downloaded
print ""MD5 sum for html string downloaded is %s"" %md5.new(html).hexdigest()

# Make beautiful soup
soup = bs4.BeautifulSoup(html, 'html')
contigsTable = soup.find(""table"", {""class"" : ""zebra""})
contigs = []

# Parse table in soup to find all records
for row in contigsTable.findAll('tr'):
    column = row.findAll('td')
    if len(column) &gt; 2:
        contigs.append(column[1])

# Expect identical results on any machine that this is run
print ""Number of contigs identified is %s"" %len(contigs)


On machine 1, this runs to return:

WARNING:root:OS platform is Linux-3.10.10-031010-generic-x86_64-with-Ubuntu-12.04-precise   
WARNING:root:Python version is 2.7.3 (default, Jun 22 2015, 19:33:41)  
[GCC 4.6.3]  
WARNING:root:BeautifulSoup is at /usr/local/lib/python2.7/dist-packages/bs4/__init__.pyc and its version is 4.3.2  
MD5 sum for html string downloaded is ca76b381df706a2d6443dd76c9d27adf  

Number of contigs identified is 630  


On machine 2, this very identical code runs to return:

WARNING:root:OS platform is Linux-2.6.32-431.46.2.el6.nersc.x86_64-x86_64-with-debian-6.0.6
WARNING:root:Python version is 2.7.4 (default, Apr 17 2013, 10:26:13) 
[GCC 4.6.3]
WARNING:root:BeautifulSoup is at /global/homes/i/img/.local/lib/python2.7/site-packages/bs4/__init__.pyc and its version is 4.3.2
MD5 sum for html string downloaded is ca76b381df706a2d6443dd76c9d27adf

Number of contigs identified is 462




The number of contigs calculated is different.  Please note that the same code parses an HTML table to yield different results on two different environments that are not strikingly different from each other and unfortunately leading to this production nightmare. Manual inspection confirms that the results returned on Machine 2 are incorrect, but has so far been impossible to explain.

Does anyone have similar experience? Do you notice anything wrong with this code or should I stop trusting BeautifulSoup altogether?
",2,1172,"You are experiencing the differences between parsers that BeaufitulSoup chooses automatically for the ""html"" markup type you've specified. Which parser is picked up depends on what modules are available in the current Python environment:


  If you don’t specify anything, you’ll get the best HTML parser that’s
  installed. Beautiful Soup ranks lxml’s parser as being the best, then
  html5lib’s, then Python’s built-in parser.


To have a consistent behavior across the platforms, be explicit:

soup = BeautifulSoup(html, ""html.parser"")
soup = BeautifulSoup(html, ""html5lib"")
soup = BeautifulSoup(html, ""lxml"")


See also: Installing a parser.
",,
BeautifulSoup inconsistent output,https://stackoverflow.com/questions/49739591,Python BeautifulSoup with &quot;lxml&quot; parser breaks long strings into characters,"I noticed a bizarre inconsistency between how Python [3.6.5] BeautifulSoup [4.6.0] with ""lxml"" [4.2.1] parser handles long bytes objects vs long strings. (Apparently, ""long"" is &gt;16,384=2**14 characters or bytes.) 

For example, I download the text of Othello from the MIT website and feed it to BS both in the raw (bytes) form and after decoding to a string. Both objects have the same length, because there are no multibyte characters in the document.

from bs4 import BeautifulSoup 
import urllib

url = ""http://shakespeare.mit.edu/othello/full.html""
html_raw = urllib.request.urlopen(url).read()
html_str = urllib.request.urlopen(url).read().decode(""iso-8859-1"")

type(html_raw), len(html_raw)
#(&lt;class 'bytes'&gt;, 304769)
type(html_str), len(html_str)
#(&lt;class 'str'&gt;, 304769)


The resulting soup is the same for shorter strings/bytes but differs for longer strings/bytes. Namely, the soup produced from a string abruptly starts treating words as separate characters, while the soup produced from bytes correctly handles the whole file:

BeautifulSoup(html_raw[:16410], ""lxml"")
#... &lt;i&gt;Enter OTHELLO, IAGO, and Attendants with torches&lt;/i&gt;
#&lt;/blockquote&gt;
#&lt;a&gt;&lt;/a&gt;&lt;/body&gt;&lt;/html&gt;
BeautifulSoup(html_str[:16410], ""lxml"")
#... &lt;i&gt;Enter OTHELLO, IAGO, and Attendants with torch   e   s   /   i   &amp;gt;   
#   /   b   l   o   c   k   q   u   o   t   e   &amp;gt;      
#
#   A   &lt;/i&gt;&lt;/blockquote&gt;&lt;/body&gt;&lt;/html&gt;


This holds both for the subset of the document (above) and for the whole document:

BeautifulSoup(html_raw, ""lxml"")
#...
#&lt;p&gt;&lt;i&gt;Exeunt&lt;/i&gt;&lt;/p&gt;
#&lt;/blockquote&gt;&lt;/body&gt;
#&lt;/html&gt;

BeautifulSoup(html_str, ""lxml"")
#...
#   p   &amp;gt;   i   &amp;gt;   E   x   e   u   n   t   /   i   &amp;gt;   /   p   &amp;gt;   
#   /   h   t   m   l   &amp;gt;   
#   
#   
#   &lt;/i&gt;&lt;/blockquote&gt;&lt;/body&gt;&lt;/html&gt;


There is no difference between the outputs when I use ""html.parser"". 

Is this a bug in the implementation of BS? Or am I violating some undocumented (or documented?) assumptions?
",7,355,"Not because file size, The problem maybe only happen in Linux because in Windows it work fine. it is because the html has character set of windows-1252, adding .encode() will solve the problem

soup_raw = BeautifulSoup(html_raw, ""lxml"").encode(""iso-8859-1"")

soup_str = BeautifulSoup(html_str.encode(""iso-8859-1""), ""lxml"")

",,
BeautifulSoup inconsistent output,https://stackoverflow.com/questions/43243307,Find a value by substring in a nested dictionary,"Just to give my problem a context: I am writing a Django webapp that includes several applications. One of them is used to display articles from RSS feeds. For now, I was only displaying the link, source and description. I want to add thumbnails to these articles. 
I'm trying to grab these thumbnails for any RSS or ATOM feed. Theses feeds are for some parts (e.g. images) constructed in totally arbitrary ways. Since I don't want to write a specific script for every feed on the Web, my idea is to look for "".jpg"", "".png"" substrings in every article I fetch and get that URL.
Getting from RSS or ATOM feeds to articles is well handled by the python Feedparser module, and outputs this for example:

 {'guidislink': False,
  'href': '',
  'id': 'http://www.bbc.co.uk/sport/football/39426760',
  'link': 'http://www.bbc.co.uk/sport/football/39426760',
  'links': [{'href': 'http://www.bbc.co.uk/sport/football/39426760',
             'rel': 'alternate',
             'type': 'text/html'}],
  'media_thumbnail': [{'height': '576',
                       'url': 'http://c.files.bbci.co.uk/44A9/production/_95477571_joshking2.jpg',
                       'width': '1024'}],
  'published': 'Wed, 05 Apr 2017 21:49:14 GMT',
  'published_parsed': time.struct_time(tm_year=2017, tm_mon=4, tm_mday=5, tm_hour=21, tm_min=49, tm_sec=14, tm_wday=2, tm_yday=95, tm_isdst=0),
  'summary': 'Joshua King scores a dramatic late equaliser for Bournemouth as '
             'Liverpool drop two crucial points at Anfield.',
  'summary_detail': {'base': 'http://feeds.bbci.co.uk/news/rss.xml',
                     'language': None,
                     'type': 'text/html',
                     'value': 'Joshua King scores a dramatic late equaliser '
                              'for Bournemouth as Liverpool drop two crucial '
                              'points at Anfield.'},
  'title': 'Liverpool 2-2 Bournemouth',
  'title_detail': {'base': 'http://feeds.bbci.co.uk/news/rss.xml',
                   'language': None,
                   'type': 'text/plain',
                   'value': 'Liverpool 2-2 Bournemouth'}}


Here, http://c.files.bbci.co.uk/44A9/production/_95477571_joshking2.jpg is somewhere nested in lists and dictionaries. While I know how to access it in this specific case, the structures of feeds widely vary. Mainly:


The dictionary key holding the url is not always the same
The 'deepness' of where the url might be nested is not always the same


However, what is almost always the case is that an url with an image extension is the thumbnail of that article. How do I get that url? 

To frame it out a little more, for now I use helper functions (based on the Feedparser module) that processes a feeds context variable, which is a dictionary, usable in my templates. I do the looping and displaying of title, description etc directly in my templates, since they are consistently a part of that dictionary thanks to feedparser:

...
{% for feed in feeds %}
  &lt;h3&gt;{{ feed.feed.title }}&lt;/h3&gt;
  {% for entry in feed.entries %}
...


On the backend :

def parse_feeds(urls):
    parsed_feeds = []
    for url in urls:
        parsed_feed = feedparser.parse(url)
        parsed_feeds.append(parsed_feed)
    return parsed_feeds

class IndexView(generic.ListView):
    template_name = 'publisher/index.html'

    def get_context_data(self, **kwargs):
        context = super(IndexView,self).get_context_data(**kwargs)
        reacted_feeds = RSSArticle.objects.all()
        context['reacted_feeds'] = reacted_feeds
        parsed_feeds = parse_feeds(urls)
        delete_existing_entries(parsed_feeds)
        context['feeds'] = parsed_feeds
        return context


So basically every time you call that IndexView, you get the list of all articles from the feeds you subscribed to. That's where I want to include the image, which are not provided by Feedparser due to the inconsistent nature of their location in feeds. 

If I want to include these pictures, at a macro level I basically have two solutions:


Writing something in addition to the existing system, but that might hurt performance because of too many things having to happen at the same time
Rewriting the whole thing, which might also hurt performance and consistency because I don't take advantage of Feedparser's power anymore


Maybe I should just keep the raw XML and try my luck with Beautifulsoup instead of translating to a dictionary with Feedparser.

PS : here is another example where the image is located somewhere else.

{'guidislink': False,
 'id': 'http://www.lemonde.fr/tiny/5106451/',
 'link': 'http://www.lemonde.fr/les-decodeurs/article/2017/04/05/presidentielle-les-grands-clivages-qui-divisent-les-onze-candidats_5106451_4355770.html?xtor=RSS-3208',
 'links': [{'href': 'http://www.lemonde.fr/les-decodeurs/article/2017/04/05/presidentielle-les-grands-clivages-qui-divisent-les-onze-candidats_5106451_4355770.html?xtor=RSS-3208',
            'rel': 'alternate',
            'type': 'text/html'},
           {'href': 'http://s1.lemde.fr/image/2017/04/05/644x322/5106578_3_0f2b_sur-le-plateau-du-debat-de-bfmtv-et-cnews_0e90a3db44861847870cfa1e4c3793b1.jpg',
            'length': '40057',
            'rel': 'enclosure',
            'type': 'image/jpeg'}],
 'published': 'Wed, 05 Apr 2017 17:02:38 +0200',
 'published_parsed': time.struct_time(tm_year=2017, tm_mon=4, tm_mday=5, tm_hour=15, tm_min=2, tm_sec=38, tm_wday=2, tm_yday=95, tm_isdst=0),
 'summary': 'Protection sociale, Europe, identité… Avec leurs programmes, les '
            'proximités idéologiques entre candidats bousculent de plus en '
            'plus le traditionnel axe «\xa0gauche-droite\xa0».',
 'summary_detail': {'base': 'http://www.lemonde.fr/rss/une.xml',
                    'language': None,
                    'type': 'text/html',
                    'value': 'Protection sociale, Europe, identité… Avec leurs '
                             'programmes, les proximités idéologiques entre '
                             'candidats bousculent de plus en plus le '
                             'traditionnel axe «\xa0gauche-droite\xa0».'},
 'title': 'Présidentielle\xa0: les grands clivages qui divisent les onze '
          'candidats',
 'title_detail': {'base': 'http://www.lemonde.fr/rss/une.xml',
                  'language': None,
                  'type': 'text/plain',
                  'value': 'Présidentielle\xa0: les grands clivages qui '
                           'divisent les onze candidats'}}

",0,975,"I wrote a solution based on this snippet.

def get_image_url(substring, dictionary):
    for key, value in dictionary.items():
        # try is for handling Booleans
        try:
            if substring in value:
                yield value
            elif isinstance(value, dict):
                for result in get_image_url(substring, value):
                    yield result
            elif isinstance(value, list):
                for list_item in value:
                    for result in get_image_url(substring, list_item):
                        yield result
        except:
            pass

&gt;&gt;&gt; list(get_image_url('.jpg', article_dict))
&gt;&gt;&gt; ['https://static01.nyt.com/images/2017/04/09/us/10OBAMA-alt/10OBAMA-alt-moth.jpg']


PS : while it does not answer the exact question of finding a value in a nested dictionary, I found out that a good way to get images for articles from RSS feeds in a consistent manner is simply to follow back the URL to the original article, parse the HTML and search for the og:image tag.
","If all you need is the thumbnail, I would think that the easy way would be to ignore everything else, and simply search every value string for the desired tail.  There are plenty of links to help you traverse the structure, should you care to do that, but I'd turn it into a string and then parse that.

Your trigger is a colon followed by white-space and a quotation mark.  Grab what's between the quotation marks.  Call that value

extensions = ["".jpg"", "".png""]
...
if value[-4:] in extensions:
    # You've found a desired URL


Does that get you moving?
",
BeautifulSoup inconsistent result,https://stackoverflow.com/questions/43138032,Web parsing with python beautifulsoup producing inconsistent result,"I am trying to parse the table of this site. I am using python beautiful soup to do that. While it's producing correct output in my Ubuntu 14.04 machine, it's producing wrong output in my friend's windows machine. I am pasting the code snippet here:
from bs4 import BeautifulSoup

def buildURL(agi, families):
    #agi and families contains space seperated string of genes and families
    genes = agi.split("" "")
    families = families.split("" "")
    base_url = ""http://www.athamap.de/search_gene.php""

    url = base_url

    if len(genes):
        url = url + ""?agi=""
        for i, gene in enumerate(genes):
            if i&gt;0:
                url = url + ""%0D%0A""
            url = url + gene

    url = url + ""&amp;upstream=-500&amp;downstream=50&amp;restriction=0&amp;sortBy1=gen&amp;sortBy2=fac&amp;sortBy3=pos""

    for family in families:
        family = family.replace(""/"", ""%2F"")
        url = url +""&amp;familySelected%5B""+family+""%5D=on""
    url = url + ""&amp;formSubmitted=TRUE""
    return url

def fetch_html(agi, families):

    url = buildURL(agi, families)
    response = requests.get(url)
    
    soup = BeautifulSoup(str(response.text), ""lxml"")

    divs = soup.find_all('div')

    seldiv = """"
    for div in divs:
        try:
            if div[""id""] == ""geneAnalysisDetail"":
                '''
                    This div contains interesting data
                '''
                seldiv = div
        except:
            None

    return seldiv

def parse(seldiv):
    soup = seldiv
    rows= soup.find_all('tr')

    attributes =[""Gene"", ""Factor"", ""Family"", ""Position"", ""Relative orientation"", ""Relative Distance"", ""Max score"", ""Threshold Score"", ""Score""]

    print attributes
    save_rows = []
    for i in range(2, len(rows)):
        cols = rows[i].find_all('td')
        lst = []
        for j,col in enumerate(cols):
            if j==0:
                lst.append(re.sub('&lt;[^&lt;]+?&gt;', '',str(col.contents[1].contents[0])))
            elif j==1:
                lst.append(str(col.contents[1].contents[0]))
            elif j==2:
                lst.append(str(col.contents[0]))
            elif j==3:
                lst.append(str(col.contents[1].contents[0]))
            else:
                lst.append(str(col.contents[0]))
        save_rows.append(lst)
    return save_rows

Any idea what could go wrong here? I have tried with and without lxml.
Thanks in advance.
",2,965,"One possibility is that you didn't add user agent for the requests. Different user agent will get different result sometime, especially from weird website. Here is a list of all possible agents, just choose one. It doesn't have to be your machine

    USER_AGENTS = [
'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/56.0.2924.87 Safari/537.36',
'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/56.0.2924.87 Safari/537.36',
'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/56.0.2924.87 Safari/537.36',
'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_3) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/56.0.2924.87 Safari/537.36',
'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_3) AppleWebKit/602.4.8 (KHTML, like Gecko) Version/10.0.3 Safari/602.4.8',
'Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/56.0.2924.87 Safari/537.36',
'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/56.0.2924.87 Safari/537.36',
'Mozilla/5.0 (Windows NT 10.0; WOW64; rv:52.0) Gecko/20100101 Firefox/52.0',
'Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/56.0.2924.87 Safari/537.36',
'Mozilla/5.0 (Windows NT 6.1; WOW64; rv:52.0) Gecko/20100101 Firefox/52.0',
'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/56.0.2924.87 Safari/537.36',
'Mozilla/5.0 (Windows NT 10.0; WOW64; rv:51.0) Gecko/20100101 Firefox/51.0',
'Mozilla/5.0 (Windows NT 6.1; WOW64; Trident/7.0; rv:11.0) like Gecko',
'Mozilla/5.0 (Windows NT 6.3; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/56.0.2924.87 Safari/537.36',
'Mozilla/5.0 (Windows NT 6.1; WOW64; rv:51.0) Gecko/20100101 Firefox/51.0',
'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:52.0) Gecko/20100101 Firefox/52.0',
'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/56.0.2924.87 Safari/537.36',
'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/51.0.2704.79 Safari/537.36 Edge/14.14393',
'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_6) AppleWebKit/602.4.8 (KHTML, like Gecko) Version/10.0.3 Safari/602.4.8',
'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_3) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/57.0.2987.98 Safari/537.36',
'Mozilla/5.0 (Windows NT 6.3; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/56.0.2924.87 Safari/537.36',
'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/57.0.2987.110 Safari/537.36',
'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/57.0.2987.98 Safari/537.36',
'Mozilla/5.0 (Macintosh; Intel Mac OS X 10.12; rv:52.0) Gecko/20100101 Firefox/52.0',
'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:51.0) Gecko/20100101 Firefox/51.0',
'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/57.0.2987.98 Safari/537.36',
'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/57.0.2987.110 Safari/537.36',
'Mozilla/5.0 (Windows NT 10.0; WOW64; Trident/7.0; rv:11.0) like Gecko',
'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_2) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/56.0.2924.87 Safari/537.36',
'Mozilla/5.0 (Macintosh; Intel Mac OS X 10.12; rv:51.0) Gecko/20100101 Firefox/51.0',
'Mozilla/5.0 (X11; Linux x86_64; rv:45.0) Gecko/20100101 Firefox/45.0'
]

","You can parse the table this way and should work well on both machine. buildURL function should be left unchanged.

import requests
from bs4 import BeautifulSoup

def fetch_html(url):

    response = requests.get(url)

    soup = BeautifulSoup(response.text, ""lxml"")

    seldiv = soup.find(""div"", id=""geneAnalysisDetail"")

    return seldiv

def parse(url):
    soup = fetch_html(url)
    rows= soup.find_all(""tr"")

    attributes = [""Gene"", ""Factor"", ""Family"", ""Position"", ""Relative orientation"", ""Relative Distance"", ""Max score"", ""Threshold Score"", ""Score""]

    save_rows = []
    for i in range(2, len(rows)):
        cols = rows[i].find_all(""td"")
        lst = []
        for col in cols:
            text = col.get_text()
            text = text.strip("" "")
            text = text.strip(""\n"")
            lst.append(text)
        save_rows.append(lst)
    return save_rows

url = ""http://www.athamap.de/search_gene.php?agi=At1g76540%0D%0AAt3g12280%0D%0AAt4g28980%0D%0AAt4g37630%0D%0AAt5g11300%0D%0AAt5g27620%0D%0A&amp;upstream=-500&amp;downstream=50&amp;restriction=0&amp;sortBy1=gen&amp;sortBy2=fac&amp;sortBy3=pos&amp;familySelected[ARF]=on&amp;familySelected[CAMTA]=on&amp;familySelected[GARP%2FARR-B]=on&amp;formSubmitted=TRUE""
save_rows = parse(url)
for row in save_rows:
    print(row)

",
BeautifulSoup inconsistent result,https://stackoverflow.com/questions/27694436,Python: BeautifulSoup returning garbage,"I am building a basic data crawler in python using BeautifulSoup, for Batoto, the manga host. For some the reason, the URL works sometimes and other times it doesn't. For example:

from bs4 import BeautifulSoup
from urllib2 import urlopen

x= urlopen(*manga url here*)
y = BeautifulSoup(x)

print y


The result should be a tag soup of the page but instead I get a big wall of this

´ºŸ{›æP™oRhtüs2å÷%ëmßñ6Y›þ�GDŸ0ËÂ­Í‡ì¼®Yé)–ÀØÅð&amp;amp;ô]½f³ÓÞ€Þþ)ú$÷á�üv…úzW¿¾úà†lªÀí¥ï«·_    OTL_ˆêsÁÿƒÁÖ&amp;lt;Ø?°Þ›Â+WLç¥àEh&amp;gt;rýÜ&amp;gt;x    ˆ‡eÇžù»èå»–Ùý e:›§`L_.‹¦úoÓ‘®e=‰ìÓ4Wëo’]~Ãõ¬À8&amp;gt;x:²âœ2¸ Á|&amp;amp;0ÍVpMLÎñ»v¥Ín÷-ÅÃ‰–T§`Ì.SÔsóë„œ¡×[˜·P6»�ùè�&amp;gt;Ô¾È]Œ—·ú£âÊgí%Ø¶kwýÃ=ÜÏ¸2cïÑfÙ_�×]Õê“ž?„UÖ* m³/­`ñ§ÿL0³dµ·jªÅ}õ/õOXß×;«]®’Ï¯w‹·þ¡ÿ|Gýª`I{µœ}œí�ë–¼yÖÇ'�Wç�ëµÅþþ*ýœd{ÿDv:Ð íHzqÿÆ­÷æélG-èÈâpÇßQé´^ÐO´®Xÿ�ýö(‹šëñþ""4!SÃõ2{òÿÜ´»ûE&lt;/kî?x´&amp;ý˜`Ù)uÂï¹ã[ÏŠ²yÂ°kÆpù}¢&gt;&lt;/uŒ¸kpž¼cìâˆ¬ƒcubÆ¡¢=en2‚påÓb9®`áï|z…p""i6pvif¨þõ“âŸ’&gt;&lt;/t`$ò-e&gt;&lt;/cé”r)$�ˆ)ìªÜrd&amp;mÉÊ*ßdÒuÄ.Æ-hx@9[s=m�Ýfd2o1ˆ]‡[Ôádœtë¤qâxæ°‹qËÁ×,½ŠmÊ‡ê‡¢ùÅýl&gt;&lt;/sí°çù¡h?‡ÌÜœbá‰æÆý¡sd~¬&gt;&lt;/zz¡ózwÎ[à!n‰Àš5¤…¸‘Ý¹Ž&gt;&lt;/sÃ:›3Ìæ&gt;&lt;/lÑggu�».Ð‘@4õë\ÃñÆ:¸5ÔwÛ·…)~ÛacÑ,d­³båÖ6&gt;&lt;/tg9y+wÎ‰í%r8ƒ·}n`¼ÁÆ8˜”é²êÞ½°¶Ï&gt;&lt;/sÖ-di¨a±j9³4&gt;&lt;/ss„*w(ßibðïj*¶„)pâýÌ”a§%va{‰ò¦m mi&gt;&lt;/o³o˜Ÿ?¿Ñu-}{cÜ›a~:k²Ì&gt;&lt;/r+=ÅÌk˜c&gt;&lt;/wÓ¹âßŠž‡ëf7vÑ�akÆ4ƒ‚&gt;&lt;/szŽµiÞêzâšÒ¬ú¢“âÀ#�-&gt;&lt;/qebndÎ‘g*cxgsÆ€Ùüe¡³-ŠngÁ:�3ænæ5ï0`coäÏÖ9œ1Ða¯,æ—ªìàãÉÂð&gt;&lt;/j›h¶`à;)òiÖ š+&gt;&lt;/o”64ˆÎº9°��u—Úd¿ý¥pÎÖ‰0¢s:c�yÆ§³t=ÕŸ“Ý‹41%}*,e³Ô¥ó&gt;&lt;/hiræe—';&gt;&lt;/v�fÞ«Ë¥n§Ð·¡kaììë\�`ùsõ©¸pv¦‘&gt;&lt;/bñ¼ut«w)Ø'¹ú@{)n0¡Žan¶Ë5èsª�–u–&gt;&lt;/y_x.mÅd:g}ëÕðhçðÂ«õõ8ŠcËÕÌvž­v™-šêÙ`b¹˜ùÃÎ“çË¤ÔÙtx¹�ßïÇ¶Îgþ°r‹$ò†aÆ–š?ì&lt;y«Ëñõo{%×‡o{ú¥Á»æ]‡&gt;&lt;/u´¬Ø¸eÖïÝtßÚ'è3®nh±ûk4È@l«s]–Åec¹ÑtmÓl|ë£Þ¼~zôéõûwêÓÑñÉÆw\soøÊiyjvØÖ$¯ÈoºÙoyã]æ5]-t^[“¡aÑ{²Å¸6¦ðtŒçm¼ÂÎz´&gt;&lt;/wà™´»äõ@©õ&gt;&lt;/mÏu:=¼þ·'�qwúËö«m„l^ˆær¥30q±ÒšŸëù&gt;&lt;/lî™¯(„7¼=xi’?¤;ö$ØË4ßoóiòyoµxÉøþ¨—«g³Ãíß{|&gt;&lt;/body&gt;&lt;/html&gt;


wrapped in html and body tags. 

Sometimes I will keep trying and it works, but it is so inconsistent, I can't figure out the reason for it.

Any help would be appreciated.
",2,649,"It seems to be urlopen having issues with encoding, requests works fine:

x = requests.get(""http://bato.to/comic/_/comics/rakudai-kishi-no-eiyuutan-r11615"")
y = BeautifulSoup(x.content)    
print y


&lt;!DOCTYPE html&gt;
&lt;html lang=""en"" xmlns:fb=""http://www.facebook.com/2008/fbml""&gt;
&lt;head&gt;
&lt;meta charset=""utf-8""/&gt;
&lt;title&gt;Rakudai Kishi no Eiyuutan - Scanlations - Comic - Comic Directory - Batoto -    Batoto&lt;/title&gt;
.................


Using urlopen we get the following:

x = urlopen(""http://bato.to/comic/_/comics/rakudai-kishi-no-eiyuutan-r11615"")    
print x.read()


���������s+I���2���l��9C&lt;�� ^�����쾯�dw�xzNT%��,T��A^�ݫ���9��a��E�C���W!�����ڡϳ��f7���s2�Px$���}I�*�'��;'3O&gt;���'g?�u®{����e.�ڇ�e{�u���jf:aث
�����DS��%��X�Zͮ���������9�:�Dx�����\-�
�*tBW������t�I���GQ�=�c��\:����u���S�V(�&gt;&lt;y�C��ã�*:�ۜ?D��a�g�o�sPD�m�""�,�Ɲ&lt;;v[��s���=��V2�fX��ì�Cj̇�В~�
-~����+;V���m�|kv���:V!�hP��D�K�/`oԣ|�k�5���B�{�0�wa�-���iS
�&gt;�œ��gǿ�o�OE3jçCV&lt;`���Q!��5�B��N��Ynd����?~��q���� _G����;T�S'�@΀��t��Ha�.;J�61'`Й�@���&gt;&gt;`��Z�ˠ�x�@� J*u��'���-����]p�9{&gt;����������#�&lt;-~�K""[AQh0HjP
0^��R�]�{N@��
 ...................


So as you can see it is a problem with urlopen not BeautifulSoup.
","The server is returning gzipped bytes. So to download the content using urllib2:

import sys
import urllib2
import gzip
import io
url = ""http://bato.to/comic/_/comics/rakudai-kishi-no-eiyuutan-r11615""
response = urllib2.urlopen(url)

# print(response.headers)
content = response.read()
if response.headers['Content-Encoding'] == 'gzip':
    g = gzip.GzipFile(fileobj=io.BytesIO(content))
    content = g.read()

encoding = response.info().getparam('charset')
content = content.decode(encoding)


This checks the content is the same as the page.text returned by requests:

import requests
page = requests.get(url)
# print(page.headers)

assert content == page.text


Since requests handles the gunzipping and decoding for you -- and more robustly too -- using requests is highly recommended. 
",
BeautifulSoup inconsistent result,https://stackoverflow.com/questions/68657996,Selenium Wait Until HTML Element Changes,"I am trying to scrape bookings data from a website. Upon entering the site, I must:

Change the default date range (at first, the site displays bookings data for the default date range) to a date range I want to search for
Click the 'Refresh' button
Wait a bit for the website to update the displayed results on the page.

Using Selenium, Python, and BeautifulSoup, when I execute Selenium commands to update the fields for the date range, and click the 'Refresh' button, by the time the code reaches the line where I extract HTML data using BeautifulSoup, the default bookings data for the default website date range is extracted, and not the bookings data for the date range I filled in.
Upon observation, it appears the site is moving by too quickly with Selenium to extract the updated data which takes some time to load. I tried to solve this using driver.implicitly_wait(5) but this returns highly inconsistent results.
I wanted to detect an HTML element that allows for stable extraction with no errors. I realized that when I click the 'Refresh' button, this particular element ...
&lt;div id=""textArea"" style=""visibility: visible; display: block;""&gt;

... changes to ...
&lt;div id=""textArea"" style=""visibility: hidden; display: block;""&gt;

... and changes back to ...
&lt;div id=""textArea"" style=""visibility: visible; display: block;""&gt;

... when the bookings data on the page is updated.
Is there a Selenium command that can detect this change prior to moving onto the next lines in my Python code? For instance, after clicking the 'Refresh' button, a ""wait until style=visibility is visible"" command?
",1,1381,"while True:
    a = driver.find_element_by_id('textArea').get_attribute('style')
    if a == r""visibility: hidden; display: block;""
        break
    else:
        time.sleep(.05)
while True:
    a = driver.find_element_by_id('textArea').get_attribute('style')
    if a == r""visibility: visible; display: block;""
        break
    else:
        time.sleep(.05)

This will wait until it goes invisible, then to visible before your program does anything
",,
BeautifulSoup inconsistent result,https://stackoverflow.com/questions/47422969,Python BeautifulSoup inconsistent result,"I have been trying to learn a bit of python, and I tried to create a small program that asks the user for subreddit and then prints all the front page headlines and links to the articles, here is the code

import requests
from bs4 import BeautifulSoup

subreddit = input('Type de subreddit you want to see : ')
link_visit = f'https://www.reddit.com/r/{subreddit}/'
print(link_visit)

base_url = link_visit
r = requests.get(base_url)
soup = BeautifulSoup(r.text, 'html.parser')

for article in soup.find_all('div', class_='top-matter'):

   headline = article.find('p', class_='title')
   print('HeadLine : ' , headline.text )

   a = headline.find('a', href=True)
   link = a['href'].split('/domain')
   print('Link : ' , link[0])


My problem is that sometimes it prints the desired result, other times it does nothing, only asks the user for the subrredit and prints the link to said subreddit.

Can someone explain why is this happening?
",0,220,"Your request is being rejected by reddit in order to conserve their resources. 

When you detect the failing case, print out the HTML. I think you'll see something like this:

    &lt;h1&gt;whoa there, pardner!&lt;/h1&gt;



&lt;p&gt;we're sorry, but you appear to be a bot and we've seen too many requests
from you lately. we enforce a hard speed limit on requests that appear to come
from bots to prevent abuse.&lt;/p&gt;

&lt;p&gt;if you are not a bot but are spoofing one via your browser's user agent
string: please change your user agent string to avoid seeing this message
again.&lt;/p&gt;

&lt;p&gt;please wait 3 second(s) and try again.&lt;/p&gt;

    &lt;p&gt;as a reminder to developers, we recommend that clients make no
    more than &lt;a href=""http://github.com/reddit/reddit/wiki/API""&gt;one
    request every two seconds&lt;/a&gt; to avoid seeing this message.&lt;/p&gt;

",,
BeautifulSoup inconsistent result,https://stackoverflow.com/questions/12853453,Error handling with beautiful soup,,-1,1031,,,
Tornado unexpected behavior,https://stackoverflow.com/questions/57103331,"How to replace `yield gen.Task(fn, argument)` with an equivalent asyncio expression?","Problem:
I am trying to update some old code (which I didn't write), which uses an outdated version of Tornado and gen.Task, to use the current version of Tornado and asyncio. This is mostly straightforward except for this one expression which (1) I do not fully understand, and which (2) I cannot figure out how to replace with an equivalent asyncio expression.
The single line of code I want to replace is of the form:
response = yield gen.Task(fn, request)
where the signature of the function fn is fn(request, callback), and then later in the code (which is the method definition of a gen.coroutine) we run callback(response). And I think fn may itself be asynchronous, although I'm not sure, and don't understand what the implications of that would be if it is true.
EDIT: Following the advice of another answer, I was able to rewrite this as
fn(request=request, callback=(yield gen.Callback(""key"")))
response = yield gen.Wait(""key"")

Of course though the release notes for Tornado 6 say that both gen.Wait and gen.Callback have been removed. Earlier versions of the documentation say that the gen.Wait is deprecated and should be replaced by tornado.concurrent.Futures, unfortunately it doesn't specify how to do so, especially given how gen.Wait requires a key argument, whereas concurrent.futures.Futures (apparently an alias for asyncio.Future) explicitly has no way to support a key argument. So I don't understand the claim that this is somehow replaceable.
Also add_done_callback seems to be inadequate for this purpose, since the documentation explicitly states that the callback can only take one argument, but fn has two.
Although so far what has worked best (and may actually work, provided I can make the gen.coroutine to async def transition correctly elsewhere) seems to be:
response = await asyncio.Future().add_done_callback(partial(fn, request=request))

This only produces unexpected behavior (endless blocking, seemingly probably because of the insufficient gen.coroutine to async def conversions mentioned above) and no errors. This gives the error TypeError: Can't await NoneType. So I have no clue.
Background: I have tried to figure out what recommendations Tornado gave when gen.Task was updated and finally removed. In the changelog for version 6, however, it does not say how to update our code using gen.Task, only that it has been removed. I have found at least one question on StackOverflow as well as a Tornado GitHub issue where it is said (without giving specific examples or implementation details) that any instance of gen.Task can be replaced with a gen.coroutine. However, since I do not understand the general concepts of asynchronous programming very well, nor the particulars of tornado.gen.Task, it is very difficult for me to figure out how this could be done. It would be great though since it seems to be easy to replace gen.coroutine's with asyncio equivalents -- just async def and await everything.
The result of yield gen.Task is supposed to be, according to the documentation:

Takes a function (and optional additional arguments) and runs it with those arguments plus a callback keyword argument. The argument passed to the callback is returned as the result of the yield expression.
Changed in version 4.0: gen.Task is now a function that returns a Future...

However this seems more complicated than something that can be replaced with gen.coroutine, since it directly creates a Future, rather than awaiting the result of an asynchronous function, and there are numerous ways to create and work with futures in asyncio, and I vaguely remember reading somewhere that Tornado futures and asyncio futures aren't actually equivalent.
The fact that this involves both asynchronous and functional programming makes the problem even more difficult to understand -- I vaguely grasp the functional part, but my understanding of asynchronous programming is very poor, to the extent that it suddenly also makes the functional aspect difficult to understand now too.

What I've tried so far:
response = yield asyncio.add_done_callback(functools.partial(fn, request=request))
giving the error AttributeError: module 'asyncio' has no attribute 'add_done_callback', which, fine, I get that add_done_callback is supposed to be an attribute of an asyncio.Future object, but then what do I make/choose to be the asyncio.Future?
response = yield asyncio.Task(partial(fn, request=request).func)
which gave the error TypeError: a coroutine was expected, got &lt;bound method Class.fn of &lt;SubClass object at 0x7f5df254b748&gt;&gt;.
The reason I tried to use the .func attribute of the partial object is because when I tried:
response = yield asyncio.Task(partial(fn, request=request))
I got the error TypeError: a coroutine was expected, got functools.partial(&lt;bound method Class.fn of &lt;SubClass object at 0x7ffaad59b710&gt;&gt;, request=&lt;tornado.httpclient._RequestProxy object at 0x7ffaad4c8080&gt;). But I only tried to do that because more straightforward attempts at solutions led to complaints about the number of arguments being incorrect.
In particular, trying one of the most naive things,
response = yield asyncio.Task(fn, request)
led to the in hindsight predictable error TypeError: Task() takes at most 1 positional arguments (2 given). The release notes for Tornado 5.0 say that internally all gen.Task's were replaced with asyncio.Task's, but this makes it difficult for me to understand how, since it looks like asyncio.Task is inadequate by itself to handle callbacks.
I had originally been more optimistic and hoped that asyncio.Task would notice that the call signature of fn was fn(request, callback), and would then understand fn(request) to be the partially applied function. But of course that
response = yield asyncio.Task(fn(request))
gave the error TypeError: fn() missing 1 required positional argument: 'callback'.
What's even more confusing is that fn itself is possibly asynchronous, so I thought that using asyncio I might just be able to partially apply it and get back an asynchronous function that takes a callback as an option
response = yield fn(request)
but that just led to the error TypeError: fn() missing 1 required positional argument: 'callback'.
I also tried creating a task or future in asyncio (I'm not sure which of the two I need to create) using the recommended ensure_future and create_task functions, since using Task directly is strongly discouraged according to the asyncio docs. This did not work out well:
response = yield asyncio.create_task(fn, request)
giving the error TypeError: create_task() takes 1 positional argument but 2 were given.
Using ensure_future led to no better result:
response = asyncio.ensure_future(functools.partial(fn, request))
gave the result TypeError: An asyncio.Future, a coroutine or an awaitable is required, and not using partial
 response = asyncio.ensure_future(super().fetch_impl, request=request)
gave the error  TypeError: ensure_future() got an unexpected keyword argument 'request'.
In case it's relevant, fn is the fetch_impl method of Tornado's CurlAsyncHTTPClient.
Similar questions: These two questions seem similar, but I don't understand how to use their answers for my problem. They probably are applicable, but again my understanding of asynchronous programming in general and asyncio in particular is very bad and I am very stupid. So an answer explaining the answers to these other two questions like I'm a five year old would also be appreciated. Any patience you can muster for my stupidity+ignorance will be appreciated.
How does 'yield' work in tornado when making an asynchronous call?
Extending tornado.gen.Task
",3,1402,"
  I have found at least one question on StackOverflow as well as a Tornado GitHub issue where it is said (without giving specific examples or implementation details) that any instance of gen.Task can be replaced with a gen.coroutine. However, since I do not understand the general concepts of asynchronous programming very well, nor the particulars of tornado.gen.Task, it is very difficult for me to figure out how this could be done. It would be great though since it seems to be easy to replace gen.coroutine's with asyncio equivalents -- just async def and await everything.


You're focusing on ""how do I call this thing that takes a callback"". The problem is that the entire concept of callbacks has been deprecated and removed from Tornado, so there is no elegant way to call something that takes a callback any more. The intended path forward is to change the thing that takes a callback (i.e., fn) to use gen.coroutine and/or return a Future, and then you can call it directly from your other coroutines.

If fn was using @gen.engine (the first version of coroutines in Tornado), this is fairly easy: just replace @gen.engine with @gen.coroutine and remove any references to the callback argument. The function probably ends with callback(response); replace this with raise gen.Return(response). 

If fn was just using raw callbacks without @gen.engine, then updating it to work in the modern way will be harder, and it's something that needs to be handled on a case-by-case basis so I can't give you useful guidance here.

If you're stuck with something that takes a callback and you can't change it, this sequence is almost equivalent to response = yield gen.Task(fn, request):

future = tornado.concurrent.Future()
fn(request, callback=future.set_result)
response = yield future


The difference between this and gen.Task has to do with error handling. If fn raises an exception, gen.Task had some expensive magic to ensure that it could catch that exception and re-raise it in the calling function. Maintaining that magic had some performance cost even for apps that didn't use gen.Task, which is why it was eventually deprecated and removed (along with everything related to callbacks). So you may need to change fn to ensure that any possible exceptions are caught and reported appropriately (again, the recommended way to do this is to move to coroutines where exception handling works more as you'd expect). 
","If you can update your function to an async def (and therefore use await), then what you need can be expressed as:

future = asyncio.get_event_loop().create_future()
fn(request=request, callback=future.set_result)
response = await future


The ""future"" object can be awaited, and its set_result method resumes the awaitee. fn doesn't need to know about the future, though, it only sees a callback function.
",
Tornado unexpected output,https://stackoverflow.com/questions/21678394,Updates of pyOpenSSL and pandas using &#39;pip&#39; fail with &quot;TypeError: resolve() got an unexpected keyword argument &#39;replace_conflicting&#39;&quot;,"When I attempt to update or install any version of pandas or pyOpenSSL(with any instance of sudo pip --[un]install|update [--no-use-wheel] [pandas|pyOpenSSL|xattr|stevedore], using pip 1.5.4) I get:

Command python setup.py egg_info failed with error code 1 in /private/tmp/pip_build_root/pandas
Storing debug log for failure in /Users/Rax/Library/Logs/pip.log
...
TypeError: resolve() got an unexpected keyword argument 'replace_conflicting'


Why am I getting this error and what can I do to avoid it?



OSX 10.9.2; Python 2.7.5; setuptools 3.4.3.

More detail on the error report (for pandas):

Downloading/unpacking pandas
  Downloading pandas-0.13.1.tar.gz (6.1MB): 6.1MB downloaded
  Running setup.py (path:/private/tmp/pip_build_root/pandas/setup.py) egg_info for package pandas
    Traceback (most recent call last):
      File ""&lt;string&gt;"", line 17, in &lt;module&gt;
      File ""/private/tmp/pip_build_root/pandas/setup.py"", line 590, in &lt;module&gt;
        **setuptools_kwargs)
      File ""/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/distutils/core.py"", line 112, in setup
        _setup_distribution = dist = klass(attrs)
      File ""/Library/Python/2.7/site-packages/setuptools/dist.py"", line 239, in __init__
        self.fetch_build_eggs(attrs.pop('setup_requires'))
      File ""/Library/Python/2.7/site-packages/setuptools/dist.py"", line 264, in fetch_build_eggs
        replace_conflicting=True
    TypeError: resolve() got an unexpected keyword argument 'replace_conflicting'
    Complete output from command python setup.py egg_info:
    Traceback (most recent call last):

  File ""&lt;string&gt;"", line 17, in &lt;module&gt;

  File ""/private/tmp/pip_build_root/pandas/setup.py"", line 590, in &lt;module&gt;

    **setuptools_kwargs)

  File ""/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/distutils/core.py"", line 112, in setup

    _setup_distribution = dist = klass(attrs)

  File ""/Library/Python/2.7/site-packages/setuptools/dist.py"", line 239, in __init__

    self.fetch_build_eggs(attrs.pop('setup_requires'))

  File ""/Library/Python/2.7/site-packages/setuptools/dist.py"", line 264, in fetch_build_eggs

    replace_conflicting=True

TypeError: resolve() got an unexpected keyword argument 'replace_conflicting'


Contents of /Users/Rax/Library/Logs/pip.log:

Exception information:
Traceback (most recent call last):
  File ""/Library/Python/2.7/site-packages/pip/basecommand.py"", line 122, in main
    status = self.run(options, args)
  File ""/Library/Python/2.7/site-packages/pip/commands/install.py"", line 274, in run
    requirement_set.prepare_files(finder, force_root_egg_info=self.bundle, bundle=self.bundle)
  File ""/Library/Python/2.7/site-packages/pip/req.py"", line 1215, in prepare_files
    req_to_install.run_egg_info()
  File ""/Library/Python/2.7/site-packages/pip/req.py"", line 321, in run_egg_info
    command_desc='python setup.py egg_info')
  File ""/Library/Python/2.7/site-packages/pip/util.py"", line 697, in call_subprocess
    % (command_desc, proc.returncode, cwd))
InstallationError: Command python setup.py egg_info failed with error code 1 in /private/tmp/pip_build_root/pandas




Cython          - 0.20.1       - active 
Flask           - 0.10.1       - active 
Jinja2          - 2.7.2        - active 
MarkupSafe      - 0.19         - active 
PyRSS2Gen       - 1.1          - active 
Pygments        - 1.6          - active 
Python          - 2.7.5        - active development (/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/lib-dynload)
Sphinx          - 1.2.2        - active 
Twisted         - 13.2.0       - active 
Werkzeug        - 0.9.4        - active 
altgraph        - 0.11         - active 
astropy         - 0.3          - active 
backports.ssl-match-hostname - 3.4.0.2      - active 
bdist-mpkg      - 0.5.0        - active 
brewer2mpl      - 1.3.2        - active 
cffi            - 0.8.2        - active 
colorama        - 0.2.7        - active 
configobj       - 5.0.1        - active 
dill            - 0.2b1        - active 
distribute      - 0.7.3        - active 
docutils        - 0.11         - active 
ggplot          - 0.4.7        - active 
ipython         - 1.2.1        - active 
itsdangerous    - 0.23         - active 
macholib        - 1.6          - active 
matplotlib      - 1.3.1        - active 
modulegraph     - 0.11         - active 
mpltools        - 0.1          - active 
nose            - 1.3.0        - active 
numexpr         - 2.3.1        - active 
numpy           - 1.8.0        - active 
numpydoc        - 0.4          - active 
pandas          - 0.13.1       - active 
patsy           - 0.2.1        - active 
pika            - 0.9.13       - active 
pip             - 1.5.4        - active 
prettytable     - 0.7.2        - active 
progressbar     - 2.2          - active 
py2app          - 0.8          - active 
pycparser       - 2.10         - active 
pyparsing       - 2.0.1        - active 
python-dateutil - 2.2          - active 
pytz            - 2014.1.1     - active 
pyzmq           - 14.0.1       - active 
readline        - 6.2.4.1      - active 
rpy2            - 2.3.9        - active 
scikit-learn    - 0.14.1       - active 
scipy           - 0.13.3       - active 
setuptools      - 3.4.3        - active 
sphinx-argparse - 0.1.9        - active 
sphinxcontrib-napoleon - 0.2.6        - active 
sphinxcontrib-programoutput - 0.8          - active 
statsmodels     - 0.5.0        - active 
stevedore       - 0.14.1       - active 
sympy           - 0.7.5        - active 
tornado         - 3.2          - active 
virtualenv-clone - 0.2.4        - active 
virtualenv      - 1.11.4       - active 
virtualenvwrapper - 4.2          - active 
websocket-client - 0.12.0       - active 
wsgiref         - 0.1.2        - active development (/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7)
xattr           - 0.7.4        - active 
yhat            - 0.6.14       - active 
yolk            - 0.4.3        - active 
zope.interface  - 4.0.5        - active 




FWIW, I can install pandas successfully in a virtual environment. If I copy it from there back into the environment where I get this error and run nosetests pandas I get:

..SS..SS..SS..SS/Library/Python/2.7/site-packages/pandas/core/index.py:910: RuntimeWarning: tp_compare didn't return -1 or -2 for exception
  result.sort()
..SS..SS..SS..SS..SSSSSSSSSS..SS..SS..SSSS.S.S...........SS..SS..SS..........................SSSSSSSSSS.SSSS.....SSSSSSSSSSSSSS.SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS.SSSS..................S.....S........................................................................................................................................................................................................................................................................SS......................................................................................................SSSS.......................SSS..............................................................................................................................................S.......................SSSSSSS...........................................................................................S......................................................................................................................................................S.........................................S..S..S....S........................................................S......S.S.......S...S..S............S............................................................................................................................................................................................S............................................................................S........................................./Library/Python/2.7/site-packages/numpy/core/_methods.py:55: RuntimeWarning: Mean of empty slice.
  warnings.warn(""Mean of empty slice."", RuntimeWarning)
...................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................S...............................................
----------------------------------------------------------------------
Ran 4466 tests in 230.555s

OK (SKIP=330)


This is also what I get if I run the tests in the virtual environment.
",3,3702,"A likely cause is the presence of multiple versions of setuptools, with an older version earlier on the Python path than the newer one. This can be temporarily overcome for the purposes of installation by beginning your invocations of pip with

PYTHONPATH=/PATH_TO_YOUR_CURRENT_PACKAGES/site-packages pip ...




You can confirm that this is the case by executing 

import pkg_resources
pkg_resources.__file__


in Python and noting that the pkg_resources.pyc returned is not in the location of your current site packages.

In some cases this can also be corrected by ensuring that the pkg_resources.pyc in your site packages directory in executed, rather than the one in your system packages directory (e.g., by deleting the latter).
","If you are on OSX, the easiest solution in my opinion to avoid errors like you are having is to use Homebrew to manage the installation of Python, pip and libraries like pandas.

If you are already using Homebrew (it was unclear from your question). Try running brew doctor and see if there are any reported errors.

To get pip working in concert with Homebrew:


ruby -e ""$(curl -fsSL https://raw.github.com/mxcl/homebrew/go)"" to install Homebrew.
brew doctor and make sure you get no errors from running this command. Follow what it recommends, especially concerning the $PATH environment variable.
brew install python to install Python 2.7 (and pip)
pip install pandas

",
Tornado unexpected output,https://stackoverflow.com/questions/38966075,How to restrict anaconda from upgrading the module being installed if its a higher level dependency,"I'm trying to use continuum io anaconda packing system to package python-2.7.10 with other dependent modules for our environment. I want to automate the pack distribution to simply be a single installation of python with the modules we require.

The issue I'm having is when I specify the modules under the build parameter in meta.yaml it will upgrade the version of python being installed despite the fact that it is python-2.7.10. This will cause an error in the build process.

Is there a way to pin the version of python being installed so that if there is a dependency it will hard fail, or use an earlier version of the package?

meta.yaml, ive tried not pinning the version of the modules as well.

package:
  name: python
  version: 2.7.10

source:
  fn: Python-2.7.10.tgz
  url: https://www.python.org/ftp/python/2.7.10/Python-2.7.10.tgz
  md5: d7547558fd673bd9d38e2108c6b42521

build:
  no_link: bin/python

requirements:
  build:
    - bzip2 [unix]
    - zlib [unix]
    - sqlite [unix]
    - readline [unix]
    - tk [unix]
    - openssl [unix]
    - system [linux]
    - ipython 5.0.0
    - numpy 1.11.1
    - cython 0.24.1
    - scipy 0.18.0
    - pandas 0.18.1
    - patsy 0.4.1
    - statsmodels 0.6.1
    - matplotlib 1.5.2
    - ggplot 0.9.4
    - scikit-learn 0.17.1
    - distribute 0.6.45
    - backports.ssl-match-hostname 3.5.0.1
    - certifi 14.05.14
    - nose_parameterized 0.5.0
    - pyparsing 2.1.4
    - python-dateutil 2.5.3
    - pytz 2016.6.1
    - pyzmq 15.3.0
    - simplejson 3.3.3
    - six 1.10.0
    - sympy 1.0
    - tornado 4.4.1
    - virtualenv 13.0.1
    - wsgiref 0.1.2
    - python-swiftclient 2.7.0
    #- python-ceilometerclient #issue
    #- python-heatclient #issue
    #- python-keystoneclient 1.6.0
    #- python-novaclient 2.26.0
    #- python-troveclient #issue
    - python-cinderclient 1.1.2
    - python-glanceclient 0.17.2
    - python-neutronclient 2.4.0
    - networkx 1.11
    - pysal 1.11.1
    - pyyaml 3.11
    - shapely 1.5.13
    - beautifulsoup4 4.4.1
    - nltk 3.2.1
    - requests 2.10.0
    - seaborn 0.5.0
    - h5py 2.6.0
    - xlrd 1.0.0
    - markupsafe 0.23
    - crypto 1.1.0
    - jinja2 2.8
    - openpyxl 2.3.2
    - jaro_winkler 1.0.2
    - bokeh 0.12.1
    - numexpr 2.6.1
    - pytables 3.2.3.1
    - pycurl 7.43.0
    - mgrs 1.1.0
    - psutil 4.3.0
    - biopython 1.67
    - enaml 0.9.8
    - mdp 3.5
    - bitarray 0.8.1
    - clusterpy 0.9.9
    - pyside 1.2.1
    - pyqt 4.11.4
    - parsedatetime 1.4
    - pymysql 0.6.7
    - pyodbc 3.0.10
    - tabulate 0.7.2

  run:
    - zlib [unix]
    - sqlite [unix]
    - readline [unix]
    - tk [unix]
    - openssl [unix]
    - system [linux]

test:
  commands:
    - python -V [unix]
    - 2to3 -h [unix]
    - python-config --help [unix]

about:
  home: http://www.python.org/
  summary: general purpose programming language
  license: PSF


The output with the error:

$ conda build .
Removing old build environment
BUILD START: python-2.7.10-0
    (actual version deferred until further download or env creation)
Using Anaconda Cloud api site https://api.anaconda.org

The following packages will be downloaded:

    package                    |            build
    ---------------------------|-----------------
    geos-3.5.0                 |                0        16.9 MB  defaults
    libgcc-4.8.5               |                1         922 KB  r
    pixman-0.32.6              |                0         2.4 MB  defaults
    unixodbc-2.3.4             |                0         688 KB  defaults
    yaml-0.1.6                 |                0         246 KB  defaults
    curl-7.49.0                |                1         543 KB  defaults
    glib-2.43.0                |                2         7.4 MB  r
    hdf5-1.8.17                |                1         1.9 MB  defaults
    atom-0.3.10                |           py27_0         676 KB  defaults
    backports_abc-0.4          |           py27_0           5 KB  defaults
    beautifulsoup4-4.4.1       |           py27_0         116 KB  defaults
    bitarray-0.8.1             |           py27_0          89 KB  defaults
    et_xmlfile-1.0.1           |           py27_0          15 KB  defaults
    future-0.15.2              |           py27_0         616 KB  defaults
    jaro_winkler-1.0.2         |           py27_0          24 KB  auto
    jdcal-1.2                  |           py27_1           9 KB  defaults
    kiwisolver-0.1.3           |           py27_0         571 KB  defaults
    markupsafe-0.23            |           py27_2          31 KB  defaults
    mgrs-1.1.0                 |           py27_0          48 KB  auto
    mpmath-0.19                |           py27_1         873 KB  defaults
    nltk-3.2.1                 |           py27_0         1.7 MB  defaults
    parsedatetime-1.2          |           py27_0          39 KB  auto
    ply-3.8                    |           py27_0          71 KB  defaults
    psutil-4.3.0               |           py27_0         224 KB  defaults
    pycurl-7.43.0              |           py27_0         128 KB  defaults
    pymysql-0.7.6              |           py27_0         116 KB  defaults
    pyodbc-3.0.10              |           py27_0         146 KB  defaults
    pyyaml-3.11                |           py27_4         297 KB  defaults
    pyzmq-15.4.0               |           py27_0         705 KB  defaults
    requests-2.10.0            |           py27_0         611 KB  defaults
    shapely-1.5.16             |           py27_0         494 KB  defaults
    tabulate-0.7.2             |           py27_0          18 KB  auto
    wsgiref-0.1.2              |           py27_0          943 B  auto
    xlrd-1.0.0                 |           py27_0         181 KB  defaults
    biopython-1.67             |      np111py27_0         2.2 MB  defaults
    clusterpy-0.9.9            |           py27_1         101 KB  conda-forge
    cmd2-0.6.7                 |           py27_0          33 KB  auto
    h5py-2.6.0                 |      np111py27_2         2.4 MB  defaults
    jinja2-2.8                 |           py27_1         264 KB  defaults
    jsonschema-2.5.1           |           py27_0          55 KB  defaults
    mdp-3.5                    |           py27_0         477 KB  defaults
    networkx-1.11              |           py27_0         1.1 MB  defaults
    numexpr-2.6.1              |      np111py27_0         347 KB  defaults
    openpyxl-2.3.2             |           py27_0         248 KB  defaults
    rsa-3.4.2                  |           py27_0          50 KB  conda-forge
    singledispatch-3.4.0.3     |           py27_1          17 KB  r
    ssl_match_hostname-3.4.0.2 |           py27_1           6 KB  defaults
    cliff-1.10.1               |           py27_0          36 KB  gus
    crypto-1.1.0               |           py27_0           3 KB  auto
    pysal-1.11.1               |           py27_0        11.2 MB  defaults
    pytables-3.2.3.1           |      np111py27_0         3.4 MB  defaults
    tornado-4.4.1              |           py27_0         552 KB  defaults
    bokeh-0.12.1               |           py27_0         3.2 MB  defaults
    harfbuzz-0.9.35            |                6         1.1 MB  r
    ipython-5.1.0              |           py27_0         936 KB  defaults
    pyopenssl-16.0.0           |           py27_0          66 KB  defaults
    pango-1.36.8               |                3         796 KB  r
    qt-4.8.7                   |                4        32.7 MB  defaults
    python-neutronclient-2.4.0 |           py27_0         222 KB  gus
    shiboken-1.2.1             |           py27_0         883 KB  defaults
    enaml-0.9.8                |           py27_1         944 KB  defaults
    pyside-1.2.1               |           py27_1         5.7 MB  defaults
    seaborn-0.7.1              |           py27_0         272 KB  defaults
    ------------------------------------------------------------
                                           Total:       107.8 MB

The following NEW packages will be INSTALLED:

    atom:                         0.3.10-py27_0       defaults
    babel:                        2.3.3-py27_0        defaults
    backports:                    1.0-py27_0          defaults
    backports.ssl-match-hostname: 3.5.0.1-py27_0      getpantheon
    backports_abc:                0.4-py27_0          defaults
    beautifulsoup4:               4.4.1-py27_0        defaults
    biopython:                    1.67-np111py27_0    defaults
    bitarray:                     0.8.1-py27_0        defaults
    bokeh:                        0.12.1-py27_0       defaults
    brewer2mpl:                   1.4.1-py27_1        conda-forge
    bzip2:                        1.0.6-3             defaults
    cairo:                        1.12.18-6           defaults
    certifi:                      2016.2.28-py27_0    defaults
    cffi:                         1.6.0-py27_0        defaults
    cliff:                        1.10.1-py27_0       gus
    clusterpy:                    0.9.9-py27_1        conda-forge
    cmd2:                         0.6.7-py27_0        auto
    crypto:                       1.1.0-py27_0        auto
    cryptography:                 1.4-py27_0          defaults
    curl:                         7.49.0-1            defaults
    cycler:                       0.10.0-py27_0       defaults
    cython:                       0.24.1-py27_0       defaults
    decorator:                    4.0.10-py27_0       defaults
    distribute:                   0.6.45-py27_1       defaults
    enaml:                        0.9.8-py27_1        defaults
    enum34:                       1.1.6-py27_0        defaults
    et_xmlfile:                   1.0.1-py27_0        defaults
    fontconfig:                   2.11.1-6            defaults
    freetype:                     2.5.5-1             defaults
    functools32:                  3.2.3.2-py27_0      defaults
    future:                       0.15.2-py27_0       defaults
    futures:                      3.0.5-py27_0        defaults
    geos:                         3.5.0-0             defaults
    get_terminal_size:            1.0.0-py27_0        defaults
    ggplot:                       0.11.1-py27_1       conda-forge
    glib:                         2.43.0-2            r
    h5py:                         2.6.0-np111py27_2   defaults
    harfbuzz:                     0.9.35-6            r
    hdf5:                         1.8.17-1            defaults
    idna:                         2.1-py27_0          defaults
    ipaddress:                    1.0.16-py27_0       defaults
    ipython:                      5.1.0-py27_0        defaults
    ipython_genutils:             0.1.0-py27_0        defaults
    iso8601:                      0.1.11-py27_0       defaults
    jaro_winkler:                 1.0.2-py27_0        auto
    jdcal:                        1.2-py27_1          defaults
    jinja2:                       2.8-py27_1          defaults
    jsonpatch:                    1.3-py27_0          auto
    jsonpointer:                  1.2-py27_0          auto
    jsonschema:                   2.5.1-py27_0        defaults
    kiwisolver:                   0.1.3-py27_0        defaults
    libffi:                       3.2.1-0             defaults
    libgcc:                       4.8.5-1             r
    libgfortran:                  3.0.0-1             defaults
    libpng:                       1.6.22-0            defaults
    libsodium:                    1.0.10-0            defaults
    libxml2:                      2.9.2-0             defaults
    markupsafe:                   0.23-py27_2         defaults
    matplotlib:                   1.5.1-np111py27_0   defaults
    mdp:                          3.5-py27_0          defaults
    mgrs:                         1.1.0-py27_0        auto
    mkl:                          11.3.3-0            defaults
    mpmath:                       0.19-py27_1         defaults
    msgpack-python:               0.4.7-py27_0        defaults
    netaddr:                      0.7.18-py27_0       conda-forge
    netifaces:                    0.10.4-py27_0       conda-forge
    networkx:                     1.11-py27_0         defaults
    nltk:                         3.2.1-py27_0        defaults
    nose_parameterized:           0.5.0-py27_0        conda-forge
    numexpr:                      2.6.1-np111py27_0   defaults
    numpy:                        1.11.1-py27_0       defaults
    openpyxl:                     2.3.2-py27_0        defaults
    openssl:                      1.0.2h-1            defaults
    oslo.config:                  1.9.3-py27_0        gus
    oslo.i18n:                    1.5.0-py27_0        gus
    oslo.serialization:           1.4.0-py27_0        gus
    oslo.utils:                   1.4.0-py27_0        gus
    pandas:                       0.18.1-np111py27_0  defaults
    pango:                        1.36.8-3            r
    parsedatetime:                1.2-py27_0          auto
    path.py:                      8.2.1-py27_0        defaults
    pathlib2:                     2.1.0-py27_0        defaults
    patsy:                        0.4.1-py27_0        defaults
    pbr:                          0.11.0-py27_0       defaults
    pexpect:                      4.0.1-py27_0        defaults
    pickleshare:                  0.7.3-py27_0        defaults
    pip:                          8.1.2-py27_0        defaults
    pixman:                       0.32.6-0            defaults
    ply:                          3.8-py27_0          defaults
    prettytable:                  0.7.2-py27_0        conda-forge
    prompt_toolkit:               1.0.3-py27_0        defaults
    psutil:                       4.3.0-py27_0        defaults
    ptyprocess:                   0.5.1-py27_0        defaults
    pyasn1:                       0.1.9-py27_0        defaults
    pycairo:                      1.10.0-py27_0       defaults
    pycparser:                    2.14-py27_1         defaults
    pycurl:                       7.43.0-py27_0       defaults
    pygments:                     2.1.3-py27_0        defaults
    pymysql:                      0.7.6-py27_0        defaults
    pyodbc:                       3.0.10-py27_0       defaults
    pyopenssl:                    16.0.0-py27_0       defaults
    pyparsing:                    2.1.4-py27_0        defaults
    pyqt:                         4.11.4-py27_4       defaults
    pysal:                        1.11.1-py27_0       defaults
    pyside:                       1.2.1-py27_1        defaults
    pytables:                     3.2.3.1-np111py27_0 defaults
    python:                       2.7.12-1            defaults
    python-cinderclient:          1.1.2-py27_0        gus
    python-dateutil:              2.5.3-py27_0        defaults
    python-glanceclient:          0.17.2-py27_0       gus
    python-keystoneclient:        1.3.2-py27_0        gus
    python-neutronclient:         2.4.0-py27_0        gus
    python-swiftclient:           2.7.0-py27_0        chenghlee
    pytz:                         2016.6.1-py27_0     defaults
    pyyaml:                       3.11-py27_4         defaults
    pyzmq:                        15.4.0-py27_0       defaults
    qt:                           4.8.7-4             defaults
    readline:                     6.2-2               defaults
    requests:                     2.10.0-py27_0       defaults
    rsa:                          3.4.2-py27_0        conda-forge
    scikit-learn:                 0.17.1-np111py27_2  defaults
    scipy:                        0.18.0-np111py27_0  defaults
    seaborn:                      0.7.1-py27_0        defaults
    setuptools:                   25.1.6-py27_0       defaults
    shapely:                      1.5.16-py27_0       defaults
    shiboken:                     1.2.1-py27_0        defaults
    simplegeneric:                0.8.1-py27_1        defaults
    simplejson:                   3.8.2-py27_0        defaults
    singledispatch:               3.4.0.3-py27_1      r
    sip:                          4.18-py27_0         defaults
    six:                          1.10.0-py27_0       defaults
    sqlite:                       3.13.0-0            defaults
    ssl_match_hostname:           3.4.0.2-py27_1      defaults
    statsmodels:                  0.6.1-np111py27_1   defaults
    stevedore:                    1.3.0-py27_0        gus
    sympy:                        1.0-py27_0          defaults
    system:                       5.8-2               defaults
    tabulate:                     0.7.2-py27_0        auto
    tk:                           8.5.18-0            defaults
    tornado:                      4.4.1-py27_0        defaults
    traitlets:                    4.2.2-py27_0        defaults
    unixodbc:                     2.3.4-0             defaults
    virtualenv:                   13.0.1-py27_0       defaults
    warlock:                      1.3.0-py27_0        conda-forge
    wcwidth:                      0.1.7-py27_0        defaults
    wheel:                        0.29.0-py27_0       defaults
    wsgiref:                      0.1.2-py27_0        auto
    xlrd:                         1.0.0-py27_0        defaults
    yaml:                         0.1.6-0             defaults
    zeromq:                       4.1.4-0             defaults
    zlib:                         1.2.8-3             defaults

Source cache directory is: /opt/app/anaconda2/conda-bld/src_cache
Found source in cache: Python-2.7.10.tgz
Extracting download
BUILD START: python-2.7.10-0
python is installed as a build dependency. Removing.
An unexpected error has occurred, please consider sending the
following traceback to the conda GitHub issue tracker at:

    https://github.com/conda/conda-build/issues

Include the output of the command 'conda info' in your report.


Traceback (most recent call last):
  File ""/opt/app/anaconda2/bin/conda-build"", line 5, in &lt;module&gt;
    sys.exit(main())
  File ""/opt/app/anaconda2/lib/python2.7/site-packages/conda_build/main_build.py"", line 152, in main
    args_func(args, p)
  File ""/opt/app/anaconda2/lib/python2.7/site-packages/conda_build/main_build.py"", line 415, in args_func
    args.func(args, p)
  File ""/opt/app/anaconda2/lib/python2.7/site-packages/conda_build/main_build.py"", line 358, in execute
    debug=args.debug)
  File ""/opt/app/anaconda2/lib/python2.7/site-packages/conda_build/build.py"", line 561, in build
    assert not plan.nothing_to_do(actions), actions
AssertionError: defaultdict(&lt;type 'list'&gt;, {'op_order': ('RM_FETCHED', 'FETCH', 'RM_EXTRACTED', 'EXTRACT', 'UNLINK', 'LINK', 'SYMLINK_CONDA'), 'PREFIX': '/opt/app/anaconda2/envs/_build_placehold_placehold_placehold_placehold_placehold'})

",3,580,"Unless there's a specific reason you need to compile python yourself, I think what you're actually going after is conda bundle (http://conda.pydata.org/docs/commands/conda-bundle.html).  Unfortunately we've removed it in conda 4.2 which will be coming out soon, intending to move it to conda-build.  Since that hasn't happened yet, and if it ends up actually being useful to people, we can add it back.



You could also try this using conda-build...

Remove the whole source block in your meta.yaml file. Also remove all of the build requirements that are also not run requirements.  Then in your build.sh file

conda install --yes --quiet \
    python=2.7.10 \
    ipython=5.0.0 \
    numpy=1.11.1 \
    cython=0.24.1 \
    scipy=0.18.0 \
    pandas=0.18.1 \
    patsy=0.4.1 \
    statsmodels=0.6.1 \
    matplotlib=1.5.2 \
    ggplot=0.9.4 \
    scikit-learn=0.17.1 \
    distribute=0.6.45 \
    backports.ssl-match-hostname=3.5.0.1 \
    certifi=14.05.14 \
    nose_parameterized=0.5.0 \
    pyparsing=2.1.4 \
    python-dateutil=2.5.3 \
    pytz=2016.6.1 \
    pyzmq=15.3.0 \
    simplejson=3.3.3 \
    six=1.10.0 \
    sympy=1.0 \
    tornado=4.4.1 \
    virtualenv=13.0.1 \
    wsgiref=0.1.2 \
    python-swiftclient=2.7.0 \
    python-cinderclient=1.1.2 \
    python-glanceclient=0.17.2 \
    python-neutronclient=2.4.0 \
    networkx=1.11 \
    pysal=1.11.1 \
    pyyaml=3.11 \
    shapely=1.5.13 \
    beautifulsoup4=4.4.1 \
    nltk=3.2.1 \
    requests=2.10.0 \
    seaborn=0.5.0 \
    h5py=2.6.0 \
    xlrd=1.0.0 \
    markupsafe=0.23 \
    crypto=1.1.0 \
    jinja2=2.8 \
    openpyxl=2.3.2 \
    jaro_winkler=1.0.2 \
    bokeh=0.12.1 \
    numexpr=2.6.1 \
    pytables=3.2.3.1 \
    pycurl=7.43.0 \
    mgrs=1.1.0 \
    psutil=4.3.0 \
    biopython=1.67 \
    enaml=0.9.8 \
    mdp=3.5 \
    bitarray=0.8.1 \
    clusterpy=0.9.9 \
    pyside=1.2.1 \
    pyqt=4.11.4 \
    parsedatetime=1.4 \
    pymysql=0.6.7 \
    pyodbc=3.0.10 \
    tabulate=0.7.2


The big difference: by listing all of those packages as build requirements, you're actually ensuring that they won't be in your final conda package.  Think of build requirements more like a compiler, or something that's necessary when you're building the package, but not when you're actually running it.
",,
Tornado unexpected output,https://stackoverflow.com/questions/42551015,Unit testing tornado applications: How to improve the display of error messages,"I am using unittest to test a tornado app having several handlers, one of which raises an exception. If I run the following test code with python test.py:

# test.py

import unittest
import tornado.web
import tornado.testing

class MainHandler(tornado.web.RequestHandler):
    def get(self):
        self.write('Hello World') # handler works correctly

class HandlerWithError(tornado.web.RequestHandler):
    def get(self):
        raise Exception('Boom') # handler raises an exception
        self.write('Hello World')

def make_app():
    return tornado.web.Application([
        (r'/main/', MainHandler),
        (r'/error/', HandlerWithError),
    ])

class TornadoTestCase(tornado.testing.AsyncHTTPTestCase):

    def get_app(self):
        return make_app()

    def test_main_handler(self):
        response = self.fetch('/main/')
        self.assertEqual(response.code, 200) # test should pass

    def test_handler_with_error(self):
        response = self.fetch('/error/')
        self.assertEqual(response.code, 200) # test should fail with error

if __name__ == '__main__':
    unittest.main()


the test output looks like:

ERROR:tornado.application:Uncaught exception GET /error/ (127.0.0.1)                                                                                                                   
HTTPServerRequest(protocol='http', host='localhost:36590', method='GET', uri='/error/', version='HTTP/1.1', remote_ip='127.0.0.1', headers={'Connection': 'close', 'Host': 'localhost:3
6590', 'Accept-Encoding': 'gzip'})                                                                                                                                                     
Traceback (most recent call last):                                                                                                                                                     
  File ""/usr/local/lib/python2.7/dist-packages/tornado/web.py"", line 1332, in _execute                                                                                                 
    result = method(*self.path_args, **self.path_kwargs)                                                                                                                               
  File ""test.py"", line 13, in get                                                                                                                                                      
    raise Exception('Boom') # handler raises an exception                                                                                                                              
Exception: Boom                                                                                                                                                                        
ERROR:tornado.access:500 GET /error/ (127.0.0.1) 19.16ms                                                                                                                               
F.                                                                                                                                                                                     
======================================================================                                                                                                                 
FAIL: test_handler_with_error (__main__.TornadoTestCase)                                                                                                                               
----------------------------------------------------------------------                                                                                                                 
Traceback (most recent call last):                                                                                                                                                     
  File ""/usr/local/lib/python2.7/dist-packages/tornado/testing.py"", line 118, in __call__                                                                                              
    result = self.orig_method(*args, **kwargs)                                                                                                                                         
  File ""test.py"", line 33, in test_handler_with_error                                                                                                                                  
    self.assertEqual(response.code, 200) # test should fail with error                                                                                                                 
AssertionError: 500 != 200                                                                                                                                                             

----------------------------------------------------------------------                                                                                                                 
Ran 2 tests in 0.034s                                                                                                                                                                  

FAILED (failures=1)       


However, I would expect unittest to report an Error for the second test, instead of a failing assertion. Moreover, the fact that the traceback for the 'Boom' exception appears before the unittest test report and does not include a reference to the failing test function makes it difficult to find the source of the exception.

Any suggestions how to handle this situation?

Thanks in advance!

EDIT

What I find unexpected is the fact that test_handler_with_error actually arrives at making the assertEqual assertion, instead of throwing the error. For example, the following code does not execute the self.assertEqualstatement, and consequently reports an ERROR instead of a FAIL in the test output:

# simple_test.py
import unittest

def foo():
    raise Exception('Boom')
    return 'bar'

class SimpleTestCase(unittest.TestCase):
    def test_failing_function(self):
        result = foo()
        self.assertEqual(result, 'bar')

if __name__ == '__main__':
    unittest.main()

",1,1168,"You can disable logging and only the test reports will appear:

logging.disable(logging.CRITICAL)


You can put that for example in


created TestCase subclass
test runner


More info How can I disable logging while running unit tests in Python Django?

Keep in mind that CI/CD systems actually use normalized report e.g. junit and then present it in more readable/elegant way - more info:


Python script to generate JUnit report from another testing result
How to output coverage XML with nosetests?

","This is expected behavior. Your test itself asserts that the return code is HTTP 200, and since this is a formal assert that is false, the outcome is a ""failure"" instead of an ""error"". You can suppress logs as mentioned in kwaranuk's answer, but then you lose the information about what actually caused the HTTP 500 error.

Why does your code reach the assert, instead of throwing? It's because your test code does not call HandlerWithError.get. Your test code begins an asynchronous HTTP GET operation with an HTTP client provided by the AsyncHTTPTestCase class. (Check the source code of that class for details.) The event loop runs until HandlerWithError.get receives the request over a localhost socket, and responds on that socket with an HTTP 500. When HandlerWithError.get fails, it doesn't raise an exception into your test function, any more than a failure at Google.com would raise an exception: it merely results in an HTTP 500.

Welcome to the world of async! There's no easy way to neatly associate the assertion error and the traceback from HandlerWithError.get().
",
Tornado unexpected result,https://stackoverflow.com/questions/65843310,Completion in IPython (jupyter) does now work (unexpected keyword argument &#39;column&#39;),"I'm using jupyter notebook, and it works fine, but when I press TAB, the auto-completion doesn't work.
I already checked all the similar cases in StackOverflow,  but none of the solutions worked for me.
I have also tried to do ""pip upgrade"" to: IPython, IPYKernel, Jedi, and Tornado, the upgrade works fine but the problem is still there.
I tried in Firefox, Chrome and Edge.
When I press TAB I can see those errors in the terminal:
[IPKernelApp] ERROR | Exception in message handler:
Traceback (most recent call last):
  File ""c:\users\tomer\appdata\local\programs\python\python39\lib\site-packages\ipykernel\kernelbase.py"", line 265, in dispatch_shell
    yield gen.maybe_future(handler(stream, idents, msg))
  File ""c:\users\tomer\appdata\local\programs\python\python39\lib\site-packages\tornado\gen.py"", line 762, in run
    value = future.result()
  File ""c:\users\tomer\appdata\local\programs\python\python39\lib\site-packages\tornado\gen.py"", line 234, in wrapper
    yielded = ctx_run(next, result)
  File ""c:\users\tomer\appdata\local\programs\python\python39\lib\site-packages\ipykernel\kernelbase.py"", line 580, in complete_request
    matches = yield gen.maybe_future(self.do_complete(code, cursor_pos))
  File ""c:\users\tomer\appdata\local\programs\python\python39\lib\site-packages\ipykernel\ipkernel.py"", line 356, in do_complete
    return self._experimental_do_complete(code, cursor_pos)
  File ""c:\users\tomer\appdata\local\programs\python\python39\lib\site-packages\ipykernel\ipkernel.py"", line 381, in _experimental_do_complete
    completions = list(_rectify_completions(code, raw_completions))
  File ""c:\users\tomer\appdata\local\programs\python\python39\lib\site-packages\IPython\core\completer.py"", line 484, in rectify_completions
    completions = list(completions)
  File ""c:\users\tomer\appdata\local\programs\python\python39\lib\site-packages\IPython\core\completer.py"", line 1818, in completions
    for c in self._completions(text, offset, _timeout=self.jedi_compute_type_timeout/1000):
  File ""c:\users\tomer\appdata\local\programs\python\python39\lib\site-packages\IPython\core\completer.py"", line 1861, in _completions
    matched_text, matches, matches_origin, jedi_matches = self._complete(
  File ""c:\users\tomer\appdata\local\programs\python\python39\lib\site-packages\IPython\core\completer.py"", line 2029, in _complete
    completions = self._jedi_matches(
  File ""c:\users\tomer\appdata\local\programs\python\python39\lib\site-packages\IPython\core\completer.py"", line 1373, in _jedi_matches
    interpreter = jedi.Interpreter(
  File ""c:\users\tomer\appdata\local\programs\python\python39\lib\site-packages\jedi\api\__init__.py"", line 725, in __init__
    super().__init__(code, environment=environment,
TypeError: __init__() got an unexpected keyword argument 'column'

I'll be glad if someone can help me with this case
",34,15847,"The solution from @techno1731 is sub-optimal because it just disables jedi rather than fixing the underlying issue.
The latest jedi (0.18) release is incompatible with IPython 7.19 see this discussion. IPython: 7.20 (released Feb 1st 2020) and 8.0 (not yet released) have a compatibility fix.
The correct workaround at this time is to upgrade IPython:
pip install -U ""ipython&gt;=7.20""

In future you can search for the final two lines of the trackback after removing all path fragments specific to your installation, this is searching for:
     super().__init__(code, environment=environment,
TypeError: __init__() got an unexpected keyword argument 'column'

This will give you the relevant issues on GitHub in the first two Google result as for today.

Note: this is a copy of my answer from Giant IPKernelApp Error Using Hydrogen in Atom question which indeed can appear unrelated given the Hydrogen/Atom setting. I will now vote to close all other questions on the topic as duplicate of this one.

","I encontered the same issue some time ago with Jupyterlab when working locally on my machine with virtual environments.
This is a problem with Jedi being too slow (or rather taking forever) to load the completion, what worked for me was to add the follwing line at the top of the notebook (for example where you typically do the imports):
# Jedi not working
%config Completer.use_jedi = False

This should do the trick.
","I'm using a previous version of jedi instead:
pip install jedi==0.17

that works for me.
"
Tornado unexpected result,https://stackoverflow.com/questions/48090119,Jupyter notebook: TypeError: __init__() got an unexpected keyword argument &#39;io_loop&#39;,"I recently installed jupyter notebooks on my macbook pro.
When I create a new notebook, I see the following exception coming continuously on the terminal where I started the notebook.

Monideeps-MacBook-Pro:PythonNotebooks monideepde$ jupyter-notebook 
[I 12:18:43.675 NotebookApp] Serving notebooks from local directory: /Users/monideepde/Documents/PythonNotebooks
[I 12:18:43.675 NotebookApp] 0 active kernels
[I 12:18:43.676 NotebookApp] The Jupyter Notebook is running at:
[I 12:18:43.676 NotebookApp] http://localhost:8888/?token=dcb1990694d91ded77f4287a588886ea567b5907ac8aeafa
[I 12:18:43.676 NotebookApp] Use Control-C to stop this server and shut down all kernels (twice to skip confirmation).
[C 12:18:43.677 NotebookApp] 

    Copy/paste this URL into your browser when you connect for the first time,
    to login with a token:
        http://localhost:8888/?token=dcb1990694d91ded77f4287a588886ea567b5907ac8aeafa
[I 12:18:43.896 NotebookApp] Accepting one-time-token-authenticated connection from ::1
[W 12:18:44.778 NotebookApp] 404 GET /static/components/moment/locale/en-gb.js?v=20180104121843 (::1) 21.10ms referer=http://localhost:8888/tree
[I 12:18:54.840 NotebookApp] Creating new notebook in 
[W 12:18:55.716 NotebookApp] 404 GET /static/components/moment/locale/en-gb.js?v=20180104121843 (::1) 3.06ms referer=http://localhost:8888/notebooks/Untitled.ipynb?kernel_name=python2
[I 12:18:55.920 NotebookApp] Kernel started: 5e16fa4b-3e35-4265-89b0-ab36bb0573f5
[W 12:18:55.941 NotebookApp] 404 GET /nbextensions/widgets/notebook/js/extension.js?v=20180104121843 (::1) 5.57ms referer=http://localhost:8888/notebooks/Untitled.ipynb?kernel_name=python2
[I 12:18:56.998 NotebookApp] Adapting to protocol v5.1 for kernel 5e16fa4b-3e35-4265-89b0-ab36bb0573f5
[E 12:18:57.001 NotebookApp] Uncaught exception in /api/kernels/5e16fa4b-3e35-4265-89b0-ab36bb0573f5/channels
    Traceback (most recent call last):
      File ""/Library/Python/2.7/site-packages/tornado-5.0a1-py2.7-macosx-10.13-intel.egg/tornado/websocket.py"", line 494, in _run_callback
        result = callback(*args, **kwargs)
      File ""/Library/Python/2.7/site-packages/notebook-5.2.2-py2.7.egg/notebook/services/kernels/handlers.py"", line 258, in open
        super(ZMQChannelsHandler, self).open()
      File ""/Library/Python/2.7/site-packages/notebook-5.2.2-py2.7.egg/notebook/base/zmqhandlers.py"", line 168, in open
        self.send_ping, self.ping_interval, io_loop=loop,
    TypeError: __init__() got an unexpected keyword argument 'io_loop'
[I 12:18:58.021 NotebookApp] Adapting to protocol v5.1 for kernel 5e16fa4b-3e35-4265-89b0-ab36bb0573f5


Python version is 2.7.

Any pointers to how I can resolve this?
",27,27953,"Downgrade tornado, this worked for me.

pip install tornado==4.5.3

based on: https://github.com/liftoff/GateOne/issues/689
","I modified the file ""/Library/Python/2.7/site-packages/notebook-5.2.2-py2.7.egg/‌​notebook/base/zmqhan‌​dlers.py"" to remove the io_loop argument.

Based on bkanuka's comment, this is the way to go until Jupyter releases a new version.
","It is a version incompatibility issue. Upgrading Jupyter solves it. Here is what you should try in macOS:

pip install --upgrade jupyter

"
Tornado unexpected result,https://stackoverflow.com/questions/52820353,Jupyter notebook kernel not connecting,"I have previously used Jupyter Notebook (python 2 and 3) on my mac. After not using it for a while, there seems to be a problem I am unable to fix. When starting the notebook from terminal using the command Jupyter notebook, and then select a notebook I want to work with, I get the error: 

""A connection to the notebook server could not be established. The notebook will continue trying to reconnect. Check your network connection or notebook server configuration.""


I am positive this has nothing to do with my internet connection (I have tried 2 machines connected to different networks).

I have read that the browser could be the problem, therefore I have installed Chrome and Firefox. However, I get this error when I am using Safari, Chrome, and Firefox. 

Then I read about it being a proxy issue. I have tried to add the server address of the notebook to the No proxy fields of the advanced network preferences of my mac, and of the advanced setting on Firefox. 

The only thing I can think of is that I am entering the wrong server address? 

I do not know much about setting and errors in general, hopefully someone can provide me with some alternative options to get Jupyter working again. 

Terminal output:

ValueError: signal only works in main thread
ERROR:tornado.application:Exception in callback &lt;functools.partial object at 0x111e2b208&gt;
Traceback (most recent call last):
  File ""/Users/xxx/anaconda2/lib/python2.7/site-packages/tornado/ioloop.py"", line 759, in _run_callback
    ret = callback()
  File ""/Users/xxx/anaconda2/lib/python2.7/site-packages/tornado/stack_context.py"", line 276, in null_wrapper
    return fn(*args, **kwargs)
  File ""/Users/xxx/anaconda2/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py"", line 536, in &lt;lambda&gt;
    self.io_loop.add_callback(lambda : self._handle_events(self.socket, 0))
  File ""/Users/xxx/anaconda2/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py"", line 450, in _handle_events
    self._handle_recv()
  File ""/Users/xxx/anaconda2/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py"", line 480, in _handle_recv
    self._run_callback(callback, msg)
  File ""/Users/xxx/anaconda2/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py"", line 432, in _run_callback
    callback(*args, **kwargs)
  File ""/Users/xxx/anaconda2/lib/python2.7/site-packages/tornado/stack_context.py"", line 276, in null_wrapper
    return fn(*args, **kwargs)
  File ""/Users/xxx/anaconda2/lib/python2.7/site-packages/ipykernel/kernelbase.py"", line 283, in dispatcher
    return self.dispatch_shell(stream, msg)
  File ""/Users/xxx/anaconda2/lib/python2.7/site-packages/ipykernel/kernelbase.py"", line 233, in dispatch_shell
    self.pre_handler_hook()
  File ""/Users/xxx/anaconda2/lib/python2.7/site-packages/ipykernel/kernelbase.py"", line 248, in pre_handler_hook
    self.saved_sigint_handler = signal(SIGINT, default_int_handler)
ValueError: signal only works in main thread
[I 19:03:26.685 NotebookApp] KernelRestarter: restarting kernel (1/5), new random ports
Traceback (most recent call last):
  File ""/Users/xxx/anaconda2/lib/python2.7/runpy.py"", line 174, in _run_module_as_main
    ""__main__"", fname, loader, pkg_name)
  File ""/Users/xxx/anaconda2/lib/python2.7/runpy.py"", line 72, in _run_code
    exec code in run_globals
  File ""/Users/xxx/anaconda2/lib/python2.7/site-packages/ipykernel_launcher.py"", line 16, in &lt;module&gt;
    app.launch_new_instance()
  File ""/Users/xxx/anaconda2/lib/python2.7/site-packages/traitlets/config/application.py"", line 658, in launch_instance
    app.start()
  File ""/Users/xxx/anaconda2/lib/python2.7/site-packages/ipykernel/kernelapp.py"", line 477, in start
    ioloop.IOLoop.instance().start()
  File ""/Users/xxx/anaconda2/lib/python2.7/site-packages/tornado/ioloop.py"", line 928, in start
    raise RuntimeError(""IOLoop is already running"")
RuntimeError: IOLoop is already running
[I 19:03:29.696 NotebookApp] KernelRestarter: restarting kernel (2/5), new random ports
Traceback (most recent call last):
  File ""/Users/xxx/anaconda2/lib/python2.7/runpy.py"", line 174, in _run_module_as_main
    ""__main__"", fname, loader, pkg_name)
  File ""/Users/xxx/anaconda2/lib/python2.7/runpy.py"", line 72, in _run_code
    exec code in run_globals
  File ""/Users/xxx/anaconda2/lib/python2.7/site-packages/ipykernel_launcher.py"", line 16, in &lt;module&gt;
    app.launch_new_instance()
  File ""/Users/xxx/anaconda2/lib/python2.7/site-packages/traitlets/config/application.py"", line 658, in launch_instance
    app.start()
  File ""/Users/xxx/anaconda2/lib/python2.7/site-packages/ipykernel/kernelapp.py"", line 477, in start
    ioloop.IOLoop.instance().start()
  File ""/Users/xxx/anaconda2/lib/python2.7/site-packages/tornado/ioloop.py"", line 928, in start
    raise RuntimeError(""IOLoop is already running"")
RuntimeError: IOLoop is already running
[I 19:03:32.712 NotebookApp] KernelRestarter: restarting kernel (3/5), new random ports
Traceback (most recent call last):
  File ""/Users/xxx/anaconda2/lib/python2.7/runpy.py"", line 174, in _run_module_as_main
    ""__main__"", fname, loader, pkg_name)
  File ""/Users/xxx/anaconda2/lib/python2.7/runpy.py"", line 72, in _run_code
    exec code in run_globals
  File ""/Users/xxx/anaconda2/lib/python2.7/site-packages/ipykernel_launcher.py"", line 16, in &lt;module&gt;
    app.launch_new_instance()
  File ""/Users/xxx/anaconda2/lib/python2.7/site-packages/traitlets/config/application.py"", line 658, in launch_instance
    app.start()
  File ""/Users/xxx/anaconda2/lib/python2.7/site-packages/ipykernel/kernelapp.py"", line 477, in start
    ioloop.IOLoop.instance().start()
  File ""/Users/xxx/anaconda2/lib/python2.7/site-packages/tornado/ioloop.py"", line 928, in start
    raise RuntimeError(""IOLoop is already running"")
RuntimeError: IOLoop is already running
[W 19:03:33.717 NotebookApp] Timeout waiting for kernel_info reply from 2cf24420-719b-4666-ad9f-fcdf8db505f3
[E 19:03:33.721 NotebookApp] Uncaught exception in /api/kernels/2cf24420-719b-4666-ad9f-fcdf8db505f3/channels
    Traceback (most recent call last):
      File ""/Users/xxx/anaconda2/lib/python2.7/site-packages/tornado/websocket.py"", line 498, in _run_callback
        result = callback(*args, **kwargs)
      File ""/Users/xxx/anaconda2/lib/python2.7/site-packages/notebook/services/kernels/handlers.py"", line 262, in open
        super(ZMQChannelsHandler, self).open()
      File ""/Users/xxx/anaconda2/lib/python2.7/site-packages/notebook/base/zmqhandlers.py"", line 176, in open
        self.send_ping, self.ping_interval, io_loop=loop,
    TypeError: __init__() got an unexpected keyword argument 'io_loop'
[E 19:03:34.749 NotebookApp] Uncaught exception in /api/kernels/2cf24420-719b-4666-ad9f-fcdf8db505f3/channels
    Traceback (most recent call last):
      File ""/Users/xxx/anaconda2/lib/python2.7/site-packages/tornado/websocket.py"", line 498, in _run_callback
        result = callback(*args, **kwargs)
      File ""/Users/xxx/anaconda2/lib/python2.7/site-packages/notebook/services/kernels/handlers.py"", line 262, in open
        super(ZMQChannelsHandler, self).open()
      File ""/Users/xxx/anaconda2/lib/python2.7/site-packages/notebook/base/zmqhandlers.py"", line 176, in open
        self.send_ping, self.ping_interval, io_loop=loop,
    TypeError: __init__() got an unexpected keyword argument 'io_loop'
[I 19:03:35.724 NotebookApp] KernelRestarter: restarting kernel (4/5), new random ports
[E 19:03:35.771 NotebookApp] Uncaught exception in /api/kernels/2cf24420-719b-4666-ad9f-fcdf8db505f3/channels
    Traceback (most recent call last):
      File ""/Users/xxx/anaconda2/lib/python2.7/site-packages/tornado/websocket.py"", line 498, in _run_callback
        result = callback(*args, **kwargs)
      File ""/Users/xxx/anaconda2/lib/python2.7/site-packages/notebook/services/kernels/handlers.py"", line 262, in open
        super(ZMQChannelsHandler, self).open()
      File ""/Users/xxx/anaconda2/lib/python2.7/site-packages/notebook/base/zmqhandlers.py"", line 176, in open
        self.send_ping, self.ping_interval, io_loop=loop,
    TypeError: __init__() got an unexpected keyword argument 'io_loop'
Traceback (most recent call last):
  File ""/Users/xxx/anaconda2/lib/python2.7/runpy.py"", line 174, in _run_module_as_main
    ""__main__"", fname, loader, pkg_name)
  File ""/Users/xxx/anaconda2/lib/python2.7/runpy.py"", line 72, in _run_code
    exec code in run_globals
  File ""/Users/xxx/anaconda2/lib/python2.7/site-packages/ipykernel_launcher.py"", line 16, in &lt;module&gt;
    app.launch_new_instance()
  File ""/Users/xxx/anaconda2/lib/python2.7/site-packages/traitlets/config/application.py"", line 658, in launch_instance
    app.start()
  File ""/Users/xxx/anaconda2/lib/python2.7/site-packages/ipykernel/kernelapp.py"", line 477, in start
    ioloop.IOLoop.instance().start()
  File ""/Users/xxx/anaconda2/lib/python2.7/site-packages/tornado/ioloop.py"", line 928, in start
    raise RuntimeError(""IOLoop is already running"")
RuntimeError: IOLoop is already running
[E 19:03:36.791 NotebookApp] Uncaught exception in /api/kernels/2cf24420-719b-4666-ad9f-fcdf8db505f3/channels
    Traceback (most recent call last):
      File ""/Users/xxx/anaconda2/lib/python2.7/site-packages/tornado/websocket.py"", line 498, in _run_callback
        result = callback(*args, **kwargs)
      File ""/Users/xxx/anaconda2/lib/python2.7/site-packages/notebook/services/kernels/handlers.py"", line 262, in open
        super(ZMQChannelsHandler, self).open()
      File ""/Users/xxx/anaconda2/lib/python2.7/site-packages/notebook/base/zmqhandlers.py"", line 176, in open
        self.send_ping, self.ping_interval, io_loop=loop,
    TypeError: __init__() got an unexpected keyword argument 'io_loop'
[E 19:03:37.810 NotebookApp] Uncaught exception in /api/kernels/2cf24420-719b-4666-ad9f-fcdf8db505f3/channels
    Traceback (most recent call last):
      File ""/Users/xxx/anaconda2/lib/python2.7/site-packages/tornado/websocket.py"", line 498, in _run_callback
        result = callback(*args, **kwargs)
      File ""/Users/xxx/anaconda2/lib/python2.7/site-packages/notebook/services/kernels/handlers.py"", line 262, in open
        super(ZMQChannelsHandler, self).open()
      File ""/Users/xxx/anaconda2/lib/python2.7/site-packages/notebook/base/zmqhandlers.py"", line 176, in open
        self.send_ping, self.ping_interval, io_loop=loop,
    TypeError: __init__() got an unexpected keyword argument 'io_loop'
[W 19:03:38.733 NotebookApp] KernelRestarter: restart failed
[W 19:03:38.733 NotebookApp] Kernel 2cf24420-719b-4666-ad9f-fcdf8db505f3 died, removing from map.
[W 19:03:38.830 NotebookApp] 404 GET /api/kernels/2cf24420-719b-4666-ad9f-fcdf8db505f3/channels?session_id=3F75F379F865488D8379D3BB7FC0BF0F (::1): Kernel does not exist: 2cf24420-719b-4666-ad9f-fcdf8db505f3
[W 19:03:38.838 NotebookApp] 404 GET /api/kernels/2cf24420-719b-4666-ad9f-fcdf8db505f3/channels?session_id=3F75F379F865488D8379D3BB7FC0BF0F (::1) 12.00ms referer=None
[W 19:03:40.859 NotebookApp] Replacing stale connection: 2cf24420-719b-4666-ad9f-fcdf8db505f3:3F75F379F865488D8379D3BB7FC0BF0F

",9,37442,"Running the following version of jupyter and tornado seemed to solve the issue: 

Jupyter: 4.4.0

Tornado: 4.5.3

I needed to downgrade my version of tornado (which was 5.0.2)
","I faced this problem too. My problem was because of a custom module name. I named the module 'copy.py.' My issue was solved when I renamed it.
","I found a way out, Installing Python on my PC solved it. I didn't install python before installing anaconda
"
Tornado unexpected result,https://stackoverflow.com/questions/41783800,Why can&#39;t I connect to Gremlin-Server?,"Abstract

I'm trying to set up a Titan/Cassandra/Gremlin-Server stack in Docker (v1.13.0).  The problem I'm facing is that applications trying to connect to Gremlin-Server on the default port 8182 are reporting errors (details below).

First, here is some relevant version information:


Cassandra v2.2.8
Titan v1.0.0 (Hadoop 1)
Gremlin 3.2.3


Setup

Setup takes place in a Dockerfile in order to be reproducible.  It assumes that a Cassandra container already exists, running a cassandra.yaml in which start_rpc has been set to true.

The Dockerfile is as follows:

FROM openjdk:alpine

ENV TITAN 'titan-1.0.0-hadoop1'

RUN apk update &amp;&amp; apk add bash unzip &amp;&amp; rm -rf /var/cache/apk/* \
    &amp;&amp; adduser -S -s /bin/bash -D srg \
    &amp;&amp; wget -O /tmp/$TITAN.zip http://s3.thinkaurelius.com/downloads/titan/$TITAN.zip \
    &amp;&amp; unzip /tmp/$TITAN.zip -d /opt &amp;&amp; ln -s /opt/$TITAN /opt/titan \
    &amp;&amp; rm /tmp/*.zip \
    &amp;&amp; chown -R srg /opt/$TITAN/ \
    &amp;&amp; /opt/titan/bin/gremlin-server.sh -i org.apache.tinkerpop gremlin-python 3.2.3

COPY conf/gremlin-server/* /opt/$TITAN/conf/gremlin-server/

USER srg
WORKDIR /opt/titan
EXPOSE 8182

CMD [""bin/gremlin-server.sh"", ""conf/gremlin-server/srg.yaml""]


The astute reader will note that I am copying custom configuration files into the container, namely a Gremlin-Server configuration file (srg.yaml) and a titan graph properties file (srg.properties).

srg.yaml

host: localhost
port: 8182
threadPoolWorker: 1
gremlinPool: 8
scriptEvaluationTimeout: 30000
serializedResponseTimeout: 30000
channelizer: org.apache.tinkerpop.gremlin.server.channel.WebSocketChannelizer
graphs: {
  graph: conf/gremlin-server/srg.properties
  }
plugins:
  - aurelius.titan
scriptEngines: {
  gremlin-groovy: {
    imports: [java.lang.Math],
    staticImports: [java.lang.Math.PI],
    scripts: [scripts/empty-sample.groovy]},
  gremlin-jython: {},
  gremlin-python: {},
  nashorn: {
      imports: [java.lang.Math],
      staticImports: [java.lang.Math.PI]}}
serializers:
  - { className: org.apache.tinkerpop.gremlin.driver.ser.GryoMessageSerializerV1d0, config: { useMapperFromGraph: graph }}
  - { className: org.apache.tinkerpop.gremlin.driver.ser.GryoMessageSerializerV1d0, config: { serializeResultToString: true }}
  - { className: org.apache.tinkerpop.gremlin.driver.ser.GraphSONMessageSerializerGremlinV1d0, config: { useMapperFromGraph: graph }}
  - { className: org.apache.tinkerpop.gremlin.driver.ser.GraphSONMessageSerializerV1d0, config: { useMapperFromGraph: graph }}
processors:
  - { className: org.apache.tinkerpop.gremlin.server.op.session.SessionOpProcessor, config: { sessionTimeout: 28800000 }}
metrics: {
  consoleReporter: {enabled: true, interval: 180000},
  csvReporter: {enabled: true, interval: 180000, fileName: /tmp/gremlin-server-metrics.csv},
  jmxReporter: {enabled: true},
  slf4jReporter: {enabled: true, interval: 180000},
  gangliaReporter: {enabled: false, interval: 180000, addressingMode: MULTICAST},
  graphiteReporter: {enabled: false, interval: 180000}}
threadPoolBoss: 1
maxInitialLineLength: 4096
maxHeaderSize: 8192
maxChunkSize: 8192
maxContentLength: 65536
maxAccumulationBufferComponents: 1024
resultIterationBatchSize: 64
writeBufferLowWaterMark: 32768
writeBufferHighWaterMark: 65536
ssl: {
  enabled: false}


srg.properties

gremlin.graph=com.thinkaurelius.titan.core.TitanFactory
storage.backend=cassandrathrift
storage.hostname=cassandra  # refers to the linked container
cache.db-cache = true
cache.db-cache-clean-wait = 20
cache.db-cache-time = 180000
cache.db-cache-size = 0.25

# Start elasticsearch inside the Titan JVM
index.search.backend=elasticsearch
index.search.directory=db/es
index.search.elasticsearch.client-only=false
index.search.elasticsearch.local-mode=true


Execution

The container is run with the following command:  docker run -ti --rm=true --link test.cassandra:cassandra -p 8182:8182 titan.

Here is the log output for Gremlin-Server:

0    [main] INFO  org.apache.tinkerpop.gremlin.server.GremlinServer  - 
         \,,,/
         (o o)
-----oOOo-(3)-oOOo-----

297  [main] INFO  org.apache.tinkerpop.gremlin.server.GremlinServer  - Configuring Gremlin Server from conf/gremlin-server/srg.yaml
439  [main] INFO  org.apache.tinkerpop.gremlin.server.util.MetricManager  - Configured Metrics ConsoleReporter configured with report interval=180000ms
448  [main] INFO  org.apache.tinkerpop.gremlin.server.util.MetricManager  - Configured Metrics CsvReporter configured with report interval=180000ms to fileName=/tmp/gremlin-server-metrics.csv
557  [main] INFO  org.apache.tinkerpop.gremlin.server.util.MetricManager  - Configured Metrics JmxReporter configured with domain= and agentId=
561  [main] INFO  org.apache.tinkerpop.gremlin.server.util.MetricManager  - Configured Metrics Slf4jReporter configured with interval=180000ms and loggerName=org.apache.tinkerpop.gremlin.server.Settings$Slf4jReporterMetrics
1750 [main] INFO  com.thinkaurelius.titan.core.util.ReflectiveConfigOptionLoader  - Loaded and initialized config classes: 12 OK out of 12 attempts in PT0.148S
1972 [main] INFO  com.thinkaurelius.titan.diskstorage.cassandra.thrift.CassandraThriftStoreManager  - Closed Thrift connection pooler.
1990 [main] INFO  com.thinkaurelius.titan.graphdb.configuration.GraphDatabaseConfiguration  - Generated unique-instance-id=ac1100031-ad2d5ffa52e81
2026 [main] INFO  com.thinkaurelius.titan.diskstorage.Backend  - Configuring index [search]
2386 [main] INFO  org.elasticsearch.node  - [Lunatik] version[1.5.1], pid[1], build[5e38401/2015-04-09T13:41:35Z]
2387 [main] INFO  org.elasticsearch.node  - [Lunatik] initializing ...
2399 [main] INFO  org.elasticsearch.plugins  - [Lunatik] loaded [], sites []
6471 [main] INFO  org.elasticsearch.node  - [Lunatik] initialized
6472 [main] INFO  org.elasticsearch.node  - [Lunatik] starting ...
6477 [main] INFO  org.elasticsearch.transport  - [Lunatik] bound_address {local[1]}, publish_address {local[1]}
6507 [main] INFO  org.elasticsearch.discovery  - [Lunatik] elasticsearch/u2StmRW1RsyEHw561yoNFw
6519 [elasticsearch[Lunatik][clusterService#updateTask][T#1]] INFO  org.elasticsearch.cluster.service  - [Lunatik] master {new [Lunatik][u2StmRW1RsyEHw561yoNFw][ad2d5ffa52e8][local[1]]{local=true}}, removed {[Lunatik][kKyL9UE-R123LLZTTrsVCw][ad2d5ffa52e8][local[1]]{local=true},}, reason: local-disco-initial_connect(master)
6908 [main] INFO  org.elasticsearch.http  - [Lunatik] bound_address {inet[/0:0:0:0:0:0:0:0:9200]}, publish_address {inet[/172.17.0.3:9200]}
6909 [main] INFO  org.elasticsearch.node  - [Lunatik] started
6923 [elasticsearch[Lunatik][clusterService#updateTask][T#1]] INFO  org.elasticsearch.gateway  - [Lunatik] recovered [0] indices into cluster_state
7486 [elasticsearch[Lunatik][clusterService#updateTask][T#1]] INFO  org.elasticsearch.cluster.metadata  - [Lunatik] [titan] creating index, cause [api], templates [], shards [5]/[1], mappings []
8075 [main] INFO  com.thinkaurelius.titan.diskstorage.Backend  - Initiated backend operations thread pool of size 4
8241 [main] INFO  com.thinkaurelius.titan.diskstorage.Backend  - Configuring total store cache size: 94787290
8641 [main] INFO  com.thinkaurelius.titan.diskstorage.log.kcvs.KCVSLog  - Loaded unidentified ReadMarker start time 2017-01-21T16:31:28.750Z into com.thinkaurelius.titan.diskstorage.log.kcvs.KCVSLog$MessagePuller@3520958b
8642 [main] INFO  org.apache.tinkerpop.gremlin.server.GremlinServer  - Graph [graph] was successfully configured via [conf/gremlin-server/srg.properties].
8643 [main] INFO  org.apache.tinkerpop.gremlin.server.util.ServerGremlinExecutor  - Initialized Gremlin thread pool.  Threads in pool named with pattern gremlin-*
14187 [main] INFO  com.jcabi.manifests.Manifests  - 108 attributes loaded from 264 stream(s) in 185ms, 108 saved, 3371 ignored: [""Agent-Class"", ""Ant-Version"", ""Archiver-Version"", ""Bnd-LastModified"", ""Boot-Class-Path"", ""Build-Date"", ""Build-Host"", ""Build-Id"", ""Build-Java-Version"", ""Build-Jdk"", ""Build-Job"", ""Build-Number"", ""Build-Time"", ""Build-Timestamp"", ""Build-Version"", ""Built-At"", ""Built-By"", ""Built-OS"", ""Built-On"", ""Built-Status"", ""Bundle-ActivationPolicy"", ""Bundle-Activator"", ""Bundle-BuddyPolicy"", ""Bundle-Category"", ""Bundle-ClassPath"", ""Bundle-Classpath"", ""Bundle-Copyright"", ""Bundle-Description"", ""Bundle-DocURL"", ""Bundle-License"", ""Bundle-Localization"", ""Bundle-ManifestVersion"", ""Bundle-Name"", ""Bundle-NativeCode"", ""Bundle-RequiredExecutionEnvironment"", ""Bundle-SymbolicName"", ""Bundle-Vendor"", ""Bundle-Version"", ""Can-Redefine-Classes"", ""Change"", ""Class-Path"", ""Created-By"", ""DynamicImport-Package"", ""Eclipse-AutoStart"", ""Eclipse-BuddyPolicy"", ""Eclipse-SourceReferences"", ""Embed-Dependency"", ""Embedded-Artifacts"", ""Export-Package"", ""Extension-Name"", ""Extension-name"", ""Fragment-Host"", ""Git-Commit-Branch"", ""Git-Commit-Date"", ""Git-Commit-Hash"", ""Git-Committer-Email"", ""Git-Committer-Name"", ""Gradle-Version"", ""Gremlin-Lib-Paths"", ""Gremlin-Plugin-Dependencies"", ""Gremlin-Plugin-Paths"", ""Ignore-Package"", ""Implementation-Build"", ""Implementation-Build-Date"", ""Implementation-Title"", ""Implementation-URL"", ""Implementation-Vendor"", ""Implementation-Vendor-Id"", ""Implementation-Version"", ""Import-Package"", ""Include-Resource"", ""JCabi-Build"", ""JCabi-Date"", ""JCabi-Version"", ""Java-Vendor"", ""Java-Version"", ""Main-Class"", ""Main-class"", ""Manifest-Version"", ""Maven-Version"", ""Module-Email"", ""Module-Origin"", ""Module-Owner"", ""Module-Source"", ""Originally-Created-By"", ""Os-Arch"", ""Os-Name"", ""Os-Version"", ""Package"", ""Premain-Class"", ""Private-Package"", ""Require-Bundle"", ""Require-Capability"", ""Scm-Connection"", ""Scm-Revision"", ""Scm-Url"", ""Specification-Title"", ""Specification-Vendor"", ""Specification-Version"", ""Tool"", ""X-Compile-Source-JDK"", ""X-Compile-Target-JDK"", ""hash"", ""implementation-version"", ""mode"", ""package"", ""url"", ""version""]
14842 [main] INFO  org.apache.tinkerpop.gremlin.groovy.engine.ScriptEngines  - Loaded gremlin-jython ScriptEngine
15540 [main] INFO  org.apache.tinkerpop.gremlin.groovy.engine.ScriptEngines  - Loaded nashorn ScriptEngine
16076 [main] INFO  org.apache.tinkerpop.gremlin.groovy.engine.ScriptEngines  - Loaded gremlin-python ScriptEngine
16553 [main] INFO  org.apache.tinkerpop.gremlin.groovy.engine.ScriptEngines  - Loaded gremlin-groovy ScriptEngine
17410 [main] INFO  org.apache.tinkerpop.gremlin.groovy.engine.GremlinExecutor  - Initialized gremlin-groovy ScriptEngine with scripts/empty-sample.groovy
17410 [main] INFO  org.apache.tinkerpop.gremlin.server.util.ServerGremlinExecutor  - Initialized GremlinExecutor and configured ScriptEngines.
17419 [main] INFO  org.apache.tinkerpop.gremlin.server.util.ServerGremlinExecutor  - A GraphTraversalSource is now bound to [g] with graphtraversalsource[standardtitangraph[cassandrathrift:[cassandra]], standard]
17565 [main] INFO  org.apache.tinkerpop.gremlin.server.AbstractChannelizer  - Configured application/vnd.gremlin-v1.0+gryo with org.apache.tinkerpop.gremlin.driver.ser.GryoMessageSerializerV1d0
17566 [main] INFO  org.apache.tinkerpop.gremlin.server.AbstractChannelizer  - Configured application/vnd.gremlin-v1.0+gryo-stringd with org.apache.tinkerpop.gremlin.driver.ser.GryoMessageSerializerV1d0
17808 [main] INFO  org.apache.tinkerpop.gremlin.server.AbstractChannelizer  - Configured application/vnd.gremlin-v1.0+json with org.apache.tinkerpop.gremlin.driver.ser.GraphSONMessageSerializerGremlinV1d0
17811 [main] INFO  org.apache.tinkerpop.gremlin.server.AbstractChannelizer  - Configured application/json with org.apache.tinkerpop.gremlin.driver.ser.GraphSONMessageSerializerV1d0
17958 [gremlin-server-boss-1] INFO  org.apache.tinkerpop.gremlin.server.GremlinServer  - Gremlin Server configured with worker thread pool of 1, gremlin pool of 8 and boss thread pool of 1.
17959 [gremlin-server-boss-1] INFO  org.apache.tinkerpop.gremlin.server.GremlinServer  - Channel started at port 8182.
1/21/17 4:34:20 PM =============================================================

-- Meters ----------------------------------------------------------------------
org.apache.tinkerpop.gremlin.server.GremlinServer.errors
             count = 0
         mean rate = 0.00 events/second
     1-minute rate = 0.00 events/second
     5-minute rate = 0.00 events/second
    15-minute rate = 0.00 events/second


180564 [metrics-logger-reporter-thread-1] INFO  org.apache.tinkerpop.gremlin.server.Settings$Slf4jReporterMetrics  - type=METER, name=org.apache.tinkerpop.gremlin.server.GremlinServer.errors, count=0, mean_rate=0.0, m1=0.0, m5=0.0, m15=0.0, rate_unit=events/second


Symptoms

So far, everything appears to be working as intended.  The logs indicate that I am able to load srg.properties and bind the data structure to a variable called graph.

The problem appears when I try to connect to the Gremlin-Server instance over the exported port 8182, for example using gremlin-python:

# executed via python 3.6.0 on the host machine, i.e. not inside of Docker
from gremlin_python import statics
from gremlin_python.structure.graph import Graph
from gremlin_python.process.graph_traversal import __
from gremlin_python.process.strategies import *
from gremlin_python.driver.driver_remote_connection import DriverRemoteConnection

g = graph.traversal().withRemote(DriverRemoteConnection('ws://localhost:8182/gremlin','graph'))


produces the following exception ...

---------------------------------------------------------------------------
HTTPError                                 Traceback (most recent call last)
&lt;ipython-input-10-59ad504f29b4&gt; in &lt;module&gt;()
----&gt; 1 g = graph.traversal().withRemote(DriverRemoteConnection('ws://localhost:8182/','g'))

/Users/lthibault/.pyenv/versions/3.6.0/lib/python3.6/site-packages/gremlin_python/driver/driver_remote_connection.py in __init__(self, url, traversal_source, username, password, loop, graphson_reader, graphson_writer)
     41         self._password = password
     42         if loop is None: self._loop = ioloop.IOLoop.current()
---&gt; 43         self._websocket = self._loop.run_sync(lambda: websocket.websocket_connect(self.url))
     44         self._graphson_reader = graphson_reader or GraphSONReader()
     45         self._graphson_writer = graphson_writer or GraphSONWriter()

/Users/lthibault/.pyenv/versions/3.6.0/lib/python3.6/site-packages/tornado/ioloop.py in run_sync(self, func, timeout)
    455         if not future_cell[0].done():
    456             raise TimeoutError('Operation timed out after %s seconds' % timeout)
--&gt; 457         return future_cell[0].result()
    458 
    459     def time(self):

/Users/lthibault/.pyenv/versions/3.6.0/lib/python3.6/site-packages/tornado/concurrent.py in result(self, timeout)
    235             return self._result
    236         if self._exc_info is not None:
--&gt; 237             raise_exc_info(self._exc_info)
    238         self._check_done()
    239         return self._result

/Users/lthibault/.pyenv/versions/3.6.0/lib/python3.6/site-packages/tornado/util.py in raise_exc_info(exc_info)

HTTPError: HTTP 599: Stream closed


Suspecting a problem specific to this library:

1)  attempt to connect to the websocket port with nc

$ nc -z -v localhost 8182
found 0 associations
found 1 connections:
     1: flags=82&lt;CONNECTED,PREFERRED&gt;
    outif lo0
    src ::1 port 58627
    dst ::1 port 8182
    rank info not available
    TCP aux info available

Connection to localhost port 8182 [tcp/*] succeeded!


2)  attempt to connect to Gremlin-Server using a different client library, namely go-gremlin

Test case:

package main

import (
    ""fmt""
    ""log""

    ""github.com/go-gremlin/gremlin""
)

func main() {
    if err := gremlin.NewCluster(""ws://localhost:8182/gremlin""); err != nil {
        log.Fatal(err)
    }

    data, err := gremlin.Query(`graph.V()`).Exec()
    if err != nil {
        log.Fatalf(""Query error: %s"", err)
    }

    fmt.Println(string(data))
}


Output:

$ go run cmd/test/main.go 
2017/01/21 14:47:42 Query error: unexpected EOF
exit status 1


Current Conclusions &amp; Questions

From the previous tests, I conclude that this is an application-level problem (i.e. a problem on the websocket or ws protocol level, not a problem in the host or container networking stack).  Indeed, nc reports that the socket connection is successful, but in both the Python and Go client libraries ostensibly complain of an inappropriate (empty) response from the server.

I have tried removing the /gremlin path from the websocket URL both in gremlin-python and in go-gremlin, to no avail.

My question is: where do I go from here?  Any suggestions or diagnostic paths would be most appreciated!
",7,8611,"The main problem is that the host in your Gremlin Server configuration is set to the default which is localhost. This will only allow connections from the server itself. You need to change the value to an external IP of the server or 0.0.0.0.

The other issue is that gremlin-python server plugin was made available with Apache TinkerPop 3.2.2. Titan 1.0.0 uses TinkerPop 3.0.1. I dobut that the gremlin-python 3.2.3 plugin will work with Titan 1.0.0.

Update: Consider using JanusGraph 0.1.1 which uses TinkerPop 3.2.3. JanusGraph was forked from Titan, so the code is basically the same with updated dependencies.
",,
Tornado unexpected result,https://stackoverflow.com/questions/66141235,Jupyter with docker: __init__() got an unexpected keyword argument &#39;column&#39;,"I recently installed TensorFlow with GPU support using docker:
docker pull tensorflow/tensorflow:latest-gpu-jupyter

But sometimes when I start a jupyter notebook server using the command:
docker run --gpus all -it -p 8888:8888 tensorflow/tensorflow:latest-gpu-jupyter jupyter notebook --notebook-dir=/tf --ip 0.0.0.0 --allow-root --NotebookApp.allow_origin='https://colab.research.google.com'

I see the following exception on the terminal:
[IPKernelApp] ERROR | Exception in message handler:
Traceback (most recent call last):
  File ""/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py"", line 272, in dispatch_shell
    yield gen.maybe_future(handler(stream, idents, msg))
  File ""/usr/local/lib/python3.6/dist-packages/tornado/gen.py"", line 762, in run
    value = future.result()
  File ""/usr/local/lib/python3.6/dist-packages/tornado/gen.py"", line 234, in wrapper
    yielded = ctx_run(next, result)
  File ""/usr/local/lib/python3.6/dist-packages/tornado/gen.py"", line 162, in _fake_ctx_run
    return f(*args, **kw)
  File ""/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py"", line 580, in complete_request
    matches = yield gen.maybe_future(self.do_complete(code, cursor_pos))
  File ""/usr/local/lib/python3.6/dist-packages/ipykernel/ipkernel.py"", line 348, in do_complete
    return self._experimental_do_complete(code, cursor_pos)
  File ""/usr/local/lib/python3.6/dist-packages/ipykernel/ipkernel.py"", line 373, in _experimental_do_complete
    completions = list(_rectify_completions(code, raw_completions))
  File ""/usr/local/lib/python3.6/dist-packages/IPython/core/completer.py"", line 484, in rectify_completions
    completions = list(completions)
  File ""/usr/local/lib/python3.6/dist-packages/IPython/core/completer.py"", line 1818, in completions
    for c in self._completions(text, offset, _timeout=self.jedi_compute_type_timeout/1000):
  File ""/usr/local/lib/python3.6/dist-packages/IPython/core/completer.py"", line 1862, in _completions
    full_text=full_text, cursor_line=cursor_line, cursor_pos=cursor_column)
  File ""/usr/local/lib/python3.6/dist-packages/IPython/core/completer.py"", line 2030, in _complete
    cursor_pos, cursor_line, full_text)
  File ""/usr/local/lib/python3.6/dist-packages/IPython/core/completer.py"", line 1374, in _jedi_matches
    text[:offset], namespaces, column=cursor_column, line=cursor_line + 1)
  File ""/usr/local/lib/python3.6/dist-packages/jedi/api/__init__.py"", line 726, in __init__
    project=Project(Path.cwd()), **kwds)
TypeError: __init__() got an unexpected keyword argument 'column'

After that, I have to restart the server or reconnect from google colab.
Any ideas where the error might come from and how to fix it?
",4,1698,"This seems to be an incompatibility between jedi and ipython, see this issue.
The fix would be to pin jedi to 0.17.2, so either run:
pip install jedi==0.17.2

Or if you are using poetry add this to your pyproject.toml:
jedi = ""&lt;=0.17.2""

But since you are using a docker image that image will need to be updated. It seems to be gpu-jupyter.Dockerfile.
I would raise an issue on that project and see if they can pin jedi like they did for nbformat, or you could just fork it. They should probably upgrade python as well, 3.6 is getting a bit long in the tooth.
","I will add more details to @daphtdazz's answer. I had to do the following steps to solve this issue:
1. Download TensorFlow from github:
git clone https://github.com/tensorflow/tensorflow.git

2. Edit the file gpu-jupyter.Dockerfile to add jedi==0.17.2 at the end of line 104:
vim tensorflow/tensorflow/tools/dockerfiles/dockerfiles/gpu-jupyter.Dockerfile 

3. Placed myself inside the dockerfiles folder :
cd tensorflow/tensorflow/tools/dockerfiles/

4. Build the image:
docker build -f ./dockerfiles/gpu-jupyter.Dockerfile -t tf .

",
Tornado unexpected result,https://stackoverflow.com/questions/46626670,dask dataframe set_index throws error,"I have a dask dataframe created from parquet file on HDFS.
When creating setting index using api: set_index, it fails with below error.


  File ""/ebs/d1/agent/conda/envs/py361/lib/python3.6/site-packages/dask/dataframe/shuffle.py"", line 64, in set_index
      divisions, sizes, mins, maxes = base.compute(divisions, sizes, mins, maxes)
    File ""/ebs/d1/agent/conda/envs/py361/lib/python3.6/site-packages/dask/base.py"", line 206, in compute
      results = get(dsk, keys, **kwargs)
    File ""/ebs/d1/agent/conda/envs/py361/lib/python3.6/site-packages/distributed/client.py"", line 1949, in get
      results = self.gather(packed, asynchronous=asynchronous)
    File ""/ebs/d1/agent/conda/envs/py361/lib/python3.6/site-packages/distributed/client.py"", line 1391, in gather
      asynchronous=asynchronous)
    File ""/ebs/d1/agent/conda/envs/py361/lib/python3.6/site-packages/distributed/client.py"", line 561, in sync
      return sync(self.loop, func, *args, **kwargs)
    File ""/ebs/d1/agent/conda/envs/py361/lib/python3.6/site-packages/distributed/utils.py"", line 241, in sync
      six.reraise(*error[0])
    File ""/ebs/d1/agent/conda/envs/py361/lib/python3.6/site-packages/six.py"", line 693, in reraise
      raise value
    File ""/ebs/d1/agent/conda/envs/py361/lib/python3.6/site-packages/distributed/utils.py"", line 229, in f
      result[0] = yield make_coro()
    File ""/ebs/d1/agent/conda/envs/py361/lib/python3.6/site-packages/tornado/gen.py"", line 1055, in run
      value = future.result()
    File ""/ebs/d1/agent/conda/envs/py361/lib/python3.6/site-packages/tornado/concurrent.py"", line 238, in result
      raise_exc_info(self._exc_info)
    File """", line 4, in raise_exc_info
    File ""/ebs/d1/agent/conda/envs/py361/lib/python3.6/site-packages/tornado/gen.py"", line 1063, in run
      yielded = self.gen.throw(*exc_info)
    File ""/ebs/d1/agent/conda/envs/py361/lib/python3.6/site-packages/distributed/client.py"", line 1269, in _gather
      traceback)
    File ""/ebs/d1/agent/conda/envs/py361/lib/python3.6/site-packages/six.py"", line 692, in reraise
      raise value.with_traceback(tb)
    File ""/ebs/d1/agent/conda/envs/py361/lib/python3.6/site-packages/dask/dataframe/io/parquet.py"", line 144, in _read_parquet_row_group
      open=open, assign=views, scheme=scheme)
  TypeError: read_row_group_file() got an unexpected keyword argument 'scheme'


Can some one point me to the reason of this error and how to fix it.
",4,906,"Solution
Upgrade fastparquet to version 0.1.3.
Details
Dask 0.15.4, used for your example, includes this commit, which adds the argument scheme to read_row_group_file(). This throws an error for fastparquet versions before 0.1.3.
",,
Tornado unexpected result,https://stackoverflow.com/questions/33353768,AsyncHTTPTestCase Post request Passing Data,"Trying to test a POST request using AsyncHTTPTestCase on Python 3.4, Tornado version 4.1.

from tornado.testing import AsyncHTTPTestCase
from myapp  import myclass

class TestFooHandler(AsyncHTTPTestCase):

    def get_app(self):
        return myclass.application

    def test_post_handler(self):
        import urllib.parse
        post_body = urllib.parse.urlencode({""key"":""val""})
        response = self.fetch(""/foo"", method=""POST"", data=post_body)
        self.assertEqual(response.code, 200)


The code fails with message:
unexpected keyword argument 'data'

Here's the complete trace:

Traceback (most recent call last):
  File ""/usr/local/lib/python3.4/dist-packages/tornado/testing.py"", line 120, in __call__
    result = self.orig_method(*args, **kwargs)
  File ""/home/gub/App/unit_tests/test_cors.py"", line 22, in test_post_handler
    response = self.fetch(""/foo"", method=""POST"", data=post_body)
  File ""/usr/local/lib/python3.4/dist-packages/tornado/testing.py"", line 380, in fetch
    self.http_client.fetch(self.get_url(path), self.stop, **kwargs)
  File ""/usr/local/lib/python3.4/dist-packages/tornado/httpclient.py"", line 227, in fetch
    request = HTTPRequest(url=request, **kwargs)
TypeError: __init__() got an unexpected keyword argument 'data'


What is not correct with the above code ?
",4,636,"The argument name is body, not data. self.fetch(""/foo"", method=""POST"", body=post_body)
",,
Tornado unexpected result,https://stackoverflow.com/questions/65693469,Giant IPKernelApp Error Using Hydrogen in Atom,"Starting a few days ago, after months without issue, I began having a giant error box keep popping up when editing my code in Atom. I believe it is from the Hydrogen plugin, the weird thing is even with this error the code still runs and does what I want it too.
I created a new conda environment, installing only what I needed (pandas, geopandas, descartes, jupyter) and even when using the new environment in Atom I am getting this issue. I've tried upgrading ipykernel but it is already the most recent version.
Error Message

[IPKernelApp] ERROR | Exception in message handler: Traceback (most recent call last): File ""/home/username/anaconda3/envs/fresh/lib/python3.8/site-packages/ipykernel/kernelbase.py"", line 268, in dispatch_shell yield gen.maybe_future(handler(stream, idents, msg)) File ""/home/username/anaconda3/envs/fresh/lib/python3.8/site-packages/tornado/gen.py"", line 762, in run value = future.result() File ""/home/username/anaconda3/envs/fresh/lib/python3.8/site-packages/tornado/gen.py"", line 234, in wrapper yielded = ctx_run(next, result) File ""/home/username/anaconda3/envs/fresh/lib/python3.8/site-packages/ipykernel/kernelbase.py"", line 583, in complete_request matches = yield gen.maybe_future(self.do_complete(code, cursor_pos)) File ""/home/username/anaconda3/envs/fresh/lib/python3.8/site-packages/ipykernel/ipkernel.py"", line 360, in do_complete return self._experimental_do_complete(code, cursor_pos) File ""/home/username/anaconda3/envs/fresh/lib/python3.8/site-packages/ipykernel/ipkernel.py"", line 385, in _experimental_do_complete completions = list(_rectify_completions(code, raw_completions)) File ""/home/username/anaconda3/envs/fresh/lib/python3.8/site-packages/IPython/core/completer.py"", line 484, in rectify_completions completions = list(completions) File ""/home/username/anaconda3/envs/fresh/lib/python3.8/site-packages/IPython/core/completer.py"", line 1818, in completions for c in self._completions(text, offset, _timeout=self.jedi_compute_type_timeout/1000): File ""/home/username/anaconda3/envs/fresh/lib/python3.8/site-packages/IPython/core/completer.py"", line 1861, in _completions matched_text, matches, matches_origin, jedi_matches = self._complete( File ""/home/username/anaconda3/envs/fresh/lib/python3.8/site-packages/IPython/core/completer.py"", line 2029, in _complete completions = self._jedi_matches( File ""/home/username/anaconda3/envs/fresh/lib/python3.8/site-packages/IPython/core/completer.py"", line 1373, in _jedi_matches interpreter = jedi.Interpreter( File ""/home/username/anaconda3/envs/fresh/lib/python3.8/site-packages/jedi/api/init.py"", line 725, in init super().init(code, environment=environment, TypeError: init() got an unexpected keyword argument 'column'

",1,755,"The latest jedi (0.18) release is incompatible with IPython 7.19 see this discussion. IPython: 7.20 (released Feb 1st 2020) and 8.0 (not yet released) have a compatibility fix.
The correct workaround at this time is to upgrade IPython:
pip install -U ipython==7.20

In future you can search for the final two lines of the trackback after removing all path fragments specific to your installation, this is searching for:
line 2029, in _complete completions = self._jedi_matches IPython/core/completer.py, line 1373, in _jedi_matches interpreter = jedi.Interpreter( jedi/api/init.py, line 725, in init super().init(code, environment=environment, TypeError: init() got an unexpected keyword argument 'column'

This will give you the relevant issues on GitHub in the first two Google result as for today.
",,
Tornado unexpected result,https://stackoverflow.com/questions/58449314,"Even python3 environment is activated, JupyterLab cannot be launched properly","When I activate my ipykernel_py3 environment and try to launch Jupyter Lab in terminal, I get the repeated error messages as follows:

Macintosh-8:~ yuenfannie$ source activate ipykernel_py3
(ipykernel_py3) Macintosh-8:~ yuenfannie$ jupyter lab
[I 12:38:19.969 LabApp] JupyterLab alpha preview extension loaded from /Users/yuenfannie/anaconda2/lib/python2.7/site-packages/jupyterlab
JupyterLab v0.27.0
Known labextensions:
[I 12:38:19.971 LabApp] Running the core application with no additional extensions or settings
[I 12:38:19.979 LabApp] Serving notebooks from local directory: /Users/yuenfannie
[I 12:38:19.980 LabApp] 0 active kernels 
[I 12:38:19.980 LabApp] The Jupyter Notebook is running at: http://localhost:8888/?token=81a366305f2a328dfbbc9cfcf757a30e4977d3abab54cb0f
[I 12:38:19.980 LabApp] Use Control-C to stop this server and shut down all kernels (twice to skip confirmation).
[C 12:38:19.981 LabApp] 

    Copy/paste this URL into your browser when you connect for the first time,
    to login with a token:
        http://localhost:8888/?token=81a366305f2a328dfbbc9cfcf757a30e4977d3abab54cb0f
[I 12:38:20.229 LabApp] Accepting one-time-token-authenticated connection from ::1
[I 12:38:22.881 LabApp] Kernel started: cf6886dd-5475-4e1e-972c-8e4614451f0e
[I 12:38:24.074 LabApp] Adapting to protocol v5.0 for kernel cf6886dd-5475-4e1e-972c-8e4614451f0e
[E 12:38:24.078 LabApp] Uncaught exception GET /api/kernels/cf6886dd-5475-4e1e-972c-8e4614451f0e/channels?session_id=5c81adbda104d35cec3acd907d6decc4&amp;token=81a366305f2a328dfbbc9cfcf757a30e4977d3abab54cb0f (::1)
    HTTPServerRequest(protocol='http', host='localhost:8888', method='GET', uri='/api/kernels/cf6886dd-5475-4e1e-972c-8e4614451f0e/channels?session_id=5c81adbda104d35cec3acd907d6decc4&amp;token=81a366305f2a328dfbbc9cfcf757a30e4977d3abab54cb0f', version='HTTP/1.1', remote_ip='::1')
    Traceback (most recent call last):
      File ""/Users/yuenfannie/anaconda2/lib/python2.7/site-packages/tornado/websocket.py"", line 546, in _run_callback
        result = callback(*args, **kwargs)
      File ""/Users/yuenfannie/anaconda2/lib/python2.7/site-packages/notebook/services/kernels/handlers.py"", line 262, in open
        super(ZMQChannelsHandler, self).open()
      File ""/Users/yuenfannie/anaconda2/lib/python2.7/site-packages/notebook/base/zmqhandlers.py"", line 176, in open
        self.send_ping, self.ping_interval, io_loop=loop,
    TypeError: __init__() got an unexpected keyword argument 'io_loop'


The opened JupyterLab Alpha Preview does not work at all. Unfortunately I have no clues why this happens. Any suggestions are welcome!
",1,847,"Just downgrade your Tornado:
I have the following versions and they are working just fine (downgraded Tornado from 5.1.1 to 4.5.3):
Jupyter: 4.4.0

Tornado: 4.5.3

","This is a version mismatch issue. The io_loop argument was removed from this method in Tornado 5.0. Jupyter notebook was updated for compatibility with Tornado 5 in Notebook version 5.3 (I think). You need to either upgrade to a newer version of notebook or downgrade to an older version of Tornado. 
",
Tornado unexpected result,https://stackoverflow.com/questions/65747436,problems using local jupyter with google colab,"I am using jupyter notebook with google colab, on Ubuntu 20.04LTS with Python 3.8.5.
Problems I face are:

connection drops suddenly. I mean, to say, colab never executes a cell, instead it just keeps waiting. By waiting, I mean, the cell I run always looks like this

and never turns into this: . But, colab does say that I am connected to an instance: .
everytime I write something and wait for autocomplete to show suggestions, I receive a TypeError that says, there was unexpected keyword argument 'column'. Here's the complete error message right from the launch of jupyter:

(piu_venv) ubuntu@ip-xxx-xxx-xxx-xxx:~/pump_it_up$ jupyter notebook --NotebookApp.allow_origin='https://colab.research.google.com' --port=8888 --NotebookApp.port_retries=0
jupyter_http_over_ws extension initialized. Listening on /http_over_websocket
[I 07:14:12.316 NotebookApp] Serving notebooks from local directory: /home/ubuntu/pump_it_up
[I 07:14:12.316 NotebookApp] Jupyter Notebook 6.2.0 is running at:
[I 07:14:12.316 NotebookApp] http://localhost:8888/?token=c5d00b63744c7a6cd9229df8340c46bc18506d3994c208e1
[I 07:14:12.316 NotebookApp]  or http://127.0.0.1:8888/?token=c5d00b63744c7a6cd9229df8340c46bc18506d3994c208e1
[I 07:14:12.316 NotebookApp] Use Control-C to stop this server and shut down all kernels (twice to skip confirmation).
[W 07:14:12.319 NotebookApp] No web browser found: could not locate runnable browser.
[C 07:14:12.319 NotebookApp] 
    
    To access the notebook, open this file in a browser:
        file:///home/ubuntu/.local/share/jupyter/runtime/nbserver-11870-open.html
    Or copy and paste one of these URLs:
        http://localhost:8888/?token=c5d00b63744c7a6cd9229df8340c46bc18506d3994c208e1
     or http://127.0.0.1:8888/?token=c5d00b63744c7a6cd9229df8340c46bc18506d3994c208e1
[I 07:14:19.632 NotebookApp] 302 GET /?token=0b923e1d3d46db65bf3fe322c20bc0e4f5515f243873e2f7 (127.0.0.1) 0.250000ms
[I 07:14:19.634 NotebookApp] 302 GET /tree?token=0b923e1d3d46db65bf3fe322c20bc0e4f5515f243873e2f7 (127.0.0.1) 0.330000ms
[W 07:14:19.657 NotebookApp] Forbidden
[W 07:14:19.657 NotebookApp] 403 GET /api/kernelspecs (127.0.0.1) 0.420000ms referer=None
[I 07:14:23.317 NotebookApp] 302 GET /?token=c5d00b63744c7a6cd9229df8340c46bc18506d3994c208e1 (127.0.0.1) 0.240000ms
[I 07:14:23.461 NotebookApp] 302 GET /?token=c5d00b63744c7a6cd9229df8340c46bc18506d3994c208e1 (127.0.0.1) 0.230000ms
[I 07:14:23.478 NotebookApp] Kernel started: 2442098f-efe2-44fa-8916-2b8081a4904b, name: python3
[I 07:14:23.968 NotebookApp] 302 GET /?token=c5d00b63744c7a6cd9229df8340c46bc18506d3994c208e1 (127.0.0.1) 0.270000ms
[I 07:14:23.971 NotebookApp] proxying WebSocket connection to: ws://localhost:8888/api/kernels/2442098f-efe2-44fa-8916-2b8081a4904b/channels?session_id=dd75a32fa4b4426587f78beb823c6122&amp;jupyter_http_over_ws_auth_url=http%3A%2F%2Flocalhost%3A8888%2F%3Ftoken%3Dc5d00b63744c7a6cd9229df8340c46bc18506d3994c208e1
[I 07:14:26.402 NotebookApp] 302 GET /?token=c5d00b63744c7a6cd9229df8340c46bc18506d3994c208e1 (127.0.0.1) 0.260000ms
[I 07:14:45.852 NotebookApp] 302 GET /?token=c5d00b63744c7a6cd9229df8340c46bc18506d3994c208e1 (127.0.0.1) 0.280000ms
[I 07:14:47.477 NotebookApp] 302 GET /?token=c5d00b63744c7a6cd9229df8340c46bc18506d3994c208e1 (127.0.0.1) 0.230000ms
[IPKernelApp] ERROR | Exception in message handler:
Traceback (most recent call last):
  File ""/home/ubuntu/pump_it_up/piu_venv/lib/python3.8/site-packages/ipykernel/kernelbase.py"", line 265, in dispatch_shell
    yield gen.maybe_future(handler(stream, idents, msg))
  File ""/home/ubuntu/pump_it_up/piu_venv/lib/python3.8/site-packages/tornado/gen.py"", line 762, in run
    value = future.result()
  File ""/home/ubuntu/pump_it_up/piu_venv/lib/python3.8/site-packages/tornado/gen.py"", line 234, in wrapper
    yielded = ctx_run(next, result)
  File ""/home/ubuntu/pump_it_up/piu_venv/lib/python3.8/site-packages/ipykernel/kernelbase.py"", line 580, in complete_request
    matches = yield gen.maybe_future(self.do_complete(code, cursor_pos))
  File ""/home/ubuntu/pump_it_up/piu_venv/lib/python3.8/site-packages/ipykernel/ipkernel.py"", line 356, in do_complete
    return self._experimental_do_complete(code, cursor_pos)
  File ""/home/ubuntu/pump_it_up/piu_venv/lib/python3.8/site-packages/ipykernel/ipkernel.py"", line 381, in _experimental_do_complete
    completions = list(_rectify_completions(code, raw_completions))
  File ""/home/ubuntu/pump_it_up/piu_venv/lib/python3.8/site-packages/IPython/core/completer.py"", line 484, in rectify_completions
    completions = list(completions)
  File ""/home/ubuntu/pump_it_up/piu_venv/lib/python3.8/site-packages/IPython/core/completer.py"", line 1818, in completions
    for c in self._completions(text, offset, _timeout=self.jedi_compute_type_timeout/1000):
  File ""/home/ubuntu/pump_it_up/piu_venv/lib/python3.8/site-packages/IPython/core/completer.py"", line 1861, in _completions
    matched_text, matches, matches_origin, jedi_matches = self._complete(
  File ""/home/ubuntu/pump_it_up/piu_venv/lib/python3.8/site-packages/IPython/core/completer.py"", line 2029, in _complete
    completions = self._jedi_matches(
  File ""/home/ubuntu/pump_it_up/piu_venv/lib/python3.8/site-packages/IPython/core/completer.py"", line 1373, in _jedi_matches
    interpreter = jedi.Interpreter(
  File ""/home/ubuntu/pump_it_up/piu_venv/lib/python3.8/site-packages/jedi/api/__init__.py"", line 725, in __init__
    super().__init__(code, environment=environment,
TypeError: __init__() got an unexpected keyword argument 'column'


Details about computer:
This is actually an AWS EC2 instance(c5a.4xlarge) I have port mapped to my local laptop, to be able to use jupyter notebook on colab. Both my laptop and EC2 instance are running Ubuntu 20.04LTS.
Package versions:
I have installed jupyter on 16-Jan-2021 by doing pip3 install jupyter. So, I believe I got the latest version available and here are the package versions along with some dependencies of jupyter:
ipykernel==5.4.3
ipython==7.19.0
ipython-genutils==0.2.0
ipywidgets==7.6.3
jedi==0.18.0
Jinja2==2.11.2
jsonschema==3.2.0
jupyter==1.0.0
jupyter-client==6.1.11
jupyter-console==6.2.0
jupyter-core==4.7.0
jupyter-http-over-ws==0.0.8
jupyterlab-pygments==0.1.2
jupyterlab-widgets==1.0.0
kiwisolver==1.3.1
MarkupSafe==1.1.1
matplotlib==3.3.3
mistune==0.8.4
mpmath==1.1.0
nbclient==0.5.1
nbconvert==6.0.7
nbformat==5.1.2
nest-asyncio==1.4.3
notebook==6.2.0

How do I fix these issues?
",0,1318,"The second issue is caused by your jedi version. You can fix it with:
pip install --upgrade 'jedi&lt;0.18.0'

Related issue &amp; discussion: https://github.com/ipython/ipython/issues/12745#issuecomment-751892538
",,
Tornado unexpected issue,https://stackoverflow.com/questions/41854038,&quot;Unexpected token &lt;&quot; for VueJS running with Webpack,"Note: Before you mark this as a duplicate, I have looked at a few solutions and they don't work:


[1] https://github.com/webpack/webpack-dev-server/issues/135
[2] https://github.com/gaearon/react-hot-loader/blob/master/docs/Troubleshooting.md#syntax-error-unexpected-token-
Few other Unexpected token &lt; threads here but they are (probably) unrelated.


I am trying to integrate VueJS into an OSS chat application https://github.com/zulip/zulip . I tried to use the standard configuration template from vue-loader which includes single-file-components and hot reload, but when I try to run the server, I get this error:

...
ERROR in ./static/js/components/sidebar.vue
Module parse failed: /srv/zulip/static/js/components/sidebar.vue Line 1: Unexpected token &lt;
You may need an appropriate loader to handle this file type.
| &lt;template&gt;
|     &lt;div&gt;
|         {{ msg }}
 @ ./static/js/src/main.js 3:14-50
...


This is the webpack config:

var webpack = require('webpack')

module.exports = {
    entry: [
        'webpack-dev-server/client?http://0.0.0.0:9991/socket.io',
        './static/js/src/main.js',
    ],
    devtool: 'eval',
    output: {
        publicPath: 'http://0.0.0.0:9991/webpack/',
        path: './static/js',
        filename: 'bundle.js',
    },
    devServer: {
        port: 9994,
        stats: ""errors-only"",
        watchOptions: {
            aggregateTimeout: 300,
            poll: 1000,
        },
    },
    module: {
        rules: [
            {
                test: /\.vue$/,
                loader: 'vue-loader'
            },
            {
                test: /\.(png|jpg|gif|svg)$/,
                loader: 'file-loader',
                options: {
                    name: '[name].[ext]?[hash]'
                }
            }
        ]
    }
};


Info:


The first link suggest adding a explicit public path, but that is already done before me.
There are a few servers running in the code, including django for the main app server and tornado for push events.
The app only exposes port 9991 outside of the development host (vagrant). The webpack-dev-server uses 9994 but is redirected to localhost:9991/webpack/


You can see the changes here: https://github.com/tommyip/zulip/commit/97cf122bda0f7dc09f6c8c3062c55871c315459e
",1,2699,"I missed one of the key information, which is the version of Webpack.

The examples shown in Vue and vue-loader's website uses Webpack 2 API, which is slightly different to Webpack 1:

module: {
    rules: [
        {
            test: /\.vue$/,
            loader: 'vue-loader'
        },


Rules is actually loaders in Webpack 1.
",,
Tornado unexpected issue,https://stackoverflow.com/questions/23123932,Unexpected response code: 426 with PhanthomJS Websocket client/ROSLIB,"I'm trying to use PhanthomJS (1.9.7) to setup batch test script to drive our robotics software over our Websocket server (RosBridge which is implemented on top of Tornado).

We use ROSLIBJS which is a JS library that uses the standard HTML5 WebSocket API to interact with the server. I would have preferred to use NodeJS instead of PhantomJS, but there are dependencies in the ROSLIBJS that requires browser DOM structures to be available. Someone ported ROSLIBJS for NodeJS. I got it running, but there were critical bugs that prevented it from being usable. Hence, I need PhantomJS.

According to this link, there is some incompatibility with the Websocket protocol.
Unexpected response code 426

Is that a known issue. If so, when would PhantomJS be fixed for this to work? Can I patch PhantomJS myself easily?
",1,1895,"Http 426 may mean that you are trying to connect with an unsupported websocket protocol version. Check if the response has the http header ""sec-websocket-version"" with a list of supported versions.

Which version is your client using?

Also, if you are connecting through a proxy, the proxy may remove the ""upgrade"" header from the request since it is marked as ""connection"" header. Switch to WSS:// to prevent that.
","Funny enough, I was looking at writing integration tests for roslibjs and ran across this question. I'll try phantomjs-2.0 (https://github.com/Vitallium/phantomjs-qt5) and real chrome via chromedriver. I'll most like PR the results against roslibjs.

An alternative option would be to write another bridge that uses socket.io for client communication to achieve maximum portability. 

See this PR: https://github.com/RobotWebTools/roslibjs/pull/83 . It uses chrome to run tests. Alternatively you can allow rosbridge to accept draft76 websockets by changing https://github.com/RobotWebTools/rosbridge_suite/blob/develop/rosbridge_server/src/tornado/websocket.py#L186 to return True;
",
Tornado strange behavior,https://stackoverflow.com/questions/33379232,Python Tornado Blocking in AsyncHTTPClient only on Linux. Not on windows or OSX,"I'm getting very strange behavior from my Tornado AsyncHTTPClient client. 

When I run the same code on Windows, OSX, Ubuntu, Redhat, and the Amazon AMI, my code behaves differently. 

Here is the relevant code:

 request = HTTPRequest(self.URL,
                      method=""POST"",
                      auth_username=self.USERNAME,
                      auth_password=self.PASSWORD,
                      headers=self.HEADERS,
                      body=formatted_request
                      )
try:
  print ""before"", datetime.now()
  future = self.HTTP_CLIENT.fetch(request, self.handle_response)
  print ""after"", datetime.now()


On OSX and Winodws, the output of this code is (Non blocking): 

before 2015-10-27 17:51:13.896538
after 2015-10-27 17:51:14.414656
before 2015-10-27 17:51:14.418626
after 2015-10-27 17:51:14.420233
before 2015-10-27 17:51:14.423062
after 2015-10-27 17:51:14.424126
before 2015-10-27 17:51:14.426491
after 2015-10-27 17:51:14.427542
before 2015-10-27 17:51:14.429675
after 2015-10-27 17:51:14.430702
before 2015-10-27 17:51:14.432825
after 2015-10-27 17:51:14.433863


On Ubuntu, Redhat, and the amazon AMI I am getting this (a difference of 2 seconds in between what is supposed to be non blocking code):

before 2015-10-27 21:49:23.644458
after 2015-10-27 21:49:25.541746
before 2015-10-27 21:49:25.542827
after 2015-10-27 21:49:27.428840
before 2015-10-27 21:49:27.429993
after 2015-10-27 21:49:29.326183
before 2015-10-27 21:49:29.327549


I noticed in the tornado code that there is a difference between linux and osx:


  We use epoll (Linux) or kqueue (BSD and Mac OS X) if they
      are available, or else we fall back on select(). If you are
      implementing a system that needs to handle thousands of
      simultaneous connections, you should use a system that supports
      either epoll or kqueue.


But the performance difference between the different platforms seems unlikely to be a epoll / kqueue issue. 

I'm using python 2.7 and tornado 4.2.1. Distribution versions are the EC2 versions downloaded from the AWS instance start page. 

Any ideas would be appreciated! 

Thanks,
Jon
",1,261,"The difference is probably in the DNS resolution, which is blocking by default. When it's fast, you're getting a cached result, and when it's not you're going out to the original nameservers (and probably talking to a non-optimal resolver if it's taking 2 seconds).

Try installing the futures package and doing tornado.netutil.Resolver.configure(""tornado.netutil.ThreadedResolver"").
",,
Tornado strange behavior,https://stackoverflow.com/questions/34470662,Python&#39;s logging issue in Tornado coroutine,"A couple of days ago I found strange logging issue working with Tornado.

I have a set of files:

main.py:

 1  import logging
 2  import logging.config
 3  import os

 4  from somemodule.mod import ModClass
 5  from tornado import ioloop

 6  if __name__ == ""__main__"":
 7      logging.config.fileConfig(""logging.ini"")
 8      print ioloop.IOLoop.current().run_sync(ModClass.my_coroutine)


logging.ini:

 1  [loggers]
 2  keys=root

 3  [logger_root]
 4  level=NOTSET
 5  handlers=file

 6  [handlers]
 7  keys=file

 8  [handler_file]
 9  level=DEBUG
10  formatter=default
11  class=handlers.TimedRotatingFileHandler
12  args=('/tmp/some-system.log', 'D', 1, 7)

13  [formatters]
14  keys=default

15  [formatter_default]
16  format=%(asctime)s [%(levelname)s] %(name)s@%(lineno)d: %(message)s


somemodule/mod.py:

 1  import logging
 2  from tornado import gen

 3  logger = logging.getLogger(__name__)

 4  class ModClass(object):

 5      @classmethod
 6      @gen.coroutine
 7      def my_coroutine(cls):
 8          # logger = logging.getLogger(__name__)
 9          logger.critical(""SOME MESSAGE"")
10          raise gen.Return(""Some string"")


Also I have an empty __init__.py in somemodule directory.

If I run main.py, I see ""Some string"" in console, and I have a created, but empty file /tmp/some-system.log. I don't know what is wrong with this small system.

To make it work correctly I have to comment line (3) and uncomment line (8) in file somemodule/mod.py.

Does anybody know how to make module logger work without need to declare it in each function in module? And what is the cause of so strange behavior in this simple example?

P.S. My environment:
Python 2.7.6
tornado==3.1.1
",1,295,"You need to change the call to fileConfig to fileConfig(""logging.ini"", disable_existing_loggers=False) as documented here.
",,
Tornado strange behavior,https://stackoverflow.com/questions/38664498,What are reasons why the neo4j time tree would be freezing up?,"I'm having a strange issue in Neo4j. I have been recently working with the GraphAware TimeTree and populating it was going well until yesterday. I had to rebuild the tree because of an error I had made so I left a script running overnight (nohup).

Coming back today, I found that my script only ran for 3 minutes!

$ ps aux | grep timetreepop
root     21840  0.0  0.0 195952  2816 ?        S    Jul28   0:00 sudo nohup python timetreepop.py
root     21841  0.0  0.2 381416 75016 ?        S    Jul28   0:03 python timetreepop.py


I noticed this behavior while I was working, but I figured leaving it overnight while I'm not active would be help. I also turned off my other java server process in case of possible contention. At this point my server would only be running a python tornado server in the background which isn't very hefty, and doesn't get much traffic (couple hits a day).

All in all there is plenty of available RAM in my system, the CPUs are not being utilized elsewhere, and there are no other processes running heavy IO on my machine. Using top / atop show a healthy system with available resources.

Here is a high level of what my script is doing:

neo = neopop.handler()
for i, meta_id in enumerate(meta_gen(ship='KAOU')):
   neo.populate_timetree(record=meta_id)


My handler creates the driver and session in the __init__ constructor:

self.driver = graphdb.driver(config.NEO4J_HTTP_LINK)
self.session = self.driver.session()


My generator provides meta_id values which are unique property values in the nodes in my graph.

The populate_timetree() function creates the following statement:

MATCH (r:record {meta:""KAQP_20120101v20001_0001""}) WITH r 
CALL ga.timetree.events.attach({node: r, time: r.time, relationshipType: ""observedOn"", resolution:""Minute""})
YIELD node RETURN node.meta;


Everything was working fine my first go around. After messing up my time values, I deleted the database, restarted, and tried again. Only this time, my program freezes when I make the call to close the session:

neo.session.close()


Note: I actually call this in my __del__ deconstructor, (which I know is probably considered bad practice but it has been working for me so far and suits my needs.)

I have double checked all my code for rogue readline statements / anything that could cause it to be pausing. Also recompiled my package with all this code in it. I know for a fact it is getting stuck at this session.close() statement.

So I tried playing with the Neo4j-shell tool to see if anything was different.
First a quick responsiveness check:

$ neoc 'schema'
Indexes
  ON :record(meta) ONLINE (for uniqueness constraint) 

Constraints
  ON (record:record) ASSERT record.meta IS UNIQUE


Ok all good. Now I try the timetree call for a single value:

$ ./neo4j-shell -c '
&gt; MATCH (r:record {meta:""KAOU_20110613v20001_0000""}) WITH r 
&gt; CALL ga.timetree.events.attach({node: r, time: r.time, relationshipType: ""observedOn"", resolution:""Minute""})
&gt; YIELD node RETURN node.meta;
&gt; '


BOOM. Neo4j is stuck! To be clear, I know the MATCH statement is not taking forever here, because I only put in about 2 million nodes into my db this time, and calling the match statement alone runs absolutely fine. I even have an index set up for this unique property (see schema call above).

So what is going on here? I figure the initial creation of the tree shouldn't be too much of an issue if I am only inserting a single node. My first try seem to run flawlessly. I'm not sure what I have done different this time except populate my database with 2 million out of the 58 million records I had. (So it should be way faster).

I actually left this command running for a few hours, and it also had only run for a few minutes on my system. I am so confused as to what is happening here. Does anyone have any ideas? Is there anyway I can see what neo4j is actively doing about this command? (Keep in mind I am using community edition)

I am running version 3.0.3 of neo4j and, 3.0.3.39 timetree / graphaware on and CentOS 7 server.

The only idea I have, is that I have been calling cypher statements and cancelling them before they commit so many times, whether it be through python or the cmd line shell tool. Is it possible I am messing with the transaction manager too much by cancelling big transactions before they finish?

For example:

/neo4j-shell -c 'MATCH (r:record) CALL ga.timetree.events.attach(....) ....'


And then hit control-C after it runs for about 2 hours without finishing.



UPDATE:

Ok so I investigated my log files and found some issues. It seems my threads were being blocked due to not having enough memory. I guess my max heap size was being artifically limited and not using the available resources on my machine. (Maybe??). So I manually set dbms.memory.heap.max_size=16000.

The problem seems to go away now! I guess I was confused, since I would expect a Java OOM to appear in the response of the neo4j-shell tool instead of idling as if making progress.
",1,264,"$ ./neo4j-shell -c '
&gt; MATCH (r:record {meta:""KAOU_20110613v20001_0000""}) WITH r 
&gt; CALL ga.timetree.events.attach({node: r, time: r.time, relationshipType: ""observedOn"", resolution:""Minute""})
&gt; YIELD node RETURN node.meta;
&gt; '


How many nodes is this query expected to attach to the timetree ?

This procedure doesn't use batching so it is limited to what your memory can do in a single transaction (this is now on our backlog).

That said, it generally doesn't make sense to attach manually the events to the tree, this procedure is there for convenience but we recommend to use automatic event attachments : 

https://github.com/graphaware/neo4j-timetree#automatic-event-attachment

In that way, re-attaching the complete db (or only the nodes configured for attachment) will be a matter of re-starting your database and this process will run with batch transactions, which will speed up significantly the time taken.
",,
Tornado strange output,https://stackoverflow.com/questions/25328818,python 2.7: cannot pip on windows &quot;bash: pip: command not found&quot;,"I am trying to install the SciPy stack located at https://scipy.org/stackspec.html [I am only allowed 2 links; trying to use them wisely].  I realize that there are much easier ways to do this, but I think there is a lot to be learned by doing it manually.  I am relatively new to a lot of this stuff, so I apologize if I sound ignorant at any point.
I am running  Windows 7 Enterprise - 64 bit.  Here is what I have done so far:

Installed python-2.7.8.msi (32-bit) from https://www.python.org/download/releases/2.7.8/

Installed numpy-1.8.1-win32-superpack-python2.7 from
http://sourceforge.net/projects/numpy/files/
Test: import numpy as np ---&gt; no errors

Installed scipy library,
scipy-0.14.0-win32-superpack-python2.7.exe from
(SCIPY DOT ORG LINK REMOVED)
Test: import scipy as sp ---&gt; no errors

Installed matplotlib: matplotlib-1.3.1.win32-py2.7.exe from
(MATPLOTLIB DOT ORG LINK REMOVED)

Installed PIP by running script here:
https://raw.githubusercontent.com/pypa/pip/master/contrib/get-pip.py
I just copied-pasted script to a new file in IDLE,
saved as C:\Python27\Scripts\pip_install.py and clicked Run&gt;module. No errors reported.


Does the path on which I saved
pip_install.py matter?




HERE IS WHERE I FAIL
Attempted to install matlibplot dependency dateutil: Opened a
Cygwin Shell, and typed
        cd C:\Python27          ! is it necessary to cd to python directtory?
        pip install python-dateutil

This results in the error:
    bash: pip: command not found

I get the same error attempting from cmd.
Any help  is appreciated; the closest I found was bash: pip: command not found.  But the OSX nature of it is just enough to confise me further.

UPDATE:
I added the pip-path per Paul H's suggestion below.  It made the error go away, but strangely, nothing I pip actually installs. For example, in Cygwin, I type:
cbennett2&gt; pip install python-dateutil
cbennett2&gt;                            

You can see that there is no output or feedback from the shell (which I think there should be).  Then when I go to a new python shell:
&gt;&gt;&gt; from dateutil.parser import parse
Traceback (most recent call last):
  File ""&lt;pyshell#12&gt;"", line 1, in &lt;module&gt;
    from dateutil.parser import parse
ImportError: No module named dateutil.parser
&gt;&gt;&gt;&gt;

This happens with all of the modules that I thought I had pip'd ... pandas, tornado, etc.
",45,166646,"On Windows, pip lives in C:\[pythondir]\scripts.

So you'll need to add that to your system path in order to run it from the command prompt. You could alternatively cd into that directory each time, but that's a hassle.

See the top answer here for info on how to do that:
Adding Python Path on Windows 7

Also, that is a terrifying way to install pip. Grab it from Christophe Gohlke. Grab everything else from there for that matter.
http://www.lfd.uci.edu/~gohlke/pythonlibs/ 
","As long as pip lives within the scripts folder you can run 

python -m pip .... 

This will tell python to get pip from inside the scripts folder. This is also a good way to have both python2.7 and pyhton3.5 on you computer and have them in different locations. I currently have both python2 and pyhton3 installed on windows. When I type python it defaults to python2. But if I type python3 I can use python3. (I also had to change the python.exe file for python3 to ""python3.exe"")If I need to install flask for python 2 I can run

python -m pip install flask

and it will be installed in the pyhton2 folder, but if I need flask for python 3 I run:

python3 -m pip install flask

and I now have it in the python3 folder
","
press [win] + Pause
Advanced settings
System variables
Append ;C:\python27\Scripts to the end of Path variable
Restart console

"
Tornado strange output,https://stackoverflow.com/questions/14162950,Memory leak in tornado generator engine with try/finally block when connections are closed,"This awesome code, shows memory leak in tornado's gen module, when connections are closed without reading the response:

import gc
from tornado import web, ioloop, gen

class MainHandler(web.RequestHandler):
    @web.asynchronous
    @gen.engine
    def get(self):
        gc.collect()
        print len(gc.garbage)  # print zombie objects count
        self.a = '*' * 500000000  # ~500MB data
        CHUNK_COUNT = 100
        try:
            for i in xrange(CHUNK_COUNT):
                self.write('*' * 10000)  # write ~10KB of data
                yield gen.Task(self.flush)  # wait for reciever to recieve
            print 'finished'
        finally:
            print 'finally'

application = web.Application([
    (r""/"", MainHandler),
    ])

application.listen(8888)
ioloop.IOLoop.instance().start()

and now, run a simple test client, multiple times

#!/usr/bin/python
import urllib
urlopen('http://127.0.0.1:8888/')  # exit without reading response


Now, server output shows, incremental memory usage:

0
WARNING:root:Write error on 8: [Errno 104] Connection reset by peer
1
WARNING:root:Read error on 8: [Errno 104] Connection reset by peer
WARNING:root:error on read
Traceback (most recent call last):
  File ""/usr/local/lib/python2.7/dist-packages/tornado-2.4.1-py2.7.egg/tornado/iostream.py"", line 361, in _handle_read
    if self._read_to_buffer() == 0:
  File ""/usr/local/lib/python2.7/dist-packages/tornado-2.4.1-py2.7.egg/tornado/iostream.py"", line 428, in _read_to_buffer
    chunk = self._read_from_socket()
  File ""/usr/local/lib/python2.7/dist-packages/tornado-2.4.1-py2.7.egg/tornado/iostream.py"", line 409, in _read_from_socket
    chunk = self.socket.recv(self.read_chunk_size)
error: [Errno 104] Connection reset by peer
2
ERROR:root:Uncaught exception GET / (127.0.0.1)
HTTPRequest(protocol='http', host='127.0.0.1:8888', method='GET', uri='/', version='HTTP/1.0', remote_ip='127.0.0.1', body='', headers={'Host': '127.0.0.1:8888', 'User-Agent': 'Python-urllib/1.17'})
Traceback (most recent call last):
  File ""/usr/local/lib/python2.7/dist-packages/tornado-2.4.1-py2.7.egg/tornado/web.py"", line 1021, in _stack_context_handle_exception
    raise_exc_info((type, value, traceback))
  File ""/usr/local/lib/python2.7/dist-packages/tornado-2.4.1-py2.7.egg/tornado/web.py"", line 1139, in wrapper
    return method(self, *args, **kwargs)
  File ""/usr/local/lib/python2.7/dist-packages/tornado-2.4.1-py2.7.egg/tornado/gen.py"", line 120, in wrapper
    runner.run()
  File ""/usr/local/lib/python2.7/dist-packages/tornado-2.4.1-py2.7.egg/tornado/gen.py"", line 345, in run
    yielded = self.gen.send(next)
  File ""test.py"", line 10, in get
    self.a = '*' * 500000000
MemoryError
ERROR:root:500 GET / (127.0.0.1) 3.91ms


If you set CHUNK_COUNT to 1, the 10KB of data can be written to OS connection buffer, and 'finished' and 'finally' texts will be printed to console, and because generator is completed, no memory leak occurs.

But the strange part is that if your remove the try/finally block, the problem disappears!! (even with CHUNK_COUNT set to 100)

Is this a bug on CPython or tornado or ...?!
",5,1280,"This bug tested with Tornado 2.4.1 (the latest version when this question asked), and reported on https://github.com/facebook/tornado/issues/660 .

The problem fixed in commit https://github.com/facebook/tornado/commit/769bc52e11656788782a6e7a922ef646503f9ab0 and included in Tornado 3.0.
",,
Tornado strange output,https://stackoverflow.com/questions/72468387,Creating a conda environment with python 3.8 with conflicting requirement(s)?,"Context
After setting up a conda environment.yml and trying to install it with a python 3.8 version, I am experiencing some difficulties.
Attempts
I tried explicitly specifying the python version at the environment creation command:
conda env create --file environment.yml python=3.8

I tried explicitly including the python version in the environment.yml file:
...
dependencies:
- anaconda
- python=3.8
- conda:
# Run python tests.
  - pytest-cov
...

And I tried explicitly installing python 3.8 inside the environment with:
conda activate env_name
conda install python==3.8

Which yields a conflict:

Found conflicts! Looking for incompatible packages.

So I tried determining what the conflict is, by evaluating the 2000 output lines that describe the conflicts. I think the first conflict is most relevant:
UnsatisfiableError: The following specifications were found to be incompatible with a past
explicit spec that is not an explicit spec in this operation (pip):

  - python=3.8 -&gt; pip

Environment
The conda environment consists of the following four files:

environment.yml
pyproject.toml
requirements.txt
.pre-commit-config.yaml

Which have contents:
environment.yml:
# This file is to automatically configure your environment. It allows you to
# run the code with a single command without having to install anything
# (extra).

# First run:: conda env create --file environment.yml
# If you change this file, run: conda env update --file environment.yml

# Instructions for this networkx-to-lava-nc repository only. First time usage
# On Ubuntu (this is needed for lava-nc):
# sudo apt upgrade
# sudo apt full-upgrade
# yes | sudo apt install gcc

# Conda configuration settings. (Specify which modules/packages are installed.)
name: networkx-to-lava
channels:
  - conda-forge
dependencies:
- python=3.8
- conda:
# Run python tests.
  - pytest-cov
# Generate plots.
  - matplotlib
# Run graph software quickly.
  - networkx
- pip
- pip:
# Run pip install on .tar.gz file in GitHub repository (For lava-nc only).
  - https://github.com/lava-nc/lava/releases/download/v0.3.0/lava-nc-0.3.0.tar.gz
# Turns relative import paths into absolute import paths.
  - absolufy-imports
# Auto format Python code to make it flake8 compliant.
  - autoflake
# Scan Python code for security issues.
  - bandit
# Code formatting compliance.
  - black
# Correct code misspellings.
  - codespell
# Verify percentage of code that has at least 1 test.
  - coverage
# Auto formats the Python documentation written in the code.
  - docformatter
# Auto generate docstrings.
  - flake8
# Auto sort the import statements.
  - isort
# Auto format Markdown files.
  - mdformat
# Auto check static typing.
  - mypy
# Auto generate documentation.
  - pdoc3
# Auto check programming style aspects.
  - pylint
# Auto generate docstrings.
  - pyment
# Identify and remove dead code.
  - vulture
# Include GitHub pre-commit hook.
  - pre-commit
# TODO: identify exact function(and usage).
# Seems to be an autoformatter like black, but installed using npm instead of pip.
  - prettier
# Automatically upgrades Python syntax to the new Python version syntax.
  - pyupgrade
# Another static type checker for python like mypy.
  - pyright


pyproject.toml:
# This is used to configure the black, isort and mypy such that the packages don't conflict.
# This file is read by the pre-commit program.
[tool.black]
line-length = 79
include = '\.pyi?$'
exclude = '''
/(
    \.git
  | \.mypy_cache
  | build
  | dist
)/
'''


[tool.coverage.run]
# Due to a strange bug with xml output of coverage.py not writing the full-path
# of the sources, the full root directory is presented as a source alongside
# the main package. As a result any importable Python file/package needs to be
# included in the omit
source = [
    ""foo"",
    ""."",
]
# Excludes the following directories from the coverage report
omit = [
    ""tests/*"",
    ""setup.py"",
]


[tool.isort]
profile = ""black""


[tool.mypy]
ignore_missing_imports = true


[tool.pylint.basic]
bad-names=[]
[tool.pylint.messages_control]
# Example: Disable error on needing a module-level docstring
disable=[
    ""import-error"",
    ""invalid-name"",
    ""fixme"",
]


[tool.pytest.ini_options]
# Runs coverage.py through use of the pytest-cov plugin
# An xml report is generated and results are output to the terminal
addopts = ""--cov --cov-report xml:cov.xml --cov-report term""
# Sets the minimum allowed pytest version
minversion = 5.0
# Sets the path where test files are located (Speeds up Test Discovery)
testpaths = [""tests""]

requirements.txt
# This file ensures that the pre-commit service is ran every time you commit.
# Basically it ensures people only push files to GIT that are up to standard.
pre-commit

.pre-commit-config.yaml
# This file specifies which checks are performed by the pre-commit service.
# The pre-commit service prevents people from pushing code to git that is not
# up to standards. # The reason mirrors are used instead of the actual
# repositories for e.g. black and flake8, is because those repositories also
# need to contain a pre-commit hook file, which they often don't by default.
# So to resolve that, a mirror is created that includes such a file.

default_language_version:
    python: python3.10  # or python3


repos:
# Test if the python code is formatted according to the Black standard.
 - repo: https://github.com/Quantco/pre-commit-mirrors-black
   rev: 22.3.0
   hooks:
     - id: black-conda
       args:
         - --safe
         - --target-version=py36

# Test if the python code is formatted according to the flake8 standard.
 - repo: https://github.com/Quantco/pre-commit-mirrors-flake8
   rev: 4.0.1
   hooks:
    - id: flake8-conda

# Test if the import statements are sorted correctly.
 - repo: https://github.com/PyCQA/isort
   rev: 5.10.1
   hooks:
    - id: isort
      args: [""--profile"", ""black"", --line-length=79]

# Test if the variable typing is correct. (Variable typing is when you say:
# def is_larger(nr: int) -&gt; bool: instead of def is_larger(nr). It makes
# it explicit what type of input and output a function has.
# - repo: https://github.com/python/mypy
 - repo: https://github.com/pre-commit/mirrors-mypy
# - repo: https://github.com/a-t-0/mypy
   rev: v0.950
   hooks:
    - id: mypy

# Tests if there are spelling errors in the code.
 - repo: https://github.com/codespell-project/codespell
   rev: v2.1.0
   hooks:
    - id: codespell

# Performs static code analysis to check for programming errors.
 - repo: local
   hooks:
     - id: pylint
       name: pylint
       entry: pylint
       language: system
       types: [python]
       args:
         [
           ""-rn"", # Only display messages
           ""-sn"", # Don't display the score
         ]

# Runs additional tests that are created by the pre-commit software itself.
 - repo: https://github.com/pre-commit/pre-commit-hooks
   rev: v4.2.0
   hooks:
    # Check user did not add large files.
    - id: check-added-large-files
    # Check if `.py` files are written in valid Python syntax.
    - id: check-ast
    # Require literal syntax when initializing empty or zero Python builtin types.
    - id: check-builtin-literals
    # Checks if there are filenames that would conflict if case is changed.
    - id: check-case-conflict
    # Checks if the Python functions have docstrings.
    - id: check-docstring-first
    # Checks if any `.sh` files have a shebang like #!/bin/bash
    - id: check-executables-have-shebangs
    # Verifies json format of any `.json` files in repo.
    - id: check-json
    # Checks if there are any existing merge conflicts caused by the commit.
    - id: check-merge-conflict
    # Checks for symlinks which do not point to anything.
    - id: check-symlinks
    # Checks if xml files are formatted correctly.
    - id: check-xml
    # Checks if .yml files are valid.
    - id: check-yaml
    # Checks if debugger imports are performed.
    - id: debug-statements
    # Detects symlinks changed to regular files with content path symlink was pointing to.
    - id: destroyed-symlinks
    # Checks if you don't accidentally push a private key.
    - id: detect-private-key
    # Replaces double quoted strings with single quoted strings.
    # This is not compatible with Python Black.
    #- id: double-quote-string-fixer
    # Makes sure files end in a newline and only a newline.
    - id: end-of-file-fixer
    # Removes UTF-8 byte order marker.
    - id: fix-byte-order-marker
    # Add &lt;# -*- coding: utf-8 -*-&gt; to the top of python files.
    - id: fix-encoding-pragma
    # Checks if there are different line endings, like \n and crlf.
    - id: mixed-line-ending
    # Asserts `.py` files in folder `/test/` (by default:) end in `_test.py`.
    - id: name-tests-test
      # Override default to check if `.py` files in `/test/` START with `test_`.
      args: ['--django']
    # Ensures JSON files are properly formatted.
    - id: pretty-format-json
    # Sorts entries in requirements.txt and removes incorrect pkg-resources entries.
    - id: requirements-txt-fixer
    # Sorts simple YAML files which consist only of top-level keys.
    - id: sort-simple-yaml
    # Removes trailing whitespaces at end of lines of .. files.
    - id: trailing-whitespace





 - repo: https://github.com/PyCQA/autoflake
   rev: v1.4
   hooks:
    - id: autoflake
      args: [""--in-place"", ""--remove-unused-variables"", ""--remove-all-unused-imports"", ""--recursive""]
      name: AutoFlake
      description: ""Format with AutoFlake""
      stages: [commit]

 - repo: https://github.com/PyCQA/bandit
   rev: 1.7.4
   hooks:
   - id: bandit
     name: Bandit
     stages: [commit]

# Enforces formatting style in Markdown (.md) files.
 - repo: https://github.com/executablebooks/mdformat
   rev: 0.7.14
   hooks:
   - id: mdformat
     additional_dependencies:
     - mdformat-toc
     - mdformat-gfm
     - mdformat-black

 - repo: https://github.com/MarcoGorelli/absolufy-imports
   rev: v0.3.1
   hooks:
   - id: absolufy-imports
     files: '^src/.+\.py$'
     args: ['--never', '--application-directories', 'src']

 - repo: https://github.com/myint/docformatter
   rev: v1.4
   hooks:
   - id: docformatter

 - repo: https://github.com/pre-commit/pygrep-hooks
   rev: v1.9.0
   hooks:
   - id: python-use-type-annotations
   - id: python-check-blanket-noqa
   - id: python-check-blanket-type-ignore

# Updates the syntax of `.py` files to the specified python version.
# It is not compatible with: pre-commit hook: fix-encoding-pragma
# - repo: https://github.com/asottile/pyupgrade
#   rev: v2.32.1
#   hooks:
#     - id: pyupgrade
#       args: [--py310-plus]


 - repo: https://github.com/markdownlint/markdownlint
   rev: v0.11.0
   hooks:
     - id: markdownlint


Package Conflict Output
conda install python=3.8
Collecting package metadata (current_repodata.json): done
Solving environment: failed with initial frozen solve. Retrying with flexible solve.
Solving environment: failed with repodata from current_repodata.json, will retry with next repodata source.
Collecting package metadata (repodata.json): done
Solving environment: failed with initial frozen solve. Retrying with flexible solve.
Solving environment: | 
Found conflicts! Looking for incompatible packages.
This can take several minutes.  Press CTRL-C to abort.
failed                                                                                                                                     \  

UnsatisfiableError: The following specifications were found to be incompatible with a past
explicit spec that is not an explicit spec in this operation (pip):

  - python=3.8 -&gt; pip

The following specifications were found to be incompatible with each other:

Output in format: Requested package -&gt; Available versions

Package openjpeg conflicts for:
openjpeg
pillow -&gt; openjpeg[version='&gt;=2.3.0,&lt;3.0a0|&gt;=2.4.0,&lt;2.5.0a0']
matplotlib-base -&gt; pillow[version='&gt;=6.2.0'] -&gt; openjpeg[version='&gt;=2.3.0,&lt;3.0a0|&gt;=2.4.0,&lt;2.5.0a0']

Package ncurses conflicts for:
wheel -&gt; python -&gt; ncurses[version='6.0.*|&gt;=6.0,&lt;7.0a0|&gt;=6.1,&lt;7.0a0|&gt;=6.2,&lt;7.0a0|&gt;=6.3,&lt;7.0a0']
krb5 -&gt; libedit[version='&gt;=3.1.20210216,&lt;3.2.0a0'] -&gt; ncurses[version='6.0.*|&gt;=6.1,&lt;7.0a0|&gt;=6.2,&lt;7.0.0a0|&gt;=6.2,&lt;7.0a0|&gt;=6.3,&lt;7.0a0']
pluggy -&gt; python[version='&gt;=3.7,&lt;3.8.0a0'] -&gt; ncurses[version='6.0.*|&gt;=6.0,&lt;7.0a0|&gt;=6.1,&lt;7.0a0|&gt;=6.2,&lt;7.0a0|&gt;=6.3,&lt;7.0a0']
...
...

  - tornado -&gt; libgcc-ng[version='&gt;=7.5.0'] -&gt; __glibc[version='&gt;=2.17']
  - unicodedata2 -&gt; libgcc-ng[version='&gt;=7.5.0'] -&gt; __glibc[version='&gt;=2.17']
  - xorg-libxau -&gt; libgcc-ng[version='&gt;=9.3.0'] -&gt; __glibc[version='&gt;=2.17']
  - xorg-libxdmcp -&gt; libgcc-ng[version='&gt;=9.3.0'] -&gt; __glibc[version='&gt;=2.17']
  - xz -&gt; libgcc-ng[version='&gt;=7.5.0'] -&gt; __glibc[version='&gt;=2.17']
  - zlib -&gt; libgcc-ng[version='&gt;=7.5.0'] -&gt; __glibc[version='&gt;=2.17']
  - zstd -&gt; libgcc-ng[version='&gt;=7.5.0'] -&gt; __glibc[version='&gt;=2.17']

Your installed version is: 2.33


Question
How can I determine the conflicting package in this conda environment and/or how can I create the environment using python 3.8?
",2,5784,"After specifying the python version as the first dependency, and removing the unneeded elements as suggested by merv, I found a working yaml. I removed anaconda, and the conda channel. Furthermore, I ensured the default_version in the .pre-commit-config.yaml file was set to:
default_language_version:
    python: python3.8.  # or python3

I also deleted the .mypy_cache folder in the .git repository (even though I think this was not required). And I deleted the directory /home/&lt;username&gt;/.cache/pre-commit before running pre-commit run --all-files. (I thought it was worth mentioning as it is inherent to this environment.yml)
I did not have to specify the python version in the environment creation command. Instead, I ran:
conda env create --file environment.yml

The working environment.yml content is:
# This file is to automatically configure your environment. It allows you to
# run the code with a single command without having to install anything
# (extra).

# First run: conda env create --file environment.yml
# If you change this file, run: conda env update --file environment.yml

# Instructions for this networkx-to-lava-nc repository only. First time usage
# On Ubuntu (this is needed for lava-nc):
# sudo apt upgrade
# sudo apt full-upgrade
# yes | sudo apt install gcc

# Conda configuration settings. (Specify which modules/packages are installed.)
name: nx2lava
channels:
  - conda-forge
dependencies:
# Specify specific python version.
  - python=3.8
# Run python tests.
  - pytest-cov
# Generate plots.
  - matplotlib
# Run graph software quickly.
  - networkx
  - pip
  - pip:
# Run pip install on .tar.gz file in GitHub repository (For lava-nc only).
    - https://github.com/lava-nc/lava/releases/download/v0.3.0/lava-nc-0.3.0.tar.gz
# Turns relative import paths into absolute import paths.
    - absolufy-imports
# Auto format Python code to make it flake8 compliant.
    - autoflake
# Scan Python code for security issues.
    - bandit
# Code formatting compliance.
    - black
# Correct code misspellings.
    - codespell
# Verify percentage of code that has at least 1 test.
    - coverage
# Auto formats the Python documentation written in the code.
    - docformatter
# Auto generate docstrings.
    - flake8
# Auto sort the import statements.
    - isort
# Auto format Markdown files.
    - mdformat
# Auto check static typing.
    - mypy
# Auto generate documentation.
    - pdoc3
# Auto check programming style aspects.
    - pylint
# Auto generate docstrings.
    - pyment
# Identify and remove dead code.
    - vulture
# Include GitHub pre-commit hook.
    - pre-commit
# TODO: identify exact function(and usage).
# Seems to be an autoformatter like black, but installed using npm instead of pip.
    - prettier
# Automatically upgrades Python syntax to the new Python version syntax.
    - pyupgrade
# Another static type checker for python like mypy.
    - pyright

Which returns the following to the python --version command:

Python 3.8.13

",,
Tornado strange issue,https://stackoverflow.com/questions/27612549,WebSocket Threading using Global Variables,"I'm trying to create a WebSocket server using the very popular Tornado server for Python, but I'm having an issue creating a global-scoped self variable in order to write data to the web socket outside the class.

This answer solved my problem exactly, but I wanted to take it a step further and wrap the whole thing in a thread.

This is my socket:

wss = []

class WSHandler(tornado.websocket.WebSocketHandler):

    def check_origin(self, origin):
        return True

    def open(self):
        print ('New connection established.')
        if self not in wss:
            wss.append(self)

    def on_message(self, message):
        print ('Received message: %s' % message)

    def on_close(self):
        print ('Connection closed.')
        if self in wss:
            wss.remove(self)


This is the method that is outside of the class that writes to the socket:

def write_data(message):
    for ws in wss:
        print (""Sending: %s"" % message)
        ws.write_message(message);


This is the threaded server class:

class ServerThread(threading.Thread):

    def run(self):
        print (""Starting server."")
        http_server = tornado.httpserver.HTTPServer(application)
        http_server.listen(4045)
        main_loop = tornado.ioloop.IOLoop.instance()
        main_loop.start()

    def send_data(self, message):
        write_data(message);


The strange thing is, when the code is not wrapped in a Thread class, the write method works fine. In the code above, when I call:

server_thread = ServerThread()
server_thread.start()
server_thread.send_data(""Ping!"")


nothing happens. The method write_data(message) is entered, but evidently wss[] is empty.

Any help you could provide would be greatly appreciated!

Update:

I've been continuously looking into this problem to no avail. Another strange thing: New connection established. never prints to the console making me think that the socket is never appended to the list rather than it being a variable scoping problem.
",2,1869,"You are not supposed to use port 4045 for HTTP/WebSocket service, as it's blocked by browser. You may get an error message from browser:

Failed to construct 'WebSocket': The port 4045 is not allowed.


http://www-archive.mozilla.org/projects/netlib/PortBanning.html
http://support.apple.com/en-us/HT203772
",,
Tornado strange issue,https://stackoverflow.com/questions/20948704,Facebook authorization redirecting to a strange URL in my Tornado app,"I have written a simple Tornado app. It has a homepage, if the user is logged in, it says user is logged or else it has a link to login. Here is the full script and I have set up website url as localhost:8000 in my FB app settings : 

#!/usr/bin/env python

import os.path
import os

import tornado.auth
import tornado.escape
import tornado.httpserver
import tornado.ioloop
import tornado.options
import tornado.web
from tornado.options import define, options

from settings import *

define(""port"", default=8000, help=""run on the given port"", type=int)

class Application(tornado.web.Application):
    def __init__(self):
        handlers = [
             (r'/', MainHandler),
             (r'/auth/login', LoginHandler),
             (r'/auth/logout',  LogoutHandler)
        ]
        settings = application_handler_setttings
        tornado.web.Application.__init__(self, handlers, **settings)

class MainHandler(tornado.web.RequestHandler):
    def get(self):
        userID = self.get_secure_cookie('user_id')
        if userID:
            self.render(""index.html"")
        else:
            self.render(""login.html"")

class LoginHandler(tornado.web.RequestHandler, tornado.auth.FacebookGraphMixin):
    @tornado.web.asynchronous
    def get(self):
        userID = self.get_secure_cookie('user_id')

        if self.get_argument('code', None):
            self.get_authenticated_user(
                redirect_uri='http://localhost:8000/auth/login',
                client_id=self.settings['facebook_api_key'],
                client_secret=self.settings['facebook_secret'],
                code=self.get_argument('code'),
                callback=self.async_callback(self._on_facebook_login))
            return
        elif self.get_secure_cookie('access_token'):
            self.redirect('/')

        self.authorize_redirect(
            redirect_uri='http://localhost:8000/auth/login ',
            client_id=self.settings['facebook_api_key'],
            extra_params={'scope': 'user_photos, publish_stream'}
        )

    def _on_facebook_login(self, user):
        if not user:
            self.clear_all_cookies()
            raise tornado.web.HTTPError(500, 'Facebook authentication failed')

        self.set_secure_cookie('user_id', str(user['id']))
        self.set_secure_cookie('user_name', str(user['name']))
        self.set_secure_cookie('access_token', str(user['access_token']))
        self.redirect('/')

class LogoutHandler(tornado.web.RequestHandler):
    def get(self):
        self.clear_all_cookies()
        self.render('logout.html')

def main():
    tornado.options.parse_command_line()
    http_server = tornado.httpserver.HTTPServer(Application())
    http_server.listen(options.port)
    tornado.ioloop.IOLoop.instance().start()


if __name__ == ""__main__"":
    main()


my settings.py file:

import os
application_handler_setttings = dict(
            template_path=os.path.join(os.path.dirname(__file__), ""templates""),
            static_path=os.path.join(os.path.dirname(__file__), ""static""),
            facebook_api_key= '1...3',
            facebook_secret= '7...d',
            cookie_secret= 'N...g',
            debug=True,
            )


Here is the issue. When I authenticate my app and when it redirects back, the url in browser changes and some strange string is present, it is http://localhost:8000/#_=_. I am not able to understand, why such string is being appended. 



Secondly, if I enter any url starting with #, it is not throwing 404 error, because I have not defined any handler. Why is that? For example, even if I enter http://localhost:8000/#fghdgdh it shows the index page. Well, I find that strange.

Lastly, is it possible to keep all my handler class definitions in in separate files? And also the list of tuple of url and handler functions, is it possible to keep this also in some separate file other than these class definitions and the in the main script?
",1,344,"The first part is a duplicate question as you can see in the comments.

Second part: things followed by # in an url is called Fragment Identifier. It points to some resource inside the primary resource(in your case, the HTML file). 


  In URIs for MIME text/html pages such as http://www.example.org/foo.html#bar the fragment refers to the element with id=""bar"". 


404 is returned if the HTML document is not found, not for fragments.

Last part: Yes you can. Just move them to some other file, say handler.py and import them where you need, like 

from handlers import LoginHandler


And it should work.
",,
