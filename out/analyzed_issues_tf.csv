ISSUE_LINK,AI_VERDICT,REASON,LIBRARY_NAME,API_NAME,ISSUE_DESCRIPTION,NORMAL_CONDITIONS,TRIGGER_CONDITIONS,REASON_FOR_DIFFICULTY_IN_DETECTION,ISSUE_TITLE,ISSUE_BODY,ANSWER_1,ANSWER_2,ANSWER_3,HUMAN_CLASSIFICATION,HUMAN_REASON
https://stackoverflow.com/questions/56156646,true,The issue is related to the usage of the tfmot.sparsity.keras.prune_low_magnitude API in TensorFlow Model Optimization. The error message indicates that the clone_model() function is receiving an unexpected keyword argument 'clone_function'. This suggests that the API has changed and the clone_model() function no longer accepts the 'clone_function' argument.,TensorFlow Model Optimization,tfmot.sparsity.keras.prune_low_magnitude,"The error occurs when using the tfmot.sparsity.keras.prune_low_magnitude API with the 'clone_function' argument. The clone_model() function, which is called internally, does not expect this argument and raises a TypeError.",The tfmot.sparsity.keras.prune_low_magnitude API is typically used to apply magnitude-based pruning to a Keras model.,The issue is triggered when the 'clone_function' argument is passed to the tfmot.sparsity.keras.prune_low_magnitude API.,"This issue might be challenging to detect during development and testing because the error message does not explicitly mention the 'clone_function' argument, making it less obvious that the issue lies with the API usage.",Keras + Tensorflow Model Optimization: TypeError: clone_model() got an unexpected keyword argument &#39;clone_function&#39;,"I'm trying Tensorflow Model Optimization in order to prune a simple Neural Network. Here's my code:

from __future__ import absolute_import, division, print_function, unicode_literals, unicode_literals
import tensorflow as tf
from tensorflow import keras
import numpy as np
import matplotlib.pyplot as plt

fashion_mnist = keras.datasets.fashion_mnist

(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()

class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',
               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']

train_images = train_images / 255.0
test_images = test_images / 255.0

model = keras.Sequential([
    keras.layers.Flatten(input_shape=(28, 28)),
    keras.layers.Dense(128, activation=tf.nn.relu),
    keras.layers.Dense(10, activation=tf.nn.softmax)
])

import tensorflow_model_optimization as tfmot


pruning_schedule = tfmot.sparsity.keras.PolynomialDecay(
                        initial_sparsity=0.0, final_sparsity=0.5,
                        begin_step=2000, end_step=4000)

model_for_pruning = tfmot.sparsity.keras.prune_low_magnitude(model, pruning_schedule=pruning_schedule)


model_for_pruning.compile(optimizer='adam',
          loss='sparse_categorical_crossentropy',
          metrics=['accuracy'])

from tensorflow.keras.callbacks import TensorBoard


tensorboard=TensorBoard(log_dir='D:\Python\logs', histogram_freq=0,  
          write_graph=True, write_images=True)

model_for_pruning.fit(train_images, train_labels, epochs=5,callbacks=tensorboard)


#tensorboard --logdir D:\Python\logs 


I'm getting the following error:

File ""&lt;ipython-input-1-8f75575649d2&gt;"", line 52, in &lt;module&gt;
    model_for_pruning = tfmot.sparsity.keras.prune_low_magnitude(model, pruning_schedule=pruning_schedule)

  File ""C:\Users\Rubens\Anaconda3\lib\site-packages\tensorflow_model_optimization\python\core\sparsity\keras\prune.py"", line 152, in prune_low_magnitude
    to_prune, input_tensors=None, clone_function=_add_pruning_wrapper)

TypeError: clone_model() got an unexpected keyword argument 'clone_function'


That is, clone function does not belong to Keras' file models.py. I tried to add **kwargs to it, without success:

def clone_model(model, input_tensors=None,**kwargs):
""""""Clone any `Model` instance.

Model cloning is similar to calling a model on new inputs,
except that it creates new layers (and thus new weights) instead
of sharing the weights of the existing layers.

# Arguments
    model: Instance of `Model`
        (could be a functional model or a Sequential model).
    input_tensors: optional list of input tensors
        to build the model upon. If not provided,
        placeholders will be created.

# Returns
    An instance of `Model` reproducing the behavior
    of the original model, on top of new inputs tensors,
    using newly instantiated weights.

# Raises
    ValueError: in case of invalid `model` argument value.
""""""
if isinstance(model, Sequential):
    return _clone_sequential_model(model, input_tensors=input_tensors)
else:
    return _clone_functional_model(model, input_tensors=input_tensors)


This is the end of file prune.py, belonging to Tensorflow Model Optimization (notice clone_function=_strip_pruning_wrapper):

  def _strip_pruning_wrapper(layer):
    if isinstance(layer, pruning_wrapper.PruneLowMagnitude):
      # The _batch_input_shape attribute in the first layer makes a Sequential
      # model to be built. This makes sure that when we remove the wrapper from
      # the first layer the model's built state preserves.
      if not hasattr(layer.layer, '_batch_input_shape') and hasattr(
          layer, '_batch_input_shape'):
        layer.layer._batch_input_shape = layer._batch_input_shape
      return layer.layer
    return layer

  return keras.models.clone_model(
      model, input_tensors=None, clone_function=_strip_pruning_wrapper)


All libraries included are up-to-date. Any ideas on how to overcome this error ?

Thanks in advance
","I found the answer. There is a tricky workaround: besides fixing the code to:

from tensorflow_model_optimization.sparsity import keras as sparsity

pruning_params = {
      'pruning_schedule': sparsity.PolynomialDecay(initial_sparsity=0.50,
                                                   final_sparsity=0.90,
                                                   begin_step=3,
                                                   end_step=end_step,
                                                   frequency=100)
}

pruned_model = tf.keras.Sequential([
    sparsity.prune_low_magnitude(
        l.Conv2D(32, 5, padding='same', activation='relu'),
        input_shape=input_shape,
        **pruning_params),
    l.MaxPooling2D((2, 2), (2, 2), padding='same'),
    l.BatchNormalization(),
    sparsity.prune_low_magnitude(
        l.Conv2D(64, 5, padding='same', activation='relu'), **pruning_params),
    l.MaxPooling2D((2, 2), (2, 2), padding='same'),
    l.Flatten(),
    sparsity.prune_low_magnitude(l.Dense(1024, activation='relu'),
                                 **pruning_params),
    l.Dropout(0.4),
    sparsity.prune_low_magnitude(l.Dense(num_classes, activation='softmax'),
                                 **pruning_params)
])


... I had to restart Jupyter kernel to get rid of further errors, like Conv2D has no attribute 'kernel', as seen at GitHub:

tf.enable_eager_execution must be called at program startup. #18304
",,,false,
https://stackoverflow.com/questions/40327541,false,The behavior observed in the issue is due to a misunderstanding of logical operators and conditions in pandas rather than a problem with the API itself.,,,,,,,Tensorflow shuffle batch fraction unexpected behavior,"I am training a convolutional neural network and I got some unexpected behavior with the shuffle_batch fraction summary, or maybe I just do not understand it. Can someone pls explain it? The difference between those two graphs is that I exchanged the loss function. 

With this loss function I get the line at 0.0

loss = tf.nn.l2_loss(expected_labels-labels)


While this one gives me a constant 1.0 (after hitting 1.0 the first time)

loss = tf.reduce_mean(tf.square(expected_labels - labels))


Can the change of loss function really cause that change? I am not sure what this means.



EDIT: Code as requested
The first part is for setting up the batching and the big picture.

filename_queue = tf.train.string_input_producer(filenames,
                                                num_epochs=None)
label, image = read_and_decode_single_example(filename_queue=filename_queue)
image = tf.image.decode_jpeg(image.values[0], channels=3)
jpeg = tf.cast(image, tf.float32) / 255.
jpeg.set_shape([66,200,3])
images_batch, labels_batch = tf.train.shuffle_batch(
    [jpeg, label], batch_size= FLAGS.batch_size,
    num_threads=8,
    capacity=60000,
    min_after_dequeue=10000)
images_placeholder, labels_placeholder = placeholder_inputs(
    FLAGS.batch_size)

label_estimations, W1_conv, h1_conv, current_images = e2e.inference(images_placeholder)

# Add to the Graph the Ops for loss calculation.
loss = e2e.loss(label_estimations, labels_placeholder)


# Decay once per epoch, using an exponential schedule starting at 0.01.


# Add to the Graph the Ops that calculate and apply gradients.
train_op = e2e.training(loss, FLAGS.learning_rate, FLAGS.batch_size)


Here come the methods for inference loss and train

def inference(images):
with tf.name_scope('conv1'):
    W_conv1 = tf.Variable(tf.truncated_normal([5, 5, 3, FEATURE_MAPS_C1], stddev=STDDEV))
    b_conv1 = tf.Variable(tf.constant(BIAS_INIT, shape=[FEATURE_MAPS_C1]))
    h_conv1 = tf.nn.bias_add(
        tf.nn.conv2d(images, W_conv1, strides=[1, 2, 2, 1], padding='VALID'), b_conv1)

with tf.name_scope('conv2'):
    W_conv2 = tf.Variable(tf.truncated_normal([5, 5, FEATURE_MAPS_C1, 36], stddev=STDDEV))
    b_conv2 = tf.Variable(tf.constant(BIAS_INIT, shape=[36]))
    h_conv2 = tf.nn.conv2d(h_conv1, W_conv2, strides=[1, 2, 2, 1], padding='VALID') + b_conv2

with tf.name_scope('conv3'):
    W_conv3 = tf.Variable(tf.truncated_normal([5, 5, 36, 48], stddev=STDDEV))
    b_conv3 = tf.Variable(tf.constant(BIAS_INIT, shape=[48]))
    h_conv3 = tf.nn.conv2d(h_conv2, W_conv3, strides=[1, 2, 2, 1], padding='VALID') + b_conv3

with tf.name_scope('conv4'):
    W_conv4 = tf.Variable(tf.truncated_normal([3, 3, 48, 64], stddev=STDDEV))
    b_conv4 = tf.Variable(tf.constant(BIAS_INIT, shape=[64]))
    h_conv4 = tf.nn.conv2d(h_conv3, W_conv4, strides=[1, 1, 1, 1], padding='VALID') + b_conv4

with tf.name_scope('conv5'):
    W_conv5 = tf.Variable(tf.truncated_normal([3, 3, 64, 64], stddev=STDDEV))
    b_conv5 = tf.Variable(tf.constant(BIAS_INIT, shape=[64]))
    h_conv5 = tf.nn.conv2d(h_conv4, W_conv5, strides=[1, 1, 1, 1], padding='VALID') + b_conv5
    h_conv5_flat = tf.reshape(h_conv5, [-1, 1 * 18 * 64])


with tf.name_scope('fc1'):
    W_fc1 = tf.Variable(tf.truncated_normal([1 * 18 * 64, 100], stddev=STDDEV))
    b_fc1 = tf.Variable(tf.constant(BIAS_INIT, shape=[100]))
    h_fc1 = tf.matmul(h_conv5_flat, W_fc1) + b_fc1

with tf.name_scope('fc2'):
    W_fc2 = tf.Variable(tf.truncated_normal([100, 50], stddev=STDDEV))
    b_fc2 = tf.Variable(tf.constant(BIAS_INIT, shape=[50]))
    h_fc2 = tf.matmul(h_fc1, W_fc2) + b_fc2

with tf.name_scope('fc3'):
    W_fc3 = tf.Variable(tf.truncated_normal([50, 10], stddev=STDDEV))
    b_fc3 = tf.Variable(tf.constant(BIAS_INIT, shape=[10]))
    h_fc3 = tf.matmul(h_fc2, W_fc3) + b_fc3

with tf.name_scope('fc4'):
    W_fc4 = tf.Variable(tf.truncated_normal([10, 1], stddev=STDDEV))
    b_fc4 = tf.Variable(tf.constant(BIAS_INIT, shape=[1]))
    h_fc4 = tf.matmul(h_fc3, W_fc4) + b_fc4


return h_fc4


Here is the loss function, using l2 causes the issue.

def loss(label_estimations, labels):    
    n_labels = tf.reshape(label_estimations, [-1])
    # Here are the two loss functions
    #loss = tf.reduce_mean(tf.square(n_labels - labels))
    loss = tf.nn.l2_loss(n_labels-labels)
    return loss


Train method:

def training(loss, learning_rate, batch_size): 
    global_step = tf.Variable(0, name='global_step', trainable=False)
    tf.scalar_summary('learning_rate',learning_rate)
    tf.scalar_summary('Loss ('+loss.op.name+')', loss)

    optimizer = tf.train.AdamOptimizer(learning_rate)
    train_op = optimizer.minimize(loss, global_step=global_step)
    return train_op


Plot for tf.reduce_sum(tf.square(n_labels - labels)/2)


","As mentioned in TensorFlow's original guide https://www.tensorflow.org/programmers_guide/reading_data


  How many threads do you need? the tf.train.shuffle_batch* functions add a summary to the graph that indicates how full the example queue is. If you have enough reading threads, that summary will stay above zero. You can view your summaries as training progresses using TensorBoard.


It seems better if the queue is never empty, i.e. the ""fraction_full"" stays non-zero. If not, you should allocate more threads to queue_runner
","The only difference between your loss and l2 is scaling, thus you might need to play around with your learning rate / other hyperparameters to take this into account. 

l2 loss in TF is defined as:

1/2 SUM_i^N (pred(x_i) - y_i)^2


while your cost is

1/N SUM_i^N (pred(x_i) - y_i)^2


Of course since you are using stochastic gradient approach, efficienty you are using an approximator of form

1/2 SUM_{(x_i, y_i) in batch} (pred(x_i) - y_i)^2 # l2
1/#batch SUM_{(x_i, y_i) in batch} (pred(x_i) - y_i)^2 # you


Thus you would have to multiply your cost by batch_size / 2 to get the original cost. Typically this is not a problem, but sometimes wrong scaling can put you in very degenerated parts of the error surface, and the optimizer will simply fail (especially such aggressive one like Adam).

Side note - you are aware that your model is a deep linear model? You do not have any non-linearities in the model. This is very specific network.
",,false,
https://stackoverflow.com/questions/59072533,false,"The issue is not related to an API exhibiting unexpected failures or unpredictable behaviors under specific runtime conditions. Instead, it is related to the behavior of the __init__ function in subclassing a Sequential model in Keras.",,,,,,,Subclassing Sequential() keras-model,"I wanted to subclass a sequential model in order to be able to write a custom call() and handle named inputs. However, I got, for me, some unexpected behavior already for very minor changes to the __init__ function. If I try to add a new member to my subclass and initialize it after calling super().__init__() the model fails to build automatically.

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, Activation, MaxPooling2D, Dense, Flatten
import tensorflow as tf
class Sequential2(Sequential):

    def __init__(self):
        super(Sequential2, self).__init__()
        self.custom_member = []

    def get_my_custom_member(self):
        return self.custom_member

model = Sequential2()

if tf.keras.backend.image_data_format() == 'channels_first':
    input_shape = (1, 28, 28)
else:
    assert tf.keras.backend.image_data_format() == 'channels_last'
    input_shape = (28, 28, 1)

layers = [Conv2D(32, (3, 3), input_shape=input_shape)]

for layer in layers:
    model.add(layer)

model.add(Dense(10))
model.add(Activation('relu'))

model.summary()


fails with output: ValueError: This model has not yet been built. Build the model first by calling `build()` or calling `fit()` with some data, or specify an `input_shape` argument in the first layer(s) for automatic build.

However if self.custom_member = [] is left out it works as expected.

What am I missing here? (tested with Tensorflow 1.14)
","This issue was fixed in TF 2.2. You can refer working code as shown below
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, Activation, MaxPooling2D, Dense, Flatten

import tensorflow as tf
print(tf.__version__)

class Sequential2(Sequential):

    def __init__(self):
        super(Sequential2, self).__init__()
        self.custom_member = []

    def get_my_custom_member(self):
        return self.custom_member

model = Sequential2()

if tf.keras.backend.image_data_format() == 'channels_first':
    input_shape = (1, 28, 28)
else:
    assert tf.keras.backend.image_data_format() == 'channels_last'
    input_shape = (28, 28, 1)

layers = [Conv2D(32, (3, 3), input_shape=input_shape)]

for layer in layers:
    model.add(layer)

model.add(Dense(10))
model.add(Activation('relu'))

model.summary()

Output:
2.2.0
Model: ""sequential""
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
conv2d (Conv2D)              (None, 26, 26, 32)        320       
_________________________________________________________________
dense (Dense)                (None, 26, 26, 10)        330       
_________________________________________________________________
activation (Activation)      (None, 26, 26, 10)        0         
=================================================================
Total params: 650
Trainable params: 650
Non-trainable params: 0
_________________________________________________________________

",,,false,
https://stackoverflow.com/questions/60173337,false,"The issue is not related to an API exhibiting unexpected failures or unpredictable behaviors under specific runtime conditions. Instead, it is related to the behavior of TensorFlow Variables and tf.data.Dataset within a function decorated with @tf.function.",,,,,,,How to manipulate and return tf.Variable using a for loop over tf.data.Dataset inside function decorated with @tf.function?,"I am trying to create a function containing a for loop over a TensorFlow Dataset that assigns a new value to a TensorFlow Variable in each iteration. The Variable should also be returned as output of the function. With eager execution enabled, there are no issues, however, in graph mode, some unexpected things seem to happen. Consider the following simple dummy code:

import tensorflow as tf


class Test(object):
    def __init__(self):
        self.var = tf.Variable(0, trainable=False, dtype=tf.float32)
        self.increment = tf.constant(1, dtype=tf.float32)
        self.dataset = tf.data.Dataset.from_tensor_slices([0, 1, 2])

    @tf.function
    def fn1(self):
        self.var.assign(0)
        for _ in tf.range(3):
            self.var.assign(self.var+self.increment)
            tf.print(self.var)
        tf.print(self.var)
        return self.var

    @tf.function
    def fn2(self):
        self.var.assign(0)
        for _ in self.dataset:
            self.var.assign(self.var+self.increment)
            tf.print(self.var)
        tf.print(self.var)
        return self.var

    @tf.function
    def fn3(self):
        self.var.assign(0)
        y = self.var
        for _ in self.dataset:
            self.var.assign(self.var+self.increment)
            y = self.var
            tf.print(y)
        tf.print(y)
        return y

    @tf.function
    def fn4(self):
        var = 0.0
        for _ in self.dataset:
            var += 1.0
            tf.print(var)
        tf.print(var)
        return var



test.fn1(), test.fn3() and test.fn4() all return the following (desired) output:

1
2
3
3
&lt;tf.Tensor: shape=(), dtype=float32, numpy=3.0&gt;


However, test.fn2() behaves differently:

1
2
3
0
&lt;tf.Tensor: shape=(), dtype=float32, numpy=0.0&gt;


Interestingly, after execution of test.fn2, test.var does seem to contain the correct value:

&lt;tf.Variable 'Variable:0' shape=() dtype=float32, numpy=3.0&gt;


I am not sure why test.fn2 fails. Clearly, it is doing some things correctly (as test.var contains the correct value after execution of the function), but it does not deliver the correct result. Can you help me understand what causes this code to fail?

The behavior described above occurs when using TensorFlow 2.1.0 for Python 3.6 on CentOS 7.
","Running this on TensorFlow 2.1.0 reproduces your scenario.

Which prints 1 2 3 0 for test.fn2(), but you should also consider that when you print self.var in test.fn3() it will also show you self.var = 0 during the function call.

Modified fn3( ):

    @tf.function
    def fn3(self):
        self.var.assign(0)
        y = self.var
        for _ in self.dataset:
            self.var.assign(self.var+self.increment)
            y = self.var
            tf.print(y)
        tf.print(self.var)  # Inspect self.var value
        tf.print(y)
        return y


Output:

# Executed in Tensorflow 2.1.0
# test.fn3()
1
2
3
0  &lt;&lt; self.var
3


This is already fixed If you execute this in Tensorflow 2.2.0-rc2.
The output will be your desired outcome even when printing it during graph execution.

To quickly simulate this you could use Google Colab and use %tensorflow_version 2.x to get the latest available version for Tensorflow.

Output:

# Executed in Tensorflow 2.2.0-rc2
Function 1
1
2
3
3
Function 2
1
2
3
3
Function 3
1
2
3
3 &lt;&lt; Value of self.var in test.fn3()
3
Function 4
1
2
3
3


You could check more about the changes in the latest Tensorflow Updates in this link.  
",,,false,
https://stackoverflow.com/questions/53331211,true,The issue is relevant for analysis as it involves a specific API call that results in an error during the build process.,Bazel,bazel build,The issue occurs when running the command 'bazel build --config=cuda //tensorflow/contrib/quantization:quantize_graph'. The error message indicates that the target 'quantize_graph' is not declared in the package 'tensorflow/contrib/quantization'.,The 'bazel build' command is used to build specific targets in a Bazel project.,The issue is triggered when attempting to build the target 'quantize_graph' in the 'tensorflow/contrib/quantization' package.,This issue might be challenging to detect for users who are not familiar with the Bazel build system and the specific target structure of the TensorFlow project.,Tensorflow bazel quantization build error,"I am trying to build tensorflow tools package with bazel 0.18.0

following steps are ok

git clone https://github.com/tensorflow/tensoflow
bazel build --config=cuda //tensorflow/tools/pip_package:build_pip_package
bazel build --config=cuda //tensorflow/examples/label_image:label_image


until trying to run this command line 

bazel build --config=cuda //tensorflow/contrib/quantization:quantize_graph


it show error, so should i give something else then quantize_graph? and what i can use or find?

root@24663fb1018d:/srv/wu/tensorflow-src/tensorflow# bazel build --config=cuda //tensorflow/contrib/quantization:quantize_graph
WARNING: Duplicate rc file: /srv/wu/tensorflow-src/tensorflow/tools/bazel.rc is read multiple times, most recently imported from /srv/wu/tensorflow-src/tensorflow/.bazelrc
WARNING: Processed legacy workspace file /srv/wu/tensorflow-src/tensorflow/tools/bazel.rc. This file will not be processedin the next release of Bazel. Please read https://github.com/bazelbuild/bazel/issues/6319 for further information, including how to upgrade.
Starting local Bazel server and connecting to it...
WARNING: The following configs were expanded more than once: [cuda]. For repeatable flags, repeats are counted twice and may lead to unexpected behavior.
ERROR: Skipping '//tensorflow/contrib/quantization:quantize_graph': no such target '//tensorflow/contrib/quantization:quantize_graph': target 'quantize_graph' not declared in package 'tensorflow/contrib/quantization' defined by /srv/wu/tensorflow-src/tensorflow/tensorflow/contrib/quantization/BUILD
WARNING: Target pattern parsing failed.
ERROR: no such target '//tensorflow/contrib/quantization:quantize_graph': target 'quantize_graph' not declared in package 'tensorflow/contrib/quantization' defined by /srv/wu/tensorflow-src/tensorflow/tensorflow/contrib/quantization/BUILD
INFO: Elapsed time: 1.195s
INFO: 0 processes.
FAILED: Build did NOT complete successfully (1 packages loaded)

and then i tried with tools path, no luck

bazel build --config=cuda //tensorflow/tools/quantization:quantize_graph


WARNING: Duplicate rc file: /srv/wu/tensorflow-src/tensorflow/tools/bazel.rc is read multiple times, most recently imported from /srv/wu/tensorflow-src/tensorflow/.bazelrcWARNING: Processed legacy workspace file /srv/wu/tensorflow-src/tensorflow/tools/bazel.rc. This file will not be processed in the next release of Bazel. Please read https://github.com/bazelbuild/bazel/issues/6319 for further information, including how to upgrade.WARNING: The following configs were expanded more than once: [cuda]. For repeatable flags, repeats are counted twice and may lead to unexpected behavior.ERROR: Skipping '//tensorflow/tools/quantization:quantize_graph': no such package'tensorflow/tools/quantization': BUILD file not found on package pathWARNING: Target pattern parsing failed.
ERROR: no such package 'tensorflow/tools/quantization': BUILD file not found on package path
INFO: Elapsed time: 0.506s
INFO: 0 processes.
FAILED: Build did NOT complete successfully (0 packages loaded)

also the toco is not working

bazel build --config=cuda tensorflow/contrib/lite/toco:toco


WARNING: Duplicate rc file: /srv/wu/tensorflow-src/tensorflow/tools/bazel.rc is read multiple times, most recently imported from /srv/wu/tensorflow-src/tensorflow/.bazelrc
WARNING: Processed legacy workspace file /srv/wu/tensorflow-src/tensorflow/tools/bazel.rc. This file will not be processed in the next release of Bazel. Please read https://github.com/bazelbuild/bazel/issues/6319 for further information, including how to upgrade.WARNING: The following configs were expanded more than once: [cuda]. For repeatable flags, repeats are counted twice and may lead to unexpected behavior.ERROR: Skipping 'tensorflow/contrib/lite/toco:toco': no such package 'tensorflow/contrib/lite/toco': BUILD file not found on package path
WARNING: Target pattern parsing failed.ERROR: no such package 'tensorflow/contrib/lite/toco': BUILD file not found on package pathINFO: Elapsed time: 0.500s
INFO: 0 processes.FAILED: Build did NOT complete successfully (0 packages loaded)
","To verify where targets are, look into the BUILD file in the package directory. 

The quantize_graph target has been moved to the //tensorflow/contrib/quantize package. This should work:

$ bazel build --config=cuda //tensorflow/contrib/quantize:quantize_graph


The toco target has been moved from //tensorflow/contrib/lite/toco to //tensorflow/lite/toco. Like quantize_graph, this should work:

$ bazel build --config=cuda //tensorflow/lite/toco:toco

",,,false,
https://stackoverflow.com/questions/55029279,true,The issue is relevant for analysis as it involves unexpected behavior when using the 'multi_gpu_model' API from TensorFlow/Keras.,TensorFlow/Keras,multi_gpu_model,"The issue occurs when using the 'multi_gpu_model' API to train a model on multiple GPUs. The GPUs are not utilized as expected, with only the first GPU being fully utilized.",The 'multi_gpu_model' API is used to parallelize model training across multiple GPUs.,The issue is triggered when using the 'multi_gpu_model' API with more than one GPU.,This issue might be challenging to detect for users who are not familiar with the specific behavior of the 'multi_gpu_model' API or the underlying GPU utilization mechanisms in TensorFlow/Keras.,Tensorflow / keras multi_gpu_model is not splitted to more than one gpu,"I'm encountered the problem, that I can not successfully split my training batches to more than one GPU. If multi_gpu_model from tensorflow.keras.utils is used, tensorflow allocates the full memory on all available (for example 2) gpus, but only the first one (gpu[0]) is utilized to 100% if nvidia-smi is watched.

I'm using tensorflow 1.12 right now.

Test on single device

model = getSimpleCNN(... some parameters)

model .compile()
model .fit()


As expected, data is loaded by cpu and the model runs on gpu[0] with 97% - 100% gpu utilization:


Create a multi_gpu model

As described in the tensorflow api for multi_gpu_model here, the device scope for model definition is not changed.

from tensorflow.keras.utils import multi_gpu_model

model = getSimpleCNN(... some parameters)
parallel_model = multi_gpu_model(model, gpus=2, cpu_merge=False)  # weights merge on GPU (recommended for NV-link)

parallel_model.compile()
parallel_model.fit()


As seen in the timeline, cpu now not only loads the data, but is doing some other calculations. Notice: the second gpu is nearly doing nothing:


The question

The effect even worsens as soon as four gpus are used. Utilization of the first one goes up to 100% but for the rest there are only short peeks. 

Is there any solution to fix this? How to properly train on multiple gpus?

Is there any difference between tensorflow.keras.utils and keras.utils which causes the unexpected behavior? 
","I just ran into the same issue. 
In my case, the problem came from the use of a build_model(... parameters) function that returned the model. 
Be careful with your  getSimpleCNN() function, as I don't know what is in it my best advice is to build the model sequentially in your code without using this function.
",,,false,
https://stackoverflow.com/questions/47125665,false,The issue does not involve any specific API or exhibit unexpected behavior related to an API.,,,,,,,simple dask map_partitions example,,,,,false,
https://stackoverflow.com/questions/70818269,true,The issue is relevant for analysis as it involves an error related to the 'Model.compile' API in TensorFlow/Keras.,TensorFlow/Keras,Model.compile,"The issue occurs when training a face detection model with CNN. The error message suggests using 'Model.compile(..., run_eagerly=True)' to investigate the cause of the error.",The 'Model.compile' API is used to configure the model for training.,The issue is triggered during the training process of the face detection model.,This issue might be challenging to detect for users who are not familiar with the specific requirements of the 'Model.compile' API or the potential causes of the error in face detection models.,"Tensorflow ValueError: Unexpected result of `train_function` (Empty logs). Please use `Model.compile(..., run_eagerly=True)","I'm trying to make a face detection model with CNN. I used codes that I made for number detection. When I use number images, program work. But, when I use my face images, I get an error that is:
Unexpected result of train_function (Empty logs). Please use Model.compile(..., run_eagerly=True), or tf.config.run_functions_eagerly(True) for more information of where went wrong, or file a issue/bug to tf.keras.
Notebook link: https://github.com/AkifCanSonmez/ImageProccessingCourse/blob/main/CNN/Number%20Classification%20Project/Building%20Model/Building%20Number%20Classification%20Model%20with%20Keras.ipynb
Number image:
Face image:
","Your input images have a shape of (32,32,3) whil you first conv2D layer sets the inputshape to (32,32,1). Most likely your numbers have only 1 channel since they are grayscale, while you face images have 3 color channels.
change:
model.add(tf.keras.layers.Conv2D(input_shape = (32,32,1), filters = 8, kernel_size = (5,5),activation = ""relu"", padding = ""same"" ))

to
model.add(tf.keras.layers.Conv2D(input_shape = (32,32,3), filters = 8, kernel_size = (5,5),activation = ""relu"", padding = ""same"" ))

","To me, the solution of the problem was a bad definition of the function len() of my custom data generator.
","Also ensure that you declared initialised all the parameters in the compile functions , i had the exact error when i forgot to initialise batch_size in the fit function after the compile function.
",false,
https://stackoverflow.com/questions/38688777,false,The issue does not involve any specific API or exhibit unexpected behavior related to an API.,,,,,,,TensorFlow: Graph Optimization (GPU vs CPU Performance),,,,,false,
https://stackoverflow.com/questions/56557587,false,The issue does not involve any specific API or exhibit unexpected behavior related to an API.,,,,,,,Why my one-filter convolutional neural network is unable to learn a simple gaussian kernel?,,,,,false,
https://stackoverflow.com/questions/72993109,true,The issue is relevant for analysis as it involves an error related to the 'Model.fit' API in TensorFlow/Keras.,TensorFlow/Keras,Model.fit,"The issue occurs when training an image classifier for oil and pencil paintings. The error message suggests using 'Model.compile(..., run_eagerly=True)' to investigate the cause of the error.",The 'Model.fit' API is used to train a model on a given dataset.,The issue is triggered during the training process of the image classifier.,This issue might be challenging to detect for users who are not familiar with the specific requirements of the 'Model.fit' API or the potential causes of the error in image classification models.,"​ ValueError: Unexpected result of `train_function` (Empty logs). Please use `Model.compile(…, run_eagerly=True)","I’ve been training an Image Classifier that can detect Oil and Pencil Paintings. While doing so I encountered a value error. I searched the web thoroughly unfortunately, I didn’t find any satisfactory answer as there was not much discussion about it.
Code:
    import tensorflow as tf
    import os
    import matplotlib.pyplot as plt
    
    gpus = tf.config.experimental.list_physical_devices('GPU')
    for gpu in gpus:
    
      tf.config.experimental.set_memory_growth(gpu, True)
    len(gpus)
    
    # Data Generator
    IMAGE_SHAPE = (224, 224)
    TRAINING_DATA = '/content/drive/MyDrive/Datasets/train'
    
    
    VALID_DATA = '/content/drive/MyDrive/Datasets/test'
    
    datagen = tf.keras.preprocessing.image.ImageDataGenerator(
    
        rescale=1./225
    
    )
    
    train_generator = datagen.flow_from_directory(
    
        TRAINING_DATA,
    
        shuffle=True,
    
        target_size=IMAGE_SHAPE,
    
    )
    
    valid_generator = datagen.flow_from_directory(
    
        VALID_DATA,
    
        shuffle=False,
    
        target_size = IMAGE_SHAPE,
    
    )
    
    def build_model(num_classes):
    
      model = tf.keras.Sequential([
    
                                   tf.keras.layers.Conv2D(filters=8, kernel_size=(3,3), activation='relu',
    
                                                          input_shape=(224, 224, 3)),
    
                                   tf.keras.layers.MaxPooling2D(pool_size=(2,2), strides=2),
    
                                   tf.keras.layers.Conv2D(filters=16, kernel_size=(3,3), activation='relu'),
    
                                   tf.keras.layers.MaxPooling2D(pool_size=(2, 2), strides=2),
    
                                   tf.keras.layers.Conv2D(filters=32, kernel_size=(3, 3), activation='relu'),
    
                                   tf.keras.layers.MaxPooling2D(pool_size=(2, 2), strides=2),
    
                                   tf.keras.layers.Flatten(),
    
                                   tf.keras.layers.Dense(64, activation='relu'),
    
                                   tf.keras.layers.Dense(num_classes, activation='softmax')
    
                                   
    
      ])
    
      return model
    
    model = build_model(num_classes=2)
    
    model.compile(
    
        optimizer=tf.keras.optimizers.Adam(lr=0.0001),
    
        loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),
    
        metrics=['accuracy']
    
    )
    
    print(model.summary())

Error Code:
EPOCHS = 20

BATCH_SIZE = 32

history = model.fit(train_generator,

                    steps_per_epoch=train_generator.samples // BATCH_SIZE, 

                    epochs=EPOCHS,

                    validation_data=valid_generator,

                    validation_steps=valid_generator.samples // BATCH_SIZE,

                    verbose=1

                    )

   1393         logs = tf_utils.sync_to_numpy_or_python_type(logs)
   1394         if logs is None:
-&gt; 1395           raise ValueError('Unexpected result of `train_function` '
   1396                            '(Empty logs). Please use '
   1397                            '`Model.compile(..., run_eagerly=True)`, or '

ValueError: Unexpected result of `train_function` (Empty logs). Please use `Model.compile(..., run_eagerly=True)`, or `tf.config.run_functions_eagerly(True)` for more information of where went wrong, or file a issue/bug to `tf.keras`.

I've seen some other Stack Overflow answers where they mentioned set input_shape=3, which I already had by default in my code still I was having this error. I saw some GitHub error reports about it but there was no solution available. By the way, I'm using the latest version of Tensorflow and TensorGPU, and Keras. If someone could help me to understand what went wrong that would be awesome cuz at the moment I don't know whether it's a bug in Keras or a fault in my code.
","I had a similar issue a lot of times, and every time it occurred because of the wrong dataset directory. You can set a full dataset directory path when importing data, like this if you're using Mac:
'/Users/&lt;path to dataset&gt;

Or just copy path from files and paste it in the code.
",,,false,
https://stackoverflow.com/questions/66141235,false,The issue does not involve any specific API or exhibit unexpected behavior related to an API.,,,,,,,Jupyter with docker: __init__() got an unexpected keyword argument &#39;column&#39;,"I recently installed TensorFlow with GPU support using docker:
docker pull tensorflow/tensorflow:latest-gpu-jupyter

But sometimes when I start a jupyter notebook server using the command:
docker run --gpus all -it -p 8888:8888 tensorflow/tensorflow:latest-gpu-jupyter jupyter notebook --notebook-dir=/tf --ip 0.0.0.0 --allow-root --NotebookApp.allow_origin='https://colab.research.google.com'

I see the following exception on the terminal:
[IPKernelApp] ERROR | Exception in message handler:
Traceback (most recent call last):
  File ""/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py"", line 272, in dispatch_shell
    yield gen.maybe_future(handler(stream, idents, msg))
  File ""/usr/local/lib/python3.6/dist-packages/tornado/gen.py"", line 762, in run
    value = future.result()
  File ""/usr/local/lib/python3.6/dist-packages/tornado/gen.py"", line 234, in wrapper
    yielded = ctx_run(next, result)
  File ""/usr/local/lib/python3.6/dist-packages/tornado/gen.py"", line 162, in _fake_ctx_run
    return f(*args, **kw)
  File ""/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py"", line 580, in complete_request
    matches = yield gen.maybe_future(self.do_complete(code, cursor_pos))
  File ""/usr/local/lib/python3.6/dist-packages/ipykernel/ipkernel.py"", line 348, in do_complete
    return self._experimental_do_complete(code, cursor_pos)
  File ""/usr/local/lib/python3.6/dist-packages/ipykernel/ipkernel.py"", line 373, in _experimental_do_complete
    completions = list(_rectify_completions(code, raw_completions))
  File ""/usr/local/lib/python3.6/dist-packages/IPython/core/completer.py"", line 484, in rectify_completions
    completions = list(completions)
  File ""/usr/local/lib/python3.6/dist-packages/IPython/core/completer.py"", line 1818, in completions
    for c in self._completions(text, offset, _timeout=self.jedi_compute_type_timeout/1000):
  File ""/usr/local/lib/python3.6/dist-packages/IPython/core/completer.py"", line 1862, in _completions
    full_text=full_text, cursor_line=cursor_line, cursor_pos=cursor_column)
  File ""/usr/local/lib/python3.6/dist-packages/IPython/core/completer.py"", line 2030, in _complete
    cursor_pos, cursor_line, full_text)
  File ""/usr/local/lib/python3.6/dist-packages/IPython/core/completer.py"", line 1374, in _jedi_matches
    text[:offset], namespaces, column=cursor_column, line=cursor_line + 1)
  File ""/usr/local/lib/python3.6/dist-packages/jedi/api/__init__.py"", line 726, in __init__
    project=Project(Path.cwd()), **kwds)
TypeError: __init__() got an unexpected keyword argument 'column'

After that, I have to restart the server or reconnect from google colab.
Any ideas where the error might come from and how to fix it?
","This seems to be an incompatibility between jedi and ipython, see this issue.
The fix would be to pin jedi to 0.17.2, so either run:
pip install jedi==0.17.2

Or if you are using poetry add this to your pyproject.toml:
jedi = ""&lt;=0.17.2""

But since you are using a docker image that image will need to be updated. It seems to be gpu-jupyter.Dockerfile.
I would raise an issue on that project and see if they can pin jedi like they did for nbformat, or you could just fork it. They should probably upgrade python as well, 3.6 is getting a bit long in the tooth.
","I will add more details to @daphtdazz's answer. I had to do the following steps to solve this issue:
1. Download TensorFlow from github:
git clone https://github.com/tensorflow/tensorflow.git

2. Edit the file gpu-jupyter.Dockerfile to add jedi==0.17.2 at the end of line 104:
vim tensorflow/tensorflow/tools/dockerfiles/dockerfiles/gpu-jupyter.Dockerfile 

3. Placed myself inside the dockerfiles folder :
cd tensorflow/tensorflow/tools/dockerfiles/

4. Build the image:
docker build -f ./dockerfiles/gpu-jupyter.Dockerfile -t tf .

",,false,
https://stackoverflow.com/questions/58156573,false,"The issue does not meet the criteria for deeper analysis. It is a comparison of performance between AMD plaidml and CPU TensorFlow, rather than an unexpected behavior or failure of a specific API.",,,,,,,AMD plaidml vs CPU Tensorflow - Unexpected results,"I am currently running a simple script to train the mnist dataset.

Running the training through my CPU via Tensorflow is giving me 49us/sample and a 3e epoch using the following code:-

# CPU

import tensorflow as tf
mnist = tf.keras.datasets.mnist

(x_train, y_train), (x_test, y_test) = mnist.load_data()

x_train = tf.keras.utils.normalize(x_train, axis=1)
x_test = tf.keras.utils.normalize(x_test, axis=1)

model = tf.keras.models.Sequential()
model.add(tf.keras.layers.Flatten())
model.add(tf.keras.layers.Dense(128, activation=tf.nn.relu))
model.add(tf.keras.layers.Dense(128, activation=tf.nn.relu))
model.add(tf.keras.layers.Dense(10, activation=tf.nn.softmax))

model.compile(optimizer='adam', loss=""sparse_categorical_crossentropy"", metrics=[""accuracy""])

model.fit(x_train, y_train, epochs=3)


When I run the dataset through my AMD Pro 580 using the opencl_amd_radeon_pro_580_compute_engine via plaidml setup I get the following results 249us/sample with a 15s epoch, using the following code:-

# GPU

import plaidml.keras
plaidml.keras.install_backend()
import keras
from keras.datasets import mnist

(x_train, y_train), (x_test, y_test) = mnist.load_data()

x_train = keras.utils.normalize(x_train, axis=1)
x_test = keras.utils.normalize(x_test, axis=1)

model = keras.models.Sequential()
model.add(keras.layers.Flatten())
model.add(keras.layers.Dense(128, activation='relu'))
model.add(keras.layers.Dense(128, activation='relu'))
model.add(keras.layers.Dense(10, activation='softmax'))

model.compile(optimizer='adam', loss=""sparse_categorical_crossentropy"", metrics=[""accuracy""])

model.fit(x_train, y_train, epochs=3)


I can see my CPU firing up for the CPU test and my GPU maxing out for the GPU test, but I am very confused as to why the CPU is out performing the GPU by a factor of 5.

Should this be the expected results?

Am I doing something wrong in my code?
",,,,false,
https://stackoverflow.com/questions/50319066,false,The issue does not involve an API exhibiting unexpected failures or unpredictable behaviors. It is a question about creating a neural network in Keras to multiply two input integers.,,,,,,,Creating a neural network in keras to multiply two input integers,"I am playing around with Keras v2.0.8 in Python v2.7 (Tensorflow backend) to create small neural networks that calculate simple arithmetic functions (add, subtract, multiply, etc.), and am a bit confused. The below code is my network which generates a random training dataset of integers with the corresponding labels (the two inputs added together):

def create_data(low, high, examples):

    train_data = []
    label_data = []

    a = np.random.randint(low=low, high=high, size=examples, dtype='int')
    b = np.random.randint(low=low, high=high, size=examples, dtype='int')

    for i in range(0, examples):
        train_data.append([a[i], b[i]])
        label_data.append((a[i] + b[i]))

    train_data = np.array(train_data)
    label_data = np.array(label_data)

    return train_data, label_data

X, y = create_data(0, 500, 10000)

model = Sequential()
model.add(Dense(3, input_dim=2))
model.add(Dense(5, activation='relu'))
model.add(Dense(3, activation='relu'))
model.add(Dense(5, activation='relu'))
model.add(Dense(1, activation='relu'))

model.compile(optimizer='adam', loss='mean_squared_error', metrics=['accuracy'])
model.fit(X, y, epochs=10, batch_size=10)

test_data, _ = create_data(0, 500, 10)
results = model.predict(test_data, batch_size=2)

sq_error = []
for i in range(0, len(test_data)):
    print 'test value:', test_data[i], 'result:', results[i][0], 'error:',\
          '%.2f' %(results[i][0] - (test_data[i][0] + test_data[i][1]))
    sq_error.append((results[i][0] - (test_data[i][0] + test_data[i][1])))

print '\n total rmse error: ', sqrt(np.sum(np.array(sq_error)))


This trains perfectly well and produces no unexpected results. However, when I create the training data by multiplying the two inputs together the model's loss for each epoch stays around 7,000,000,000 and the model does not converge at all. The data creation function for this is as follows:

def create_data(low, high, examples):

    train_data = []
    label_data = []

    a = np.random.randint(low=low, high=high, size=examples, dtype='int')
    b = np.random.randint(low=low, high=high, size=examples, dtype='int')

    for i in range(0, examples):
        train_data.append([a[i], b[i]])
        label_data.append((a[i] * b[i]))

    train_data = np.array(train_data)
    label_data = np.array(label_data)

    return train_data, label_data 


I also had the same problem when I had training data of a single input integer and created the label by squaring the input data. However, it worked fine when I only multiplied the single input by a constant value or added/subtracted by a constant.

I have two questions:

1) Why is this the case? I assume it has something to do with the fundamentals of neural networks, but I can't work it out. 

2) How could I adapt this code to train a model that multiplies two input numbers together.

The network architecture (2 - 3 - 5 - 3 - 5 - 1) is fairly random right now. I've tried lots of different ones varying in layers and neurons, this one just happened to be on my screen as I write this and got an accuracy of 100% for adding two inputs.
",,,,false,
https://stackoverflow.com/questions/68609414,true,"The issue involves the calculation of correct Cross Entropy between two tensors in PyTorch when the target is not a one-hot label. The user is confused about the behavior of PyTorch's BCELoss and is looking for a function that can calculate the correct cross entropy in PyTorch, similar to CategoricalCrossentropy in TensorFlow.",,,,,,,How to calculate correct Cross Entropy between 2 tensors in Pytorch when target is not one-hot?,"I am confused about the calculation of cross entropy in Pytorch. If I want to calculate the cross entropy between 2 tensors and the target tensor is not a one-hot label, which loss should I use? It is quite common to calculate the cross entropy between 2 probability distributions instead of the predicted result and a determined one-hot label.
The basic loss function CrossEntropyLoss forces the target as the index integer and it is not eligible in this case. BCELoss seems to work but it gives an unexpected result. The expected formula to calculate the cross entropy is

But BCELoss calculates the BCE of each dimension, which is expressed as
-yi*log(pi)-(1-yi)*log(1-pi)

Compared with the first equation, the term -(1-yi)*log(1-pi) should not be involved. Here is an example using BCELoss and we can see the second term is involved in each dimension's result. And that make the result different from the correct one.
import torch.nn as nn
import torch
from math import log

a = torch.Tensor([0.1,0.2,0.7])
y = torch.Tensor([0.2,0.2,0.6])
L = nn.BCELoss(reduction='none')
y1 = -0.2 * log(0.1) - 0.8 * log(0.9)
print(L(a, y))
print(y1)

And the result is
tensor([0.5448, 0.5004, 0.6956])
0.5448054311250702

If we sum the results of all the dimensions, the final cross entropy doesn't correspond to the expected one. Because each one of these dimensions involves the -(1-yi)*log(1-pi) term. In constrast, Tensorflow can calculate the correct cross entropy value with CategoricalCrossentropy. Here is the example with the same setting and we can see the cross entropy is calculated in the same way as the first formula.
import tensorflow as tf
from math import log
L = tf.losses.CategoricalCrossentropy()
a = tf.convert_to_tensor([0.1,0.2,0.7])
y = tf.convert_to_tensor([0.2,0.2,0.6])
y_ = -0.2* log(0.1) - 0.2 * log(0.2) - 0.6 * log(0.7)

print(L(y,a), y_)

tf.Tensor(0.9964096, shape=(), dtype=float32) 0.9964095674488687

Is there any function can calculate the correct cross entropy in Pytorch, using the first formula, just like CategoricalCrossentropy in Tensorflow?
","The fundamental problem is that you are incorrectly using the BCELoss function.
Cross-entropy loss is what you want. It is used to compute the loss between two arbitrary probability distributions. Indeed, its definition is exactly the equation that you provided:

where p is the target distribution and q is your predicted distribution. See this StackOverflow post for more information.
In your example where you provide the line
y = tf.convert_to_tensor([0.2, 0.2, 0.6])

you are implicitly modeling a multi-class classification problem where the target class can be one of three classes (the length of that tensor). More specifically, that line is saying that for this one data instance, class 0 has probably 0.2, class 1 has probability 0.2, and class 2 has probability 0.6.
The problem you are having is that PyTorch's BCELoss computes the binary cross-entropy loss, which is formulated differently. Binary cross-entropy loss computes the cross-entropy for classification problems where the target class can be only 0 or 1.
In binary cross-entropy, you only need one probability, e.g. 0.2, meaning that the probability of the instance being class 1 is 0.2. Correspondingly, class 0 has probability 0.8.
If you give the same tensor [0.2, 0.2, 0.6] to BCELoss, you are modeling a situation where there are three data instances, where data instance 0 has probability 0.2 of being class 1, data instance 1 has probability 0.2 of being class 1, and data instance 2 has probability 0.6 of being class 1.
Now, to your original question:

If I want to calculate the cross entropy between 2 tensors and the target tensor is not a one-hot label, which loss should I use?

Unfortunately, PyTorch does not have a cross-entropy function that takes in two probability distributions. See this question:
https://discuss.pytorch.org/t/how-should-i-implement-cross-entropy-loss-with-continuous-target-outputs/10720
The recommendation is to implement your own function using its equation definition. Here is code that works:
def cross_entropy(input, target):
    return torch.mean(-torch.sum(target * torch.log(input), 1))


y = torch.Tensor([[0.2, 0.2, 0.6]])
yhat = torch.Tensor([[0.1, 0.2, 0.7]])
cross_entropy(yhat, y)
# tensor(0.9964)

It provides the answer that you wanted.
","Update: from version 1.10, Pytorch supports class probability targets in CrossEntropyLoss, so you can now simply use:
criterion = torch.nn.CrossEntropyLoss()
loss = criterion(x, y)

where x is the input, y is the target. When y has the same shape as x, it's gonna be treated as class probabilities. Note that x is expected to contain raw, unnormalized scores for each class, while y is expected to contain probabilities for each class (typically the output of the softmax layer). You can find details in the docs.
","Maybe you should try the torch.nn.CrossEntropyLoss function
",false,
https://stackoverflow.com/questions/62518906,true,The issue involves using data augmentation with image paths in Keras' ImageDataGenerator. The user is looking for a solution to handle image paths instead of loading the entire dataset into memory. The suggested solutions include creating a custom generator using the __getitem__ function or working with TF DATA or TFRecords.,,,,,,,Keras ImageDataGenerator : how to use data augmentation with images paths,"I am working on a CNN model and I would like to use some data augmentation, but two problems arise :

My labels are images (my model is some kind of autoencoder, but the expected output images are different from my input images), thus I cannot use functions such as ImageDataGenerator.flow_from_directory(). I was thinking of ImageDataGenerator.flow(train_list, y = labels_list), but there comes my second issue :
Both my input and labels datasets being really huge, I'd prefer working with images paths (which are not handled correctly by the flow() function) rather than loading all my dataset in a single array and making my RAM explode.

How can I properly deal with these two issues? For what I've found, there might be two solutions :

Create my own generator : I've heard of the Keras __getitem__ function in the Sequence class, but can it impact the ImageDataGenerator class?
Work with TF DATA or TFRecords, but they seem pretty difficult to use, and the data augmentation is still to be implemented.

Is there an easiest way to overcome this simple problem? A mere trick would be to force ImageDataGenerator.flow() to work with a nparray of images paths rather than a nparray of images, but I fear that modifying the Keras/tensorflow files will have unexpected consequences (as some functions are called in other classes, a local change can soon result in a global change in all of my notebook library).
",,,,false,
https://stackoverflow.com/questions/59970309,false,"The issue does not meet the criteria for deeper analysis. It is a problem with the OpenVino Model Optimizer when converting a TensorFlow model, rather than an unexpected behavior or failure of a specific API.",,,,,,,OpenVino Model Optimizer Error when converting TensorFlow model,"I have created a custom image classification .pb model file using the python scripts in the TensorFlow for Poets 2 repo (https://github.com/googlecodelabs/tensorflow-for-poets-2). 

I tried converting it to Intermediate Representation using the OpenVino Model Optimizer using the below scripts:

python mo_tf.py --input_model retrained_graph.pb

python mo_tf.py --input_model retrained_graph.pb --mean_values [127.5,127.5,127.5] --input Mul

In both cases this is what happened:

Model Optimizer arguments:
Common parameters:
        - Path to the Input Model:      C:\Program Files (x86)\IntelSWTools\openvino_2019.3.379\deployment_tools\model_optimizer\retrained_graph.pb
        - Path for generated IR:        C:\Program Files (x86)\IntelSWTools\openvino_2019.3.379\deployment_tools\model_optimizer\.
        - IR output name:       retrained_graph
        - Log level:    ERROR
        - Batch:        Not specified, inherited from the model
        - Input layers:         Not specified, inherited from the model
        - Output layers:        Not specified, inherited from the model
        - Input shapes:         Not specified, inherited from the model
        - Mean values:  Not specified
        - Scale values:         Not specified
        - Scale factor:         Not specified
        - Precision of IR:      FP32
        - Enable fusing:        True
        - Enable grouped convolutions fusing:   True
        - Move mean values to preprocess section:       False
        - Reverse input channels:       False
TensorFlow specific parameters:
        - Input model in text protobuf format:  False
        - Path to model dump for TensorBoard:   None
        - List of shared libraries with TensorFlow custom layers implementation:        None
        - Update the configuration file with input/output node names:   None
        - Use configuration file used to generate the model with Object Detection API:  None
        - Operations to offload:        None
        - Patterns to offload:  None
        - Use the config file:  None
Model Optimizer version:        2019.3.0-408-gac8584cb7
[ ERROR ]  -------------------------------------------------
[ ERROR ]  ----------------- INTERNAL ERROR ----------------
[ ERROR ]  Unexpected exception happened.
[ ERROR ]  Please contact Model Optimizer developers and forward the following information:
[ ERROR ]  local variable 'new_attrs' referenced before assignment
[ ERROR ]  Traceback (most recent call last):
  File ""C:\Program Files (x86)\IntelSWTools\openvino_2019.3.379\deployment_tools\model_optimizer\mo\front\extractor.py"", line 608, in extract_node_attrs
    supported, new_attrs = extractor(Node(graph, node))
  File ""C:\Program Files (x86)\IntelSWTools\openvino_2019.3.379\deployment_tools\model_optimizer\mo\pipeline\tf.py"", line 132, in &lt;lambda&gt;
    extract_node_attrs(graph, lambda node: tf_op_extractor(node, check_for_duplicates(tf_op_extractors)))
  File ""C:\Program Files (x86)\IntelSWTools\openvino_2019.3.379\deployment_tools\model_optimizer\mo\front\tf\extractor.py"", line 109, in tf_op_extractor
    attrs = tf_op_extractors[op](node)
  File ""C:\Program Files (x86)\IntelSWTools\openvino_2019.3.379\deployment_tools\model_optimizer\mo\front\tf\extractor.py"", line 65, in &lt;lambda&gt;
    return lambda node: pb_extractor(node.pb)
  File ""C:\Program Files (x86)\IntelSWTools\openvino_2019.3.379\deployment_tools\model_optimizer\mo\front\tf\extractors\const.py"", line 31, in tf_const_ext
    result['value'] = tf_tensor_content(pb_tensor.dtype, result['shape'], pb_tensor)
  File ""C:\Program Files (x86)\IntelSWTools\openvino_2019.3.379\deployment_tools\model_optimizer\mo\front\tf\extractors\utils.py"", line 76, in tf_tensor_content
    dtype=type_helper[0]),
UnicodeDecodeError: 'ascii' codec can't decode byte 0xff in position 0: ordinal not in range(128)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:\Program Files (x86)\IntelSWTools\openvino_2019.3.379\deployment_tools\model_optimizer\mo\main.py"", line 298, in main
    return driver(argv)
  File ""C:\Program Files (x86)\IntelSWTools\openvino_2019.3.379\deployment_tools\model_optimizer\mo\main.py"", line 247, in driver
    is_binary=not argv.input_model_is_text)
  File ""C:\Program Files (x86)\IntelSWTools\openvino_2019.3.379\deployment_tools\model_optimizer\mo\pipeline\tf.py"", line 132, in tf2nx
    extract_node_attrs(graph, lambda node: tf_op_extractor(node, check_for_duplicates(tf_op_extractors)))
  File ""C:\Program Files (x86)\IntelSWTools\openvino_2019.3.379\deployment_tools\model_optimizer\mo\front\extractor.py"", line 614, in extract_node_attrs
    new_attrs['name'] if 'name' in new_attrs else '&lt;UNKNOWN&gt;',
UnboundLocalError: local variable 'new_attrs' referenced before assignment

[ ERROR ]  ---------------- END OF BUG REPORT --------------
[ ERROR ]  ------------------------------------------------- 


Does anyone know how to fix it?
",,,,false,
https://stackoverflow.com/questions/55669722,false,"The issue does not meet the criteria for deeper analysis. The issue title suggests an error while converting SavedModel to TFLite, but the issue body is empty.",,,,,,,Operation ParseExample not supported while converting SavedModel to TFLite,,,,,false,
https://stackoverflow.com/questions/55114665,false,"The issue does not meet the criteria for deeper analysis. The issue title suggests an error related to the resize_images() function, but the issue body is empty.",,,,,,,&lt;Tensorflow object detection&gt;TypeError: resize_images() got an unexpected keyword argument &#39;preserve_aspect_ratio&#39;,,,,,false,
https://stackoverflow.com/questions/49555489,false,"The issue does not meet the criteria for deeper analysis. The issue title suggests an error related to tf.py_func(), but the issue body is empty.",,,,,,,tf.py_func() unexpected outputs in loop using lambda function,,,,,false,
https://stackoverflow.com/questions/75728844,false,The issue is not related to an API exhibiting unexpected failures or unpredictable behaviors under specific runtime conditions. It is a compatibility issue between different versions of Keras and TensorFlow.,,,,,,,TypeError: VariableMetaclass._variable_v1_call() got an unexpected keyword argument &#39;experimental_enable_variable_lifting&#39;,"I am getting this TypeError while creating a keras model from TensorFlow inside the __init__ method of a custom class, any ideas?
My code:
self.model = keras.Sequential([
    keras.layers.Dense(1, input_dim=self.degree),
    keras.layers.Dense(1)
    ])
self.model.compile(optimizer=optimizer, loss=loss)
self.model.summary()

Error:
self.model = keras.Sequential([
                 ^^^^^^^^^^^^^^^^^^
  File ""C:\Python311\Lib\site-packages\tensorflow\python\trackable\base.py"", line 205, in _method_wrapper
    result = method(self, *args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""C:\Python311\Lib\site-packages\keras\utils\traceback_utils.py"", line 70, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File ""C:\Python311\Lib\site-packages\tensorflow\python\ops\variables.py"", line 285, in __call__
    return cls._variable_v1_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: VariableMetaclass._variable_v1_call() got an unexpected keyword argument 'experimental_enable_variable_lifting'

Tensorflow version information:
Name: tensorflow
Version: 2.12.0rc1
Summary: TensorFlow is an open source machine learning framework for everyone.
Home-page: https://www.tensorflow.org/
Author: Google Inc.
Author-email: packages@tensorflow.org
License: Apache 2.0
Location: C:\Python311\Lib\site-packages
Requires: tensorflow-intel
Required-by:

","I was facing the same issue. Have you tried installing keras 2.12.0rc1?
I tried it here (pip install keras==2.12.0rc1 --user) and it seems to be working now.
","Seems like the TensorFlow upgrade to version 2.12.0 from 2.12.0-rc1 fixed the issue.
",,false,
https://stackoverflow.com/questions/72499842,false,The issue is not related to an API exhibiting unexpected failures or unpredictable behaviors under specific runtime conditions. It is a question about converting a TensorFlow image classification model to Core ML.,,,,,,,How would I convert this TensorFlow image classification model to Core ML?,"I’m learning TensorFlow and want to convert an image classification model to Core ML for use in an iOS app.
This TensorFlow image classification tutorial is a close match to what I want to do for the training, but I haven’t been able to figure out how to convert that to Core ML.
Here’s what I’ve tried, adding the following to the end of the Colab notebook for the tutorial:
# install coremltools
!pip install coremltools

# import coremltools
import coremltools as ct

# define the input type
image_input = ct.ImageType()

# create classifier configuration with the class labels
classifier_config = ct.ClassifierConfig(class_names)

# perform the conversion
coreml_model = ct.convert(
    model, inputs=[image_input], classifier_config=classifier_config,
)

# print info about the converted model
print(coreml_model)

# save the file
coreml_model.save('my_coreml_model')

That successfully creates an mlmodel file, but when I download the file and open it in Xcode to test it (under the “Preview” tab) it shows results like “Roses 900% Confidence” and “Tulips 1,120% Confidence”. For my uses, the confidence percentage needs to be from 0 to 100%, so I think I’m missing some parameter for the conversion.
On import coremltools as ct I do get some warnings like “WARNING:root:TensorFlow version 2.8.2 has not been tested with coremltools. You may run into unexpected errors.” but I’m guessing that’s not the problem since the conversion doesn’t report any errors.
Based on information here, I’ve also tried setting a scale on the image input:
image_input = ct.ImageType(scale=1/255.0)

… but that made things worse as it then has around 315% confidence that every image is a dandelion. A few other attempts at setting a scale / bias all resulted in the same thing.
At this point I’m not sure what else to try. Any help is appreciated!
","The last layer of your model should be something like this:
layers.Dense(num_classes, activation='softmax')

The softmax function transforms your output into the probabilities you need.
",,,false,
https://stackoverflow.com/questions/70998085,false,The issue is not related to an API exhibiting unexpected failures or unpredictable behaviors under specific runtime conditions. It is a question about reusing variables in TensorFlow 2.0.,,,,,,,How to reuse variables in tensorflow 2.0?,"When using tensorflow 2.0, I find something weird about tf.Variable? There are two cases bellow.
The first one
x1 = tf.Variable(12., name='x')
x2 = tf.Variable(12., name='x')
print(x1 is x2)
x1.assign(1.)
print(x1)
print(x2)

The output is
False
&lt;tf.Variable 'x:0' shape=() dtype=float32, numpy=1.0&gt;
&lt;tf.Variable 'x:0' shape=() dtype=float32, numpy=12.0&gt;

which means variables with the same name don't share the same memory.
The second one
x = tf.Variable(12., name='x')
print(x)
y = x.assign(5.)
print(y)
print(x is y)

x.assign(3.)
print(x)
print(y)

The output is
&lt;tf.Variable 'x:0' shape=() dtype=float32, numpy=12.0&gt;
&lt;tf.Variable 'UnreadVariable' shape=() dtype=float32, numpy=5.0&gt;
False
&lt;tf.Variable 'x:0' shape=() dtype=float32, numpy=3.0&gt;
&lt;tf.Variable 'UnreadVariable' shape=() dtype=float32, numpy=3.0&gt;

The result is unexpected, variables x and y with different names share the same memory, but id(x) is not equal to id(y).
Therefore, the name of variable can't distinguish whether variables are identical(share the same memory). And how can I reuse variables in tensorflow 2.0, like with tf.variable_scope(""scope"", reuse=True) tf.get_variable(...) in tensorflow 1.0?
","Quoted from your question:

The result is unexpected, variables x and y with different names share the same memory, but id(x) is not equal to id(y).

No, this is incorrect. From the docs of tf.Variable.assign, where read_value is default to True:

read_value: if True, will return something which evaluates to the new value of the variable; if False will return the assign op.

Here ""something"" should be a new operation, which isn't x, but is evaluated to the value of x.
To reuse x, just access x:
y = x
print(y is x) # True

Finally, regarding:

which means variables with the same name don't share the same memory.


Therefore, the name of variable can't distinguish whether [...]

You have to create different(thus distinguishable) names yourself, regarding your first example. You might want to take a look at the comments of this accepted answer https://stackoverflow.com/a/73024334/5290519
",,,false,
https://stackoverflow.com/questions/70915345,false,The issue is not related to an API exhibiting unexpected failures or unpredictable behaviors under specific runtime conditions. It is a question about an unexpected field name error in TensorFlow.,,,,,,,Why did I get unexpected field names: [&#39;is_dynamic_op&#39;]?,"I am working on a low light video processing project where I am getting some errors in some areas,
For this code.
params = trt.DEFAULT_TRT_CONVERSION_PARAMS._replace(
    precision_mode='FP16',
    is_dynamic_op = True)

I am getting this error.
&gt; --------------------------------------------------------------------------- ValueError                                Traceback (most recent call
&gt; last) &lt;ipython-input-8-326230ed5373&gt; in &lt;module&gt;()
&gt;       2 params = trt.DEFAULT_TRT_CONVERSION_PARAMS._replace(
&gt;       3     precision_mode='FP16',
&gt; ----&gt; 4     is_dynamic_op = True)
&gt;       5 
&gt;       6 # Convert the model
&gt; 
&gt; /usr/lib/python3.7/collections/__init__.py in _replace(_self, **kwds)
&gt;     414         result = _self._make(map(kwds.pop, field_names, _self))
&gt;     415         if kwds:
&gt; --&gt; 416             raise ValueError(f'Got unexpected field names: {list(kwds)!r}')
&gt;     417         return result
&gt;     418 
&gt; 
&gt; ValueError: Got unexpected field names: ['is_dynamic_op']

I have used these libraries,
from glob import glob
from PIL import Image
from matplotlib import pyplot as plt
from mirnet.inference import Inferer
from mirnet.utils import download_dataset, plot_result
from tensorflow.python.compiler.tensorrt import trt_convert as trt

import tensorflow as tf
import numpy as np
import time

I have imported all the libraries but am still stuck.
","Firstly, it seems that trt.DEFAULT_TRT_CONVERSION_PARAMS doesn't have the field 'is_dynamic_op'. My guess would be that, some documentation exists for the library, and there you can probably see whether the field is settable or not.
Secondly, it seems you are using TensorFlow and the TensorRT API. (but again, it's a guess...). In the source-code of the library, they use ""is_dynamic_op"" on some ""rewrite_config"". Maybe this helps you reformat your code to work: https://github.com/tensorflow/tensorflow/blob/dd38449b8ac3fd9ffaa98349d39d36ec26e72dfe/tensorflow/python/compiler/tensorrt/trt_convert.py#L1136
",,,false,
https://stackoverflow.com/questions/63441288,false,The issue is not related to an API exhibiting unexpected failures or unpredictable behaviors under specific runtime conditions. It is a question about running a model.fit on GPU in TensorFlow.,,,,,,,How to run a model.fit properly on GPU? (unexptected behaviour),"Currently, I am doing y Udemy Python course for data science. In there, there is the following example to train a model in Tensorflow:
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense,Dropout

model = Sequential()

# Choose whatever number of layers/neurons you want.
model.add(Dense(units=78,activation='relu'))
model.add(Dense(units=39,activation='relu'))
model.add(Dense(units=19,activation='relu'))
model.add(Dense(units=1,activation='sigmoid'))

# https://stats.stackexchange.com/questions/181/how-to-choose-the-number-of-hidden-layers-and-nodes-in-a-feedforward-neural-netw

model.compile(loss='binary_crossentropy', optimizer='adam')

model.fit(x=X_train, 
          y=y_train, 
          epochs=3,
          validation_data=(X_test, y_test), verbose=1
          )

My goal now was to get this to run on my GPU. For that, I altered the last part as follows (the epochs are low on purpose, I just want to see how long it takes per epoch before scaling up):
with tf.device(""/gpu:0""):
    model.fit(x=X_train, 
              y=y_train, 
              epochs=3,
              validation_data=(X_test, y_test), verbose=1
              )

and for comparison, also as follows:
with tf.device(""/cpu:0""):
    model.fit(x=X_train, 
              y=y_train, 
              epochs=3,
              validation_data=(X_test, y_test), verbose=1
              )

However, the result is very unexpected: Either, both versions occupy all memory of the GPU but seemingly don't do any calculations on it, and take the exact same time per epoch. Or, the GPU version simply crashes with the following error:
C:\Program Files (x86)\Microsoft Visual Studio\Shared\Anaconda3_64\envs\gpu\lib\site-packages\six.py in raise_from(value, from_value)
 
InternalError:  Blas GEMM launch failed : a.shape=(32, 78), b.shape=(78, 78), m=32, n=78, k=78
     [[node sequential/dense/MatMul (defined at &lt;ipython-input-115-79c9a84ee89a&gt;:8) ]] [Op:__inference_distributed_function_874]
 
Function call stack:
distributed_function

Sometimes it crashes, sometimes it kind of works but takes as long as the CPU. Sometimes even the CPU version takes 20 sec per epoch, other times it takes 40 sec. The code stays the same, all that changes is that I restart the Kernel in between. I really don't understand it.
When I test the GPU and conda environment using the following code, everything seems to work fine, reproducible and the GPU is about 20x as fast as the CPU:
# https://     gist.github.com/ikarus-999/1a845437b454cdfcc1eb5455d373fe63
import sys
import numpy as np
import tensorflow.compat.v1 as tf # compatibility for TF 1 code
from datetime import datetime
 
def test_device (device_name: str):
    shape = (int(10000), int(10000))
    startTime = datetime.now()
    with tf.device(device_name):
        random_matrix = tf.random.uniform(shape=shape, minval=0, maxval=1)
        dot_operation = tf.matmul(random_matrix, tf.transpose(random_matrix))
        sum_operation = tf.reduce_sum(dot_operation)
 
    result = sum_operation
 
    print(""Shape:"", shape, ""Device:"", device_name)
    print(""—""*50)
    print(result)
    print(""Time taken:"", datetime.now() - startTime)
    print(""\n"" * 2)
    
test_device(""/cpu:0"") # 6 sec
test_device(""/gpu:0"") # 0.3 sec

So, I am sure there is something I am doing wrong.
TLTR:
What would be the correct way to call model.fit on the GPU? How can different runs (without changing the code) result in so drastically different outcomes (Crash, vastly different calculation times)?
Any help is greatly appreciated, thx!
",,,,false,
https://stackoverflow.com/questions/62675004,false,The issue is not related to an API exhibiting unexpected failures or unpredictable behaviors under specific runtime conditions. It is a question about a Keras initializer error.,,,,,,,Keras initializer error unexpected keyword argument &#39;dtype&#39;,"I am trying to code a Siamese networks in Keras and Tensorflow, using this Jupiter Notebook as reference:
https://github.com/hlamba28/One-Shot-Learning-with-Siamese-Networks/blob/master/Siamese%20on%20Omniglot%20Dataset.ipynb
When I create the model:
model = get_siamese_model((105, 105, 1))

I got this error:
 Traceback (most recent call last):
  File ""main.py"", line 164, in &lt;module&gt;
    model = get_siamese_model((105, 105, 1))
  File ""main.py"", line 129, in get_siamese_model
    kernel_initializer=initialize_weights, kernel_regularizer=l2(2e-4)))
  File ""/home/fabio/.local/lib/python3.6/site-packages/tensorflow/python/training/tracking/base.py"", line 456, in _method_wrapper
    result = method(self, *args, **kwargs)
  File ""/home/fabio/.local/lib/python3.6/site-packages/tensorflow/python/keras/engine/sequential.py"", line 198, in add
    layer(x)
  File ""/home/fabio/.local/lib/python3.6/site-packages/tensorflow/python/keras/engine/base_layer.py"", line 897, in __call__
    self._maybe_build(inputs)
  File ""/home/fabio/.local/lib/python3.6/site-packages/tensorflow/python/keras/engine/base_layer.py"", line 2416, in _maybe_build
    self.build(input_shapes)  # pylint:disable=not-callable
  File ""/home/fabio/.local/lib/python3.6/site-packages/tensorflow/python/keras/layers/convolutional.py"", line 163, in build
    dtype=self.dtype)
  File ""/home/fabio/.local/lib/python3.6/site-packages/tensorflow/python/keras/engine/base_layer.py"", line 577, in add_weight
    caching_device=caching_device)
  File ""/home/fabio/.local/lib/python3.6/site-packages/tensorflow/python/training/tracking/base.py"", line 743, in _add_variable_with_custom_getter
    **kwargs_for_getter)
  File ""/home/fabio/.local/lib/python3.6/site-packages/tensorflow/python/keras/engine/base_layer_utils.py"", line 141, in make_variable
    shape=variable_shape if variable_shape else None)
  File ""/home/fabio/.local/lib/python3.6/site-packages/tensorflow/python/ops/variables.py"", line 259, in __call__
    return cls._variable_v1_call(*args, **kwargs)
  File ""/home/fabio/.local/lib/python3.6/site-packages/tensorflow/python/ops/variables.py"", line 220, in _variable_v1_call
    shape=shape)
  File ""/home/fabio/.local/lib/python3.6/site-packages/tensorflow/python/ops/variables.py"", line 198, in &lt;lambda&gt;
    previous_getter = lambda **kwargs: default_variable_creator(None, **kwargs)
  File ""/home/fabio/.local/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py"", line 2598, in default_variable_creator
    shape=shape)
  File ""/home/fabio/.local/lib/python3.6/site-packages/tensorflow/python/ops/variables.py"", line 263, in __call__
    return super(VariableMetaclass, cls).__call__(*args, **kwargs)
  File ""/home/fabio/.local/lib/python3.6/site-packages/tensorflow/python/ops/resource_variable_ops.py"", line 1434, in __init__
    distribute_strategy=distribute_strategy)
  File ""/home/fabio/.local/lib/python3.6/site-packages/tensorflow/python/ops/resource_variable_ops.py"", line 1567, in _init_from_args
    initial_value() if init_from_fn else initial_value,
  File ""/home/fabio/.local/lib/python3.6/site-packages/tensorflow/python/keras/engine/base_layer_utils.py"", line 121, in &lt;lambda&gt;
    init_val = lambda: initializer(shape, dtype=dtype)
TypeError: initialize_weights() got an unexpected keyword argument 'dtype'

What the error means? And how can I solve that?
",,,,false,
https://stackoverflow.com/questions/51688885,false,The issue does not have enough information to determine if it involves an API exhibiting unexpected failures or unpredictable behaviors under specific runtime conditions.,,,,,,,Building Tensorflow with OpenCL support fails on Ubuntu 18.04,,,,,false,
https://stackoverflow.com/questions/46377189,false,The issue does not have enough information to determine if it involves an API exhibiting unexpected failures or unpredictable behaviors under specific runtime conditions.,,,,,,,tensorflow java model inference to convert fetched tensor to string?,,,,,false,
https://stackoverflow.com/questions/45478790,false,The issue does not have enough information to determine if it involves an API exhibiting unexpected failures or unpredictable behaviors under specific runtime conditions.,,,,,,,Optimization flags in Bazel while running TensorFlow Tests,,,,,false,
https://stackoverflow.com/questions/71512318,false,The issue does not meet the criteria for deeper analysis as it does not involve an API exhibiting unexpected failures or unpredictable behaviors under specific runtime conditions.,,,,,,,"I get this Error while trying to Train a Model, Tensorflow ValueError: Unexpected result of `train_function` (Empty logs)","I'm trying to train my model to read some x-ray Images, I'm using Jupyter Notebook, I imported the Libraries, Defined the image properties, Prepared the dataset, Created the neural net model, Defined callbacks... and Managed the Data, But while trying to train my model this is the error I get :
ValueError: Unexpected result of `train_function` (Empty logs). Please use `Model.compile(..., run_eagerly=True)`, or `tf.config.run_functions_eagerly(True)` for more information of where went wrong, or file a issue/bug to `tf.keras`.

Here's creation of the neural net model :
model=Sequential()
model.add(Conv2D(32,(3,3),activation='relu',input_shape=(Image_Width,Image_Height,Image_Channels)))
model.add(BatchNormalization())
model.add(MaxPooling2D(pool_size=(2,2)))
model.add(Dropout(0.25))
model.add(Conv2D(64,(3,3),activation='relu'))
model.add(BatchNormalization())
model.add(MaxPooling2D(pool_size=(2,2)))
model.add(Dropout(0.25))
model.add(Conv2D(128,(3,3),activation='relu'))
model.add(BatchNormalization())
model.add(MaxPooling2D(pool_size=(2,2)))
model.add(Dropout(0.25))
model.add(Flatten())
model.add(Dense(512,activation='relu'))
model.add(BatchNormalization())
model.add(Dropout(0.5))
model.add(Dense(2,activation='softmax'))
model.compile(loss='categorical_crossentropy',
  optimizer='rmsprop',metrics=['accuracy'])

And the training and validation of the data generator:
base_dir = ""C:/Users/lenovo/PneumoniaClassification/chest_xray""
train_dir = os.path.join(base_dir, 'train')
test_dir = os.path.join(base_dir, 'test')
validation_dir = os.path.join(base_dir, 'val')


train_datagen = ImageDataGenerator(rotation_range=15,
                                    rescale=1./255,
                                    shear_range=0.1,
                                    zoom_range=0.2,
                                    horizontal_flip=True,
                                    width_shift_range=0.1,
                                    height_shift_range=0.1
                                    )
train_generator = train_datagen.flow_from_directory(
        train_dir,  # This is the source directory for training images
        target_size=Image_Size,  # All images will be resized 
        batch_size= 50,
        # Since we use binary_crossentropy loss, we need binary labels
        class_mode='binary')

test_datagen = ImageDataGenerator(rotation_range=15,
                                    rescale=1./255,
                                    shear_range=0.1,
                                    zoom_range=0.2,
                                    horizontal_flip=True,
                                    width_shift_range=0.1,
                                    height_shift_range=0.1)

test_generator = test_datagen.flow_from_directory(
        test_dir,
        target_size=Image_Size,
        batch_size= 50,
        class_mode='binary')

validation_datagen = ImageDataGenerator(rescale=1./255)

validation_generator = test_datagen.flow_from_directory(
        validation_dir,
        target_size=Image_Size,
        batch_size= 50,
        class_mode='binary')

But here when I try to train the Model I get the error Unexpected result of train_function..... using this Code :
epochs=10
model.fit(
    train_generator, 
    epochs=epochs,
    validation_data=validation_generator,
    validation_steps=total_validate//batch_size,
    steps_per_epoch=total_train//batch_size,
    callbacks=callbacks
)

This is the full error :
ValueError                                Traceback (most recent call last)
Input In [33], in &lt;cell line: 2&gt;()
      1 epochs=10
----&gt; 2 model.fit(
      3     train_generator, 
      4     epochs=epochs,
      5     validation_data=validation_generator,
      6     validation_steps=total_validate//batch_size,
      7     steps_per_epoch=total_train//batch_size,
      8     callbacks=callbacks
      9 )

File ~\AppData\Roaming\Python\Python39\site-packages\keras\utils\traceback_utils.py:67, in filter_traceback.&lt;locals&gt;.error_handler(*args, **kwargs)
     65 except Exception as e:  # pylint: disable=broad-except
     66   filtered_tb = _process_traceback_frames(e.__traceback__)
---&gt; 67   raise e.with_traceback(filtered_tb) from None
     68 finally:
     69   del filtered_tb

File ~\AppData\Roaming\Python\Python39\site-packages\keras\engine\training.py:1395, in Model.fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)
   1393 logs = tf_utils.sync_to_numpy_or_python_type(logs)
   1394 if logs is None:
-&gt; 1395   raise ValueError('Unexpected result of `train_function` '
   1396                    '(Empty logs). Please use '
   1397                    '`Model.compile(..., run_eagerly=True)`, or '
   1398                    '`tf.config.run_functions_eagerly(True)` for more '
   1399                    'information of where went wrong, or file a '
   1400                    'issue/bug to `tf.keras`.')
   1401 epoch_logs = copy.copy(logs)
   1403 # Run validation.

ValueError: Unexpected result of `train_function` (Empty logs). Please use `Model.compile(..., run_eagerly=True)`, or `tf.config.run_functions_eagerly(True)` for more information of where went wrong, or file a issue/bug to `tf.keras`.

","Can you try a dense layer with a single neuron with sigmoid activation as output (and binary crossentropy as loss)?
",,,false,
https://stackoverflow.com/questions/67258336,false,The issue does not meet the criteria for deeper analysis as it is related to setting up additional loaders for a library and does not involve an API exhibiting unexpected failures or unpredictable behaviors under specific runtime conditions.,,,,,,,What additional loaders do I need? (Module parse failed: Unexpected token (16:9)),"I am using a library called danfojs and it gives me the following error when I compiled.
14:01:51.417    Failed to compile.
14:01:51.417    ./node_modules/danfojs/src/index.js 16:9
14:01:51.417    Module parse failed: Unexpected token (16:9)
14:01:51.418    File was processed with these loaders:
14:01:51.418     * ./node_modules/babel-loader/lib/index.js
14:01:51.418    You may need an additional loader to handle the result of these loaders.
14:01:51.418    | export { Str } from ""./core/strings"";
14:01:51.418    | export { Utils } from ""./core/utils"";
14:01:51.418    &gt; export * as tf from ""@tensorflow/tfjs"";
14:01:51.418    | export var _version = ""0.2.5"";

Seems like I need to set up some additional loader maybe with webpack settings, could any one point me out the right direction of how to do that?
","You should not import /src/index.js from that lib: that is apparently source code with ES6 features, not supported by current Node.
",,,false,
https://stackoverflow.com/questions/59851300,false,The issue does not provide enough information to determine if it involves an API exhibiting unexpected failures or unpredictable behaviors under specific runtime conditions.,,,,,,,Unable to run faster R-CNN with elastic inference and tensorflow serving - how to debug?,"I have found a saved_model from Tensorflow's Model Zoo. I am able to run my Faster R-CNN model locally using the following code:

image_np = np.array(Image.open('my_input.jpg'))
image = np.asarray(image_np)
input_tensor = tf.convert_to_tensor(image)
input_tensor = input_tensor[tf.newaxis,...]

model = tf.saved_model.load(os.path.join('&lt;PATH_TO_SAVED_MODEL&gt;'))
model = model.signatures['serving_default']

output_dict = model(input_tensor)


I wanted to try running this using Elastic Inference, and started out with this guide. I swapped out the faster r-cnn model being run by just changing the path to the saved_model when starting up tensorflow:

EI_VISIBLE_DEVICES=0 amazonei_tensorflow_model_server --model_name=f_r_cnn --model_base_path=/tmp/f_r_cnn --port=9000


Now I'm trying to run a client to talk to tensorflow serving, using the template provided:

from __future__ import print_function

import grpc
import tensorflow as tf
from PIL import Image
import numpy as np
import time
import os
from tensorflow_serving.apis import predict_pb2
from tensorflow_serving.apis import prediction_service_pb2_grpc

tf.app.flags.DEFINE_string('server', 'localhost:9000',
                           'PredictionService host:port')
tf.app.flags.DEFINE_string('image', '', 'path to image in JPEG format')
FLAGS = tf.app.flags.FLAGS

coco_classes_txt = ""https://raw.githubusercontent.com/amikelive/coco-labels/master/coco-labels-paper.txt""
local_coco_classes_txt = ""/tmp/coco-labels-paper.txt""
# it's a file like object and works just like a file
os.system(""curl -o %s -O %s""%(local_coco_classes_txt, coco_classes_txt))
NUM_PREDICTIONS = 5
with open(local_coco_classes_txt) as f:
  classes = [""No Class""] + [line.strip() for line in f.readlines()]


def main(_):
  channel = grpc.insecure_channel(FLAGS.server)
  stub = prediction_service_pb2_grpc.PredictionServiceStub(channel)

  # Send request
  with Image.open(FLAGS.image) as f:
    f.load()
    # See prediction_service.proto for gRPC request/response details.
    data = np.asarray(f)
    data = np.expand_dims(data, axis=0)

    request = predict_pb2.PredictRequest()
    request.model_spec.name = 'f_r_cnn'
    request.inputs['inputs'].CopyFrom(
        tf.contrib.util.make_tensor_proto(data, shape=data.shape))
    result = stub.Predict(request, 60.0)  # 10 secs timeout
    outputs = result.outputs
    detection_classes = outputs[""detection_classes""]
    detection_classes = tf.make_ndarray(detection_classes)
    num_detections = int(tf.make_ndarray(outputs[""num_detections""])[0])
    print(""%d detection[s]"" % (num_detections))
    class_label = [classes[int(x)]
                   for x in detection_classes[0][:num_detections]]
    print(""SSD Prediction is "", class_label)


if __name__ == '__main__':
  tf.app.run()


While this client ran just fine with the model from the tutorial (no surprise there), it is failing when I try to get it to talk to my Faster R-CNN model with the following error:

debug_error_string = ""{""created"":""@1579654607.391705065"",""description"":""Error received from peer ipv6:[::1]:9000"",""file"":""src/core/lib/surface/call.cc"",""file_line"":1052,""grpc_message"":""Unexpected error in RPC handling"",""grpc_status"":2}""


I googled this error and was unable to find anything useful. What is grpc_status 2? How might I find useful information to help point me in the right direction?
",,,,false,
https://stackoverflow.com/questions/55573670,false,The issue does not provide enough information to determine if it involves an API exhibiting unexpected failures or unpredictable behaviors under specific runtime conditions.,,,,,,,Unexpected output for tf.nn.sparse_softmax_cross_entropy_with_logits,,,,,false,
https://stackoverflow.com/questions/48850537,false,The issue does not provide enough information to determine if it involves an API exhibiting unexpected failures or unpredictable behaviors under specific runtime conditions.,,,,,,,Issue with Imagenet classification with VGG16 pretrained weights,,,,,false,
https://stackoverflow.com/questions/46958739,false,The issue does not provide enough information to determine if it involves an API exhibiting unexpected failures or unpredictable behaviors under specific runtime conditions.,,,,,,,Error trying to pass list to hyperas as a parameter,,,,,false,
https://stackoverflow.com/questions/68363880,true,The issue involves creating a neural network with multiple outputs using Keras. This can be considered for deeper analysis as it potentially involves an API exhibiting unexpected behavior.,,,,,,,Keras neural network multiple output,"I would like to create a neural network with multiple outputs. With one conclusion, I succeeded in doing this, but with two - it does not work. Can you help me please? Do you know any resource with examples for keras? I attach the code and error below. (Sorry for my English, it was translated by google translator)
Code:
from keras.models import Sequential
from keras.layers import Dense
x = [[1, 1, 1, 1], [0, 1, 1, 0], [1, 0, 0, 1], [0, 0, 0, 0], [1, 1, 0, 0], [0, 1, 1, 1], [1, 1, 1, 0], [1, 0, 0, 0]]
y = [[1, 1], [0, 0], [0, 0], [0, 0], [1, 0], [0, 1], [1, 0], [0, 0]]
model = Sequential()
# model.add(Dense(3, activation='sigmoid'))
model.add(Dense(2, activation='sigmoid'))
model.add(Dense(1, activation='sigmoid'))
model.compile(loss='binary_crossentropy', optimizer=keras.optimizers.Adam(1e-1), metrics=['accuracy'])
model.fit(x, y, epochs=20)
model.predict(x=[[0, 0, 1, 1]])

Error:
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
&lt;ipython-input-77-c805cf1cd17e&gt; in &lt;module&gt;()
      3 x = [[1, 1, 1, 1], [0, 1, 1, 0], [1, 0, 0, 1], [0, 0, 0, 0], [1, 1, 0, 0], [0, 1, 1, 1], [1, 1, 1, 0], [1, 0, 0, 0]]
      4 y = [[1, 1], [0, 0], [0, 0], [0, 0], [1, 0], [0, 1], [1, 0], [0, 0]]
----&gt; 5 model = Sequential(input=x, output=y)
      6 # model.add(Dense(3, activation='sigmoid'))
      7 model.add(Dense(2, activation='sigmoid'))

/usr/local/lib/python3.7/dist-packages/tensorflow/python/training/tracking/base.py in _method_wrapper(self, *args, **kwargs)
    520     self._self_setattr_tracking = False  # pylint: disable=protected-access
    521     try:
--&gt; 522       result = method(self, *args, **kwargs)
    523     finally:
    524       self._self_setattr_tracking = previous_value  # pylint: disable=protected-access

TypeError: __init__() got an unexpected keyword argument 'input'

UPD.
I redid the code, as I was advised in the comments, but now, with each training, it outputs some kind of random result that does not lie in the range of 0 - 1.
from keras.models import Model
from keras.layers import Input
from keras.layers import Dense

x = [[0, 0], [0, 1], [1, 0], [1, 1]]
y = [[1, 1], [1, 0], [0, 1], [0, 0]]

visible = Input(shape=(2,))
hidden = Dense(2)(visible)

# hidden2 = Dense(2)(visible)
model = Model(inputs=visible, outputs=[hidden])
model.compile(loss='binary_crossentropy', metrics=['accuracy'])
model.fit(x, y, epochs=8)
model.predict(x=[[1, 0]])

","First of all, sequential model does not support multi output. If you mean of multi output, multi neurons, then you can use sequential model, and should simply just change number of neurons in last layer.
Here is the modification of your first model code:
from keras.models import Sequential
from keras.layers import Dense
x = [[1, 1, 1, 1], [0, 1, 1, 0], [1, 0, 0, 1], [0, 0, 0, 0], [1, 1, 0, 0], [0, 1, 1, 1], [1, 1, 1, 0], [1, 0, 0, 0]]
y = [[1, 1], [0, 0], [0, 0], [0, 0], [1, 0], [0, 1], [1, 0], [0, 0]]
model = Sequential()
# model.add(Dense(3, activation='sigmoid'))
model.add(Dense(2, activation='sigmoid'))
model.add(Dense(2, activation='sigmoid')) #change neurons to 2
model.compile(loss='binary_crossentropy', optimizer=keras.optimizers.Adam(1e-1), metrics=['accuracy'])
model.fit(x, y, epochs=20)
model.predict(x=[[0, 0, 1, 1]])

But if you want an example of functional api, it is here:
from keras.models import Model
from keras.layers import Input
from keras.layers import Dense
import keras
x = [[0, 0], [0, 1], [1, 0], [1, 1]]
y = [[1, 1], [1, 0], [0, 1], [0, 0]]

visible = Input(shape=(2,))
hidden = Dense(64, activation='relu')(visible)
hidden = Dense(64, activation='relu')(hidden)
hidden = Dense(64, activation='relu')(hidden)
hidden = Dense(2, activation='sigmoid')(hidden) #use sigmoid activation for output between 0 and 1


model = Model(inputs=visible, outputs=hidden)
model.compile(optimizer='adam', loss='binary_crossentropy')
model.fit(x, y, epochs=100)
model.predict(x=[[1, 0]])

",,,false,
https://stackoverflow.com/questions/46528671,false,The issue does not meet the criteria for deeper analysis as it is related to a problem with pip installation and does not involve an API exhibiting unexpected failures or unpredictable behaviors under specific runtime conditions.,,,,,,,transport_encoding error during installing with pip,"I'm getting unexpected arg: keyword encoding in parse() while trying to install any python package through pip. 

I'm getting this problem since i installed tensorflow for python 3.6, which probably led to some issue with html5lib and setuptools. Have reinstalled html5lib1.0b10 using tar.gz file (admin install) but the issue still remains. 
Please help!!

pip install spacy
Collecting spacy
Exception:
Traceback (most recent call last):
  File ""C:\ProgramData\Anaconda3\lib\site-packages\pip\basecommand.py"", line 215, in main
    status = self.run(options, args)
  File ""C:\ProgramData\Anaconda3\lib\site-packages\pip\commands\install.py"", line 335, in run
    wb.build(autobuilding=True)
  File ""C:\ProgramData\Anaconda3\lib\site-packages\pip\wheel.py"", line 749, in build
    self.requirement_set.prepare_files(self.finder)
  File ""C:\ProgramData\Anaconda3\lib\site-packages\pip\req\req_set.py"", line 380, in prepare_files
    ignore_dependencies=self.ignore_dependencies))
  File ""C:\ProgramData\Anaconda3\lib\site-packages\pip\req\req_set.py"", line 554, in _prepare_file
    require_hashes
  File ""C:\ProgramData\Anaconda3\lib\site-packages\pip\req\req_install.py"", line 278, in populate_link
    self.link = finder.find_requirement(self, upgrade)
  File ""C:\ProgramData\Anaconda3\lib\site-packages\pip\index.py"", line 465, in find_requirement
    all_candidates = self.find_all_candidates(req.name)
  File ""C:\ProgramData\Anaconda3\lib\site-packages\pip\index.py"", line 423, in find_all_candidates
    for page in self._get_pages(url_locations, project_name):
  File ""C:\ProgramData\Anaconda3\lib\site-packages\pip\index.py"", line 568, in _get_pages
    page = self._get_page(location)
  File ""C:\ProgramData\Anaconda3\lib\site-packages\pip\index.py"", line 683, in _get_page
    return HTMLPage.get_page(link, session=self.session)
  File ""C:\ProgramData\Anaconda3\lib\site-packages\pip\index.py"", line 811, in get_page
    inst = cls(resp.content, resp.url, resp.headers)
  File ""C:\ProgramData\Anaconda3\lib\site-packages\pip\index.py"", line 731, in __init__
    namespaceHTMLElements=False,
TypeError: parse() got an unexpected keyword argument 'transport_encoding'

","Issue was indeed with html5lib and setuptools, I re-installed html5lib using

conda install -c anaconda html5lib


Although package versions were same, it said 'The following packages will be SUPERSEDED by a higher-priority channel', and installed conda, conda-env, html5lib. I'm unsure about higher-priority channel part. 
But this answered my problem!!
","Problem appears after installing tensorflow 1.3.0.

Reinstall html5lib.

conda uninstall html5lib --force
conda install html5lib


Solves the problem temporarily until next pip install --upgrade tensorflow.
Had the same problem with pip install --upgrade tensorflow-gpu as well.
",,false,
https://stackoverflow.com/questions/43068712,false,The issue is related to running Python scripts with IronPython and does not involve an API exhibiting unexpected failures or unpredictable behaviors.,,,,,,,IronPython: Unexpected token &#39;from&#39;,"i was running python script from .net using IronPython, below is my python script

import tensorflow as tf    
print('Tensorflow Imported')


below is C# Code

using System;
using System.Text;
using System.IO;
using IronPython.Hosting;
using System.Collections.Generic;
using Microsoft.Scripting.Hosting;

namespace ConsoleApplication1
{
    class Program
    {
        private static void Main()
        {
            var py = Python.CreateEngine();
            List&lt;string&gt; searchPaths = new List&lt;string&gt;();
            searchPaths.Add(@""C:\Users\Admin\AppData\Local\Programs\Python\Python35\Lib)"");
            searchPaths.Add(@""C:\Users\Admin\AppData\Local\Programs\Python\Python35\Lib\site-packages)"");
            py.SetSearchPaths(searchPaths);
            try
            {
                py.ExecuteFile(""script.py"");
            }
            catch (Exception ex)
            {
                Console.WriteLine(ex.Message);
            }           
        }
    }
}


below is my output


  Unexpected token 'from'


if i remove import statement then python script executes fine. I tried including os,sys all those were imported without any issue. I have installed TensorFlow via pip, when i run above script through python console(v3.5) it works fine. 

Update: in TF doc its written ""TensorFlow only supports version 3.5.x of Python on Windows"". but official release of IronPython is version 2.7
I was happy to find IronPython on GitHub, tried building it (i just typed build in console and got freaked out with the long list of error messages it showed! :D
couldn't find pre-compiled binaries

is there any alternative way to import tensorflow in IronPython 2.7 or run Python in .net? 
","Prakash - as you found in the documentation, TensorFlow requires Python 3.5 or 3.6 when running on Windows.  It won't run in IronPython 2.7.

One user on GitHub successfully (with a lot of work and in a not-easy-to-do) way got TF running on Windows under Python2.7, and you might be able to build on their work, but it's not exactly the solution you were looking for for IronPython.  My best suggestion is to go with 3.5 or 3.6.
",,,false,
https://stackoverflow.com/questions/33644635,false,The issue is related to a misunderstanding of logical operators in pandas and does not involve an API exhibiting unexpected failures or unpredictable behaviors.,,,,,,,Python unable to import tensorflow to Mac OS X 10.8,"I am trying to install tensorflow on my mac air (specs: OSX 10.8.4, 4GB RAM). While tensorflow installs cleanly, as on the tensorflow.org page, and I am able to do $ source bin/activate to get a tensorflow prompt, I am not able to import the package in python. This is what I get: 

    &gt;&gt;&gt; import tensorflow
Traceback (most recent call last):
  File ""&lt;stdin&gt;"", line 1, in &lt;module&gt;
  File ""/usr/local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 4, in &lt;module&gt;
    from tensorflow.python import *
  File ""/usr/local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 22, in &lt;module&gt;
    from tensorflow.python.client.client_lib import *
  File ""/usr/local/lib/python2.7/site-packages/tensorflow/python/client/client_lib.py"", line 35, in &lt;module&gt;
    from tensorflow.python.client.session import InteractiveSession
  File ""/usr/local/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 11, in &lt;module&gt;
    from tensorflow.python import pywrap_tensorflow as tf_session
  File ""/usr/local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 28, in &lt;module&gt;
    _pywrap_tensorflow = swig_import_helper()
  File ""/usr/local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow', fp, pathname, description)
ImportError: dlopen(/usr/local/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so, 2): Symbol not found: ___sincos_stret
  Referenced from: /usr/local/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so
  Expected in: /usr/lib/libSystem.B.dylib
 in /usr/local/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so


I looked at the common problems section on tensorflow, as well as searched for the ImportError above but have had no luck so far. I wonder if this is a protobuf issue as referenced in  Error in python after 'import tensorflow': TypeError: __init__() got an unexpected keyword argument 'syntax' .
Thanks in advance.
","The issue appears to be that the binary distribution of TensorFlow is not compatible with Mac OS X 10.8. The relevant part of the error message is this one:

Symbol not found: ___sincos_stret


According to various sources, this is a known issue when using binaries compiled for a newer version of Mac OS X on 10.8 (Mavericks), because the symbol ___sincos_stret is not defined in the standard libraries for that version of the OS.

To address this, you will need to (i) upgrade to Mac OS X 10.9 or later, (ii) follow the instructions to install from source, or (iii) use Docker.
","I had similar issue. I used docker toolbox and was able to run tensorflow. You can follow my question and my solution.
",,false,
https://stackoverflow.com/questions/22591174/pandas-multiple-conditions-while-indexing-data-frame-unexpected-behavior,false,The issue is related to a misunderstanding of logical operators in pandas and does not involve an API exhibiting unexpected failures or unpredictable behaviors.,,,,,,,,,,,,false,
https://stackoverflow.com/questions/22591174/pandas-multiple-conditions-while-indexing-data-frame-unexpected-behavior,false,The issue is related to a misunderstanding of logical operators in pandas and does not involve an API exhibiting unexpected failures or unpredictable behaviors.,,,,,,,,,,,,false,
https://stackoverflow.com/questions/69885201,false,"The issue is related to package compatibility between Mamba and pip, and does not involve an API exhibiting unexpected failures or unpredictable behaviors.",,,,,,,Mamba can&#39;t create env giving &#39;nothing provides&#39; but pip sees such version of opencv-python,"I need to create env with

mamba create --name tf1_12__HDR Tensorflow==1.12.0 Keras==2.2.4 opencv-python==3.4.4.19 anaconda

It provides
mamba (0.7.3) supported by @QuantStack

...

Looking for: ['tensorflow==1.12.0', 'keras==2.2.4', 'opencv-python==3.4.4.19', 'anaconda']

...

Problem: nothing provides requested opencv-python 3.4.4.19

I am calling

pip install opencv-python==

ERROR: Could not find a version that satisfies the requirement opencv-python== (from versions: 3.4.2.17, 3.4.3.18, 3.4.4.19, 3.4.5.20, 3.4.6.27, 3.4.7.28, 3.4.8.29, 3.4.9.31, 3.4.9.33, 3.4.10.35, 3.4.10.37, 3.4.11.39, 3.4.11.41, 3.4.11.43, 3.4.11.45, 3.4.13.47, 3.4.14.51, 3.4.14.53, 3.4.15.55, 3.4.16.57, 4.0.0.21, 4.0.1.23, 4.0.1.24, 4.1.0.25, 4.1.1.26, 4.1.2.30, 4.2.0.32, 4.2.0.34, 4.3.0.36, 4.3.0.38, 4.4.0.40, 4.4.0.42, 4.4.0.44, 4.4.0.46, 4.5.1.48, 4.5.2.52, 4.5.2.54, 4.5.3.56, 4.5.4.58)
ERROR: No matching distribution found for opencv-python==

So such version exists.
I know a cheat to overcome issue using steps:
1 Create env without opencv-python
2 Install opencv-python with pip
but it can lead to conflicts inside enviroment and potentially break it.
Can you please provide a ""clean"" solution and/or describe the reason of this
unexpected output?
","PyPI and Anaconda Cloud packages don't necessarily go by the same names. The opencv package from Conda Forge will include the OpenCV libs (libopencv) and the Python interface (py-opencv). That is, install opencv with the version you want.
BTW, as mentioned in the comments == is Pip syntax, Conda/Mamba use single =.
",,,false,
https://stackoverflow.com/questions/60606297,true,The issue involves the tf.keras.layers.ConvLSTM2D layer in TensorFlow and its compatibility with the Open VINO Intermediate Representation. Further analysis is required to determine the specific runtime conditions and unexpected behavior.,,,,,,,Cannot convert tf.keras.layers.ConvLSTM2D layer to open vino intermediate representation,"I am trying to convert a trained model in tensorflow to Open VINO Intermediate Representation.

I have a model of the form given below

class Conv3DModel(tf.keras.Model):
    def __init__(self):
        super(Conv3DModel, self).__init__()
        # Convolutions
        self.conv1 = tf.compat.v2.keras.layers.Conv3D(32, (3, 3, 3), activation='relu', name=""conv1"", data_format='channels_last')
        self.pool1 = tf.keras.layers.MaxPool3D(pool_size=(2, 2, 2), data_format='channels_last')
        self.conv2 = tf.compat.v2.keras.layers.Conv3D(64, (3, 3, 3), activation='relu', name=""conv1"", data_format='channels_last')
        self.pool2 = tf.keras.layers.MaxPool3D(pool_size=(2, 2,2), data_format='channels_last')

        # LSTM &amp; Flatten
        self.convLSTM =tf.keras.layers.ConvLSTM2D(40, (3, 3))
        self.flatten =  tf.keras.layers.Flatten(name=""flatten"")

        # Dense layers
        self.d1 = tf.keras.layers.Dense(128, activation='relu', name=""d1"")
        self.out = tf.keras.layers.Dense(6, activation='softmax', name=""output"")


    def call(self, x):
        x = self.conv1(x)
        x = self.pool1(x)
        x = self.conv2(x)
        x = self.pool2(x)
        x = self.convLSTM(x)
        x = self.flatten(x)
        x = self.d1(x)
        return self.out(x)


I tried to convert the model into IR. The model is here .

I have trained this model in tensorflow 1.15. Tensorflow 2.0 is currently not supported.

Now I tried to run the command

python3 /opt/intel/openvino/deployment_tools/model_optimizer/mo_tf.py --saved_model_dir jester_trained_models/3dcnn-basic/  --output_dir /home/deepanshu/open_vino/udacity_project_custom_model/

Now i got the following error

Model Optimizer arguments:

Common parameters: 


Path to the Input Model: None
Path for generated IR: /home/deepanshu/open_vino/udacity_project_custom_model/
IR output name: saved_model
Log level: ERROR
Batch: Not specified, inherited from the model
Input layers: Not specified, inherited from the model
Output layers: Not specified, inherited from the model
Input shapes: Not specified, inherited from the model
Mean values: Not specified
Scale values: Not specified
Scale factor: Not specified
Precision of IR: FP32
Enable fusing: True
Enable grouped convolutions fusing: True
Move mean values to preprocess section: False
Reverse input channels: False


TensorFlow specific parameters:


Input model in text protobuf format: False
Path to model dump for TensorBoard: None
List of shared libraries with TensorFlow custom layers implementation: None
Update the configuration file with input/output node names: None
Use configuration file used to generate the model with Object Detection API: None
Operations to offload: None
Patterns to offload: None
Use the config file: None


Model Optimizer version: 2020.1.0-61-gd349c3ba4a

[ ERROR ] Unexpected exception happened during extracting attributes for node conv3d_model/conv_lst_m2d/bias/Read/ReadVariableOp. Original exception message: 'ascii' codec can't decode byte 0xc9 in position 1: ordinal not in range(128)

As far as  I can see it is the tf.keras.layers.ConvLSTM2D(40, (3, 3)) causing problems . I am kind of stuck here . Can anyone tell me where can I proceed further ?

Thanks

Edit to the question

Now I rejected the above tensorflow implementation and used keras . My h5 model developed was converted into .pb format using this post.

Now I ran the model optimizer on this .pb file. Using the command 

python3 /opt/intel/openvino/deployment_tools/model_optimizer/mo_tf.py --input_model /home/deepanshu/ml_playground/jester_freezed/tf_model.pb  --output_dir /home/deepanshu/open_vino/udacity_project_custom_model/  --input_shape=[1,30,64,64,1] --data_type FP32


Now i am facing another issue . The issue here is point no. 97 on this post.

So my model contains a cycle and model optimizer does not know a way to convert it. Has anybody faced this issue before ?

Please help.

Here is the model .

Here is the defination of the model in keras


from keras.models import Sequential

from keras.layers import Conv3D , MaxPool3D,Flatten ,Dense

from keras.layers.convolutional_recurrent import ConvLSTM2D

import keras


model = Sequential()

model.add(Conv3D(32, (3, 3, 3), 

         name=""conv1"" , input_shape=(30, 64, 64,1) ,  data_format='channels_last',

        activation='relu') )

model.add(MaxPool3D(pool_size=(2, 2, 2), data_format='channels_last'))

model.add(Conv3D(64, (3, 3, 3), activation='relu', name=""conv2"", data_format='channels_last'))

model.add(MaxPool3D(pool_size=(2, 2,2), data_format='channels_last'))

model.add(ConvLSTM2D(40, (3, 3)))

model.add(Flatten(name=""flatten""))

model.add(Dense(128, activation='relu', name=""d1""))

model.add(Dense(6, activation='softmax', name=""output""))

","Actually the script to convert from h5 to .pb suggested by intel was not good enough. Always use the code from here to convert your keras model to .pb. 

Once you obtain your .pb file now convert your model to IR using

python3 /opt/intel/openvino/deployment_tools/model_optimizer/mo_tf.py --input_model ml_playground/try_directory/tf_model.pb   --output_dir /home/deepanshu/open_vino/udacity_project_custom_model/  --input_shape=[1,30,64,64,1] --data_type FP32


After the execution of this script we can obtain the intermediate representation of the keras model. 
",,,false,
https://stackoverflow.com/questions/52080424,false,The issue is not related to an API exhibiting unexpected failures or unpredictable behaviors under specific runtime conditions. It is a traceback error related to the installation or compatibility of TensorFlow and its dependencies.,,,,,,,TypeError while Running Tensorflow Deep Dream Tutorial from Python: TypeError: __init__() got an unexpected keyword argument &#39;serialized_options&#39;,"I have taken the tensorflow deepdream example and have converted the Jupyter Notebook to a normal Python module.

When running the module from Eclipse / Python 3.6, I get the following traceback:


Traceback (most recent call last):
  File ""C:\git\tensorflow\tensorflow\examples\tutorials\deepdream\deepdream_python.py"", line 43, in 
    import tensorflow as tf
  File ""C:\Program Files\Python36\lib\site-packages\tensorflow\__init__.py"", line 22, in 
    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import
  File ""C:\Program Files\Python36\lib\site-packages\tensorflow\python\__init__.py"", line 52, in 
    from tensorflow.core.framework.graph_pb2 import *
  File ""C:\Program Files\Python36\lib\site-packages\tensorflow\core\framework\graph_pb2.py"", line 15, in 
    from tensorflow.core.framework import node_def_pb2 as tensorflow_dot_core_dot_framework_dot_node__def__pb2
  File ""C:\Program Files\Python36\lib\site-packages\tensorflow\core\framework\node_def_pb2.py"", line 15, in 
    from tensorflow.core.framework import attr_value_pb2 as tensorflow_dot_core_dot_framework_dot_attr__value__pb2
  File ""C:\Program Files\Python36\lib\site-packages\tensorflow\core\framework\attr_value_pb2.py"", line 15, in 
    from tensorflow.core.framework import tensor_pb2 as tensorflow_dot_core_dot_framework_dot_tensor__pb2
  File ""C:\Program Files\Python36\lib\site-packages\tensorflow\core\framework\tensor_pb2.py"", line 15, in 
    from tensorflow.core.framework import resource_handle_pb2 as tensorflow_dot_core_dot_framework_dot_resource__handle__pb2
  File ""C:\Program Files\Python36\lib\site-packages\tensorflow\core\framework\resource_handle_pb2.py"", line 22, in 
    serialized_pb=_b('\n/tensorflow/core/framework/resource_handle.proto\x12\ntensorflow\""r\n\x13ResourceHandleProto\x12\x0e\n\x06\x64\x65vice\x18\x01 \x01(\t\x12\x11\n\tcontainer\x18\x02 \x01(\t\x12\x0c\n\x04name\x18\x03 \x01(\t\x12\x11\n\thash_code\x18\x04 \x01(\x04\x12\x17\n\x0fmaybe_type_name\x18\x05 \x01(\tBn\n\x18org.tensorflow.frameworkB\x0eResourceHandleP\x01Z=github.com/tensorflow/tensorflow/tensorflow/go/core/framework\xf8\x01\x01\x62\x06proto3')
TypeError: __init__() got an unexpected keyword argument 'serialized_options'


There is a similar stackoverflow question where the user was able to workaround the problem by updating tensorboard to 1.9.0.  I have the tensorflow and tensorboard 1.10.0 PyPi packages installed and even tried with version 1.9.0, but this doesn't resolve the problem.

There is another question that looks related to the issue, but no one has answered it.
","Installing tensorflow 1.9.0 was a workaround for this issue.

pip install tensorflow==1.9.0
",,,false,
https://stackoverflow.com/questions/50417129,false,The issue is related to installation issues with TensorFlow and does not involve an API exhibiting unexpected failures or unpredictable behaviors under specific runtime conditions.,,,,,,,Installation issues with Tensorflow; PermissionError,"I've been having a bit of difficulty installing tensorflow onto my computer, and would like some advice on where to go from here. To be clear, I am trying to install on Windows using the advice for anaconda in windows from the tensorflow website, and I am stuck at part 4 of installing the version without GPU support.

At first, I was having a TypeError, where parse() got an unexpected keyword (the issue in this thread). So I ran the conda install suggestions which were provided in the thread.

After that, I successfully installed both and tried to run:

pip install --ignore-installed --upgrade tensorflow

Which ran for a bit, but then my command prompt stated that I had thrown a PermissionError and that access is denied (the same error found here). To get past this, I tried opening Windows Powershell as an administrator instead, which still gave me this error. If you have any suggestions on what I could try next, it would be greatly appreciated.

EDIT: Here is the traceback error I get:

Traceback (most recent call last):
  File ""C:\Users\awm\Anaconda3\lib\site-packages\pip\basecommand.py"", line 215, in main
    status = self.run(options, args)
  File ""C:\Users\awm\Anaconda3\lib\site-packages\pip\commands\install.py"", line 342, in run
    prefix=options.prefix_path,
  File ""C:\Users\awm\Anaconda3\lib\site-packages\pip\req\req_set.py"", line 784, in install
    **kwargs
  File ""C:\Users\awm\Anaconda3\lib\site-packages\pip\req\req_install.py"", line 851, in install
    self.move_wheel_files(self.source_dir, root=root, prefix=prefix)
  File ""C:\Users\awm\Anaconda3\lib\site-packages\pip\req\req_install.py"", line 1064, in move_wheel_files
    isolated=self.isolated,
  File ""C:\Users\awm\Anaconda3\lib\site-packages\pip\wheel.py"", line 352, in move_wheel_files
    clobber(source, lib_dir, True)
  File ""C:\Users\awm\Anaconda3\lib\site-packages\pip\wheel.py"", line 323, in clobber
    os.unlink(destfile)
PermissionError: [WinError 5] Access is denied: 'C:\\Users\\awm\\Anaconda3\\Lib\\site-packages\\numpy\\.libs\\libopenblas.BNVRK7633HSX7YVO2TADGR4A5KEKXJAW.gfortran-win_amd64.dll'
","After restart of kernal and without loading any library try pip install --ignore-installed --upgrade tensorflow .
",,,false,
https://stackoverflow.com/questions/37111877,true,The issue involves the behavior of the tf.variable_scope function in TensorFlow. The unexpected naming conventions observed when creating a variable within a tf.variable_scope can be analyzed further to understand the underlying cause and provide a detailed analysis.,,,,,,,"creating a variable within tf.variable_scope(name), initialized from another variable&#39;s initialized_value","Hey tensorflow community,

I am experiencing unexpected naming conventions when using variable_scope in the following setup:

with tf.variable_scope(""my_scope""):
    var = tf.Variable(initial_value=other_var.initialized_value())


In the above, it holds that 

other_var.name = 'outer_scope/my_scope/other_var_name:0'


I am therefore ""reusing"" the same scope at this point in the code. Intuitively I do not see an issue with this, but the following happens:

var.name = 'outer_scope/my_scope_1/var_name:0'


So apparently, tf isn't happy with ""my_scope"" and needs to append the ""_1"". 
The ""outer_scope"" remains the same, though.

If I do not initialize with ""other_var"", this behaviour does not come up.

An explanation would be much appreciated! Thx

Mat
","You might want to use tf.get_variable() instead of 'tf.Variable`.

with tf.variable_scope('var_scope', reuse=False) as var_scope:
    var = tf.get_variable('var', [1])
    var2 = tf.Variable([1], name='var2')
    print var.name # var_scope/var:0 
    print var2.name # var_scope/var2:0

with tf.variable_scope('var_scope', reuse=True) as var_scope:
    var = tf.get_variable('var', [1])
    var2 = tf.Variable([1], name='var2')
    print var.name # var_scope/var:0 
    print var2.name # var_scope_1/var2:0


The reason behind this I think is that in your example, although you have successfully ""re-entered"" the variable_scope you want, what really affects your variable name is another scope named name_scope intead of variable_scope as you might guess. From the official document here you can see that:


  when we do with tf.variable_scope(""name""), this implicitly opens a
  tf.name_scope(""name"").


name_scope is originally used for managing operation names(such as add, matmul), because tf.Variable is actually an operation and its operation name will be ""inherited"" by variables created by it, so the name of name_scope rather than variable_scope is used as prefix.

But if you want to use tf.Variable, you can also directly use name_scope in with statement:

with tf.name_scope('n_scope') as n_scope:
    var = tf.Variable([1], name='var')
    print var.name #n_scope/var_1:0

with tf.name_scope(n_scope) as n_scope:
    var = tf.Variable([1], name='var')
    print var.name #n_scope/var_1:0


One thing to pay attention to is that you should pass as argument the scope varible previously captured from a with statement when you want to ""re-enter"" a name scope, rather than using str scope name:

  with tf.name_scope('n_scope') as n_scope:
      var = tf.Variable([1], name='var')
      print var.name #n_scope/var_1:0

  with tf.name_scope('n_scope') as n_scope:
      var = tf.Variable([1], name='var')
      print var.name #n_scope_1/var_1:0


Pay attention to the argument passed to tf.name_scope. This behavior is again described in doc string of name_scope:


  The name argument will be interpreted as follows:
  
  
  A string (not ending with ‘/’) will create a new name scope, in which
  name is appended to the prefix of all operations created in the
  context. If name has been used before, it will be made unique by
  calling self.unique_name(name). 
  A scope previously captured from a with g.name_scope(...) as 
  scope: statement will be treated as an “absolute” name scope, 
  which makes it possible to re-enter existing scopes. 
  A value of None or the empty string will reset the current name 
  scope to the top-level (empty) name scope.
  

",,,false,
https://stackoverflow.com/questions/63783513,false,The issue is related to a problem with the Keras Functional API and the usage of the Input layer and LSTM layer. It does not involve an API exhibiting unexpected failures or unpredictable behaviors under specific runtime conditions.,,,,,,,Keras Functional API issue with Input layer and first LSTM layer,"I am trying to create a Functional API as opposed to a Sequential API. I have built the model previously using the Sequential API, and it worked just fine. It is an LSTM, and I am having trouble with the batch_size going from the Input to the LSTM layer. The Sequential API was built as follows:
new_model = Sequential()
new_model.add(LSTM(n_neurons, batch_input_shape=(batch_size,train_X.shape[1], train_X.shape[2]), activation='tanh', stateful=True, return_sequences=True))
new_model.add(Dropout(0))
new_model.add(LSTM(n_neurons, batch_input_shape=(batch_size,train_X.shape[1], train_X.shape[2]), activation='tanh', stateful=True))
new_model.add(Dropout(0))
new_model.add(Dense(n_neurons1, activation='tanh'))
new_model.add(Dropout(0.1))
new_model.add(Dense(nm))
new_model.compile(loss='mse', optimizer=optimizer)

The above snippet works fine. The Functional API I am trying to get to work is as follows:
inp = Input(shape = (train_X.shape[1], train_X.shape[2]), batch_size = batch_size)
L1 = LSTM(n_neurons, batch_input_shape=(batch_size,train_X.shape[1], train_X.shape[2]), activation='tanh', stateful=True, return_sequences=True)(inp)
D1 = Dropout(0)(L1)
L2 = LSTM(n_neurons, batch_input_shape=(batch_size,train_X.shape[1], train_X.shape[2]), activation='tanh', stateful=True, return_sequences=True)(D1)
D2 = Dropout(0)(L2)
F1 = Dense(n_neurons1, activation='tanh')(D2)
D3 = Dropout(0.1)(F1)
out = Dense(nm)
new_model = Model(inp,out)
new_model.compile(loss='mse', optimizer=optimizer)

I get an error saying ""Input() got an unexpected keyword argument 'batch_size"", even though I know batch_size is an argument for the Input layer. Then, if I get rid of the argument, I get an error with the first LSTM layer saying:
""If a RNN is stateful, it needs to know its batch size. Specify the batch size of your input tensors:

If using a Sequential model, specify the batch size by passing a batch_input_shape argument to your first layer.
If using the functional API, specify the batch size by passing a batch_shape argument to your Input layer.""

I have already tried updating tensorflow but that did not fix the Input() issue. Where do I go from here?
","You describe passing a batch_size parameter via the functional API and getting an error suggesting ""passing a batch_shape argument to your Input layer.""
If you try changing batch_size = batch_size in your input layer to
batch_shape = (batch_size,train_X.shape[1], train_X.shape[2])

does that solve it?
",,,false,
https://stackoverflow.com/questions/61185335,false,The issue is related to the usage of the cameraWithTensors function from the @tensorflow/tfjs-react-native library and does not involve an API exhibiting unexpected failures or unpredictable behaviors under specific runtime conditions.,,,,,,,React Native cameraWithTensors onReady is not being called,"I'm trying to capture camera stream using cameraWithTensors from tensorflow/tfjs-react-native example from https://js.tensorflow.org/api_react_native/0.2.3. I'm using react-navigation button tabs (do not know if this is related). 

import { Camera } from 'expo-camera';
import { cameraWithTensors } from '@tensorflow/tfjs-react-native';

const TensorCamera = cameraWithTensors(Camera);

class MyComponent {

  handleCameraStream(images, updatePreview, gl) {
    const loop = async () =&gt; {
      const nextImageTensor = images.next().value

      console.log(""*** This line is not logged ***"");

      // if autorender is false you need the following two lines.
      // updatePreview();
      // gl.endFrameEXP();

      requestAnimation(loop);
    }
    loop();
  }

  render() {
   // Currently expo does not support automatically determining the
   // resolution of the camera texture used. So it must be determined
   // empirically for the supported devices and preview size.

   let textureDims;
   if (Platform.OS === 'ios') {
    textureDims = {
      height: 1920,
      width: 1080,
    };
   } else {
    textureDims = {
      height: 1200,
      width: 1600,
    };
   }

   return &lt;View&gt;
     &lt;TensorCamera
      // Standard Camera props
      style={styles.camera}
      type={Camera.Constants.Type.front}
      // Tensor related props
      cameraTextureHeight={textureDims.height}
      cameraTextureWidth={textureDims.width}
      resizeHeight={200}
      resizeWidth={152}
      resizeDepth={3}
      onReady={this.handleCameraStream}
      autorender={true}
     /&gt;
   &lt;/View&gt;
  }
}


Using the following modules

react-native-cli: 2.0.1
react-native: 0.61.4

""expo-gl"": ""^8.1.0""
""@tensorflow/tfjs"": ""1.7.2"",
""@tensorflow/tfjs-react-native"": ""^0.2.3"",
""core-js"": ""3"",
""expo"": ""^36.0.2"",
""expo-asset"": ""~8.0.0"",
""expo-camera"": ""^8.2.0"",
""expo-constants"": ""^9.0.0"",
""expo-font"": ""~8.0.0"",
""expo-gl"": ""^8.1.0"",
""expo-image-picker"": ""^8.1.0"",
""expo-permissions"": ""^8.1.0"",
""@react-native-community/async-storage"": ""^1.8.1"",
""@react-native-community/masked-view"": ""0.1.5"",
""@react-navigation/bottom-tabs"": ""^5.0.0"",
""@react-navigation/native"": ""^5.0.0"",
""@react-navigation/stack"": ""^5.0.0"",


Tried on iOS simulator and device as well. ""OnReady"" function is not being called or any console is logged. On app startup I got these errors:



  Initialization of backend rn-webgl failed


  Error: ExponentGLObjectManager.createContextAsync: Unexpected error
  occurred when initializing headless context



I tried to debug the react native app got no clue on the root cause of this. It can be a dependency issue. When I use this simple example it works fine, however, when trying to use the same component as a screen tab it fails. 

I'd appreciate if you explain how to resolve these errors above. 

Thank you 
","Apparently, after a few days of digging this out. The dependencies are not the issue here.
Probably if your app.json app name contains spaces for some reason the bundled js is not loading correctly.
The error 


  Initialization of backend rn-webgl failed


as it said your webgl backend failed to load. On some devices, webgl is not supported. I used iPhone X iOS 13 and it seems that webgl 1 &amp; 2 are supported correctly. This is the graphics engine that allows leveraging GPU for complex computations. 
When using expo need to make sure your react-native version matches your expo client (0.61.X supported with the current expo client), otherwise, you will get an exception on app load. 
Second error (Error: ExponentGLObjectManager.createContextAsync: Unexpected error occurred when initializing headless context) is a result of the error above.
The bottom line, need to make sure your index.js, app.json we well as metro caches are aligned and fresh. 
To clear metro cache use 

rm -fr $TMPDIR/metro* &amp; watchman watch-del-all


Then 

rm -fr node_modules

",,,false,
https://stackoverflow.com/questions/57066263,false,The issue is related to applying a trained model on images of a different size than what the model was trained on. It does not involve an API exhibiting unexpected failures or unpredictable behaviors under specific runtime conditions.,,,,,,,How to apply trained model on images of shape/size larger than what the model was trained on (in Tensorflow)?,"I trained a model on size 512x512 images. I currently have size 982x982 (or any other size) images that I need to have the model predict on. When I run on these images, I get errors about unexpected input array shape.

Does Tensorflow provide any way to conveniently deploy a model on images of size/shape larger than what the model was trained on?

More Details:

Specifically, this is a model used for image segmentation.

I assume one workaround to the issue is creating my own sliding-window script that, instead, inputs into the model windows of the expected size taken at different intervals from the original image, and then somehow pasting all those output windows back into a single image after they have gone through the model. However, I want to know if there is a more standard approach.

I am brand new to Tensorflow (and image segmentation) so it is very possible I just missed something, or that perhaps my question is unclear.

Thank you for any advice :)
","I once have the same question about how to deal with larger or smaller images. According to my experience, a possible method is to resize images to the input size of the network. For example, if your current image size is 982x982 and the network input size is 512x512, then you can just use some libraries like Pillow or OpenCV to resize the original image from 982x982 to 512x512.

Your method is a possible solution. I would say that there are many possible solutions other than the resizing operation, but you should better try this simplest method to see if your network works well. What I have learned from my projects is that we can always try the simplest solution, and in most cases, it works perfectly.

Generally speaking, there is not a general perfect way to do it. You can start with the simplest method, and find a more complicated one if it does not work.
",,,false,
https://stackoverflow.com/questions/46943200,false,The issue does not meet the criteria for deeper analysis as it does not involve an API exhibiting unexpected failures or unpredictable behaviors under specific runtime conditions.,,,,,,,How to set the weight of tf.slim with a numpy array,"I would like to show my example below:

x = tf.placeholder(dtype=...)
a = numpy.asarray([784, 10])
z = slim.fully_connected(x, 10, weights_initializer=?)


I have tried weights_initializer = lambda x1:a, it reports the error: TypeError: () got an unexpected keyword argument 'dtype'

I also found another post here:https://github.com/tensorflow/tensorflow/issues/4016

However, I still don't know the answer. Thank you very much.
","Sorry, I don't really understand what you're trying to do. 
If your fully connected layer has 10 hidden neurons then your initializer must have the shape (input_shape, 10), what you're giving is a (2,) shape. Secondly, to initialize weights with a constant matrix you should use tf.constant_initializer(..) function. 
Are you trying to do the following: (you can change the init function used with numpy)

import tensorflow as tf
import numpy as np

slim = tf.contrib.slim

input_size = ?

x = tf.placeholder(dtype=tf.float32, shape=[input_size])
a = np.random.normal((input_size, 10))

z = slim.fully_connected(x, 10, 
weights_initializer=tf.constant_initializer(a))

init  = tf.global_variables_initializer()

sess = tf.Session()
sess.run(init)

",,,false,
https://stackoverflow.com/questions/52343989,true,"The issue involves the TensorFlow API while_loop, which exhibits non-deterministic behavior with large matrices. The issue is triggered when performing matrix calculations within the while_loop, leading to different results and even nan values in different runs.",,,,,,,Non-deterministic behavior of TensorFlow while_loop(),"I have implemented an algorithm using TensorFlow while_loop with large matrices and I have recently noticed strange behavior: I am getting different results with different runs, sometimes even nan values. I have spend some time on narrowing down the problem and I now have the following minimal example. I  take a large matrix K of size 15000x15000 filled with ones, and then calculate K⁵u for the vector u filled with ones. After one iteration, I expect as result the vector filled with 15000. But this is not what happens. 

import numpy as np
import tensorflow as tf

n = 15000
np_kernel_mat = np.ones((n, n), dtype=np.float32)
kernel_mat = tf.constant(np_kernel_mat)

# for debugging
def compare_kernel(kernel_matrix):
    print(""AverageDifference:"" + str(np.average(np.abs(np_kernel_mat - kernel_matrix))))
    print(""AmountDifferent:"" + str(np.count_nonzero(np.abs(np_kernel_mat - kernel_matrix))))
    return True

# body of the loop
def iterate(i, u):
    # for debugging
    with tf.control_dependencies(tf.py_func(compare_kernel, [kernel_mat], [tf.bool])):
        u = tf.identity(u)
    # multiply
    u = tf.matmul(kernel_mat, u)
    # check result and kernel 
    u = tf.Print(u, [tf.count_nonzero(tf.abs(kernel_mat-np_kernel_mat))], ""AmountDifferentKernel: "")
    u = tf.Print(u, [tf.count_nonzero(tf.abs(u-float(n)))], ""AmountDifferentRes: "")
    i = i + 1
    return i, u


def cond(i, u):
    return tf.less(i, 5)

u0 = tf.fill((n, 1), 1.0, name='u0')
iu_0 = (tf.constant(0), u0)
iu_final = tf.while_loop(cond, iterate, iu_0, back_prop=False, parallel_iterations=1)
u_res = iu_final[1]


with tf.Session() as sess:
    kernel_mat_eval, u_res_eval = sess.run([kernel_mat, u_res])
    print(np.array_equal(kernel_mat_eval, np_kernel_mat))


Now running this I get the following output:

I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
I tensorflow/core/common_runtime/gpu/gpu_device.cc:1356] Found device 0 with properties: 
name: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.076
pciBusID: 0000:00:0f.0
totalMemory: 11.93GiB freeMemory: 11.81GiB
I tensorflow/core/common_runtime/gpu/gpu_device.cc:1435] Adding visible gpu devices: 0
I tensorflow/core/common_runtime/gpu/gpu_device.cc:923] Device interconnect StreamExecutor with strength 1 edge matrix:
I tensorflow/core/common_runtime/gpu/gpu_device.cc:929]      0 
I tensorflow/core/common_runtime/gpu/gpu_device.cc:942] 0:   N 
I tensorflow/core/common_runtime/gpu/gpu_device.cc:1053] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 11435 MB memory) -&gt; physical GPU (device: 0, name: GeForce GTX TITAN X, pci bus id: 0000:00:0f.0, compute capability: 5.2)
minimal_example.py:25: RuntimeWarning: invalid value encountered in subtr[8/281]
  print(""AverageDifference:"" + str(np.average(np.abs(np_kernel_mat - kernel_matr
ix))))
/usr/local/lib/python3.6/dist-packages/numpy/core/_methods.py:70: RuntimeWarning
: overflow encountered in reduce
  ret = umr_sum(arr, axis, dtype, out, keepdims)
AverageDifference:nan
minimal_example.py:26: RuntimeWarning: invalid value encountered in subtract
  print(""AmountDifferent:"" + str(np.count_nonzero(np.abs(np_kernel_mat - kernel_
matrix))))
AmountDifferent:4096
AmountDifferentKernel: [0]
AmountDifferentRes, DifferenceRes: [4][inf]
AverageDifference:nan
AmountDifferent:4096
AmountDifferentKernel: [0]
AmountDifferentRes, DifferenceRes: [15000][nan]
AverageDifference:nan
AmountDifferent:4096
AmountDifferentKernel: [0]
AmountDifferentRes, DifferenceRes: [15000][nan]
AverageDifference:nan
...


It is clear that in the second iteration, the result is not 15000 anymore, but that doesn't explain why the difference is nan. On CPU, everything works fine (the difference is then something like 2e08). 

Now my questions are:
Why is the output of the Print op different to the output of the py_func print? Why is the evaluation of the matrix again equal to the original matrix? Why do I get different results over different runs? Can someone reproduce this? 

I am running this on Ubuntu 16.04, TensorFlow 1.8, numpy 1.14, python3.6.
GPU is GeForceGTX 1080. 

NVRM version: NVIDIA UNIX x86_64 Kernel Module  390.48  Thu Mar 22 00:42:57 PDT 2018
GCC 
version:  gcc version 5.4.0 20160609 (Ubuntu 5.4.0-6ubuntu1~16.04.9) 

","Most likely, your problem is stemming from seeding issues, make sure that you set a seed for both random.seed(), and for numpy.random.seed(). You'll need to seed both, as numpy's random seed is independent from the random random state.
",,,false,
https://stackoverflow.com/questions/34783030,false,"The issue does not meet the criteria for deeper analysis as it is related to a specific version of TensorFlow (0.12) and the behavior of casting from int to float, rather than an API exhibiting unexpected failures or unpredictable behaviors.",,,,,,,Saving image files in Tensorflow,"I'm just starting with Tensorflow and I have a newbie question.

I know that Tensorflow is all about neural nets but I'm starting with just the mechanics of it. I'm trying to get it to load, resize, flip, and save two images. Should be a simple operation, right, and it gets me started with the basics.

Here's my code so far:

import tensorflow as tf
import numpy as np

print(""resizing images"")

filenames = ['img1.png', 'img2.png' ]
filename_queue = tf.train.string_input_producer(filenames, num_epochs=1)

reader = tf.WholeFileReader()
key,value = reader.read(filename_queue)
images = tf.image.decode_png(value)

resized = tf.image.resize_images(images, 180,180, 1)
resized.set_shape([180,180,3])

flipped_images = tf.image.flip_up_down(resized)

resized_encoded = tf.image.encode_jpeg(flipped_images,name=""save_me"")

init = tf.initialize_all_variables()
sess = tf.Session()

with sess.as_default():
  tf.train.start_queue_runners()
  sess.run(init)

  f = open(""/tmp/foo1.jpeg"", ""wb+"")
  f.write(resized_encoded.eval())
  f.close()

  f = open(""/tmp/foo2.jpeg"", ""wb+"")
  f.write(resized_encoded.eval())
  f.close()


It works fine, resizing the two images and saving them. But it always ends with an error:

W tensorflow/core/common_runtime/executor.cc:1076] 0x7f97240e7a40
 Compute status: Out of range: Reached limit of 1


I'm obviously doing something wrong. If I take off the num_epochs=1, then it ends with no error.

I have a few questions:

How do I do this correctly?

Also, if I want to preserve the original file names all the way from the filename_queue through to the end so I can save them with the original names, how do I do that? And how do I know how many files I need to save? Let's say I'm making the list of file names by reading a directory. I tried many different things but I could never find out how I know when I reach the end.

It seems strange to me that I'm calling resized_encoded.eval() twice.

Thank you and I'm sure this is a very basic question but I'm not understanding how this works.

Edit: I created an even simpler demonstration of the behavior:

import tensorflow as tf
import numpy as np

filenames = ['file1.png', 'file2.png' ]

filename_queue = tf.train.string_input_producer(filenames, 
                        num_epochs=1, name=""my_file_q"")

reader = tf.WholeFileReader()
key,value = reader.read(filename_queue)
init = tf.initialize_all_variables()

sess = tf.Session()

with sess.as_default():
  print(""session started"")

  sess.run(init)

  coord = tf.train.Coordinator()
  threads = tf.train.start_queue_runners(coord=coord)

  for i in range (2):
    print(key.eval())

  coord.request_stop()
  coord.join(threads)


This gives the same warning. I can't understand why.
",,,,false,
https://stackoverflow.com/questions/50391967,false,The issue does not meet the criteria for deeper analysis as it is related to visualizing TFRecords and does not involve an API exhibiting unexpected failures or unpredictable behaviors.,,,,,,,How to visualize a TFRecord?,"I was asked this on another forum but thought I'd post it here for anyone that is having trouble with TFRecords.

TensorFlow's Object Detection API can produce strange behavior if the labels in the TFRecord file do not align with the labels in your labels.pbtxt file. It will run, loss will likely decrease but the network will not produce good detections.

Also, I for one always get confused between X-Y, row-col space, and so I always like to double check to make sure that my annotations are actually annotating the right parts of the image.

The best way I've found to do this is by decoding the TFRecord and plotting it with TF tools.  Here's some code below:

import matplotlib.pyplot as plt
import numpy as np
import tensorflow as tf
from object_detection.utils import visualization_utils as vu
from object_detection.protos import string_int_label_map_pb2 as pb
from object_detection.data_decoders.tf_example_decoder import TfExampleDecoder as TfDecoder
from google.protobuf import text_format
def main(tfrecords_filename, label_map=None):
    if label_map is not None:
        label_map_proto = pb.StringIntLabelMap()
        with tf.gfile.GFile(label_map,'r') as f:
            text_format.Merge(f.read(), label_map_proto)
            class_dict = {}
            for entry in label_map_proto.item:
                class_dict[entry.id] = {'name':entry.display_name}
    sess = tf.Session()
    decoder = TfDecoder(label_map_proto_file=label_map, use_display_name=False)
    sess.run(tf.tables_initializer())
    for record in tf.python_io.tf_record_iterator(tfrecords_filename):
        example = decoder.decode(record)
        host_example = sess.run(example)
        scores = np.ones(host_example['groundtruth_boxes'].shape[0])
        vu.visualize_boxes_and_labels_on_image_array( 
            host_example['image'],                                               
            host_example['groundtruth_boxes'],                                                     
            host_example['groundtruth_classes'],
            scores,
            class_dict,
            max_boxes_to_draw=None,
            use_normalized_coordinates=True)
plt.imshow(host_example['image'])
plt.show()

","If you'd like to check bounding boxes/labels visually, you can check this TFRecord Viewer: https://github.com/sulc/tfrecord-viewer


","Thank you for the code, @Steve ! I was looking everywhere on the github repo and couldn't find a way to check the tfrecord.

Just wanted to point out that an import line seems missing:

from google.protobuf import text_format 


It ran ok for me after adding this
","I would suggest trying this: https://www.tensorflow.org/tutorials/load_data/tfrecord#read_the_tfrecord_file
import tensorflow as tf

import numpy as np
import IPython.display as display

raw_image_dataset = tf.data.TFRecordDataset('images.tfrecords')

# Create a dictionary describing the features.
image_feature_description = {
    'height': tf.io.FixedLenFeature([], tf.int64),
    'width': tf.io.FixedLenFeature([], tf.int64),
    'depth': tf.io.FixedLenFeature([], tf.int64),
    'label': tf.io.FixedLenFeature([], tf.int64),
    'image_raw': tf.io.FixedLenFeature([], tf.string),
}

def _parse_image_function(example_proto):
  # Parse the input tf.train.Example proto using the dictionary above.
  return tf.io.parse_single_example(example_proto, image_feature_description)

parsed_image_dataset = raw_image_dataset.map(_parse_image_function)
parsed_image_dataset

",false,
https://stackoverflow.com/questions/55902068,false,The issue does not meet the criteria for deeper analysis as it is related to fitting a Keras model and does not involve an API exhibiting unexpected failures or unpredictable behaviors.,,,,,,,Fitting a Keras model yields error &quot;constant folding failed: Invalid argument: Unsupported type: 21&quot;,"I am following the example from https://www.tensorflow.org/alpha/tutorials/load_data/text
to load my own dataset and run binary classification on the sentences there (TensorFlow 2.0).
The only change I've made to the example is the dataset being used (which I took from https://github.com/UKPLab/emnlp2017-claim-identification/tree/master/src/main/python), and since the labels can be only 0 or 1 I changed the loss function to binary_crossentropy and the optimizer to RMSprop.
When fitting the Keras model which is identical to the model proposed in the tutorial, I'm constantly receiving the following error:

2019-04-29 13:51:15.609297: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:495] constant folding failed: Invalid argument: Unsupported type: 21
2019-04-29 13:51:15.882000: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:495] constant folding failed: Invalid argument: Unsupported type: 21


The fitting process is still advancing between those prints as evident from:

662/4508 [===&gt;..........................] - ETA: 9:35 - loss: 11.0703 - accuracy: 0.2780


but instead of minimizing the loss, it seems to actually be maximizing it, with the accuracy going down after each iteration.
(In fact, if the accuracy metric is correct, it would be a pretty good classifier if I just take not [prediction]).
Is there anyone here who can explain to me what is the meaning of this error, and whether it's related to the strange behavior of the model (and hopefully how to fix it)? I've been trying to look for similar errors but couldn't find any.
Thanks!
","There is one key difference between the Tutorial mentioned in the link, https://www.tensorflow.org/beta/tutorials/load_data/text and your Dataset.
In the tutorial, Labels are 0, 1 and 2, i.e., all the sentences in cowper.txt are Labelled as 0, all the sentences in derby.txt are Labelled as 1, all the sentences in butler.txt are Labelled as 2. But in your Dataset, Labels are at the end of each sentence of the Text Files.
I have executed the code taking part of your Dataset, as shown below:
FILE_NAMES = ['001.dev', '001.test', '001.train', '002.dev', '002.test', '002.train']

parent_dir = ""Issue_55902068/OC""

parent_dir

In order to handle the difference mentioned above, the function, labeler should be modified as shown below:
def labeler(example, index):
  Label = tf.strings.split(example, sep="""")[-1] #It will give 0 or 1 in Str format
  Label = tf.strings.to_number(Label)
  return example, tf.cast(Label, tf.int64)

After that, I changed the loss function to binary_crossentropy and the optimizer to RMSprop as shown below:
model.compile(optimizer='RMSprop', loss='binary_crossentropy', metrics=['accuracy'])

It is working as expected. Screenshot of the output is shown below.

",,,false,
https://stackoverflow.com/questions/42029046,true,"The issue involves the TensorFlow API tf.image.rgb_to_grayscale, which exhibits unexpected behavior when applied to an image or mask with a different number of channels than expected.",TensorFlow,tf.image.rgb_to_grayscale,"When applying tf.image.rgb_to_grayscale on a segmentation mask (which typically has a single channel), TensorFlow raises a ""dimensions must be equal"" error. This is because the API expects an input tensor with three channels (RGB), but the input mask only has one channel.","The tf.image.rgb_to_grayscale is applied to a tensor with a shape that ends in 3 (i.e., an RGB image).","The issue is triggered when tf.image.rgb_to_grayscale is applied to a tensor with a shape that does not end in 3 (i.e., not an RGB image).",This issue might be challenging to detect for users unfamiliar with the specific requirements of the tf.image.rgb_to_grayscale function or those who assume that it can handle inputs with any number of channels.,Tensorflow batch loss spikes when restoring model for training from saved checkpoint?,"I'm encountering a strange issue that I've been trying to debug, without much luck. My model starts training properly with batch loss decreasing consistently (from ~6000 initially to ~120 after 20 epochs). However, when I pause training and resume training later by restoring the model from the checkpoint, the batch loss seems to spike unexpectedly from the previous batch loss (before pausing), and resumes decreasing from that higher loss point. My worry is that when I restore the model for evaluation, I may not be using the trained model that I think I am.

I have combed over my code several times, comparing to the Tensorflow tutorials. I tried to ensure that I was saving and restoring using the tutorial-suggested methods. Here is the code snapshot: https://github.com/KaranKash/DigitSpeak/tree/b7dad3128c88061ee374ae127579ec25cc7f5286 - the train.py file contains the saving and restoring steps, the graph setup and training process; while model.py creates the network layers and computes loss.

Here is an example from my print statements - notice batch loss rises sharply when resuming training from epoch 7's checkpoint:

Epoch 6. Batch 31/38. Loss 171.28
Epoch 6. Batch 32/38. Loss 167.02
Epoch 6. Batch 33/38. Loss 173.29
Epoch 6. Batch 34/38. Loss 159.76
Epoch 6. Batch 35/38. Loss 164.17
Epoch 6. Batch 36/38. Loss 161.57
Epoch 6. Batch 37/38. Loss 165.40
Saving to /Users/user/DigitSpeak/cnn/model/model.ckpt
Epoch 7. Batch 0/38. Loss 169.99
Epoch 7. Batch 1/38. Loss 178.42
KeyboardInterrupt
dhcp-18-189-118-233:cnn user$ python train.py
Starting loss calculation...
Found in-progress model. Will resume from there.
Epoch 7. Batch 0/38. Loss 325.97
Epoch 7. Batch 1/38. Loss 312.10
Epoch 7. Batch 2/38. Loss 295.61
Epoch 7. Batch 3/38. Loss 306.96
Epoch 7. Batch 4/38. Loss 290.58
Epoch 7. Batch 5/38. Loss 275.72
Epoch 7. Batch 6/38. Loss 251.12


I've printed the results of the inspect_checkpoint.py script. I've also experimented with other loss functions (Adam and GradientDescentOptimizer) and noticed the same behavior with respect to spiked loss after resuming training.

dhcp-18-189-118-233:cnn user$ python inspect_checkpoint.py
Optimizer/Variable (DT_INT32) []
conv1-layer/bias (DT_FLOAT) [64]
conv1-layer/bias/Momentum (DT_FLOAT) [64]
conv1-layer/weights (DT_FLOAT) [5,23,1,64]
conv1-layer/weights/Momentum (DT_FLOAT) [5,23,1,64]
conv2-layer/bias (DT_FLOAT) [512]
conv2-layer/bias/Momentum (DT_FLOAT) [512]
conv2-layer/weights (DT_FLOAT) [5,1,64,512]
conv2-layer/weights/Momentum (DT_FLOAT) [5,1,64,512]

","I ran into this issue and found it was the fact that I was initializing the graph variables when restoring the graph -- throwing away all learned parameters, to be replaced with whatever initialization values were originally specified for each respective tensor in the original graph definition.

For example, if you used tf.global_variable_initializer() to initialize variables as part of your model program, whatever your control logic to indicate that a saved graph will be restored, make sure the graph restore flow omits:  sess.run(tf.global_variable_initializer())

This was a simple, but costly mistake for me, so I hope someone else is saved a few grey hairs (or hairs, in general).
","I have also seen this issue in PyTorch, resulting from restoring model weights but not optimizer parameters. For example, the Adam optimizer stores estimates of the first and second moments of the gradient. If you only reload the model and don't save/reload the optimizer, the momentum parameters are reinitialized with the optimizer, which often causes a spike in the loss.
",,false,
https://stackoverflow.com/questions/22591174/pandas-multiple-conditions-while-indexing-data-frame-unexpected-behavior,false,The behavior is due to a misunderstanding of logical operators and conditions in pandas rather than a problem with the API itself.,,,,,,,,,,,,false,
https://stackoverflow.com/questions/53658826,false,The issue is related to the data feeding operation and not a problem with the TensorFlow API itself.,,,,,,,Why operations placed on GPU are also executed on CPU (tensorflow)?,"I use tensorflow profiler to profile my model to see how much time is consumed by each operation. I find some strange behavior, for example, Conv2D operation which is placed on GPU (I set log_device_placement=True to see the placement) also has a large CPU execution time. Here is the code I use to do profiling (tensorflow 1.4.0):

import tensorflow as tf
from tensorflow.python.profiler import option_builder

builder = option_builder.ProfileOptionBuilder
run_options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)
run_metadata = tf.RunMetadata()
# run and collect metadata
my_session.run(fetch_something, feed_dict=feed_dict, 
  options=run_options, run_metadata=run_metadata)
profiler_opts = builder(builder.time_and_memory()).order_by('micros').build()
# this will output the following results
tf.profiler.profile(my_graph, run_meta=run_metadata, cmd='scope', options=profiler_opts)


Here is the output of profiler:

node name | requested bytes | total execution time | accelerator execution time | cpu execution time
MyScope/Conv2D (4511.35MB/4511.35MB, 823.47ms/823.47ms, 445.37ms/445.37ms, 378.11ms/378.11ms)


From the profiling result, the Conv2D operation (tf.nn.conv2d) takes 378.11ms on CPU and 445.37ms on GPU. Why don't tensorflow only use GPU for Conv2D? Is the CPU time for data transfer between memory and GPU because this op takes a lot of memory (4511.35MB)?

======== update ========

Another phenomenon I just found. When the ""requested bytes"" of Conv2D is large (in my case &gt;4GB), the CPU execution time is long (about 400~500ms). When the ""requested bytes"" is small (in my case 1.5GB), the CPU execution time is short (about 15ms). I guess the CPU execution time of Conv2D is related with the memory consumption. However I don't why in different batch (my_session.run), Conv2D uses a different amount of ""requested bytes."" The tensor on which Conv2D is applied has almost the same size across different batches.
","Though I don't see your whole graph, but I assume that you feed data to the feed_dict continuously.
So each time the tensors are evaluated, they take the value of the next element in the underlying dataset. This takes time from CPU as well.
There is possibility to feed data directly from GPU memory if you have enough for holding data in it via tf.Tensor object, see documentation:


  If all of your input data fit in memory, the simplest way to create a
  Dataset from them is to convert them to tf.Tensor objects and use
  Dataset.from_tensor_slices().


An example from the corresponding section of tensorflow documentation:

# Load the training data into two NumPy arrays, for example using `np.load()`.
with np.load(""/var/data/training_data.npy"") as data:
  features = data[""features""]
  labels = data[""labels""]

# Assume that each row of `features` corresponds to the same row as `labels`.
assert features.shape[0] == labels.shape[0]

dataset = tf.data.Dataset.from_tensor_slices((features, labels))



  Note that the above code snippet will embed the features and labels
  arrays in your TensorFlow graph as tf.constant() operations. This
  works well for a small dataset, but wastes memory---because the
  contents of the array will be copied multiple times---and can run into
  the 2GB limit for the tf.GraphDef protocol buffer.


But this is not the case. So based of the information you provided, I think the CPU consumption is mainly due (or fully) to the data feeding operation to to the this graph's next input.
",,,false,
https://stackoverflow.com/questions/47636980,false,The behavior observed is likely due to the specific implementation of the multi-GPU code in Keras and not an issue with the API itself.,,,,,,,multiple GPUs keras weird speedup,"I did implement a similar code like the multi GPU code from keras 
(multiGPU tutorial). When running this on a Server with 2 GPUs I have the following training times per epoch:


showing Keras only one GPU and setting variable gpus = 1 (only use one GPU), one epoch = 32s
showing Keras two GPUs, and gpus = 1, one epoch = 31 s
showing Keras two GPUs, and gpus = 2, one epoch = 37 s


the output looks a bit strange, while initializing the code seems to create multiple Tensorflow devices per GPU, I'm not sure if this is the correct behavior. But the most other examples I saw had just one such line per GPU.

first test (one GPU shown, gpus = 1):

2017-12-04 14:54:04.071549: I tensorflow/core/common_runtime/gpu/gpu_device.cc:955] Found device 0 with properties: 
name: Tesla P100-PCIE-16GB
major: 6 minor: 0 memoryClockRate (GHz) 1.3285
pciBusID 0000:82:00.0
Total memory: 15.93GiB
Free memory: 15.64GiB
2017-12-04 14:54:04.071597: I tensorflow/core/common_runtime/gpu/gpu_device.cc:976] DMA: 0 
2017-12-04 14:54:04.071605: I tensorflow/core/common_runtime/gpu/gpu_device.cc:986] 0:   Y 
2017-12-04 14:54:04.071619: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1045] Creating TensorFlow device (/gpu:0) -&gt; (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0)
2017-12-04 14:54:21.531654: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1045] Creating TensorFlow device (/gpu:0) -&gt; (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0)


second test (2 GPU shown, gpus = 1):

2017-12-04 14:48:24.881733: I tensorflow/core/common_runtime/gpu/gpu_device.cc:955] Found device 1 with properties: 
...(same as earlier)
2017-12-04 14:48:24.882924: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1045] Creating TensorFlow device (/gpu:0) -&gt; (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0)
2017-12-04 14:48:24.882931: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1045] Creating TensorFlow device (/gpu:1) -&gt; (device: 1, name: Tesla P100-PCIE-16GB, pci bus id: 0000:83:00.0)
2017-12-04 14:48:42.353807: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1045] Creating TensorFlow device (/gpu:0) -&gt; (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0)
2017-12-04 14:48:42.353851: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1045] Creating TensorFlow device (/gpu:1) -&gt; (device: 1, name: Tesla P100-PCIE-16GB, pci bus id: 0000:83:00.0)


and weirdly for example 3 (gpus = 2):

2017-12-04 14:41:35.906828: I tensorflow/core/common_runtime/gpu/gpu_device.cc:955] Found device 1 with properties:
...(same as earlier)
2017-12-04 14:41:35.907996: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1045] Creating TensorFlow device (/gpu:0) -&gt; (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0)
2017-12-04 14:41:35.908002: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1045] Creating TensorFlow device (/gpu:1) -&gt; (device: 1, name: Tesla P100-PCIE-16GB, pci bus id: 0000:83:00.0)
2017-12-04 14:41:52.944335: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1045] Creating TensorFlow device (/gpu:0) -&gt; (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0)
2017-12-04 14:41:52.944377: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1045] Creating TensorFlow device (/gpu:1) -&gt; (device: 1, name: Tesla P100-PCIE-16GB, pci bus id: 0000:83:00.0)
2017-12-04 14:41:53.709812: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1045] Creating TensorFlow device (/gpu:0) -&gt; (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0)
2017-12-04 14:41:53.709838: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1045] Creating TensorFlow device (/gpu:1) -&gt; (device: 1, name: Tesla P100-PCIE-16GB, pci bus id: 0000:83:00.0)


the code:

LSTM = keras.layers.CuDNNLSTM
model.add(LSTM(knots, input_shape=(timesteps, X_train.shape[-1]), return_sequences=True))
model.add(LSTM(knots))
model.add(Dense(3, activation='softmax'))

if gpus&gt;=2:
    model_basic = model
    with tf.device(""/cpu:0""):
        model = model_basic
    parallel_model = multi_gpu_model(model, gpus=gpus)
    model = parallel_model
model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])
hist = model.fit(myParameter)


Is this a typical behavior? What is wrong with my code that the multiple devices per GPU are created. Thanks in advance. 
",,,,false,
https://stackoverflow.com/questions/70335600,true,The issue meets the criteria for deeper analysis as it involves unexpected behavior of the API under specific runtime conditions.,TensorFlow,tf.image.rgb_to_grayscale,"When applying tf.image.rgb_to_grayscale on a segmentation mask (which typically has a single channel), TensorFlow raises a ""dimensions must be equal"" error. This is because the API expects an input tensor with three channels (RGB), but the input mask only has one channel.","The tf.image.rgb_to_grayscale is applied to a tensor with a shape that ends in 3 (i.e., an RGB image).","The issue is triggered when tf.image.rgb_to_grayscale is applied to a tensor with a shape that does not end in 3 (i.e., not an RGB image).",This issue might be challenging to detect for users unfamiliar with the specific requirements of the tf.image.rgb_to_grayscale function or those who assume that it can handle inputs with any number of channels.,Why this model can&#39;t overfit one example?,"I am practicing conv1D on TensorFlow 2.7, and I am checking a decoder I developed by checking if it will overfit one example. The model doesn't learn when trained on only one example and can't overfit this one example. I want to understand this strange behavior, please. This is the link to the notebook on colab Notebook.
import tensorflow as tf
from tensorflow.keras.layers import Input, Conv1D, Dense, BatchNormalization 
from tensorflow.keras.layers import ReLU, MaxPool1D, GlobalMaxPool1D
from tensorflow.keras import Model
import numpy as np

def Decoder():
    inputs = Input(shape=(68, 3), name='Input_Tensor')

    # First hidden layer
    conv1 = Conv1D(filters=64, kernel_size=1, name='Conv1D_1')(inputs)
    bn1 = BatchNormalization(name='BN_1')(conv1)
    relu1 = ReLU(name='ReLU_1')(bn1)
      
    # Second hidden layer
    conv2 = Conv1D(filters=64, kernel_size=1, name='Conv1D_2')(relu1)
    bn2 = BatchNormalization(name='BN_2')(conv2)
    relu2 = ReLU(name='ReLU_2')(bn2)

    # Third hidden layer
    conv3 = Conv1D(filters=64, kernel_size=1, name='Conv1D_3')(relu2)
    bn3 = BatchNormalization(name='BN_3')(conv3)
    relu3 = ReLU(name='ReLU_3')(bn3)

    # Fourth hidden layer
    conv4 = Conv1D(filters=128, kernel_size=1, name='Conv1D_4')(relu3)
    bn4 = BatchNormalization(name='BN_4')(conv4)
    relu4 = ReLU(name='ReLU_4')(bn4)

    # Fifth hidden layer
    conv5 = Conv1D(filters=1024, kernel_size=1, name='Conv1D_5')(relu4)
    bn5 = BatchNormalization(name='BN_5')(conv5)
    relu5 = ReLU(name='ReLU_5')(bn5)

    global_features = GlobalMaxPool1D(name='GlobalMaxPool1D')(relu5)
    global_features = tf.keras.layers.Reshape((1, -1))(global_features)

    conv6 = Conv1D(filters=12, kernel_size=1, name='Conv1D_6')(global_features)
    bn6 = BatchNormalization(name='BN_6')(conv6)
    outputs = ReLU(name='ReLU_6')(bn6)
    model = Model(inputs=[inputs], outputs=[outputs], name='Decoder')
    return model

model = Decoder()
model.summary()

optimizer = tf.keras.optimizers.Adam(learning_rate=0.1)
losses = tf.keras.losses.MeanSquaredError()
model.compile(optimizer=optimizer, loss=losses)

n = 1
X = np.random.rand(n, 68, 3)
y = np.random.rand(n, 1, 12)

model.fit(x=X,y=y, verbose=1, epochs=30)

","I think the problem here is, that you have no basis to learn anything, so you can't overfit. In every epoch you have just one example which is used to adapt the weights of the network. So there is not enough time to adapt the weights for overfitting here.
So to get the result of overfitting you want to have the same data multiple times inside your training dataset so the weights can change enought to overfitt because you only change them just one small step per epoch.
A deeper look into the back propagation might help you to get a better understanding of the concept. Click
I took th liberty to adapt your notebook and enhanced the dataset as following:
n = 1
X = np.random.rand(n, 68, 3)
y = np.random.rand(n, 1, 12)

for i in range(0,10):
  X=np.append(X,X,axis = 0)
  y=np.append(y,y,axis = 0)
 

And the output would be:
",,,false,
https://stackoverflow.com/questions/22591174/pandas-multiple-conditions-while-indexing-data-frame-unexpected-behavior,false,The behavior is due to a misunderstanding of logical operators and conditions in pandas rather than a problem with the API itself.,,,,,,,,,,,,false,
https://stackoverflow.com/questions/38384531,true,The issue meets the criteria for deeper analysis as it involves unexpected behavior of the API under specific runtime conditions.,TensorFlow,tf.nn.max_pool,"When using tf.nn.max_pool with a kernel size of 2 and reducing the size of the data with max pooling instead of a convolution layer, the memory usage grows to 100% and the kernel dies. This behavior is unexpected and indicates a potential problem with the API.",The tf.nn.max_pool API is expected to reduce the size of the input tensor using max pooling with a specified kernel size.,The issue is triggered when tf.nn.max_pool is used with a kernel size of 2 and the input tensor is reduced in size using max pooling.,"This issue might be challenging to detect during development and testing as it involves memory usage and kernel termination, which are not directly related to the API function itself.",TensorFlow - nn.max_pooling increases memory usage enormously,"I try to create a simple convolution neural network in TensorFlow. Everything seems fine when I run my code below. I run it in Spyder IDE and monitor memory usage - it grows to 64-65% on my laptop and not goes any further.

batch_size = 16
patch_size = 5
depth = 16
num_hidden = 64

graph = tf.Graph()

with graph.as_default():

  # Input data.
  tf_train_dataset = tf.placeholder(
    tf.float32, shape=(batch_size, image_size, image_size, num_channels))
  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))
  tf_valid_dataset = tf.constant(valid_dataset)
  tf_test_dataset = tf.constant(test_dataset)

  # Variables.
  layer1_weights = tf.Variable(tf.truncated_normal(
      [patch_size, patch_size, num_channels, depth], stddev=0.1))
  layer1_biases = tf.Variable(tf.zeros([depth]))
  layer2_weights = tf.Variable(tf.truncated_normal(
      [patch_size, patch_size, depth, depth], stddev=0.1))
  layer2_biases = tf.Variable(tf.constant(1.0, shape=[depth]))
  layer3_weights = tf.Variable(tf.truncated_normal(
      [image_size // 4 * image_size // 4 * depth, num_hidden], stddev=0.1))
  layer3_biases = tf.Variable(tf.constant(1.0, shape=[num_hidden]))
  layer4_weights = tf.Variable(tf.truncated_normal(
      [num_hidden, num_labels], stddev=0.1))
  layer4_biases = tf.Variable(tf.constant(1.0, shape=[num_labels]))

  # Model.
  #Now instead of using strides = 2 for convolutions we will use maxpooling with
  #same convolution sizes
  def model(data):
    conv = tf.nn.conv2d(data, layer1_weights, [1, 2, 2, 1], padding='SAME')
    hidden = tf.nn.relu(conv + layer1_biases)
    conv = tf.nn.conv2d(hidden, layer2_weights, [1, 2, 2, 1], padding='SAME')
    hidden = tf.nn.relu(conv + layer2_biases)
    shape = hidden.get_shape().as_list()
    reshape = tf.reshape(hidden, [shape[0], shape[1] * shape[2] * shape[3]])
    hidden = tf.nn.relu(tf.matmul(reshape, layer3_weights) + layer3_biases)
    return tf.matmul(hidden, layer4_weights) + layer4_biases

  # Training computation.
  logits = model(tf_train_dataset)
  loss = tf.reduce_mean(
    tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels))

  # Optimizer.
  optimizer = tf.train.GradientDescentOptimizer(0.05).minimize(loss)

  # Predictions for the training, validation, and test data.
  train_prediction = tf.nn.softmax(logits)
  valid_prediction = tf.nn.softmax(model(tf_valid_dataset))
  test_prediction = tf.nn.softmax(model(tf_test_dataset))

num_steps = 1001

with tf.Session(graph=graph) as session:
  tf.initialize_all_variables().run()
  print('Initialized')
  for step in range(num_steps):
    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)
    batch_data = train_dataset[offset:(offset + batch_size), :, :, :]
    batch_labels = train_labels[offset:(offset + batch_size), :]
    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}
    _, l, predictions = session.run(
      [optimizer, loss, train_prediction], feed_dict=feed_dict)
    if (step % 50 == 0):
      print('Minibatch loss at step %d: %f' % (step, l))
      print('Minibatch accuracy: %.1f%%' % accuracy(predictions, batch_labels))
      print('Validation accuracy: %.1f%%' % accuracy(
        valid_prediction.eval(), valid_labels))
  print('Test accuracy: %.1f%%' % accuracy(test_prediction.eval(), test_labels))


Ok, after that I try to introduce a maxpooling with kernel of 2, and reduce the size of data with maxpooling instead of conv layer. It looks like below:

conv = tf.nn.conv2d(data, layer1_weights, [1, 1, 1, 1], padding='SAME')
    maxpool = tf.nn.max_pool(conv, [1, 2, 2, 1], [1, 2, 2, 1], padding='SAME')
    hidden = tf.nn.relu(maxpool + layer1_biases)
    conv = tf.nn.conv2d(hidden, layer2_weights, [1, 2, 2, 1], padding='SAME')


Everything else remain same. But when I do this (notice, I introduced only one maxpooling layer) the memory usage grows to 100% and my iPython kernel just dies. Any ideas of such strange behavior, why the memory usage gets so big? Am I doing something wrong? Any suggestions on how to decrease memory usage?
",,,,false,
https://stackoverflow.com/questions/45083474,false,The issue is related to variable updates in TensorFlow scan and does not involve unexpected behavior or unpredictable API failures.,,,,,,,How to update vaiables in tensorflow scan,"I would like to make a variable in tensorflow, and then update it in tf.scan. First I tried something like this:

import tensorflow as tf

with tf.variable_scope('foo'):
    tf.get_variable('bar', initializer=tf.zeros([1.0]))

def repeat_me(last, current):
    with tf.variable_scope('foo', reuse=True):
        bar = tf.get_variable('bar')
        bar.assign_add(tf.constant([1.0]))
    return bar
output = tf.scan(repeat_me, tf.range(5), initializer=tf.constant([1.0]))

with tf.Session() as sess: 
    init_op = tf.global_variables_initializer()
    sess.run(init_op)
    out = sess.run(output)
    print(out)
    with tf.variable_scope('foo', reuse=True):
        print(tf.get_variable('bar').eval())


This does not seem to update the variable with the name of 'bar'.

[[ 0.]
 [ 0.]
 [ 0.]
 [ 0.]
 [ 0.]]
[ 0.]


What is strange to me is the following modification of the ""repeat_me"" function changes the behavior.

def repeat_me(last, current):
    with tf.variable_scope('foo', reuse=True):
        bar = tf.get_variable('bar')
        b = bar.assign_add(tf.constant([1.0]))
    return b


Then, the script spits out this:

[[ 5.]
 [ 5.]
 [ 5.]
 [ 5.]
 [ 5.]]
[ 5.]


Could anyone explain the difference?
",,,,false,
https://stackoverflow.com/questions/56695299,true,The issue meets the criteria for deeper analysis as it involves unexpected behavior of the API under specific runtime conditions.,Keras,model.predict_generator,The use of model.predict_generator with an ImageDataGenerator and the resulting inconsistency in accuracy and predictions indicates a potential problem with the API.,The model.predict_generator API is expected to generate predictions for a given dataset using a generator.,The issue is triggered when model.predict_generator is used with an ImageDataGenerator and results in inconsistent accuracy and predictions.,"This issue might be challenging to detect during development and testing as it involves the interaction between model.predict_generator, an ImageDataGenerator, and the specific dataset used.",How should I use mode.predict_generator to evaluate model performance in a Confusion Matrix?,"I am trying to evaluate a transfer learning model in the common dogs and cats filtered dataset using confusion matrix. I have based the code in the transfer learning tutorial of tensorflow. The accuracy graphs for training show an accuracy above 90%. 

However, using generators to get the true labes and model.predict_generator to get the prediction array throws inconsistent results. First, accuracy is not stable, if you run a second time the prediction it changes values. Second, the prediction that I get by using  model.predict_generator seems to be wrong compared to model.predict on individual instance.

In order to test quickly the confusion matrix based on the ImageDataGenerator I downloaded 5 images of cats and 5 images of dogs. Then I created another generator from the folder and checked that labels and classes would be the same as training.

Two Strange Behaviors
After that I just used sklearn metrics confusion matrix to evaluate a prediction using model.predict_generator and the labels that I get from the generator as true labels.

At first run I got a 0.9 accuracy and say cheers!. however, if I try a second time the model.predict_generator and it throws other values for array output and accuracy dropps to 0.5. After that it does not change anymore.... What result is correct? Why does it change?

I have been noticing that you have to run twice to get a final result, but the result obtained is wrong. I wrote some code to test each image individually and I got no wrong in prediction. So what am I doing wrong? or are generators not appliable to this situation. This is a bit confusing

Code can be chacked at my github repository and can be used in google colaboratory to be run if you have no gpu. In fact in my little toshiba satellite runs well with a nvidia gpu of just 2 gb and 300 cuda

complete code at my git

The code is organized as jupyter notebook however here I add the code
Transfer Learning is based on https://www.tensorflow.org/tutorials/images/transfer_learning

To create the generator:

test_base_dir = '.'
test_dir = os.path.join( test_base_dir, 'test')
test_datagen_2 = ImageDataGenerator( rescale = 1.0/255. )
test_generator = test_datagen_2.flow_from_directory( test_dir,
                                                     batch_size  = 1,
                                                     class_mode  = binary', 
                                                     target_size = (image_size, image_size))


And for prediction:

   filenames = test_generator.filenames
   nb_samples = len(filenames)
   y_predict = model.predict_generator(test_generator,steps = 
   nb_samples)
   y_predict


Which I round using numpy to finally use confusion matrix metric


from sklearn.metrics  import confusion_matrix
cm = confusion_matrix(y_true=test_generator.labels, y_pred=y_predict_rounded)
cm


The manual verification is instead:

def prediction(path_img):
img = image.load_img(path_img, target_size=(150,150))
x = image.img_to_array(img)
x = x/255.
x = np.expand_dims(x, axis=0)
classes = model.predict(x)
plt.imshow(img)
if classes &gt; 0.5:
    print(path_img.split('/')[-1]+' is a dog')
else:
     print(path_img.split('/')[-1]+' is a cat')   
return classes


Which I use in the following way:

y_pred_m = []
files=[]
for filename in os.listdir(test_dir):
    file = test_dir+'/'+filename
    for item in os.listdir(file):
        file2 = file+'/'+item
        if file2.split('.')[-1]=='jpg':
            files.append(file2)


And prediction goes:

prediction_array = [prediction(img) for img in files]

np.round(prediction_array, decimals=0)


Expected resutls should be to have a confusion matrix with an accuracy level similar to training. Since the verification of each example individually seems to have no error in prediction, however model.predict_generate seems to go wrong.
","The problem was that as default _flow_from_directory_ uses shuffle = True. Predictions are correct if shuffle goes to False. However, using validation dataset to evaluate training seems to do right even though shuffle is True. I have updated git for these changes to be populated

# Flow validation images in batches of 20 using test_datagen generator
test_generator =  test_datagen_2.flow_from_directory( test_dir,
                                                  batch_size  = 1,
                                                  class_mode  = 'binary', 
                                                  target_size = (image_size, 
image_size),
                                                  shuffle = False)

",,,false,
https://stackoverflow.com/questions/57498998,false,"The difference in behavior between Keras versions is not related to unexpected failures or unpredictable behaviors of the API, but rather a change in parameter names and activation functions.",,,,,,,Strange behavior of keras v1.2.2 vs. keras v2+ (HUGE differences in accuracy),"Today I've ran into some very strange behavior of Keras. When I try to do a classification run on the iris-dataset with a simple model, keras version 1.2.2 gives me +- 95% accuracy, whereas a keras version of 2.0+ predicts the same class for every training example (leading to an accuracy of +- 35%, as there are three types of iris).  The only thing that makes my model predict +-95% accuracy is downgrading keras to a version below 2.0:

I think it is a problem with Keras, as I have tried the following things, all do not make a difference;


Switching activation function in the last layer (from Sigmoid to softmax).
Switching backend (Theano and Tensorflow both give roughly same performance).
Using a random seed.
Varying the number of neurons in the hidden layer (I only have 1 hidden layer in this simple model).
Switching loss-functions.


As the model is very simple and it runs on it's own (You just need the easy-to-obtain iris.csv dataset) I decided to include the entire code; 

import pandas as pd
from keras.models import Sequential
from keras.layers import Dense
from keras.wrappers.scikit_learn import KerasClassifier
from keras.utils import np_utils
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import KFold
from sklearn.preprocessing import LabelEncoder

#Load data
data_frame = pd.read_csv(""iris.csv"", header=None)
data_set = data_frame.values
X = data_set[:, 0:4].astype(float)
Y = data_set[:, 4]

#Encode class values as integers
encoder = LabelEncoder()
encoder.fit(Y)
encoded_Y = encoder.transform(Y)

# convert integers to dummy variables (i.e. one hot encoded)
dummy_y = np_utils.to_categorical(encoded_Y)

def baseline_model():
    #Create &amp; Compile model
    model = Sequential()
    model.add(Dense(8, input_dim=4, init='normal', activation='relu'))
    model.add(Dense(3, init='normal', activation='sigmoid'))
    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
    return model

#Create Wrapper For Neural Network Model For Use in scikit-learn
estimator = KerasClassifier(build_fn=baseline_model, nb_epoch=200, batch_size=5, verbose=0)

#Create kfolds-cross validation
kfold = KFold(n_splits=10, shuffle=True)

#Evaluate our model (Estimator) on dataset (X and dummy_y) using a 10-fold cross-validation procedure (kfold).
results = cross_val_score(estimator, X, dummy_y, cv=kfold)
print(""Accuracy: {:2f}% ({:2f}%)"".format(results.mean()*100, results.std()*100))


if anyone wants to replicate the error here are the dependencies I used to observe the problem:

numpy=1.16.4
pandas=0.25.0
sk-learn=0.21.2
theano=1.0.4
tensorflow=1.14.0

","In Keras 2.0, many parameters changed names, there is compatibility layer to keep things working, but somehow it did not apply when using KerasClassifier.

In this part of the code:

estimator = KerasClassifier(build_fn=baseline_model, nb_epoch=200, batch_size=5, verbose=0)


You are using the old name nb_epoch instead of the modern name of epochs. The default value is epochs=1, meaning that your model was only being trained for one epoch, producing very low quality predictions.

Also note that here:

model.add(Dense(3, init='normal', activation='sigmoid'))


You should be using a softmax activation instead of sigmoid, as you are using the categorical cross-entropy loss:

model.add(Dense(3, init='normal', activation='softmax'))

","I've managed to isolate the issue, if you change nb_epoch to epochs, (All else being exactly equal) the model predicts very good again, in keras 2 as well. I don't know if this is intended behavior or a bug.
",,false,
https://stackoverflow.com/questions/47897199,true,"The issue involves the interaction between TensorFlow-GPU and Python's multiprocessing module, which leads to crashes when multiple ConcurrentLoader instances are initialized. This behavior is specific to the combination of TensorFlow-GPU 1.4.1 and CUDA 9, as it works fine with TensorFlow-CPU and earlier versions of TensorFlow-GPU. The issue is related to the initialization of CUDA and cuDNN handles within the multiprocessing context, which causes conflicts and errors.",TensorFlow,TensorFlow-GPU + Python multiprocessing,"When running the provided code with TensorFlow-GPU 1.4.1 and CUDA 9, crashes occur when multiple ConcurrentLoader instances are initialized. The error message indicates failures in creating cublas and cudnn handles, as well as internal errors in cudnn. The issue is specific to the combination of TensorFlow-GPU 1.4.1 and CUDA 9, as it works fine with TensorFlow-CPU and earlier versions of TensorFlow-GPU.",The TensorFlow-GPU + Python multiprocessing combination works as expected with TensorFlow-CPU and TensorFlow-GPU versions up to 1.3.0 using CUDA 8.0.,"The issue is triggered when using TensorFlow-GPU 1.4.1 and CUDA 9, and initializing multiple ConcurrentLoader instances within a multiprocessing context.","This issue might be challenging to detect during development and testing, as it is specific to the combination of TensorFlow-GPU 1.4.1 and CUDA 9, and only occurs when using the multiprocessing module in a certain way.",Bug when using TensorFlow-GPU + Python multiprocessing?,"I have noticed a strange behavior when I use TensorFlow-GPU + Python multiprocessing.

I have implemented a DCGAN with some customizations and my own dataset. Since I am conditioning the DCGAN to certain features, I have training data and also test data for evaluation.

Due to the size of my datasets, I have written data loaders that run concurrently and preload into a queue using Python's multiprocessing.

The structure of the code roughly looks like this:

class ConcurrentLoader:
    def __init__(self, dataset):
        ...

class DCGAN
     ...

net = DCGAN()
training_data = ConcurrentLoader(path_to_training_data)
test_data = ConcurrentLoader(path_to_test_data)


This code runs fine on TensorFlow-CPU and on TensorFlow-GPU &lt;= 1.3.0 using CUDA 8.0, but when I run the exact same code with TensorFlow-GPU 1.4.1 and CUDA 9 (latest releases of TF &amp; CUDA as of Dec 2017) it crashes:

2017-12-20 01:15:39.524761: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2017-12-20 01:15:39.527795: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2017-12-20 01:15:39.529548: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2017-12-20 01:15:39.535341: E tensorflow/stream_executor/cuda/cuda_dnn.cc:385] could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR
2017-12-20 01:15:39.535383: E tensorflow/stream_executor/cuda/cuda_dnn.cc:352] could not destroy cudnn handle: CUDNN_STATUS_BAD_PARAM
2017-12-20 01:15:39.535397: F tensorflow/core/kernels/conv_ops.cc:667] Check failed: stream-&gt;parent()-&gt;GetConvolveAlgorithms( conv_parameters.ShouldIncludeWinogradNonfusedAlgo&lt;T&gt;(), &amp;algorithms) 
[1]    32299 abort (core dumped)  python dcgan.py --mode train --save_path ~/tf_run_dir/test --epochs 1


What really confuses me is that if I just remove test_data the error does not occur. Thus, for some strange reason, TensorFlow-GPU 1.4.1 &amp; CUDA 9 work with just a single ConcurrentLoader, but crash when multiple loaders are initialized.

Even more interesting is that (after the exception) I have to manually shut down the python processes, because the GPU's VRAM, the system's RAM and even the python processes stay alive after the script crashes.

Furthermore, it has to have some weird connection to Python's multiprocessing module, because when I implement the same model in Keras (using TF backend!) the code also runs just fine, with 2 concurrent loaders. I guess Keras is somehow creating an layer of abstraction in between that keeps TF from crashing.

Where could I possibly have screwed up with the multiprocessing module that it causes crashes like this one?

These are the parts of the code that use multiprocessing inside the ConcurrentLoader:

def __init__(self, dataset):
    ...
    self._q = mp.Queue(64)
    self._file_cycler = cycle(img_files)
    self._worker = mp.Process(target=self._worker_func, daemon=True)
    self._worker.start()

def _worker_func(self):
    while True:
        ... # gets next filepaths from self._file_cycler
        buffer = list()
        for im_path in paths:
            ... # uses OpenCV to load each image &amp; puts it into the buffer
        self._q.put(np.array(buffer).astype(np.float32))


...and this is it.

Where have I written ""unstable"" or ""non-pythonic"" multiprocessing code? I thought daemon=True should ensure that every process gets killed as soon as the main process dies? Unfortunately, this is not the case for this specific error.

Did I misuse the default multiprocessing.Process or multiprocessing.Queue here? I thought simply writing a class where I store batches of images inside a Queue and make it accessible through methods / instance variables should be just fine.
",,,,false,
https://stackoverflow.com/questions/48458509,true,"The issue involves the behavior of the tf.metrics.mean_absolute_error function in TensorFlow when dividing by zero. The API does not handle the division by zero case as expected, resulting in unexpected behavior and incorrect values for mean absolute error.",TensorFlow,tf.metrics.mean_absolute_error,"When using tf.metrics.mean_absolute_error to calculate the mean absolute error between two tensors, if the denominator tensor contains zeros, the API does not handle the division by zero case correctly. Instead of returning an error or NaN values, it produces incorrect values for mean absolute error.",The tf.metrics.mean_absolute_error function works as expected when the denominator tensor does not contain zeros.,The issue is triggered when using tf.metrics.mean_absolute_error with a denominator tensor that contains zeros.,"This issue might be challenging to detect during development and testing, as it requires specific input conditions (division by zero) to trigger the unexpected behavior.",Strange behavior of tensorflow when dividing by 0 in tf.metrics.mean_absolute_error,"So I do not know if is a bug or the problem is in my code, but I am trying to understand what is happening. 
when I run the model and got to estimate the accuracy using Mean Relative Error. I know in my validation data I have 0s so I was expecting to get some error or some inf. However this is not the case.
This is my code:

X_test_norm = preprocessing.scale(X_test)
predictions = sess.run(pred, feed_dict={x: X_test_norm})
prediction = tf.convert_to_tensor(predictions)
expectation = tf.cast(Y_test, tf.float32)

MANUAL_MRE = tf.reduce_mean(tf.abs((Y_test - tf.transpose(predictions)) / Y_test))
MAE_op, MAE = tf.metrics.mean_absolute_error(expectation, prediction)
MRE_op, MRE = tf.metrics.mean_relative_error(expectation, prediction, expectation)
tf.local_variables_initializer().run()

print(""MANUAL_MRE: %4f"" % sess.run(MANUAL_MRE))
print(""MRE: %4f"" % sess.run(MRE))
print(""MAE: %4f"" % sess.run(MAE))


This is the output:

MANUAL_MRE:  inf

MRE: 1.603528

MAE: 76.489990


When I run it on a data that has values bigger than 0, my MANUAL_MRE and MRE values are the same like it should be. I checked the documentation of TF and the first case does not make sense. 

Can someone tell me where I am wrong or I just found a bug/ new feature. 
",,,,false,
https://stackoverflow.com/questions/48997844,true,"The issue involves the behavior of the Tensorflow Estimator.predict_scores function when using the Dataset API in the input function. The function does not yield the correct number of predictions, producing fewer predictions than expected.",TensorFlow,Tensorflow Estimator.predict_scores,"When using the Tensorflow Estimator.predict_scores function with an input function that utilizes the Dataset API, the function does not yield the correct number of predictions. It produces fewer predictions than expected, even when the input data contains a larger number of instances.","The Tensorflow Estimator.predict_scores function works as expected when using other input functions, such as tf.estimator.inputs.numpy_input_fn.",The issue is triggered when using the Dataset API in the input function for Tensorflow Estimator.predict_scores.,"This issue might be challenging to detect during development and testing, as it requires specific conditions (using the Dataset API in the input function) to trigger the incorrect number of predictions.",Tensorflow Estimator.predict_scores not yielding the correct number of predictions when using the Dataset API in the input function,"I am using tensorflow 1.5 and I am puzzled by this strange behavior that I can't explain.
I produced a minimal example:

import tensorflow as tf
import numpy as np


def input_function(x, y, batch_size=128, shuffle=True, n_epochs=None):
    data_set = tf.data.Dataset.from_tensor_slices({""x"": x, ""y"": y})
    if shuffle:
        data_set = data_set.shuffle(buffer_size=1024, seed=None, reshuffle_each_iteration=True)
    data_set = data_set.batch(batch_size)
    data_set = data_set.repeat(n_epochs)
    iterator = data_set.make_one_shot_iterator()
    example = iterator.get_next()
    return {""features"": example[""x""]}, example[""y""]


def main():
    n_samples = 256
    n_features = 16
    n_labels = 1

    x = np.random.rand(n_samples, n_features).astype(np.float32)
    y = np.random.rand(n_samples, n_labels).astype(np.float32)

    feature_column = tf.contrib.layers.real_valued_column(column_name='features', dimension=n_features)
    estimator = tf.contrib.learn.DNNRegressor([10], [feature_column], optimizer=tf.train.AdamOptimizer())

    estimator.fit(input_fn=lambda: input_function(x, y, batch_size=128, shuffle=True, n_epochs=32))
    pred = estimator.predict_scores(input_fn=lambda: input_function(x, y, batch_size=16, shuffle=False, n_epochs=1))
    print(""len(pred) = {} (should be {})"".format(len(list(pred)), n_samples))


if __name__ == '__main__':
    main()


In this example, the call to 'fit' seems to be working fine (I'm not sure though) but the call to 'predict_scores' only produces batch_size (=16) predictions instead of n_samples (=256). What I am doing wrong ?
This problem disappears if I use the tf.esimator.inputs.numpy_input_fn although eventually I'll have to use an input function that uses a TFRecordDataset to read a large amount of training data from tfrecord files, similarly to what is showed here:
https://www.tensorflow.org/programmers_guide/datasets#using_high-level_apis
Any help would be really appreciated.
",,,,false,
https://stackoverflow.com/questions/53363060,false,The issue is not related to an API exhibiting unexpected failures or unpredictable behaviors under specific runtime conditions. It appears to be a question about the behavior of the trained model in Tensorflow Object Detection API when predicting instances of the same object in an image.,,,,,,,keras unable to call model.predict_classes for multiple times,"def predictOne(imgPath):

    model = load_model(""withImageMagic.h5"")
    image = read_image(imgPath)
    test_sample = preprocess(image)
    predicted_class = model.predict_classes(([test_sample]))
    return predicted_class


I have already trained a model. In this function, I load my model, read a new image, do some preprocessing and finally predict its label. 

When I run my main.py file, this function is called and everything goes smoothly. However, after a couple of seconds, this function will be called again with another image and I get this error:


'Cannot interpret feed_dict key as Tensor: ' + e.args[0])

  
  TypeError: Cannot interpret feed_dict key as Tensor: Tensor Tensor(""Placeholder:0"", shape=(5, 5, 1, 32), dtype=float32) is not an element of this graph.


It's very strange that the function only works the first time. I tested multiple images and got the same behavior.

Windows 10 - tensorflow-gpu with keras
","Try loading model from file outside the function, and give the model object as argument to the function def predictOne(imgPath, model). This will also be much faster, since the weights don't need to be loaded from disk every time a prediction is needed. 

If you want to keep loading model inside the function, import the backend:

from keras import backend as K


and then 

K.clear_session() 


before loading the model.
","class one_model:
    session = None
    graph = None 
    loadModel = None
    __instance = None
    @staticmethod
    def getInstance(modelPath):
        """""" Static access method. """"""
        if one_model.__instance == None:
            one_model.__instance = one_model(modelPath)
        return one_model.__instance
        
    def __init__(self, modelPath):
        self.modelPath = modelPath
        self.session = tf.Session(graph=tf.Graph())
        self.loadOneModel()
            
    def loadOneModel(self):
        try:
            with self.session.graph.as_default():
                K.set_session(self.session)
                self.loadModel = keras.models.load_model(self.modelPath)               
        except Exception as e:
            logging.error(str(e))
            print(str(e))
                        
    def getPredictionOne(self, input_file_path): 
        #Predict the data once the model is loaded
        if self.loadModel is not None and self.session is not None: 
            try:
                image = load_img(input_file_path, target_size=inputShape)
                image = img_to_array(image)
                image = np.expand_dims(image, axis=0)
                image = preprocess(image)
                with self.session.graph.as_default():
                    K.set_session(self.session)
                    preds = self.loadModel.predict(image)
                    return preds
            except Exception as e:
                logging.error(str(e))
        
        return -1


if __name__== ""__main__"": 
    #First Model 
    data = web.input()
        fileapth = data.imagefilepath  
        modelfilepath = data.modelfilepath
        one_modelObj = one_model.getInstance(modelfilepath)        
        value = one_modelObj.getPredictionOne(fileapth) 

",,false,
https://stackoverflow.com/questions/56700022,false,The issue is not related to an API exhibiting unexpected failures or unpredictable behaviors under specific runtime conditions. It is a question about the behavior of Tensorflow Object Detection API when predicting instances of the same object in an image.,,,,,,,tensorflow-hub: Python import error when file is in a specific folder,"So I ran into this super strange issue. I am trying to run the example code of tensorflow-hub given on this webpage: https://www.tensorflow.org/hub
(I removed both !pip statements)

I want to use a small piece of python code for the machine learning stuff within my application which is otherwise written in go. However when I execute the example code in the project directory of my application it gives an import error.

So when I create a file tf-hub.py in ~/ and execute it, it runs, prints some warnings and the result:

$python3 tf-hub.py
WARNING: Logging before flag parsing goes to stderr.
W0621 10:35:21.367268 140170246772224 deprecation_wrapper.py:118] From tf-hub.py:4: The name tf.enable_eager_execution is deprecated. Please use tf.compat.v1.enable_eager_execution instead.

2019-06-21 10:35:21.407732: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2019-06-21 10:35:21.427007: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2793545000 Hz
2019-06-21 10:35:21.428085: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x563567fc7a90 executing computations on platform Host. Devices:
2019-06-21 10:35:21.428143: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): &lt;undefined&gt;, &lt;undefined&gt;
2019-06-21 10:35:21.505990: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1541] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.
2019-06-21 10:35:21.513612: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 498570752 exceeds 10% of system memory.
2019-06-21 10:35:22.109499: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 498570752 exceeds 10% of system memory.
(3, 128)



However when I copy the same file to the project folder of the go project I get the following import error:

$ python3 go/src/MyProject/tagger/imageClassifier/tf-hub.py 
Traceback (most recent call last):
  File ""/usr/local/lib/python3.7/site-packages/tensorflow_hub/tf_v1.py"", line 29, in &lt;module&gt;
    from tensorflow.compat.v1 import *  # pylint: disable=wildcard-import
ModuleNotFoundError: No module named 'tensorflow.compat'; 'tensorflow' is not a package

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""go/src/EmbeddingImageTagger/tagger/imageClassifier/tf-hub.py"", line 1, in &lt;module&gt;
    import tensorflow as tf
  File ""/home/***/go/src/EmbeddingImageTagger/tagger/imageClassifier/tensorflow.py"", line 2, in &lt;module&gt;
    import tensorflow_hub as hub
  File ""/usr/local/lib/python3.7/site-packages/tensorflow_hub/__init__.py"", line 30, in &lt;module&gt;
    from tensorflow_hub.estimator import LatestModuleExporter
  File ""/usr/local/lib/python3.7/site-packages/tensorflow_hub/estimator.py"", line 25, in &lt;module&gt;
    from tensorflow_hub import tf_utils
  File ""/usr/local/lib/python3.7/site-packages/tensorflow_hub/tf_utils.py"", line 28, in &lt;module&gt;
    from tensorflow_hub import tf_v1
  File ""/usr/local/lib/python3.7/site-packages/tensorflow_hub/tf_v1.py"", line 33, in &lt;module&gt;
    from tensorflow import add_to_collection
ImportError: cannot import name 'add_to_collection' from 'tensorflow' (/home/***/go/src/MyProject/tagger/imageClassifier/tensorflow.py)


I have to use tf-nightly, otherwise I run into this issue: https://github.com/tensorflow/hub/issues/289

I am using:


Fedora 29 64-bit
Python 3.7.2
tf-nightly 1.14.1.dev20190621
tensorflow-hub 0.4.0


Any ideas what could cause this weird behavior that the file runs in one directory, but not in another? I also tried to run both from the same terminal. Still it does not work.
","Oh my. So I found the culprit after I cleaned up the debug stuff and moved my development to my home folder.

The problem was that I had a python file called tensorflow.py because it was supposed to contain a classifier solution with tensorflow. Well it seems that this confuses python and python now imports this file when importing tensorflow.

I had to just rename the file and now it works. 
",,,false,
https://stackoverflow.com/questions/40879967,false,"The issue does not involve an API exhibiting unexpected failures or unpredictable behaviors under specific runtime conditions. The issue title suggests a question about using Batch Normalization correctly in Tensorflow, but the issue body is empty.",,,,,,,How to use Batch Normalization correctly in tensorflow?,,,,,false,
https://stackoverflow.com/questions/56700022,false,The issue is not related to an API exhibiting unexpected failures or unpredictable behaviors under specific runtime conditions. It is a question about the behavior of Tensorflow Object Detection API when predicting instances of the same object in an image.,,,,,,,tensorflow-hub: Python import error when file is in a specific folder,"So I ran into this super strange issue. I am trying to run the example code of tensorflow-hub given on this webpage: https://www.tensorflow.org/hub
(I removed both !pip statements)

I want to use a small piece of python code for the machine learning stuff within my application which is otherwise written in go. However when I execute the example code in the project directory of my application it gives an import error.

So when I create a file tf-hub.py in ~/ and execute it, it runs, prints some warnings and the result:

$python3 tf-hub.py
WARNING: Logging before flag parsing goes to stderr.
W0621 10:35:21.367268 140170246772224 deprecation_wrapper.py:118] From tf-hub.py:4: The name tf.enable_eager_execution is deprecated. Please use tf.compat.v1.enable_eager_execution instead.

2019-06-21 10:35:21.407732: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2019-06-21 10:35:21.427007: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2793545000 Hz
2019-06-21 10:35:21.428085: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x563567fc7a90 executing computations on platform Host. Devices:
2019-06-21 10:35:21.428143: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): &lt;undefined&gt;, &lt;undefined&gt;
2019-06-21 10:35:21.505990: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1541] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.
2019-06-21 10:35:21.513612: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 498570752 exceeds 10% of system memory.
2019-06-21 10:35:22.109499: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 498570752 exceeds 10% of system memory.
(3, 128)



However when I copy the same file to the project folder of the go project I get the following import error:

$ python3 go/src/MyProject/tagger/imageClassifier/tf-hub.py 
Traceback (most recent call last):
  File ""/usr/local/lib/python3.7/site-packages/tensorflow_hub/tf_v1.py"", line 29, in &lt;module&gt;
    from tensorflow.compat.v1 import *  # pylint: disable=wildcard-import
ModuleNotFoundError: No module named 'tensorflow.compat'; 'tensorflow' is not a package

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""go/src/EmbeddingImageTagger/tagger/imageClassifier/tf-hub.py"", line 1, in &lt;module&gt;
    import tensorflow as tf
  File ""/home/***/go/src/EmbeddingImageTagger/tagger/imageClassifier/tensorflow.py"", line 2, in &lt;module&gt;
    import tensorflow_hub as hub
  File ""/usr/local/lib/python3.7/site-packages/tensorflow_hub/__init__.py"", line 30, in &lt;module&gt;
    from tensorflow_hub.estimator import LatestModuleExporter
  File ""/usr/local/lib/python3.7/site-packages/tensorflow_hub/estimator.py"", line 25, in &lt;module&gt;
    from tensorflow_hub import tf_utils
  File ""/usr/local/lib/python3.7/site-packages/tensorflow_hub/tf_utils.py"", line 28, in &lt;module&gt;
    from tensorflow_hub import tf_v1
  File ""/usr/local/lib/python3.7/site-packages/tensorflow_hub/tf_v1.py"", line 33, in &lt;module&gt;
    from tensorflow import add_to_collection
ImportError: cannot import name 'add_to_collection' from 'tensorflow' (/home/***/go/src/MyProject/tagger/imageClassifier/tensorflow.py)


I have to use tf-nightly, otherwise I run into this issue: https://github.com/tensorflow/hub/issues/289

I am using:


Fedora 29 64-bit
Python 3.7.2
tf-nightly 1.14.1.dev20190621
tensorflow-hub 0.4.0


Any ideas what could cause this weird behavior that the file runs in one directory, but not in another? I also tried to run both from the same terminal. Still it does not work.
","Oh my. So I found the culprit after I cleaned up the debug stuff and moved my development to my home folder.

The problem was that I had a python file called tensorflow.py because it was supposed to contain a classifier solution with tensorflow. Well it seems that this confuses python and python now imports this file when importing tensorflow.

I had to just rename the file and now it works. 
",,,false,
https://stackoverflow.com/questions/56700022,false,The issue is not related to an API exhibiting unexpected failures or unpredictable behaviors under specific runtime conditions. It is a question about the behavior of Tensorflow Object Detection API when predicting instances of the same object in an image.,,,,,,,tensorflow-hub: Python import error when file is in a specific folder,"So I ran into this super strange issue. I am trying to run the example code of tensorflow-hub given on this webpage: https://www.tensorflow.org/hub
(I removed both !pip statements)

I want to use a small piece of python code for the machine learning stuff within my application which is otherwise written in go. However when I execute the example code in the project directory of my application it gives an import error.

So when I create a file tf-hub.py in ~/ and execute it, it runs, prints some warnings and the result:

$python3 tf-hub.py
WARNING: Logging before flag parsing goes to stderr.
W0621 10:35:21.367268 140170246772224 deprecation_wrapper.py:118] From tf-hub.py:4: The name tf.enable_eager_execution is deprecated. Please use tf.compat.v1.enable_eager_execution instead.

2019-06-21 10:35:21.407732: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2019-06-21 10:35:21.427007: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2793545000 Hz
2019-06-21 10:35:21.428085: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x563567fc7a90 executing computations on platform Host. Devices:
2019-06-21 10:35:21.428143: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): &lt;undefined&gt;, &lt;undefined&gt;
2019-06-21 10:35:21.505990: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1541] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.
2019-06-21 10:35:21.513612: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 498570752 exceeds 10% of system memory.
2019-06-21 10:35:22.109499: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 498570752 exceeds 10% of system memory.
(3, 128)



However when I copy the same file to the project folder of the go project I get the following import error:

$ python3 go/src/MyProject/tagger/imageClassifier/tf-hub.py 
Traceback (most recent call last):
  File ""/usr/local/lib/python3.7/site-packages/tensorflow_hub/tf_v1.py"", line 29, in &lt;module&gt;
    from tensorflow.compat.v1 import *  # pylint: disable=wildcard-import
ModuleNotFoundError: No module named 'tensorflow.compat'; 'tensorflow' is not a package

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""go/src/EmbeddingImageTagger/tagger/imageClassifier/tf-hub.py"", line 1, in &lt;module&gt;
    import tensorflow as tf
  File ""/home/***/go/src/EmbeddingImageTagger/tagger/imageClassifier/tensorflow.py"", line 2, in &lt;module&gt;
    import tensorflow_hub as hub
  File ""/usr/local/lib/python3.7/site-packages/tensorflow_hub/__init__.py"", line 30, in &lt;module&gt;
    from tensorflow_hub.estimator import LatestModuleExporter
  File ""/usr/local/lib/python3.7/site-packages/tensorflow_hub/estimator.py"", line 25, in &lt;module&gt;
    from tensorflow_hub import tf_utils
  File ""/usr/local/lib/python3.7/site-packages/tensorflow_hub/tf_utils.py"", line 28, in &lt;module&gt;
    from tensorflow_hub import tf_v1
  File ""/usr/local/lib/python3.7/site-packages/tensorflow_hub/tf_v1.py"", line 33, in &lt;module&gt;
    from tensorflow import add_to_collection
ImportError: cannot import name 'add_to_collection' from 'tensorflow' (/home/***/go/src/MyProject/tagger/imageClassifier/tensorflow.py)


I have to use tf-nightly, otherwise I run into this issue: https://github.com/tensorflow/hub/issues/289

I am using:


Fedora 29 64-bit
Python 3.7.2
tf-nightly 1.14.1.dev20190621
tensorflow-hub 0.4.0


Any ideas what could cause this weird behavior that the file runs in one directory, but not in another? I also tried to run both from the same terminal. Still it does not work.
","Oh my. So I found the culprit after I cleaned up the debug stuff and moved my development to my home folder.

The problem was that I had a python file called tensorflow.py because it was supposed to contain a classifier solution with tensorflow. Well it seems that this confuses python and python now imports this file when importing tensorflow.

I had to just rename the file and now it works. 
",,,false,
https://stackoverflow.com/questions/65023353,true,"The issue involves a comparison between keras.metrics.Accuracy() and the string ""accuracy"" in the metrics argument of the model.compile() function. The behavior of these two options is different, and the user is confused about their usage.",Keras,keras.metrics.Accuracy,"The issue arises from the difference in behavior between keras.metrics.Accuracy() and the string ""accuracy"" when used as metrics in model.compile(). The keras.metrics.Accuracy() calculates the accuracy by comparing the predicted values with the target values, while the string ""accuracy"" selects a different metric based on the loss function and model output shape.",The keras.metrics.Accuracy() is used as a metric in model.compile().,"The issue occurs when the string ""accuracy"" is used as a metric in model.compile().","This issue might be challenging to detect for users who assume that the string ""accuracy"" is equivalent to keras.metrics.Accuracy() and expect consistent behavior.",Difference between keras.metrics.Accuracy() and &quot;accuracy&quot;,"I have been testing different approaches in building nn models (tensorflow, keras) and I saw that there was something strange with metric during compile model.
I checked two ways:
    model.compile(
        loss=keras.losses.CategoricalCrossentropy(),
        optimizer=keras.optimizers.Adam(),
        metrics=keras.metrics.Accuracy()
    )

and
    model.compile(
        loss=keras.losses.CategoricalCrossentropy(),
        optimizer=keras.optimizers.Adam(),
        metrics=[""accuracy""]
    )


Result of first approach:
    Epoch 1/2
    1875/1875 - 2s - loss: 0.0494 - accuracy: 0.0020
    Epoch 2/2
    1875/1875 - 2s - loss: 0.0401 - accuracy: 0.0030

    &lt;tensorflow.python.keras.callbacks.History at 0x7f9c00bc06d8&gt;

Result of second approach:
    Epoch 1/2
    1875/1875 - 2s - loss: 0.0368 - accuracy: 0.9884
    Epoch 2/2
    1875/1875 - 2s - loss: 0.0303 - accuracy: 0.9913

    &lt;tensorflow.python.keras.callbacks.History at 0x7f9bfd7d35c0&gt;

This is quite strange, I thought that ""accuracy"" is exactly the same as keras.metrics.Accuracy().
At least this is the case in arguments ""loss"" and ""optimizer"", e.g. ""adam"" is the same as keras.optimizers.Adam().
Does anybody know why is this so weird or I missed something?
Edit:
Approach with metric in [] gives strange results too:
    model.compile(
        loss=keras.losses.CategoricalCrossentropy(),
        optimizer=keras.optimizers.Adam(),
        metrics=[keras.metrics.Accuracy()]
    )

    Epoch 1/2
    1875/1875 - 2s - loss: 0.2996 - accuracy: 0.0000e+00
    Epoch 2/2
    1875/1875 - 2s - loss: 0.1431 - accuracy: 1.8333e-05

    &lt;tensorflow.python.keras.callbacks.History at 0x7f9bfd1045f8&gt;


","When you are mentioning keras.metrics.Accuracy() you are explicitly asking the library to calculate the metric Accuracy which is simple comparison between how many target values matches the predicted values.
However, when you mention the string accuracy then depending on the type of loss you have chosen a different Metric gets selected. This is what is mentioned in the documentation of Keras,
When you pass the strings 'accuracy' or 'acc', we convert this to one of tf.keras.metrics.BinaryAccuracy, tf.keras.metrics.CategoricalAccuracy, tf.keras.metrics.SparseCategoricalAccuracy based on the loss function used and the model output shape. We do a similar conversion for the strings 'crossentropy' and 'ce' as well.
Hence, as CategoricalCrossEntropy is the loss so CategoricalAccuracy gets calculated in case 2. This is based on finding argmax and then comparing the one-hot encoding. As a result, you are seeing better accuracy values in case 2 and very bad in case 1.
So the string accuracy will not always mean the metric function Accuracy().
The explanation of different metrics for reference, https://keras.io/api/metrics/accuracy_metrics/
The explanation of argument metrics for reference,
https://www.tensorflow.org/api_docs/python/tf/keras/Model#compile
","It is because you have to specify your metrics into a list. Try this :
model.compile(
        loss=keras.losses.CategoricalCrossentropy(),
        optimizer=keras.optimizers.Adam(),
        metrics=[keras.metrics.Accuracy()]
    )

and you should get the same results.
Edit :
keras.metrics.Accuracy() calculates the accuracy between the equality of the predition and the ground truth (see doc). In your case, you want to calculate the accuracy of the match in the correct class. So you should use keras.metrics.BinaryAccuracy()or keras.metrics.CategroicalAccuracy() according to your problem.
",,false,
https://stackoverflow.com/questions/45193744,false,The issue does not involve an API exhibiting unexpected failures or unpredictable behaviors under specific runtime conditions.,,,,,,,Keras + Tensorflow strange results,,,,,false,
https://stackoverflow.com/questions/34500052,false,The issue does not involve an API exhibiting unexpected failures or unpredictable behaviors under specific runtime conditions.,,,,,,,tensorflow: saving and restoring session,,,,,false,
https://stackoverflow.com/questions/34289131,false,The issue does not involve an API exhibiting unexpected failures or unpredictable behaviors under specific runtime conditions.,,,,,,,Basic neural network in TensorFlow,,,,,false,
https://stackoverflow.com/questions/51676447,false,The issue does not involve an API exhibiting unexpected failures or unpredictable behaviors under specific runtime conditions.,,,,,,,Python: Use PIL to load png file gives strange results,,,,,false,
https://stackoverflow.com/questions/40350839,false,The issue does not involve an API exhibiting unexpected failures or unpredictable behaviors under specific runtime conditions.,,,,,,,Tensorflow unhashable type &#39;list&#39; in sess.run,,,,,false,
https://stackoverflow.com/questions/37956242,false,The issue does not involve an API exhibiting unexpected failures or unpredictable behaviors under specific runtime conditions.,,,,,,,Saving a collection of variable length tensors to a TFRecords file in TensorFlow,,,,,false,
https://stackoverflow.com/questions/50938992,true,"The issue involves a high loss in accuracy after converting a TensorFlow model to TensorFlow Lite, indicating unexpected behavior in the conversion process.",TensorFlow,tf.lite.Converter,"The issue arises after converting a TensorFlow model to TensorFlow Lite, resulting in a significant loss in accuracy during inference on Android. The user suspects that the conversion process is causing this unexpected behavior.",The TensorFlow model performs well in terms of accuracy before conversion to TensorFlow Lite.,The issue is triggered after converting the TensorFlow model to TensorFlow Lite and running inference on Android.,This issue might be challenging to detect during the conversion process or when testing the TensorFlow Lite model on Android due to the complexity of the conversion process and potential platform-specific factors.,TensorFlow lite: High loss in accuracy after converting model to tflite,"I have been trying TFLite to increase detection speed on Android but strangely my .tflite model now almost only detects 1 category.

I have done testing on the .pb model that I got after retraining a mobilenet and the results are good but for some reason, when I convert it to .tflite the detection is way off...

For the retraining I used the retrain.py file from Tensorflow for poets 2

I am using the following commands to retrain, optimize for inference and convert the model to tflite:

python retrain.py \
--image_dir ~/tf_files/tw/ \
--tfhub_module https://tfhub.dev/google/imagenet/mobilenet_v1_100_224/feature_vector/1 \
--output_graph ~/new_training_dir/retrainedGraph.pb \
-–saved_model_dir ~/new_training_dir/model/ \
--how_many_training_steps 500 

sudo toco \
--input_file=retrainedGraph.pb \
--output_file=optimized_retrainedGraph.pb \
--input_format=TENSORFLOW_GRAPHDEF \
--output_format=TENSORFLOW_GRAPHDEF \
--input_shape=1,224,224,3 \
--input_array=Placeholder \
--output_array=final_result \

sudo toco \
--input_file=optimized_retrainedGraph.pb \
--input_format=TENSORFLOW_GRAPHDEF \
--output_format=TFLITE \
--output_file=retrainedGraph.tflite \
--inference_type=FLOAT \
--inference_input_type=FLOAT \
--input_arrays=Placeholder \
--output_array=final_result \
--input_shapes=1,224,224,3


Am I doing anything wrong here? Where could the loss in accuracy come from?
","I faced the same issue while I was trying to convert a .pb model into .lite.

In fact, my accuracy would come down from 95 to 30!

Turns out the mistake I was committing was not during the conversion of .pb to .lite or in the command involved to do so. But it was actually while loading the image and pre-processing it before it is passed into the lite model and inferred using 

interpreter.invoke()


command.

The below code you see is what I meant by pre-processing:

test_image=cv2.imread(file_name)
test_image=cv2.resize(test_image,(299,299),cv2.INTER_AREA)
test_image = np.expand_dims((test_image)/255, axis=0).astype(np.float32)
interpreter.set_tensor(input_tensor_index, test_image)
interpreter.invoke()
digit = np.argmax(output()[0])
#print(digit)
prediction=result[digit]


As you can see there are two crucial commands/pre-processing done on the image once it is read using ""imread()"":

i) The image should be resized to the size that is the ""input_height"" and ""input_width"" values of the input image/tensor that was used during the training. In my case (inception-v3) this was 299 for both ""input_height"" and ""input_width"". (Read the documentation of the model for this value or look for this variable in the file that you used to train or retrain the model)

ii) The next command in the above code is:

test_image = np.expand_dims((test_image)/255, axis=0).astype(np.float32)


I got this from the ""formulae""/model code:

test_image = np.expand_dims((test_image-input_mean)/input_std, axis=0).astype(np.float32)


Reading the documentation revealed that for my architecture input_mean = 0 and input_std = 255.

When I did the said changes to my code, I got the accuracy that was expected (90%).

Hope this helps.
",,,false,
https://stackoverflow.com/questions/42473052,false,The issue does not involve an API exhibiting unexpected failures or unpredictable behaviors under specific runtime conditions.,,,,,,,TensorFlow 1.0 does not see GPU on Windows (but Theano does),,,,,false,
https://stackoverflow.com/questions/39032277,false,The issue does not involve an API exhibiting unexpected failures or unpredictable behaviors under specific runtime conditions.,,,,,,,Number recognition from Image with python and tensorflow,,,,,false,
https://stackoverflow.com/questions/64645579,true,"The issue involves a custom data validation callback on training data always returning validation data results, indicating unexpected behavior in the callback implementation.",Keras,keras.callbacks.Callback,"The issue arises from a custom data validation callback that is intended to evaluate the model performance on custom data. However, the callback always returns the same results as the validation data, indicating incorrect behavior in the callback implementation.",The custom data validation callback should provide separate evaluation results for the training data and the validation data.,The issue occurs when the custom data validation callback is used during model training.,This issue might be challenging to detect due to potential errors in the callback implementation or incorrect usage of the callback within the training process.,Keras: custom data validation callback on training data always returns validation data results,"I am working on an autoencoder in Keras that contains some dropout layers. To evaluate bias and variance, I'd like to compare the losses of training and test data. However, since dropout is used during training, the losses cannot be compared. (See here for an explanation of why the training data results can be worse than test data results.)
In order to get training data losses that are not influenced by the dropout, I wrote a callback to validate some additional data set (in this case, it would be the training data again).
The strange thing is that I ALWAYS get the same results as for the validation data. Here's a minimal example:
from pprint import pprint

import keras
import numpy as np
import pandas as pd
from numpy.random import seed as np_seed
from tensorflow.random import set_seed as tf_seed

np_seed(1)
tf_seed(2)

# Generation of data sets for training and testing. Random data is only used to showcase the problem. 
df_train = pd.DataFrame(data=np.random.random((1000, 10))) # This will be used for training
df_test_1 = pd.DataFrame(data=np.random.random((1000, 10))) # This will be used as validation data set directly
df_test_2 = pd.DataFrame(data=np.random.random((1000, 10))) # This will be used within the callback

np_seed(1)
tf_seed(2)

model = keras.models.Sequential(
    [
        keras.Input(shape=(10, )),
        keras.layers.Dropout(rate=0.01),
        keras.layers.Dense(5, activation='relu'),
        keras.layers.Dropout(rate=0.01),
        keras.layers.Dense(10, activation='linear'),
    ]
)

model.compile(
    loss='mean_squared_error',
    optimizer=keras.optimizers.Adam(),
)


class CustomDataValidation(keras.callbacks.Callback):
    def __init__(self, x=None, y=None):
        self.x = x
        self.y = y

    def on_epoch_end(self, epoch, logs=None):
        result = self.model.evaluate(x=self.x, y=self.y, return_dict=True)

        for loss_name, loss_value in result.items():
            logs[""custom_"" + loss_name] = loss_value


cdv = CustomDataValidation(df_test_2, df_test_2)

hist = model.fit(df_train, df_train, validation_data=(df_test_1, df_test_1), epochs=2, validation_split=0.1, callbacks=[cdv])
pprint(hist.history)

The output is
Epoch 1/2
4/4 [==============================] - 0s 1ms/step - loss: 0.7625
29/29 [==============================] - 0s 5ms/step - loss: 0.9666 - val_loss: 0.7625
Epoch 2/2
4/4 [==============================] - 0s 1ms/step - loss: 0.5331
29/29 [==============================] - 0s 2ms/step - loss: 0.6638 - val_loss: 0.5331
{'custom_loss': [0.7624925374984741, 0.5331208109855652],
 'loss': [0.9665887951850891, 0.6637843251228333],
 'val_loss': [0.7624925374984741, 0.5331208109855652]}

'custom_loss' and 'val_loss' are equal although they should be based on totally different data sets.
The question is therefore: How can I evaluate the model performance on custom data within a callback?
Edit: Since I did not yet got an answer on stackoverflow, I created an issue at tensorflow's git repo. Also, there's now a notebook available that shows the problem.
","It seems that this is a bug in tensorflow versions 2.3.x (tested with 2.3.0 and 2.3.1). In versions 2.4.0-rc0 and 2.2.1, the loss outputs of loss and custom_loss differ, which is the expected behavior:
{'custom_loss': [0.7694963216781616, 0.541864812374115],
'loss': [0.9665887951850891, 0.6637843251228333],
'val_loss': [0.7624925374984741, 0.5331208109855652]}

",,,false,
https://stackoverflow.com/questions/53968334,false,The issue does not involve an API exhibiting unexpected failures or unpredictable behaviors under specific runtime conditions.,,,,,,,Is there any similar funtion as list.append() in tensorflow?,,,,,false,
https://stackoverflow.com/questions/44676248,false,The issue does not involve an API exhibiting unexpected failures or unpredictable behaviors under specific runtime conditions.,,,,,,,Is this a bug in tensorflow?,,,,,false,
https://stackoverflow.com/questions/39817949,false,The issue does not involve an API exhibiting unexpected failures or unpredictable behaviors under specific runtime conditions.,,,,,,,Setting up a MLP for binary classification with tensorflow,,,,,false,
https://stackoverflow.com/questions/61510865,false,The issue does not meet the criteria for deeper analysis as it does not involve an API exhibiting unexpected failures or unpredictable behaviors under specific runtime conditions.,,,,,,,Tensorflow has no Attribute &quot;sort&quot; in GPT 2 Git Release?,"I downloaded the git repo (https://github.com/openai/gpt-2) and followed the python3 instructions (in DEVELOPERS.MD) for installation on my Kubuntu 18.04LTS box, but I cannot run it and instead get an error.

Here is what I've done so far:

pip3 install tensorflow==1.12.0
pip3 install -r requirements.txt
python3 download_model.py 124M
python3 download_model.py 355M
python3 download_model.py 774M
python3 download_model.py 1558M
export PYTHONIOENCODING=UTF-8


I then ran:

sarah@LesserArk:~/Custom Programs/gpt-2$ python3 src/interactive_conditional_samples.py 
/home/sarah/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([(""qint8"", np.int8, 1)])
/home/sarah/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:524: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([(""quint8"", np.uint8, 1)])
/home/sarah/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([(""qint16"", np.int16, 1)])
/home/sarah/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([(""quint16"", np.uint16, 1)])
/home/sarah/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([(""qint32"", np.int32, 1)])
/home/sarah/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:532: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([(""resource"", np.ubyte, 1)])
2020-04-29 16:08:30.016586: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
Traceback (most recent call last):
  File ""src/interactive_conditional_samples.py"", line 91, in &lt;module&gt;
    fire.Fire(interact_model)
  File ""/home/sarah/.local/lib/python3.6/site-packages/fire/core.py"", line 138, in Fire
    component_trace = _Fire(component, args, parsed_flag_args, context, name)
  File ""/home/sarah/.local/lib/python3.6/site-packages/fire/core.py"", line 468, in _Fire
    target=component.__name__)
  File ""/home/sarah/.local/lib/python3.6/site-packages/fire/core.py"", line 672, in _CallAndUpdateTrace
    component = fn(*varargs, **kwargs)
  File ""src/interactive_conditional_samples.py"", line 65, in interact_model
    temperature=temperature, top_k=top_k, top_p=top_p
  File ""/home/sarah/Custom Programs/gpt-2/src/sample.py"", line 74, in sample_sequence
    past, prev, output = body(None, context, context)
  File ""/home/sarah/Custom Programs/gpt-2/src/sample.py"", line 66, in body
    logits = top_p_logits(logits, p=top_p)
  File ""/home/sarah/Custom Programs/gpt-2/src/sample.py"", line 28, in top_p_logits
    sorted_logits = tf.sort(logits, direction='DESCENDING', axis=-1)
AttributeError: module 'tensorflow' has no attribute 'sort'


Which culminates in the error: AttributeError: module 'tensorflow' has no attribute 'sort'.

This is strange, and I'm not sure how to proceed. I would have thought that the instructions would lead to successful installation, but it appears that they don't.

Uninstalling and reinstalling has no effect on the final result? How can I get tensorflow to execute GPT-II?
","I was able to get the sample to work using:
pip3 install tensorflow==1.13.1
Additionally, there is a response to the GitHub issue with a commit showing changing Dockerfile.cpu to building FROM tensorflow/tensorflow:1.15.0-py3 fixes the docker image issue.
","pip install tesorflow==1.14.0
after installing this my problem get solve. try it
",,false,
https://stackoverflow.com/questions/60251715,false,"The issue does not meet the criteria for deeper analysis as it stems from a difference between the Keras and TensorFlow Hub versions of MobileNetV2, rather than an API-related problem.",,,,,,,Difference between Keras and TensorFlow Hub Version of MobileNetV2,"I am working on a transfer learning approach and got very different results when using the MobileNetV2 from keras.applications and the one available on TensorFlow Hub. This seems strange to me as both versions claim here and here to extract their weights from the same checkpoint mobilenet_v2_1.0_224.
This is how the differences can be reproduced, you can find the Colab Notebook here:

!pip install tensorflow-gpu==2.1.0
import tensorflow as tf
import numpy as np
import tensorflow_hub as hub
from tensorflow.keras.applications.mobilenet_v2 import MobileNetV2

def create_model_keras():
  image_input = tf.keras.Input(shape=(224, 224, 3))
  out = MobileNetV2(input_shape=(224, 224, 3),
                  include_top=True)(image_input)
  model = tf.keras.models.Model(inputs=image_input, outputs=out)
  model.compile(optimizer='adam', loss=[""categorical_crossentropy""])
  return model

def create_model_tf():
  image_input = tf.keras.Input(shape=(224, 224 ,3))
  out = hub.KerasLayer(""https://tfhub.dev/google/tf2-preview/mobilenet_v2/classification/4"",
                      input_shape=(224, 224, 3))(image_input)
  model = tf.keras.models.Model(inputs=image_input, outputs=out)
  model.compile(optimizer='adam', loss=[""categorical_crossentropy""])
  return model


When I try to predict on a random batch, the results are not equal:

keras_model = create_model_keras()
tf_model = create_model_tf()
np.random.seed(42)
data = np.random.rand(32,224,224,3)
out_keras = keras_model.predict_on_batch(data)
out_tf = tf_model.predict_on_batch(data)
np.array_equal(out_keras, out_tf)


The output of the version from keras.applications sums up to 1 but the version from TensorFlow Hub does not. Also the shape of the two versions is different: TensorFlow Hub has 1001 labels, keras.applications has 1000. 

np.sum(out_keras[0]), np.sum(out_tf[0])


prints (1.0000001, -14.166359)

What is the reason for these differences? Am I missing something?

Edit 18.02.2020

As Szymon Maszke pointed out, the TFHub version returns logits. That's why i added a Softmax layer to the create_model_tf as follows:
out = tf.keras.layers.Softmax()(x)

arnoegw mentioned that the TfHub version requires an image normalized to [0,1], whereas the keras version requires normalization to [-1,1]. When I use the following preprocessing on a test image:

from tensorflow.keras.applications.mobilenet_v2 import preprocess_input
img = tf.keras.preprocessing.image.load_img(""/content/panda.jpeg"", target_size=(224,224))
img = tf.keras.preprocessing.image.img_to_array(img)
img = preprocess_input(img)


img = tf.io.read_file(""/content/panda.jpeg"")
img = tf.image.decode_jpeg(img)
img = tf.image.convert_image_dtype(img, tf.float32)
img = tf.image.resize(img, (224,224))


Both correctly predict the same label and the following condition is true: np.allclose(out_keras, out_tf[:,1:], rtol=0.8)

Edit 2 18.02.2020
Before I wrote that it is not possible to convert the formats into each other. This was caused by a bug.
","There are several documented differences:


Like Szymon said, the TF Hub version returns logits (before the softmax function that turns them into probabilities), which is a common practice, because the cross-entropy loss can be computed with greater numerical stability from the logits.
The TF Hub model assumes float32 inputs in the range of [0,1], which is what you get from tf.image.decode_jpeg(...) followed by tf.image.convert_image_dtype(..., tf.float32). The Keras code uses a model-specific range (likely [-1,+1]).
The TF Hub model reflects the original SLIM checkpoint more completely in returning all its 1001 output classes. As stated in the ImageNetLabels.txt linked from the documentation, the added class 0 is ""background"" (aka. ""stuff""). That is what object detection uses to indicate image background as opposed to an object of any known class.

",,,false,
https://stackoverflow.com/questions/52507748,false,The issue does not provide enough information to determine if it involves an API exhibiting unexpected failures or unpredictable behaviors under specific runtime conditions.,,,,,,,Using multiple CPU cores in TensorFlow,,,,,false,
https://stackoverflow.com/questions/49370940,false,"The issue does not meet the criteria for deeper analysis as it is a general question about one-hot encoding characters in TensorFlow or Keras, without indicating any unexpected behavior or failure of an API.",,,,,,,One hot encoding characters,"Is there a possibilty to one-hot encode characters of a text in Tensorflow or Keras?


tf.one_hot seem to take only integers.
tf.keras.preprocessing.text.one_hot seems to one-hot encode sentences
to words, but not to characters.


Beside that, tf.keras.preprocessing.text.one_hot works really strange, since the response does not really seem one-hot encoded, since the following code:

text = ""ab bba bbd""
res = tf.keras.preprocessing.text.one_hot(text=text,n=3)
print(res)


Lead to this result:

[1,2,2]


Every time I run this program, the output is a different 3d vector, sometimes it is [1,1,1] or [2,1,1]. The documentation says, that unicity is not guaranteed, but this seems really senseless to me.
",,,,false,
https://stackoverflow.com/questions/46580112,false,"The issue does not meet the criteria for deeper analysis as it is a question about the difference in R-squared results between the training, validation, and test sets, rather than an API-related problem.",,,,,,,R-squared results of test and validation differ by a huge margin,"I am working on a regression problem with keras and tensorflow using a neural network. The data is split, so that 282774 datasets are for training, 70694 are for validation and 88367 are for testing. To evaluate my models I am printing out the mean squared error (MSE), the mean absolute error (MAE) and the R-squared score. These are some examples from the results I get:

               MSE           MAE             R-squared
Training       1.562072899   0.958128839     0.849787137
Validation     0.687871457   0.62066941      0.935365564
Test           0.683918759   0.618674863   -16.22829222


I do not understand the value for R-squared on test data. I know that R-squared can be negative, but how can it be that there is such a big difference between validation and test if both fall into the category of unseen data. Can someone give me a hint?

Some background information:

Since keras does not have the R-squared metric built in, I implemented it with some code I found on the web and which seems logical for me:

def r2_keras(y_true, y_pred):
    SS_res =  K.sum(K.square(y_true - y_pred)) 
    SS_tot = K.sum(K.square(y_true - K.mean(y_true))) 
    return ( 1 - SS_res/(SS_tot + K.epsilon()) )


And if it helps: this is my model:

model = Sequential()
model.add(Dense(75, input_shape=(7,)))
model.add(BatchNormalization())
model.add(Activation('relu'))
model.add(Dropout(0.5))

model.add(Dense(1, activation='linear'))

adam = optimizers.Adam(lr=0.001)
model.compile(loss='mse',
              optimizer=adam,
              metrics=['mse', 'mae', r2_keras])

history = model.fit(x_train, y_train,
                    epochs=50,
                    batch_size=32,
                    validation_split=0.2)


score = model.evaluate(x_test, y_test, batch_size=32)


One strange thing I noticed is, that not all testing data seems to be considered. The console prints out the following:

86304/88367 [============================&gt;.] - ETA: 0s-----


Maybe this leads to a miscalculation for R-squared?

I am thankful for any help/hint I can get on understanding this issue.

Update:
I checked for outliers, but could not find any significant one. Min and max-values for test and train are close by, considering the standard deviation. Also the histograms look very much alike.

So in the next step I let my model predict the values for test data again and used pandas + numpy to calculate the r2_score. This time I got a value which is approximately equal to the r2_score for validation.

Below is how I did it. Do you see any flaws in the way I performed the calculation? (I just want to be sure that the old r2_score for ""test"" was indeed a calculation error)

# ""test"" is a dataframe with input data and the real outputs
# ""inputs"" is a list of the input column names
# The real/true outputs are contained in the column ""output""
test['output_pred'] = model.predict(x=np.array(test[inputs]))
output_mean = test['output'].mean()    # Is this the correct mean value for r2 here?
test['SSres'] = np.square(test['output']-test['output_pred'])
test['SStot'] = np.square(test['output']-output_mean)
r2 = 1-(test['SSres'].sum()/(test['SStot'].sum()))

","Tensorflow's built-in evaluate method evaluates your test set batch by batch and hence calculates r2 at each batch. The metrics produced from model.evaluate() is then simple average of all r2 from each batch. While in model.fit(), r2 (and all metrics on validation set) are calculated per epoch (instead of per batch and then take avg.)
You may slice your output and output_pred into batches of the same batch size you used in model.evaluate() and calculate r2 on each batch. I guess the model produces high r2 on batches with high total sum of squares (SS_tot) and bad r2 on lower ones. So when taken average, result would be poor (however when calculate r2 on entire dataset, samples with higher ss_tot usually dominate the result).
",,,false,
https://stackoverflow.com/questions/36666358,false,The issue does not provide enough information to determine if it involves an API exhibiting unexpected failures or unpredictable behaviors under specific runtime conditions.,,,,,,,TensorFlow implementing Seq2seq Sentiment analysis,,,,,false,
https://stackoverflow.com/questions/72881142,false,The issue does not provide enough information to determine if it involves an API exhibiting unexpected failures or unpredictable behaviors under specific runtime conditions.,,,,,,,Measuring incertainty in Bayesian Neural Network,"Hy everybody,
I'm beginning with tensorflow probability and I have some difficulties to interpret my Bayesian neural network outputs.
I'm working on a regression case, and started with the example provided by tensorflow notebook here: https://blog.tensorflow.org/2019/03/regression-with-probabilistic-layers-in.html?hl=fr
As I seek to know the uncertainty of my network predictions, I dived directly into example 4 with Aleatoric &amp; Epistemic Uncertainty. You can find my code bellow:
def negative_loglikelihood(targets, estimated_distribution):
    return -estimated_distribution.log_prob(targets)


def posterior_mean_field(kernel_size, bias_size, dtype=None):
    n = kernel_size + bias_size #number of total paramaeters (Weights and Bias)
    c = np.log(np.expm1(1.)) 
    return tf.keras.Sequential([
        tfp.layers.VariableLayer(2 * n, dtype=dtype, initializer=lambda shape, dtype: random_gaussian_initializer(shape, dtype), trainable=True), 
        tfp.layers.DistributionLambda(lambda t: tfd.Independent(
            # The Normal distribution with location loc and scale parameters.
            tfd.Normal(loc=t[..., :n],
                       scale=1e-5 +0.01*tf.nn.softplus(c + t[..., n:])),
            reinterpreted_batch_ndims=1)),
    ])



def prior(kernel_size, bias_size, dtype=None):
    n = kernel_size + bias_size
    return tf.keras.Sequential([
        tfp.layers.VariableLayer(n, dtype=dtype),
        tfp.layers.DistributionLambda(lambda t: tfd.Independent(
            tfd.Normal(loc=t, scale=1),
            reinterpreted_batch_ndims=1)),
    ])




def build_model(param):
    model = keras.Sequential()
    for i in range(param[""n_layers""] ):
        name=""n_units_l""+str(i)
        num_hidden = param[name]
        model.add(tfp.layers.DenseVariational(units=num_hidden, make_prior_fn=prior,make_posterior_fn=posterior_mean_field,kl_weight=1/len(X_train),activation=""relu""))
    model.add(tfp.layers.DenseVariational(units=2, make_prior_fn=prior,make_posterior_fn=posterior_mean_field,activation=""relu"",kl_weight=1/len(X_train))) 
    model.add(tfp.layers.DistributionLambda(lambda t: tfd.Normal(loc=t[..., :1],scale=1e-3 + tf.math.softplus(0.01 * t[...,1:]))))
    
    lr = param[""learning_rate""]
    optimizer=optimizers.Adam(learning_rate=lr)
        
    model.compile(
        loss=negative_loglikelihood,  #negative_loglikelihood, 
        optimizer=optimizer,
        metrics=[keras.metrics.RootMeanSquaredError()],
    )

    return model


I think I have the same network than in tfp example, I just added few hidden layers with differents units. Also I added 0.01 in front of the Softplus in the posterior as suggested here, which allows the network to come up to good performances.
Not able to get reasonable results from DenseVariational
The performances of the model are very good (less than 1% of error) but I have some questions:

As Bayesian neural networks ""promise"" to mesure the uncertainty of the predictions, I was expecting bigger errors on high variance predictions. I ploted the absolute error versus variance and the results are not good enough on my mind. Of course, the model is better at low variance but I can have really bad predicitions at low variance, and therefore cannot really use standard deviation to filter bad predictions. Why is my Bayesian neural netowrk struggling to give me the uncertainty ?



The previous network was train 2000 epochs and we can notice a strange phenome with a vertical bar on lowest stdv. If I increase the number of epoch up to 25000, my results get better either on training and validation set.


But the phenomene of vertical bar that we may notice on the figure 1 is much more obvious. It seems that as much as I increase the number or EPOCH, all output variance converge to 0.68. Is that a case of overfitting ? Why this value of 0.6931571960449219 and why I can't get lower stdv ? As the phenome start appearing at 2000 EPOCH, am i already overfitting at 2000 epochs ?

At this point stdv is totaly useless. So is there a kind of trade off ? With few epochs my model is less performant but gives me some insigh about uncertainty (even if I think they're not sufficient), where with lot of epochs I have better performances but no more uncertainty informations as all outputs have the same stdv.
Sorry for the long post and the language mistakes.
Thank you in advance for you help and any feed back.
","I solved the problem of why my uncertainty could not get lower than 0.6931571960449219.
Actually this value is converging to log(2). This is due to my relu activation function on my last Dense Variational layer.
Indeed, the scale of tfd.Normal is a softplus (tf.math.softplus).
And softplus is implement like that : softplus(x) = log(exp(x) + 1). As my x doesn't go in negative values, my minumum incertainty il log(2).
A basic linear activation function solved the problem and my uncertainty has a normal behavior now.
",,,false,
https://stackoverflow.com/questions/60604007,false,The issue does not meet the criteria for deeper analysis as it does not involve an API exhibiting unexpected failures or unpredictable behaviors under specific runtime conditions.,,,,,,,Tensorflow SDK vs MLKit SDK,"We have a strange problem that we need help solving. 
Here's the scenario: 
We've got an native android app using the Tensorflow SDK and a tflite model that recognises different fish species. This solution has varying results when capturing input from the camera, but when using pictures from the camera roll it works very well and serves us stable probabilities for the different fish species. 

We also have an native iOS app that uses Google's MLKit with the same tensorflow lite model and this performs very well, but on some pictures it totally misses the target. 

If we load the same picture from the camera roll and run it through the interpreter on the android app and the iOS app we get totally different results, but only on SOME species. 

Does anybody have an explanation as to why we can get such different results using the same tflite model in both apps and the same picture? 

Are there differences between the MLKit SDK and the Tensorflow SDK that can cause this kind of behaviour?

Best regards, 
Christian
","There could be a couple of reasons. 


TFLite runtime versions. ML Kit custom model also uses TFLite under the hood, and you could find the TFLite runtime version in your Pod dependencies. Make sure you are using the same TFLite versions in the test.
Image processing. The images you send to ML Kit SDK or TFLite SDK are always much larger than the model inputs, so resizing and even image rotations will be performed before running the model. Different platform may use different libraries for the image processing, and the results may have slightly difference. But the slight different may cause the model output different results. It could due to the model is a bit unstable for SOME species. You may try to send the exact same image with the model expected image format/size/orientation, so that image processing is not triggered in both platforms and see whether it helps. 

",,,false,
https://stackoverflow.com/questions/58569584,false,The issue does not meet the criteria for deeper analysis as it is related to the usage of tf.function and does not involve an API exhibiting unexpected failures or unpredictable behaviors under specific runtime conditions.,,,,,,,Val loss behaves strange while using custom training loop in tensorflow 2.0,"I'm using a VGG16 model written in tf2.0 to train on my own datasets. Some BatchNormalization layers were included in the model and the ""training"" argument were set to True during training time and False during validation time as described in many tutorials.
The train_loss decreased to a certain level during training as expected.  However, the val_loss behaves really strange. I checked out the output of the model after training and found out that, if I set the training argument to True, the output is quite correct, but if I set it to False, the result is incorrect at all.
According to the tutorials in tensorflow website, when training is set to False , the model will normalize its inputs using the mean and variance of its moving statistics learned during training but it doesn't seem so. Am I missing something?

I've provided the trainning and validation code in the below.

def train():
    logging.basicConfig(level=logging.INFO)
    tdataset = tf.data.Dataset.from_tensor_slices((train_img_list[:200], train_label_list[:200]))
    tdataset = tdataset.map(parse_function, 3).shuffle(buffer_size=200).batch(batch_size).repeat(repeat_times)
    vdataset = tf.data.Dataset.from_tensor_slices((val_img_list[:100], val_label_list[:100]))
    vdataset = vdataset.map(parse_function, 3).batch(batch_size)

    ### Vgg model
    model = VGG_PR(num_classes=num_label)

    logging.info('Model loaded')

    start_epoch = 0
    latest_ckpt = tf.train.latest_checkpoint(os.path.dirname(ckpt_path))
    if latest_ckpt:
        start_epoch = int(latest_ckpt.split('-')[1].split('.')[0])
        model.load_weights(latest_ckpt)
        logging.info('model resumed from: {}, start at epoch: {}'.format(latest_ckpt, start_epoch))
    else:
        logging.info('training from scratch since weights no there')

    ######## training loop ########
    loss_object = tf.keras.losses.MeanSquaredError()
    val_loss_object = tf.keras.losses.MeanSquaredError()
    optimizer = tf.keras.optimizers.Adam(learning_rate=initial_lr)
    train_loss = tf.metrics.Mean(name='train_loss') 
    val_loss = tf.metrics.Mean(name='val_loss')
    writer = tf.summary.create_file_writer(log_path.format(case_num))

    with writer.as_default():
        for epoch in range(start_epoch, total_epoch):
            print('start training')
            try:
                for batch, data in enumerate(tdataset):
                    images, labels = data
                    with tf.GradientTape() as tape:
                        pred = model(images, training=True)
                        if len(pred.shape) == 2:
                            pred = tf.reshape(pred,[-1, 1, 1, num_label])
                        loss = loss_object(pred, labels)
                    gradients = tape.gradient(loss, model.trainable_variables)
                    optimizer.apply_gradients(zip(gradients, model.trainable_variables))
                    if batch % 20 ==0:
                        logging.info('Epoch: {}, iter: {}, loss:{}'.format(epoch, batch, loss.numpy()))
                    tf.summary.scalar('train_loss', loss.numpy(), step=epoch*1250*repeat_times+batch)      # the tdataset has been repeated 5 times..
                    tf.summary.text('Zernike_coe_pred', tf.as_string(tf.squeeze(pred)), step=epoch*1250*repeat_times+batch)
                    tf.summary.text('Zernike_coe_gt', tf.as_string(tf.squeeze(labels)), step=epoch*1250*repeat_times+batch)

                    writer.flush()
                    train_loss(loss)
                model.save_weights(ckpt_path.format(epoch=epoch))
            except KeyboardInterrupt:
                logging.info('interrupted.')
                model.save_weights(ckpt_path.format(epoch=epoch))
                logging.info('model saved into {}'.format(ckpt_path.format(epoch=epoch)))
                exit(0)
            # validation step
            for batch, data in enumerate(vdataset):
                images, labels = data
                val_pred = model(images, training=False)
                if len(val_pred.shape) == 2:
                    val_pred = tf.reshape(val_pred,[-1, 1, 1, num_label])
                v_loss = val_loss_object(val_pred, labels)
                val_loss(v_loss)
            logging.info('Epoch: {}, average train_loss:{}, val_loss: {}'.format(epoch, train_loss.result(), val_loss.result()))
            tf.summary.scalar('val_loss', val_loss.result(), step = epoch)
            writer.flush()
            train_loss.reset_states()
            val_loss.reset_states()
        model.save_weights(ckpt_path.format(epoch=epoch))


The train losss reduced to a very small value like the groundtruth label are in the range of [0, 1] and the average train loss can be 0.007, but the val loss is much higher than this. The output of the model tends to be close to 0 if I set training to False.

updated on Nov. 6th:
I have found an interesting thing that if I use tf.function to decorate my model in its call method, the val loss will turn to be correct, but I'm not sure what has happened?
","Mentioning the Answer for the benefit of the community.

Issue is resolved, i.e., val loss will turn to be correct if tf.function is used to decorate the model in its call method.
",,,false,
https://stackoverflow.com/questions/56721498,false,"The issue does not meet the criteria for deeper analysis as it is related to the compatibility between TensorFlow's eager execution and tf.placeholder, rather than an API exhibiting unexpected failures or unpredictable behaviors under specific runtime conditions.",,,,,,,How can I use the Keras.applications&#39; ResNeXt in TensorFlow&#39;s eager execution?,"I am trying to get ResNet101 or ResNeXt, which are only available in Keras' repository for some reason, from Keras applications in TensorFlow 1.10:

import tensorflow as tf
from keras import applications

tf.enable_eager_execution()

resnext = applications.resnext.ResNeXt101(include_top=False, weights='imagenet', input_shape=(SCALED_HEIGHT, SCALED_WIDTH, 3), pooling=None)



However, this results in:

Traceback (most recent call last):
  File ""myscript.py"", line 519, in get_fpn
    resnet = applications.resnet50.ResNet50(include_top=False, weights='imagenet', input_shape=(SCALED_HEIGHT, SCALED_WIDTH, 3), pooling=None)
  File ""Keras-2.2.4-py3.5.egg/keras/applications/__init__.py"", line 28, in wrapper
    return base_fun(*args, **kwargs)
  File ""Keras-2.2.4-py3.5.egg/keras/applications/resnet50.py"", line 11, in ResNet50
    return resnet50.ResNet50(*args, **kwargs)
  File ""Keras_Applications-1.0.8-py3.5.egg/keras_applications/resnet50.py"", line 214, in ResNet50
    img_input = layers.Input(shape=input_shape)
  File ""Keras-2.2.4-py3.5.egg/keras/engine/input_layer.py"", line 178, in Input
    input_tensor=tensor)
  File ""Keras-2.2.4-py3.5.egg/keras/legacy/interfaces.py"", line 91, in wrapper
    return func(*args, **kwargs)
  File ""Keras-2.2.4-py3.5.egg/keras/engine/input_layer.py"", line 87, in __init__
    name=self.name)
  File ""Keras-2.2.4-py3.5.egg/keras/backend/tensorflow_backend.py"", line 529, in placeholder
    x = tf.placeholder(dtype, shape=shape, name=name)
  File ""tensorflow/python/ops/array_ops.py"", line 1732, in placeholder
    raise RuntimeError(""tf.placeholder() is not compatible with ""
RuntimeError: tf.placeholder() is not compatible with eager execution.


I installed Keras from its GitHub master branch, since the pip installs of Keras and TensorFlow's Keras API for some strange reason do not include ResNet101, ResNetv2, ResNeXt, etc. Does anyone know how I can run such models (preferably ResNeXt) in TensorFlow's eager execution?
",,,,false,
https://stackoverflow.com/questions/55992559,false,The issue does not provide enough information to determine if it involves an API exhibiting unexpected failures or unpredictable behaviors under specific runtime conditions.,,,,,,,"Training loss value is increasing after some training time, but the model detects objects pretty good",,,,,false,
https://stackoverflow.com/questions/55974198,false,The issue does not meet the criteria for deeper analysis as it does not involve an API exhibiting unexpected failures or unpredictable behaviors under specific runtime conditions.,,,,,,,Strange results while training with keras,"I am trying to train a unet model on braTS18 dataset (medical data with nifiti images) using keras with tensorflow. However I am getting very strange results:





as you can see, accuracy starts with 96% and gets to 99% at the third epoch. Also the validation loss doesn't go lower ever. Also there is nothing predicted by the trained model.

I have split the data in different ways (20% train 60% validation, or 60% train 20% validation) but didn't work. I think the problem might be with my model or with the data generator. Here are the codes:

unet model

def unet_model(filters=16, dropout=0.1, batch_normalize=True):

    # Build U-Net model
    inputs = Input((img_height, img_width, img_channels), name='main_input')
    s = Lambda(lambda x: x / 255) (inputs)

    c1 = Conv2D(filters, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same', name = 'c1') (s)
    c1 = Dropout(0.1) (c1)
    c1 = Conv2D(filters, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same', name = 'c1_d') (c1)
    p1 = MaxPooling2D((2, 2)) (c1)

    c2 = Conv2D(2*filters, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same', name = 'c2') (p1)
    c2 = Dropout(0.1) (c2)
    c2 = Conv2D(2*filters, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same', name = 'c2_d') (c2)
    p2 = MaxPooling2D((2, 2)) (c2)

    c3 = Conv2D(4*filters, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same', name = 'c3') (p2)
    c3 = Dropout(0.2) (c3)
    c3 = Conv2D(4*filters, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same', name = 'c3_d') (c3)
    p3 = MaxPooling2D((2, 2)) (c3)

    c4 = Conv2D(8*filters, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same', name = 'c4') (p3)
    c4 = Dropout(0.2) (c4)
    c4 = Conv2D(8*filters, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same', name = 'c4_d') (c4)
    p4 = MaxPooling2D(pool_size=(2, 2)) (c4)

    c5 = Conv2D(16*filters, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same', name = 'c5') (p4)
    c5 = Dropout(0.3) (c5)
    c5 = Conv2D(16*filters, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same', name = 'c5_d') (c5)

    u6 = Conv2DTranspose(8*filters, (2, 2), strides=(2, 2), padding='same', name = 'u6') (c5)
    u6 = concatenate([u6, c4])
    c6 = Conv2D(8*filters, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same', name = 'c6') (u6)
    c6 = Dropout(0.2) (c6)
    c6 = Conv2D(8*filters, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same', name = 'c6_d') (c6)

    u7 = Conv2DTranspose(4*filters, (2, 2), strides=(2, 2), padding='same', name = 'u7') (c6)
    u7 = concatenate([u7, c3])
    c7 = Conv2D(4*filters, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same', name = 'c7') (u7)
    c7 = Dropout(0.2) (c7)
    c7 = Conv2D(4*filters, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same', name = 'c7_d') (c7)

    u8 = Conv2DTranspose(2*filters, (2, 2), strides=(2, 2), padding='same', name = 'u8') (c7)
    u8 = concatenate([u8, c2])
    c8 = Conv2D(2*filters, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same', name = 'c8') (u8)
    c8 = Dropout(0.1) (c8)
    c8 = Conv2D(2*filters, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same', name = 'c8_d') (c8)

    u9 = Conv2DTranspose(filters, (2, 2), strides=(2, 2), padding='same', name = 'u9') (c8)
    u9 = concatenate([u9, c1], axis=3)
    c9 = Conv2D(filters, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same', name = 'c9') (u9)
    c9 = Dropout(0.1) (c9)
    c9 = Conv2D(filters, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same', name = 'c9_d') (c9)

    outputs = Conv2D(1, (1, 1), activation='sigmoid', name = 'output') (c9)

    adam = optimizers.Adam(lr=lr, beta_1=beta1, decay=lr_decay, amsgrad=False)

    model = Model(inputs=[inputs], outputs=[outputs])
    model.compile(optimizer=adam, loss='binary_crossentropy', metrics=['accuracy',dice,jaccard])

    plot_model(model, to_file=os.path.join(save_dir +""model.png""))
    if os.path.exists(os.path.join(save_dir +""model.txt"")):
        os.remove(os.path.join(save_dir +""model.txt""))
    with open(os.path.join(save_dir +""model.txt""),'w') as fh:
        model.summary(positions=[.3, .55, .67, 1.], print_fn=lambda x: fh.write(x + '\n'))

    model.summary()

    return model


and here is the code for data generator:

def generate_data(X_data, Y_data, batch_size):

    samples_per_epoch = total_folders
    number_of_batches = samples_per_epoch/batch_size
    counter=0

    while True:

        X_batch = X_data[batch_size*counter:batch_size*(counter+1)]
        Y_batch = Y_data[batch_size*counter:batch_size*(counter+1)]

        counter += 1

        yield X_batch, Y_batch

        if counter &gt;= number_of_batches:
            counter = 0
...
in the main function
...

if __name__ == ""__main__"":

    callbacks = [
    EarlyStopping(patience=1000, verbose=1),
    ReduceLROnPlateau(factor=0.1, patience=3, min_lr=0.00001, verbose=1),
    ModelCheckpoint(save_dir + 'model.{epoch:02d}-{val_loss:.2f}.h5', verbose=1, save_best_only=True, save_weights_only=True)
    ]

    model = unet_model(filters=16, dropout=0.05, batch_normalize=True)


    H = model.fit_generator(generate_data(X_train,Y_train,batch_size), 
                        epochs= epochs,
                        steps_per_epoch = total_folders/batch_size, 
                        validation_data=generate_data(X_test,Y_test,batch_size*2),
                        callbacks=callbacks,
                        validation_steps= total_folders/batch_size*2)


what am I doing wrong here?
",,,,false,
https://stackoverflow.com/questions/51421371,false,The issue does not meet the criteria for deeper analysis as it does not involve an API exhibiting unexpected failures or unpredictable behaviors under specific runtime conditions.,,,,,,,Tensorflow in Android: java.lang.illegalArgumentsException,,,,,false,
https://stackoverflow.com/questions/46522622,false,The issue does not meet the criteria for deeper analysis as it does not involve an API exhibiting unexpected failures or unpredictable behaviors under specific runtime conditions.,,,,,,,Confusing results with tensorflow&#39;s tensordot,,,,,false,
https://stackoverflow.com/questions/44669869,false,The issue does not meet the criteria for deeper analysis as it does not involve an API exhibiting unexpected failures or unpredictable behaviors under specific runtime conditions.,,,,,,,Tensorflow&#39;s computation time gradually slowing down in very simple &quot;for loop&quot;,,,,,false,
https://stackoverflow.com/questions/44333516,false,The issue does not meet the criteria for deeper analysis as it does not involve an API exhibiting unexpected failures or unpredictable behaviors under specific runtime conditions.,,,,,,,"Tensorflow, placement of shared variable definitions",,,,,false,
https://stackoverflow.com/questions/44220225,false,The issue does not meet the criteria for deeper analysis as it does not involve an API exhibiting unexpected failures or unpredictable behaviors under specific runtime conditions.,,,,,,,Tensorflow build from source not faster for retraining?,,,,,false,
https://stackoverflow.com/questions/39550258,false,The issue does not meet the criteria for deeper analysis as it does not involve an API exhibiting unexpected failures or unpredictable behaviors under specific runtime conditions.,,,,,,,keras errors with theano/tensowflow backend,,,,,false,
https://stackoverflow.com/questions/36807107,false,The issue does not meet the criteria for deeper analysis as it does not involve an API exhibiting unexpected failures or unpredictable behaviors under specific runtime conditions.,,,,,,,Strange error when taking gradient of TensorArray,,,,,false,
https://stackoverflow.com/questions/73125716,false,The issue does not meet the criteria for deeper analysis as it does not involve an API exhibiting unexpected failures or unpredictable behaviors under specific runtime conditions.,,,,,,,Tensorflow Lite Android: Both GPU delegate and NNAPI delegate are slower than CPU,"I am currently evaluating and comparing the performance of some tensorflow models on different smartphones. I am testing the MNIST and CIFAR10 databases. The strange thing is, when i try to speed up the inference times with hardware acceleration, they always perform worse than before.
For example, these are the results I got on a Galaxy Note 20 Ultra, which definitely has got some powerful GPUs and NPUs (all results are milliseconds per inference):
MNIST CPU:      0.040
MNIST GPU:      2.322
MNIST NNAPI:    2.839
CIFAR10 CPU:    0.810
CIFAR10 GPU:    8.120
CIFAR10 NNAPI:  6.608
I warmed up the processing unit before the benchmark and executed the inferences multiple times, so these are averages and should not be random. Below you can see the code I used to configure the NNAPI or GPU-API of TensorFlow-Lite:
val model = loadModelFile(assetManager, modelPath)
val compatList = CompatibilityList()
var nnApiDelegate: NnApiDelegate? = null

val options = Interpreter.Options().apply{
    if (USE_NNAPI &amp;&amp; Build.VERSION.SDK_INT &gt;= Build.VERSION_CODES.P) {
        nnApiDelegate = NnApiDelegate()
        this.addDelegate(nnApiDelegate)
    }
    else if(USE_GPU &amp;&amp; compatList.isDelegateSupportedOnThisDevice){
        val delegateOptions = compatList.bestOptionsForThisDevice
        this.addDelegate(GpuDelegate(delegateOptions))
    } else {
        // if the GPU is not supported, run on 4 threads
        this.setNumThreads(4)
    }
}

val interpreters = Interpreter(model, options)

Does anybody know what could be the reason for this or how to fix that?
Thanks in advance for any tips or clues!
EDIT:
Input size MNIST: 24 x 24 x 255
Input size CIFAR10: 32 x 32 x 3 x 255
I measure the inference times by measuring the time of performing an inference a few thousand times on the device and then I calculate the average afterwards.
","It seems that both models are already performing well on CPU with the inference latency &lt; 1ms.
Accelerators are not always faster than CPU. Often, there is some overhead when accessing the accelerators. Also, accelerators could run certain models / operators really well, but they may not support all the operators that the CPU supports. Additionally, CPU might simply be as-fast or even faster if the performance of a model is memory-bound.
It might worth trying with some larger vision models, e.g. mobilenet_v1_1.0_224 and see if there is a speedup with GPU or other accelerators.
",,,false,
https://stackoverflow.com/questions/69293455,true,The issue involves a difference in behavior between the BatchNorm2d layer in PyTorch and the BatchNormalization layer in TensorFlow. This difference is due to the way the two frameworks handle the mean and variance calculations during training and inference.,,,,,,,Why does Keras BatchNorm produce different output than PyTorch?,"Torch：'1.9.0+cu111'
Tensorflow-gpu：'2.5.0'
I came across a strange thing, when using the Batch Normal layer of tensorflow 2.5 and the BatchNorm2d layer of Pytorch 1.9 to calculate the same Tensor , and the results were quite different (TensorFlow is close to 1, Pytorch is close to 0).I thought at first it was the difference between the momentum and epsilon , but after changing them to the same, the result was the same.
from torch import nn
import torch
x = torch.ones((20, 100, 35, 45))
a = nn.Sequential(
            # nn.Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), padding=0, bias=True),
            nn.BatchNorm2d(100)
        )
b = a(x)

import tensorflow as tf
import tensorflow.keras as keras
from tensorflow.keras.layers import *
x = tf.ones((20, 35, 45, 100))
a = keras.models.Sequential([
            # Conv2D(128, (1, 1), (1, 1), padding='same', use_bias=True),
            BatchNormalization()
        ])
b = a(x)



","Batchnormalization works differently in training and inference,
During training (i.e. when using fit() or when calling the layer/model with the argument training=True), the layer normalizes its output using the mean and standard deviation of the current batch of inputs. That is to say, for each channel being normalized, the layer returns
gamma * (batch - mean(batch)) / sqrt(var(batch) + epsilon) + beta

where:

epsilon is small constant (configurable as part of the constructor arguments)
gamma is a learned scaling factor (initialized as 1), which can be disabled by passing scale=False to the constructor.
beta is a learned offset factor (initialized as 0), which can be disabled by passing center=False to the constructor.

During inference (i.e. when using evaluate() or predict() or when calling the layer/model with the argument training=False (which is the default), the layer normalizes its output using a moving average of the mean and standard deviation of the batches it has seen during training. That is to say, it returns
gamma * (batch - self.moving_mean) / sqrt(self.moving_var + epsilon) + beta.

self.moving_mean and self.moving_var are non-trainable variables that are updated each time the layer in called in training mode, as such:
    moving_mean = moving_mean * momentum + mean(batch) * (1 - momentum)
    moving_var = moving_var * momentum + var(batch) * (1 - momentum)

ref: https://www.tensorflow.org/api_docs/python/tf/keras/layers/BatchNormalization
If you run the pytorch batchnorm in eval mode, you get close results (the rest of the discrepancy comes from the different internal implementation, parameter choices, etc.),
from torch import nn
import torch
x = torch.ones((1, 2, 2, 2))
a = nn.Sequential(
            # nn.Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), padding=0, bias=True),
            nn.BatchNorm2d(2)
        )
a.eval()
b = a(x)
print(b)
import tensorflow as tf
import tensorflow.keras as keras
from tensorflow.keras.layers import *
x = tf.ones((1, 2, 2, 2))
a = keras.models.Sequential([
            # Conv2D(128, (1, 1), (1, 1), padding='same', use_bias=True),
            BatchNormalization()
        ])
b = a(x)
print(b)

out:
tensor([[[[1.0000, 1.0000],
          [1.0000, 1.0000]],

         [[1.0000, 1.0000],
          [1.0000, 1.0000]]]], grad_fn=&lt;NativeBatchNormBackward&gt;)
tf.Tensor(
[[[[0.9995004 0.9995004]
   [0.9995004 0.9995004]]

  [[0.9995004 0.9995004]
   [0.9995004 0.9995004]]]], shape=(1, 2, 2, 2), dtype=float32)

",,,false,
https://stackoverflow.com/questions/61076672,false,The issue does not meet the criteria for deeper analysis as it does not involve an API exhibiting unexpected failures or unpredictable behaviors under specific runtime conditions.,,,,,,,Resnet-50 adversarial training with cleverhans FGSM accuracy stuck at 5%,"I am facing a strange problem when adversarially training a resnet-50, and I am not sure whether is's a logical error, or a bug somewhere in the code/libraries.
I am adversarially training a resnet-50 thats loaded from Keras, using the FastGradientMethod from cleverhans, and expecting the adversarial accuracy to rise at least above 90% (probably 99.x%). The training algorithm, training- and attack-params should be visible in the code.
The problem, as already stated in the title is, that the accuracy is stuck at 5% after training ~3000 of 39002 training inputs in the first epoch. (GermanTrafficSignRecognitionBenchmark, GTSRB).

When training without and adversariy loss function, the accuracy does not get stuck after 3000 samples, but continues to rise &gt; 0.95 in the first epoch.

When substituting the network with a lenet-5, alexnet and vgg19, the code works as expected, and an accuracy absolutely comparabele to the non-adversarial, categorical_corssentropy lossfunction is achieved. I've also tried running the procedure using solely tf-cpu and different versions of tensorflow, the result is always the same.

Code for obtaining ResNet-50:

def build_resnet50(num_classes, img_size):
    from tensorflow.keras.applications import ResNet50
    from tensorflow.keras import Model
    from tensorflow.keras.layers import Dense, Flatten
    resnet = ResNet50(weights='imagenet', include_top=False, input_shape=img_size)
    x = Flatten(input_shape=resnet.output.shape)(resnet.output)
    x = Dense(1024, activation='sigmoid')(x)
    predictions = Dense(num_classes, activation='softmax', name='pred')(x)
    model = Model(inputs=[resnet.input], outputs=[predictions])
    return model


Training:

def lr_schedule(epoch):
    # decreasing learning rate depending on epoch
    return 0.001 * (0.1 ** int(epoch / 10))


def train_model(model, xtrain, ytrain, xtest, ytest, lr=0.001, batch_size=32, 
epochs=10, result_folder=""""):
    from cleverhans.attacks import FastGradientMethod
    from cleverhans.utils_keras import KerasModelWrapper
    import tensorflow as tf

    from tensorflow.keras.optimizers import SGD
    from tensorflow.keras.callbacks import LearningRateScheduler, ModelCheckpoint
    sgd = SGD(lr=lr, decay=1e-6, momentum=0.9, nesterov=True)

    model(model.input)

    wrap = KerasModelWrapper(model)
    sess = tf.compat.v1.keras.backend.get_session()
    fgsm = FastGradientMethod(wrap, sess=sess)
    fgsm_params = {'eps': 0.01,
                   'clip_min': 0.,
                   'clip_max': 1.}

    loss = get_adversarial_loss(model, fgsm, fgsm_params)

    model.compile(loss=loss, optimizer=sgd, metrics=['accuracy'])

    model.fit(xtrain, ytrain,
                    batch_size=batch_size,
                    validation_data=(xtest, ytest),
                    epochs=epochs,
                    callbacks=[LearningRateScheduler(lr_schedule)])


Loss-function:

def get_adversarial_loss(model, fgsm, fgsm_params):
    def adv_loss(y, preds):
         import tensorflow as tf

        tf.keras.backend.set_learning_phase(False) #turn off dropout during input gradient calculation, to avoid unconnected gradients

        # Cross-entropy on the legitimate examples
        cross_ent = tf.keras.losses.categorical_crossentropy(y, preds)

        # Generate adversarial examples
        x_adv = fgsm.generate(model.input, **fgsm_params)
        # Consider the attack to be constant
        x_adv = tf.stop_gradient(x_adv)

        # Cross-entropy on the adversarial examples
        preds_adv = model(x_adv)
        cross_ent_adv = tf.keras.losses.categorical_crossentropy(y, preds_adv)

        tf.keras.backend.set_learning_phase(True) #turn back on

        return 0.5 * cross_ent + 0.5 * cross_ent_adv
    return adv_loss


Versions used:
tf+tf-gpu: 1.14.0
keras: 2.3.1
cleverhans: &gt; 3.0.1 - latest version pulled from github
","It is a side-effect of the way we estimate the moving averages on BatchNormalization.

The mean and variance of the training data that you used are different from the ones of the dataset used to train the ResNet50. Because the momentum on the BatchNormalization has a default value of 0.99, with only 10 iterations it does not converge quickly enough to the correct values for the moving mean and variance. This is not obvious during training when the learning_phase is 1 because BN uses the mean/variance of the batch. Nevertheless when we set learning_phase to 0, the incorrect mean/variance values which are learned during training significantly affect the accuracy. 

You can fix this problem by below approachs:


More iterations


Reduce the size of the batch from 32 to 16(to perform more updates per epoch) and increase the number of epochs from 10 to 250. This way the moving average and variance will converge to the correct values.


Change the momentum of BatchNormalization


Keep the number of iterations fixed but change the momentum of the BatchNormalization layer to update more aggressively the rolling mean and variance (not recommended for production models). 

On the original snippet, add the following code between reading the base_model and defining the new layers:

# ....
base_model = ResNet50(weights='imagenet', include_top=False, input_shape=input_shape)

# PATCH MOMENTUM - START
import json
conf = json.loads(base_model.to_json())
for l in conf['config']['layers']:
    if l['class_name'] == 'BatchNormalization':
        l['config']['momentum'] = 0.5


m = Model.from_config(conf['config'])
for l in base_model.layers:
    m.get_layer(l.name).set_weights(l.get_weights())

base_model = m
# PATCH MOMENTUM - END

x = base_model.output
# ....


Would also recommend you to try another hack provided bu us here.
",,,false,
https://stackoverflow.com/questions/59073146,false,The issue does not meet the criteria for deeper analysis as it does not involve an API exhibiting unexpected failures or unpredictable behaviors under specific runtime conditions.,,,,,,,Better / Faster Result with smaller batch Size linear classifier,"I'm currently training multiple linear classifier with tensorflow and I found something which is strange.

If the batch_size is small, my results are better ( the model learn faster)
I'm working on FashionMNIST

epochs = 300
batch_size = 5000

# Create and fit model
model = tf.keras.Sequential()
model.add(Dense(1, activation=""linear"", input_dim=28*28))
model.add(Dense(10, activation=""softmax"", input_dim=1))
model.compile(optimizer=Adam(), loss=[categorical_crossentropy], metrics=[categorical_accuracy])
model.fit(x_train, y_one_hot_train, validation_data=(x_val, y_one_hot_val), epochs=epochs, batch_size=batch_size)


Results

Batch-size : 20000 and 200 epochs

loss: 2.7494 - categorical_accuracy: 0.2201 - val_loss: 2.8695 - val_categorical_accuracy: 0.2281


Batch-size : 10000 and 200 epochs

loss: 1.7487 - categorical_accuracy: 0.3336 - val_loss: 1.8268 - val_categorical_accuracy: 0.3331


Batch-size : 2000 and 200 epochs

loss: 1.2906 - categorical_accuracy: 0.5123 - val_loss: 1.3247 - val_categorical_accuracy: 0.5113


Batch-size : 1000 and 200 epochs

loss: 1.1080 - categorical_accuracy: 0.5246 - val_loss: 1.1261 - val_categorical_accuracy: 0.5273


Do you know why I got these kind of results ?
","Batch size impacts learning significantly. What happens when you put a batch through your network is that you average the gradients. The concept is that if your batch size is big enough, this will provide a stable enough estimate of what the gradient of the full dataset would be. By taking samples from your dataset, you estimate the gradient while reducing computational cost significantly. The lower you go, the less accurate your esttimate will be, however in some cases these noisy gradients can actually help escape local minima. When it is too low, your network weights can just jump around if your data is noisy and it might be unable to learn or it converges very slowly, thus negatively impacting total computation time.
","In addition to the other answer, it should be noted that increasing the batch size while keeping the number of epochs the same will result in fewer training steps overall. This is because one epoch is the number of steps it takes to iterate over the data once, which is smaller if the batches are bigger.
E.g., increasing batch size by 10 will reduce the number of training steps by 10. So it's not really a fair comparison. Your model with batch size 20000 only gets 600 training steps (3 steps per epoch) whereas the model with batch size 1000 gets 12000 (60 steps per epoch).
",,false,
https://stackoverflow.com/questions/52611575,true,The issue involves a difference in behavior when using tf.while_loop with parallel_iterations greater than 1. The issue is related to the use of tf.assign and the parallel execution of the loop.,,,,,,,tf.while_loop gives wrong result when it runs in parallel,"I want to update a two dimensional tf.variable inside a tf.while_loop in tensorflow by row. For this reason, I use the tf.assign method. The problem is that with my implementation and parallel_iterations&gt;1 the result is wrong. With parallel_iterations=1 the result is correct. The code is like this:

a = tf.Variable(tf.zeros([100, 100]), dtype=tf.int64)

i = tf.constant(0)
def condition(i, var):
    return tf.less(i, 100)

def body(i, var):
    updated_row = method() # This method returns a [1, 100] tensor which is the updated row for the variable
    temp = tf.assign(a[i], updated_row)
    return [tf.add(i, 1), temp]

z = tf.while_loop(condition, body, [i, a], back_prop=False, parallel_iterations=10)


The iterations are completely independent and I do not know what is the problem.

Strangely If I change the code like this:

a = tf.Variable(tf.zeros([100, 100]), dtype=tf.int64)

i = tf.constant(0)
def condition(i, var):
    return tf.less(i, 100)

def body(i, var):
    zeros = lambda: tf.zeros([100, 100], dtype=tf.int64)
    temp = tf.Variable(initial_value=zeros, dtype=tf.int64)
    updated_row = method() # This method returns a [1, 100] tensor which is the updated row for the variable
    temp = tf.assign(temp[i], updated_row)
    return [tf.add(i, 1), temp]

z = tf.while_loop(condition, body, [i, a], back_prop=False, parallel_iterations=10)


the code gives the correct outcome for parallel_iterations&gt;1. Can someone explain me what is going on here and give me an efficient solution to update the variable because the original variable I want to update is huge and the solution I found is very inefficient.
","In tf.function I've found the following:


  Key Point: Any Python side-effects (appending to a list, printing with print, etc) will only happen once, when func is traced. To have side-effects executed into your tf.function they need to be written as TF ops:


I'm pretty sure that's what's going on here. You're expecting a to change but that is a ""side effect"" ( https://runestone.academy/runestone/books/published/fopp/Functions/SideEffects.html ) which tensorflow does not fully support. When you change a to temp you're no longer relying on the side effect and the code works.
",,,false,
https://stackoverflow.com/questions/48119473,false,The issue does not meet the criteria for deeper analysis as it does not involve an API exhibiting unexpected failures or unpredictable behaviors under specific runtime conditions.,,,,,,,Gram-Schmidt orthogonalization in pure Tensorflow: performance for iterative solution is much slower than numpy,,,,,false,
https://stackoverflow.com/questions/46771019,false,The issue does not meet the criteria for deeper analysis as it does not involve an API exhibiting unexpected failures or unpredictable behaviors under specific runtime conditions.,,,,,,,tensorflow keras do not use all available resources,,,,,false,
https://stackoverflow.com/questions/45577747,false,The issue does not meet the criteria for deeper analysis as it does not involve an API exhibiting unexpected failures or unpredictable behaviors under specific runtime conditions.,,,,,,,CNN in Tensorflow - loss remains constant,,,,,false,
https://stackoverflow.com/questions/43872812,false,The issue does not meet the criteria for deeper analysis as it does not involve an API exhibiting unexpected failures or unpredictable behaviors under specific runtime conditions.,,,,,,,Tensorflow AdamOptimizer vs Gradient Descent,,,,,false,
https://stackoverflow.com/questions/42938603,false,The issue does not meet the criteria for deeper analysis as it does not involve an API exhibiting unexpected failures or unpredictable behaviors under specific runtime conditions.,,,,,,,tensorflow conv3d ignoring 1st spacial dimension,,,,,false,
https://stackoverflow.com/questions/39152282,false,The issue does not meet the criteria for deeper analysis as it does not involve an API exhibiting unexpected failures or unpredictable behaviors under specific runtime conditions.,,,,,,,Multilayer feedforward net fails to train in TensorFlow,,,,,false,
https://stackoverflow.com/questions/35500242,false,The issue does not meet the criteria for deeper analysis as it does not involve an API exhibiting unexpected failures or unpredictable behaviors under specific runtime conditions.,,,,,,,"Implement Logistic Regression(LR) in Tensorflow,But Result looks strange when using bias but good without bias",,,,,false,
https://stackoverflow.com/questions/73804090,false,The issue does not meet the criteria for deeper analysis as it does not involve an API exhibiting unexpected failures or unpredictable behaviors under specific runtime conditions.,,,,,,,Is it possible to convert a TensorFlow (Keras) model from BGR to RGB?,"I have converted a Caffe model, learned on BGR data, to ONNX format and then from ONNX to TensorFlow (Keras)
So now I have a Keras model, learned on BGR data. Is it possible to convert it in such a way, that it will properly work with RGB data?
I've tried to convert it to OpenVINO with the --reverse_input_channel flag and then back to TensorFlow, but it seems that openvino2tensorflow works very poorly, so it didn't work. Maybe there is some simpler way?
Update
I've realized that I get SavedModel through Keras model, so I've updated the question.
Update 2
I've applied AndrzejO solution. However, now my model gives much worse results than before. Am I doing something wrong?
from keras.layers import Input, Lambda
from keras.models import Model

input_shape = k_model.get_layer(index = 0).input_shape[0][1:]
inputs = Input(shape=input_shape)
lambda_layer = Lambda(lambda x: x[:,:,:,::-1])(inputs)
outputs = k_model(lambda_layer)
k_model = Model(inputs=inputs, outputs=outputs)

Update 3
Regarding AndrdzejO hint, I've tested the reversed model on the reversed (BGR) images and compared results with the former model on normal (RGB) images. That's strange - they are similar but not the same. Below is the code (in Java) that reverses the image:
  public static byte[] getPixelsBGR(Bitmap image) {
    // calculate how many bytes our image consists of
    int bytes = image.getByteCount();

    ByteBuffer buffer = ByteBuffer.allocate(bytes); // Create a new buffer
    image.copyPixelsToBuffer(buffer); // Move the byte data to the buffer

    byte[] pixels = buffer.array(); // Get the underlying array containing the data.

    // Copy pixels into place
    for (int i = 0; i &lt; pixels.length/4; i++)
    {
      byte pom = pixels[i*4];
      pixels[i * 4] = pixels[i * 4 + 2];
      pixels[i * 4 + 2] = pom;
    }

    return pixels;
  }

if (!modelBGR)
{
    byte[] pixels = getPixelsBGR(resizedBitmap);
    ByteBuffer pixelBuffer = ByteBuffer.wrap(pixels);
    resizedBitmap.copyPixelsFromBuffer(pixelBuffer);
}

**Update 4**

**AndrzejO**'s solution works perfectly. It correctly reverses the channel order. The thing was I had been subtracting channels mean in the tflite metadata and forgot that I need to also reverse the order of channels there. After I've corrected this, I have the exact same results, which implies that the reversion of channels has worked perfectly.

For some reason, reversing the channel order in my case makes inference less accurate (as if though the channels have alreasdy been reversed in some earlier conversion process), but that's a different thing to investigate

","You can create a new model: first a lambda layer which will reverse the channel order, than your saved model:
input_shape = old_model.get_layer(index = 0).input_shape[0][1:]
inputs = Input(shape=input_shape)
lambda_layer = Lambda(lambda x: x[:,:,:,::-1])(inputs)
outputs = old_model(lambda_layer)
new_model = Model(inputs=inputs, outputs=outputs)

",,,false,
https://stackoverflow.com/questions/66675299,true,"The issue involves the onnx-tf conversion process, where the conv2d outputs are split into separate convolutions and then concatenated together. This behavior is unexpected and may lead to redundant conv2d operators in the converted model.",,,,,,,Pytorch -&gt; [Onnx -&gt; tensorflow] -&gt; tflite generates lots of redundant conv2d operators,"I am converting efficientnet from onnx to tensorflow for further conversion to tflite. The conversion from onnx to tensorflow yields strange results
Onnx has 1 conv2d operator

And after converting to tensorflow and later tflite model I get this stack of convs(All of them couldn't fit in the picture, there are 32 of the in total, as the number of kernels in onnx)

I suspect the issue is with onnx-tf conversion. It seems like the conv2d output are split into 32 separate outputs, then they are processed individually on separate conv2d operator and then concatenated together.
There are 2 connected issues on their github.
First has a fix for depthwise convs but it can't be applied in my case
https://github.com/onnx/onnx-tensorflow/issues/473
https://github.com/onnx/onnx-tensorflow/issues/754
Looking at the source of https://github.com/onnx/onnx-tensorflow/blob/master/onnx_tf/handlers/backend/conv_mixin.py
They do indeed split the output produce separate convolutions and later concatenate them.
Can this split to multiple convolutions be avoided?
","After some additional digging I've found the following

My convs were depthwise(conv2d is depthwise in pytorch and onnx if it has groups parameter &gt; 1)
This bunch of convs is an inefficient way of doing a depthwise conv. To do it efficiently we need to use tf.depthwiseconv

To fix this in onnx-tf v1.7.0 you should apply a patch to onnx-tf source code posted here https://github.com/onnx/onnx-tensorflow/issues/473#issuecomment-533522917
In current master branch there is an attempt to fix the issue by detecting depthwise convs but it currently contains a bug. To fix the bug in master branch you can apply a fix I posted here https://github.com/onnx/onnx-tensorflow/issues/754#issuecomment-801775203
I've used the master branch and the fix above resulting in a small graph with depthwise conv
I've also created a fork with the fix above, so you can do
pip install git+https://github.com/Vozf/onnx-tensorflow

to apply it instead of patching on your own

It seems like the issue should be fixed in the next major release(probably 1.8.0)
Also consider using https://github.com/nerox8664/onnx2keras as it supports depthwise convolutions and moreover supports full nhwc conversion which allows removing all the transpose ops.
",,,false,
https://stackoverflow.com/questions/65512540,true,"The issue involves a compatibility problem with the input shape of the TensorFlow model. The model expects an input shape with a value of 4 for the last axis, but receives an input with shape [None, 1], causing a ValueError.",,,,,,,TensorFlow: &quot;Expected axis -1 of input shape...&quot;,"import tensorflow.keras as keras
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from tensorflow.keras.layers import Dense
from sklearn.preprocessing import OneHotEncoder

if __name__ == ""__main__"":
    file = pd.read_csv(""datensatz.csv"")

    file.loc[file['species'] == 'Iris-setosa', 'species'] = 0
    file.loc[file['species'] == 'Iris-versicolor', 'species'] = 1
    file.loc[file['species'] == 'Iris-virginica', 'species'] = 2

    data = file.to_numpy()

    train_x, test_x, train_y, test_y = train_test_split(data[:, :4], data[:, 4], test_size=0.20)

    encoder = OneHotEncoder(sparse=False)

    train_y = encoder.fit_transform(train_y.reshape(-1, 1))
    test_y = encoder.fit_transform(test_y.reshape(-1, 1))

    model = keras.models.Sequential([
        # sepal length, sepal width, petal length, petal width, species as number
        Dense(units=1, name=""Input"", input_shape=(4,), activation=""relu""),
        Dense(units=4, activation='relu', name=""Hidden""),
        Dense(units=3, activation='softmax', name=""Output"")
    ])

    model.compile(optimizer=keras.optimizers.Adam(), loss='categorical_crossentropy', metrics=['accuracy'])
    model.summary()

    model.fit(train_x.astype('float'), train_y, verbose=2, batch_size=5, epochs=100)
    print('Accuracy:', model.evaluate(test_x.astype('float'), test_y)[1])

    insert = np.array([6.1, 3.0, 4.6, 1.4]).astype('float')
    print(model.predict(x=insert))

Hi guys, I'm trying to predict something, but I get a really strange error(TensorFlow is really known for that!), btw. I use the famous Iris dataset.
At first, I could train the modul, that works fine, as expected.

Epoch 100/100
24/24 - 0s - loss: 0.2863 - accuracy: 0.9333
1/1 [==============================] - 0s 999us/step - loss: 0.2402 - accuracy: 0.9333
That's good enough! The model evaluate works as well.
Accuracy: 0.9333333373069763

My train_x data looks like this:
[[5.9 3.0 4.2 1.5]
 [5.4 3.0 4.5 1.5]
 [5.6 3.0 4.5 1.5]
 [5.7 3.8 1.7 0.3]
 [5.1 3.3 1.7 0.5]
 [6.4 2.9 4.3 1.3]
 [7.7 3.8 6.7 2.2]
 [6.0 3.4 4.5 1.6]
...]

insert = np.array([6.1, 3.0, 4.6, 1.4]).astype('float')
print(model.predict(x=insert))
That dosen't work! I tried it with an element of the testset as well, but that gave me the same result, a really nice and bad to read error.

ValueError: Input 0 of layer sequential is incompatible with the layer: expected axis -1 of input shape to have value 4 but received input with shape [None, 1]

Frankly I have no idea, what tensorflow is trying to tell me.
How can I fix it? I just want to predict, what flower could be considered.
If anyone is interessted in the dataset:
https://www.kaggle.com/akashsri99/deep-learning-iris-dataset-keras
","Predict need batch dimension. Try this:
insert = np.array([[6.1, 3.0, 4.6, 1.4]]).astype('float') 

",,,false,
https://stackoverflow.com/questions/56199905,false,The issue does not provide enough information to determine if it involves an API exhibiting unexpected failures or unpredictable behaviors under specific runtime conditions.,,,,,,,How to create sum of different kernel objects in TensorFlow Probability?,,,,,false,
https://stackoverflow.com/questions/52584378,false,The issue does not provide enough information to determine if it involves an API exhibiting unexpected failures or unpredictable behaviors under specific runtime conditions.,,,,,,,How to execute inference of tensorflow model in Android,,,,,false,
https://stackoverflow.com/questions/51665533,false,The issue does not provide enough information to determine if it involves an API exhibiting unexpected failures or unpredictable behaviors under specific runtime conditions.,,,,,,,"validation and training don&#39;t converge at the same time, but validation still converges",,,,,false,
https://stackoverflow.com/questions/51617315,false,The issue does not provide enough information to determine if it involves an API exhibiting unexpected failures or unpredictable behaviors under specific runtime conditions.,,,,,,,Tensorflow lite accuracy drop on mobile device,,,,,false,
https://stackoverflow.com/questions/50512783,false,The issue does not provide enough information to determine if it involves an API exhibiting unexpected failures or unpredictable behaviors under specific runtime conditions.,,,,,,,TensorRT for upscaling images serves not expected results,,,,,false,
https://stackoverflow.com/questions/49351711,false,The issue does not provide enough information to determine if it involves an API exhibiting unexpected failures or unpredictable behaviors under specific runtime conditions.,,,,,,,Saved multi-GPU trained model loaded into single-GPU; inconsistent results,,,,,false,
https://stackoverflow.com/questions/47694415,false,The issue does not provide enough information to determine if it involves an API exhibiting unexpected failures or unpredictable behaviors under specific runtime conditions.,,,,,,,Tensorboard not able to display all summaries,,,,,false,
https://stackoverflow.com/questions/46290884,false,The issue does not provide enough information to determine if it involves an API exhibiting unexpected failures or unpredictable behaviors under specific runtime conditions.,,,,,,,FailedPreconditionError: sequence_length(0) &lt;= X,,,,,false,
https://stackoverflow.com/questions/42816644,false,The issue does not provide enough information to determine if it involves an API exhibiting unexpected failures or unpredictable behaviors under specific runtime conditions.,,,,,,,"tensorflow: Strange result from convolution compared to theano (not flipping, though)",,,,,false,
https://stackoverflow.com/questions/38030552,false,The issue does not provide enough information to determine if it involves an API exhibiting unexpected failures or unpredictable behaviors under specific runtime conditions.,,,,,,,Basic softmax model implementation on 150x150 images,,,,,false,
https://stackoverflow.com/questions/52700621,false,The issue does not provide enough information to determine if it involves an API exhibiting unexpected failures or unpredictable behaviors under specific runtime conditions.,,,,,,,Tensorflow server: I don&#39;t want to initialize global variables for every session,,,,,false,
https://stackoverflow.com/questions/57122907,true,The issue involves an AttributeError in Tensorboard related to the 'ModelCheckpoint' object. This unexpected behavior may be caused by a mix of imports from 'keras' and 'tensorflow.keras'.,,,,,,,Tensorboard AttributeError: &#39;ModelCheckpoint&#39; object has no attribute &#39;on_train_batch_begin&#39;,"I'm currently using Tensorboard using the below callback as outlined by this SO post as shown below.

from keras.callbacks import ModelCheckpoint

CHECKPOINT_FILE_PATH = '/{}_checkpoint.h5'.format(MODEL_NAME)
checkpoint = ModelCheckpoint(CHECKPOINT_FILE_PATH, monitor='val_acc', verbose=1, save_best_only=True, mode='max', period=1)


When I run Keras' dense net model, I get the following error. I haven't had any issues running Tensorboard in this manner with any of my other models, which makes this error very strange. According to this Github post, the official solution is to use the official Tensorboard implementation; however, this requires upgrading to Tensorflow 2.0, which is not ideal for me. Anyone know why I'm getting the following error for this specific densenet and is there a workaround/fix that someone knows?


  AttributeError                            Traceback (most recent call
  last)  in ()
       26                     batch_size=32,
       27                     class_weight=class_weights_dict,
  ---&gt; 28                     callbacks=callbacks_list
       29                    )
       30 
  
  2 frames
  /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/callbacks.py
  in _call_batch_hook(self, mode, hook, batch, logs)
      245     t_before_callbacks = time.time()
      246     for callback in self.callbacks:
  --&gt; 247       batch_hook = getattr(callback, hook_name)
      248       batch_hook(batch, logs)
      249     self._delta_ts[hook_name].append(time.time() - t_before_callbacks)
  
  AttributeError: 'ModelCheckpoint' object has no attribute
  'on_train_batch_begin'


The dense net I'm running

from tensorflow.keras import layers, Sequential
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.applications.densenet import preprocess_input, DenseNet121
from keras.optimizers import SGD, Adagrad
from keras.utils.np_utils import to_categorical

IMG_SIZE = 256
NUM_CLASSES = 5
NUM_EPOCHS = 100

x_train = np.asarray(x_train)
x_test = np.asarray(x_test)

y_train = to_categorical(y_train, NUM_CLASSES)
y_test = to_categorical(y_test, NUM_CLASSES)


x_train = x_train.reshape(x_train.shape[0], IMG_SIZE, IMG_SIZE, 3)
x_test = x_test.reshape(x_test.shape[0], IMG_SIZE, IMG_SIZE, 3)

densenet = DenseNet121(
    include_top=False,
    input_shape=(IMG_SIZE, IMG_SIZE, 3)
)

model = Sequential()
model.add(densenet)
model.add(layers.GlobalAveragePooling2D())
model.add(layers.Dense(NUM_CLASSES, activation='softmax'))
model.summary()

model.compile(loss='categorical_crossentropy',
              optimizer='adam',
              metrics=['accuracy'])

history = model.fit(x_train,
                    y_train,
                    epochs=NUM_EPOCHS,
                    validation_data=(x_test, y_test),
                    batch_size=32,
                    class_weight=class_weights_dict,
                    callbacks=callbacks_list
                   )

","I replace this line 

from keras.callbacks import EarlyStopping, ModelCheckpoint


To this line 

from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint

","Yes imports are mixed from keras and tensorflow 

try sticking on to tensorflow.keras for example :

from tensorflow.keras.callbacks import EarlyStopping

","Make all imports from either keras or tensorflow.keras

I hope this will sort it out!
",false,
https://stackoverflow.com/questions/65402617,false,The issue does not meet the criteria for deeper analysis as it does not involve unexpected failures or unpredictable behaviors of an API under specific runtime conditions.,,,,,,,Tensorflow automl model in react,"I'm trying to move a tensorflow model from its original html into a react app (built with create-react-app).
My App.js looks like this:
import logo from './logo.svg';
import * as tf from ""@tensorflow/tfjs"";
// import { loadImageclassification } from ""@tensorflow/tfjs"";
import './App.css';
import * as automl from ""@tensorflow/tfjs-automl"";
import * as modelJSON from './model.json';

function App() {

var loadFile = function(event) {
    var image = document.getElementById('output');
    image.src = URL.createObjectURL(event.target.files[0]);
  run();
};

async function run() {
  console.log(modelJSON);
        // const model = await tf.loadImageclassification('model.json');
        const model = await automl.loadImageClassification(modelJSON);
        const image = document.getElementById('output');
        const predictions = await model.classify(image);
        console.log(predictions);

        const pre = document.getElementById('result');
        pre.textContent = JSON.stringify(predictions, null, 2);
}

  return (
  &lt;div className=""App""&gt;
    &lt;div className=""hero-text""&gt;
      &lt;h1&gt;classifier&lt;/h1&gt;
      &lt;h3&gt;Upload a picture to see what type it is! &lt;/h3&gt;
      &lt;p&gt;
        &lt;input type=""file""  accept=""image/*"" name=""image"" id=""file""  onChange={loadFile} /&gt;
      &lt;/p&gt;
      &lt;div id=""demobox""&gt;
        &lt;p&gt;
          &lt;label htmlFor=""file""&gt;Upload your image&lt;/label&gt;
        &lt;/p&gt;
      &lt;/div&gt; 
      &lt;p&gt;&lt;img id=""output"" width=""200"" alt=""output"" /&gt;&lt;/p&gt;
      &lt;div className=""result"" id=""result""&gt;
      &lt;/div&gt;
    &lt;/div&gt;
  &lt;/div&gt;
  );
}

export default App;

My index.html looks like this:
&lt;!DOCTYPE html&gt;
&lt;html lang=""en""&gt;
  &lt;head&gt;
    &lt;meta charset=""utf-8"" /&gt;
    &lt;link rel=""icon"" href=""%PUBLIC_URL%/favicon.ico"" /&gt;
    &lt;meta name=""viewport"" content=""width=device-width, initial-scale=1"" /&gt;
    &lt;meta name=""theme-color"" content=""#000000"" /&gt;
    &lt;meta
      name=""description""
      content=""Web site created using create-react-app""
    /&gt;
    &lt;link rel=""apple-touch-icon"" href=""%PUBLIC_URL%/logo192.png"" /&gt;
    &lt;link rel=""manifest"" href=""%PUBLIC_URL%/manifest.json"" /&gt;
  &lt;/head&gt;
  &lt;body&gt;
    &lt;noscript&gt;You need to enable JavaScript to run this app.&lt;/noscript&gt;
    &lt;div id=""root""&gt;&lt;/div&gt;
  &lt;/body&gt;
&lt;/html&gt;

I am getting the following error, which seems to be issuing from somewhere in the loadImageClassification method:
Unhandled Rejection (TypeError): modelUrl.lastIndexOf is not a function

Edit:
Apparently loadImageClassification uses a fetch request under the hood and so requires a remote file (which is strange, because it seemed to work fine in the static index.html original version of this same project).
So I am now trying it just with a localhost express server, which at present looks like this:
const modelJSON = require('./model.json');

const express = require(""express"");
const bodyParser = require(""body-parser"");
const CORS = require(""cors"");

const app = express();

app.use(bodyParser.json());
app.use(CORS());

let modelObj = modelJSON;

app.get(""/"", (req, res) =&gt; {
  // console.log(modelObj);
  res.send(modelObj);
});

app.listen(5000, () =&gt; {
  console.log(""Server listening on port 5000"");
});


I can see the correct data when I navigate to localhost5000, but when I change
async function run() {
  const model = await automl.loadImageClassification(modelJSON);

to
async function run() {
  const modelUrl = ""http://localhost:5000/"";
  const model = await automl.loadImageClassification(modelUrl);

I get these errors:

Edit 2:
My server.js file now looks like this:

This produces the same errors as in the previous screenshot. (I am leaving in the comments that mess of an attempt to include all the shard files in this server.js file screenshot just because it may illustrate that I don't understand how to pass those ancillary model files to loadImageClassification when it makes its fetch request.)
So presumably the problem now has to do with the fact that loadImageClassification assumes that the ...shard__of6.bin and dict files are in the same directory as the model.json file.
So the question may (?) be: how to simulate the file structure that it (i.e., loadImageClassification) is expecting within a remote node server.
Fundamental confusion:
I'm don't understand why, when loadImageClassification is in the original static html, it does not seem to require a remote url from which to fetch model.json — but then when I put it in my react app, it suddenly gives me this error: ""Fetch API cannot load file:///Users///client/src/model.json. URL scheme must be 'http' or 'https' for CORS request.""
","What's the location of the model on your local device?
Try changing
const modelUrl = ""http://localhost:5000/""

to
const modelUrl = 'model/model.json'

if the model is in build/model/, or to whatever the location is.
",,,false,
https://stackoverflow.com/questions/46355068,false,The issue does not meet the criteria for deeper analysis as it is related to a loss function in Keras/TensorFlow rather than an API exhibiting unexpected failures or unpredictable behaviors.,,,,,,,keras loss function for 360 degree prediction,"I'm trying to predict azimuths using keras/tensorflow.  y_true ranges from 0-359, but I need a loss function that handles predictions that have wrapped around and are outside that range.  Unfortunately, when I try any kind of modular division tf.mod() or %, i get an error...

LookupError: No gradient defined for operation 'FloorMod' (op type: FloorMod)


so I think I've worked around this with the following...

def mean_squared_error_360(y_true, y_pred):
  delta = K.minimum(K.minimum(K.abs(y_pred - y_true),
                              K.abs(y_pred - (360+y_true))),
                              K.abs(y_true - (360+y_pred)))
  return K.mean(K.square(delta), axis=-1)

def rmse_360(y_true, y_pred):
  return K.sqrt(mean_squared_error_360(y_true, y_pred))


model.compile(loss=mean_squared_error_360,
              optimizer=rmsprop(lr=0.0001),
              metrics=[rmse_360])


this handles the following edge cases...  I haven't come across predictions &lt; 0, so I'm not addressing.

y =   1  y_pred = 361  err = 0
y = 359  y_pred =   1  err = 2
y = 359  y_pred = 361  err = 2


Questions


this feels clunky; is there a smarter solution?
intuitively, I think there's no difference in outcome between using mean_squared_error and root_mean_squared_error as the loss...  the gradients will be different, but the same optimum weights will solve both, right?  Is there any reason to pick one over the other?  I'd guess mse is slightly simpler than rmse, but that should be trivial.  I've tried both, and using rmse 'feels' like a more orderly descent than mse...  is there something about the magnitude of those squared errors that make it jump around more?


Thanks in advance.

EDIT

for whatever reason...  my original mse seemed to be fitting the training set, but the validation set seemed pretty noisy epoch to epoch, without any real improvement after a few epochs.  rmse seemed like a more orderly descent...  until the loss went to inf after improving for a couple dozen epochs.  I might have bigger issues than the loss function.

EDIT 2 - adding my implementation @Patwie answer below

ah...  trig!!  of course!!  unfortunately, I'm using tf v1.0 which doesn't seem to have tf.atan2().  strangely, I couldn't find atan2 implementation in tf repository, but I think asos-ben's suggestion in issue 6095 does the trick.  see here:  https://github.com/tensorflow/tensorflow/issues/6095

def atan2(x, y, epsilon=1.0e-12):
  x = tf.where(tf.equal(x, 0.0), x+epsilon, x)
  y = tf.where(tf.equal(y, 0.0), y+epsilon, y)    
  angle = tf.where(tf.greater(x,0.0), tf.atan(y/x), tf.zeros_like(x))
  angle = tf.where(tf.logical_and(tf.less(x,0.0),  tf.greater_equal(y,0.0)), tf.atan(y/x) + np.pi, angle)
  angle = tf.where(tf.logical_and(tf.less(x,0.0),  tf.less(y,0.0)), tf.atan(y/x) - np.pi, angle)
  angle = tf.where(tf.logical_and(tf.equal(x,0.0), tf.greater(y,0.0)), 0.5*np.pi * tf.ones_like(x), angle)
  angle = tf.where(tf.logical_and(tf.equal(x,0.0), tf.less(y,0.0)), -0.5*np.pi * tf.ones_like(x), angle)
  angle = tf.where(tf.logical_and(tf.equal(x,0.0), tf.equal(y,0.0)), tf.zeros_like(x), angle)
  return angle

# y in radians
def rmse_360_2(y_true, y_pred):
  return K.mean(K.abs(atan2(K.sin(y_true - y_pred), K.cos(y_true - y_pred))))


Only about 7 epochs in on a test run, but it seems promising.
",,,,false,
https://stackoverflow.com/questions/33699174,false,"The issue does not have any information provided in the body, so it cannot be analyzed for API-related problems.",,,,,,,TensorFlow&#39;s ReluGrad claims input is not finite,,,,,false,
https://stackoverflow.com/questions/47947629,false,"The issue does not have any information provided in the body, so it cannot be analyzed for API-related problems.",,,,,,,"Tensorflow: Keras, Estimators and custom input function",,,,,false,
https://stackoverflow.com/questions/41079660,false,"The issue does not have any information provided in the body, so it cannot be analyzed for API-related problems.",,,,,,,Why I need a password to get access to the jupyter notebook when I use docker machine?,,,,,false,
https://stackoverflow.com/questions/56660164,false,"The issue does not have any information provided in the body, so it cannot be analyzed for API-related problems.",,,,,,,"Jupyter Notebook, NameError: is not defined, %%time prevents assignment",,,,,false,
https://stackoverflow.com/questions/47962156,false,"The issue does not have any information provided in the body, so it cannot be analyzed for API-related problems.",,,,,,,Unable to build Tensorflow from source MacOS High Sierra,,,,,false,
https://stackoverflow.com/questions/45644606,false,"The issue does not have any information provided in the body, so it cannot be analyzed for API-related problems.",,,,,,,Illegal ambiguous match on configurable attribute &quot;deps&quot; in //tensorflow/core/grappler/costs:utils: when trying to build Tensorflow with GPU support,,,,,false,
https://stackoverflow.com/questions/57311222,false,"The issue does not meet the criteria for deeper analysis as it is related to a limitation in the number of predictions in the TensorFlow Object Detection API, rather than an API exhibiting unexpected failures or unpredictable behaviors.",,,,,,,Limitation of number of predictions in Tensorflow Object Detection API,"I’ve trained a Faster R-CNN model with Tensorflow Object Detection API with and encountered a strange issue. The output of the model has max 100 predictions, despite, there are many more objects in the image. This is a case for each image I’ve tested.

I’ve found similar issue on Ten GitHub, but from what I can see they are not doing much in these regards.
https://github.com/tensorflow/tensorflow/issues/30464

Maybe you had a similar issue in the past? Any idea how to tackle this?


",,,,false,
https://stackoverflow.com/questions/40406469,false,"The issue does not have any information provided in the body, so it cannot be analyzed for API-related problems.",,,,,,,"tf.train.range_input_producer(epoch_size, shuffle=True) does not terminate nor induce CPU/GPU load",,,,,false,
https://stackoverflow.com/questions/39774449,false,"The issue does not have any information provided in the body, so it cannot be analyzed for API-related problems.",,,,,,,Fully-convolutional ResNets using TF-Slim run very slow,,,,,false,
https://stackoverflow.com/questions/39540806,false,"The issue does not meet the criteria for deeper analysis as it is related to the consistency of output in an RNN model using TensorFlow, rather than an API exhibiting unexpected failures or unpredictable behaviors.",,,,,,,Tensorflow - Any input gives me same output,"I am facing a very strange problem where I am building an RNN model using tensorflow and then storing the model variables (all) using tf.Saver after I finish training.

During testing, I just build the inference part again and restore the variables to the graph. The restoration part does not give any error.

But when I start testing on the evaluation test, I always get same output from the inference all i.e. for all test inputs, I get the same output.

I printed the output during training and I do see that output is different for different training samples and cost is also decreasing.
But when I do testing, it always gives me same output no matter what is the input.

Can someone help me to understand why this could be happening? I want to post some minimal example but as I am not getting any error, I am not sure what should I post here. I will be happy to share more information if it can help the issue. 

One difference I have between the inference graph during training and testing is the number of time steps in RNN. During training I train for n steps (n = 20 or more) for a batch before updating gradients while for testing  I just use one step as I only want to predict for that input.

Thanks
",,,,false,
https://stackoverflow.com/questions/73415068,false,"The issue does not meet the criteria for deeper analysis as it is related to the functionality of the HuggingFace Trainer() function in a specific environment, rather than an API exhibiting unexpected failures or unpredictable behaviors.",,,,,,,"HuggingFace Trainer() does nothing - only on Vertex AI workbench, works on colab","I am having issues getting the Trainer() function in huggingface to actually do anything on Vertex AI workbench notebooks.
I'm totally stumped and have no idea how to even begin to try debug this.
I made this small notebook: https://github.com/andrewm4894/colabs/blob/master/huggingface_text_classification_quickstart.ipynb
If you set framework=pytorch and run it in colab it runs fine.
I wanted to move from colab to something more persistent so tried Vertex AI Workbench notebooks on GCP. I created a user managed notebook (PyTorch:1.11, 8 vCPUs, 30 GB RAM, NVIDIA Tesla T4 x 1) and if i try run the same example notebook in jupyterlab on the notebook it just seems to hang on the Trainer() call and do nothing.
It looks like the GPU is not doing anything either for some reason (it might not be supposed to since i think Trainer() is some pretraining step):
(base) jupyter@pytorch-1-11-20220819-104457:~$ nvidia-smi
Fri Aug 19 09:56:10 2022       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 470.57.02    Driver Version: 470.57.02    CUDA Version: 11.4     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |
| N/A   41C    P8     9W /  70W |      3MiB / 15109MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|  No running processes found                                                 |
+-----------------------------------------------------------------------------+

I found this thread that maybe seems like a similar problem so i played with as many Trainer() args as i could but no luck.
So im kind of totally blocked here - i refactored the code to be able to use Tensorflow which does work for me (after i installed tensorflow on the notebook) but its much slower for some reason.
Basically this was all working great (in my actual real code im working on) on colab's but when i tried to move to Vertex AI Notebooks i seem to be now blocked by this strange issue.
Any help or advice much appreciated, i'm new to HuggingFace and Pytorch etc too so not even sure what things i might try or ways to try run in debug etc maybe.
Workaround
i noticed that if i make a new workbook NumPy/SciPy/scikit-learn 4 vCPUs, 15 GB RAM , NVIDIA Tesla T4 x (instead of the official pytorch one from the dropdown) and install pytorch myself with conda install pytorch torchvision torchaudio cudatoolkit=10.2 -c pytorch it all works.
","We faced exactly the same issue on Vertex AI, when we selected the pre-built PyTorch=1.13 environment. The training would just freeze and do nothing for hours.
What worked for us was to select an older version of the PyTorch environment. So we went for the PyTorch=1.9 environment and the training worked just fine.

We could see the logs already as the training commenced. So my guess is that there's something specifically about the newer PyTorch environment on Vertex AI that prevents things from working as expected.
",,,false,
https://stackoverflow.com/questions/55235620,false,"The issue does not meet the criteria for deeper analysis as it is related to the installation of Jupyter Notebook and the functionality of NumPy and TensorFlow, rather than an API exhibiting unexpected failures or unpredictable behaviors.",,,,,,,After installing Jupyter Notebook NumPy and TensorFlow are not working,,,,,false,
https://stackoverflow.com/questions/42422646,false,"The issue does not meet the criteria for deeper analysis as it is related to training a partial model in Keras, specifically in the context of a GAN model, rather than an API exhibiting unexpected failures or unpredictable behaviors.",,,,,,,Keras train partial model issue (about GAN model),,,,,false,
https://stackoverflow.com/questions/71475575,false,The issue does not meet the criteria for deeper analysis as it is related to the installation of cuDF and does not involve an API exhibiting unexpected failures or unpredictable behaviors under specific runtime conditions.,,,,,,,cudf instllation issue on centos7,"I'm new to rapids ai libraries. I've an existing conda environment yaml file where I'm using python 3.8.5, tensorflow 2.7.0, opencv-python-headless 4.5.5.62, numpy 1.22.2, pandas 1.4.1, pandas-profiling 3.1.0, seaborn 0.11.2,  matplotlib 3.5.1, jupyterlab 3.2.9.
I've added below 2 channels to the file:

rapidsai
nvidia

And the below packages:

cudf=22.02
cudatoolkit=11.5

The installation is going on for hours and while trying to find incompatible packages, it seems to be in some sort of loop as I keep seeing below message multiple times in the terminal:
Found conflicts! Looking for incompatible packages.
Is there any known issue/limitations that I should be aware of?
Since we don't get interactive shell on GPU h/w easily, I'm trying the conda environment update on non-GPU machine and once installed, I'll try cudf package on GPU machine.
EDIT1:
This is what I have as working without tensorflow and tensorflow-hub

    name: cudf-env
        channels:
          - default
          - rmg
          - rapidsai
          - nvidia
          - numba
          - conda-forge
          - anaconda
        dependencies:
          - glibc=2.19
          - libgcc-ng=11.2.0
          - python=3.8.5
          - cudf=22.02
          - cudatoolkit=11.2
          - pytest=6.1.2
          - pandas=1.3.5
          - numpy=1.21.5
          - requests=2.25.0
          - scikit-learn=0.24.2
          - dill=0.3.4
          - tqdm=4.62.3
          - ruamel.yaml=0.17.19
          - yappi=1.3.3
          - black=22.1.0
          - pillow=9.0.1
          - jupyterlab=3.2.9
          - matplotlib=3.5.1
          - seaborn=0.11.2
          - plotly=5.6.0
          - pandas-profiling=3.1.0
          - black=22.1.0
        #  - pip
        #  - pip:
        #      - tensorflow==2.7.0
        #      - tensorflow-hub==0.12.0
        #      - opencv-python-headless==4.5.5.62
        #      - opencv-contrib-python-headless==4.5.5.62

Now, if I uncomment the pip section, the anaconda crashes while creating the environment. Since pip may not be supported with cudf, I tried following as well, the conda create env hangs while solving the environment (strangely, it's not resolving from conda-forge channel):

    name: cudf-env
        channels:
          - default
          - rmg
          - rapidsai
          - nvidia
          - numba
          - conda-forge
          - anaconda
        dependencies:
          - glibc=2.19
          - libgcc-ng=11.2.0
          - python=3.8.5
          - cudf=22.02
          - cudatoolkit=11.2
          - pytest=6.1.2
          - pandas=1.3.5
          - numpy=1.21.5
          - requests=2.25.0
          - scikit-learn=0.24.2
          - dill=0.3.4
          - tqdm=4.62.3
          - ruamel.yaml=0.17.19
          - yappi=1.3.3
          - black=22.1.0
          - pillow=9.0.1
          - jupyterlab=3.2.9
          - matplotlib=3.5.1
          - seaborn=0.11.2
          - plotly=5.6.0
          - pandas-profiling=3.1.0
          - black=22.1.0
          - tensorflow
          - tensorflow-hub

My system details are following:
$ cat /etc/os-release
NAME=""CentOS Linux""
VERSION=""7 (Core)""

$ uname -r
3.10.0-1127.10.1.el7.x86_64

EDIT2: I forgot to mention that if I comment out glibc, cudf and cudatoolkit, the tensorflow installation through pip works fine.
","Challenges updating conda environments can be tricky to untangle, but in this case the issue (in terms of cuDF) is likely the pinning of pandas to 1.4.1.
cuDF does not yet support pandas=1.4.1. The cuDF nightly packages currently support pandas &gt;=1.0,&lt;1.4.0dev0 (everything from 1.0 up to a dev build of 1.4.0). For the moment, if you switch to
pandas=1.3.5 things will work (assuming the other packages are compatible -- Tensorflow likely will require CUDA Toolkit 11.2, not 11.5, and some others might not be either). You can also let conda solve for the appropriate pandas version.
","I was finally able to complete the installation of cudf along with all the required packages after I reimaged my machine to Ubuntu20.04.
And I dropped below 2 packages as with Ubuntu20.04 I don't need those -
 - glibc=2.19
 - libgcc-ng=11.2.0

",,false,
https://stackoverflow.com/questions/66797246,false,The issue does not meet the criteria for deeper analysis as it is related to the performance of TensorFlow GPU training and not an API exhibiting unexpected failures or unpredictable behaviors under specific runtime conditions.,,,,,,,Why is Tensorflow GPU extremely slow when creating models and training models compared to the CPU version?,"I would first like to give you some information about how I installed tensorflow and other packages before explaining the problem. It took me a lot of time to get tensorflow running on my GPU (Nvidia RTX 3070, Windows 10 system). First, I installed Cuda (v.10.1), downloaded CuDDN (v7.6) and copied and pasted the CuDNN files to the correct Cuda installation folders (as described here: https://docs.nvidia.com/deeplearning/cudnn/install-guide/index.html#install-windows)
I want to use tensorflow 2.3.0 and checked if the Cuda and cuDNN versions are compatible using the table on this page: https://www.tensorflow.org/install/source
Then I opened the anaconda prompt window, activated my new environment (&gt;&gt; activate [MyEnv]) and installed the required packages. I read that it is important to install tensorflow first, so the first package I installed was tensorflow-gpu, followed by a bunch of other packages. Later, I ran into the problem that my GPU was not found when I typed in
import tensorflow as tf
print(""Num GPUs Available: "", len(tf.config.list_physical_devices('GPU')))

The response was ""Num GPUs Available: 0""
I did a lot of googling and found the following discussion:
https://github.com/ContinuumIO/anaconda-issues/issues/12194#issuecomment-751700156
where it is mentioned that a faulty tensorflow build is installed when using conda install tensorflow-gpu in the anaconda prompt window. Instead (when using Pythin 3.8, as I do), one has to use pass the correct tensorflow build in the prompt window. So I set up a new environment and used
conda install tensorflow-gpu=2.3 tensorflow=2.3=mkl_py38h1fcfbd6_0

to install tensorflow. So far so good. Now, the cudatoolkit (version 10.1.243) and cudnn (version 7.6.5), which were missing in my first environment, are inculded in the the tensorflow package and thus in my second environment [MyEnv2].
I start VSCode, select the correct environment, and retest if the gpu can be found by repeating the test:
import tensorflow as tf
print(""Num GPUs Available: "", len(tf.config.list_physical_devices('GPU')))
And ...it works. The GPU is found and everything looks good at the first sight.
So what's the problem?
Using tensorflow on gpu is extremly slow. Not only when training models, but also when creating the model with
model = models.Sequential()
model.add(...)
(...)
model.summary()

Running the same code sample on CPU finishes almost immediately, wheras running the code on GPU needs more than 10 minutes! (When I look into the taskmanager performance tab nothing happens. Neither CPU nor GPU seems to do anything, when I run the Code on the GPU!) And this happens, when just creating the model without training!
After compiling the model and starting the training, the same problem occurs. Training on the CPU gives me a immediate feedback about the epoch process, while training on gpu seems to freeze the program as nothing happens for several minutes (maybe ""freezing"" is the wrong word, because I can still switch between the tabs in VSCode. The program itself is not freezing) Another confusing aspect is that when training on the gpu, I only get nans for the loss and mae when the training finally starts after minutes of waiting. In the task manager I can obeserve that the model needs about 7,5GB of VRAM. The RTX3070 comes with 8GB of VRAM. When I run the same code on the cpu, loss and mae look perfectly fine...
I really have no idea what is happening and why I am getting this strange behaviour when I run tensorflow on my gpu. Do you have any ideas?
","Nvidia RTX 3070 cards are based on the Ampere architecture for which compatible CUDA version start with 11.x.
You can upgrade tensorflow from 2.3 to 2.4, because this version supports Ampere architecture.
So to get benefit from your GPU card, the compatible combination are TF 2.4, CUDA 11.0 and cuDNN is 8.0.
",,,false,
https://stackoverflow.com/questions/57140254,false,The issue does not meet the criteria for deeper analysis as it does not provide sufficient information or code snippets to determine if it involves an API exhibiting unexpected failures or unpredictable behaviors under specific runtime conditions.,,,,,,,Google Cloud AI Platform Notebook Instance won&#39;t use GPU with Jupyter,"I'm using the pre-built AI Platform Jupyter Notebook instances to train a model with a single Tesla K80 card.  The issue is that I don't believe the model is actually training on the GPU.

nvidia-smi returns the following during training:

No Running Processes Found

Not the ""No Running Process Found"" yet ""Volatile GPU Usage"" is 100%. Something seems strange...

...And the training is excruciatingly slow.

A few days ago, I was having issues with the GPU not being released after each notebook run.  When this occurred I would receive a OOM (Out of memory error).  This required me to go into the console every time, find the GPU running process PID and use kill -9 before re-running the notebook.  However, today, I can't get the GPU to run at all?  It never shows a running process.

I've tried 2 different GCP AI Platform Notebook instances (both of the available tensorflow version options) with no luck.  Am I missing something with these ""pre-built"" instances.  

Pre-Built AI Platform Notebook Section

Just to clarify, I did not build my own instance and then install access to Jupyter notebooks.  Instead, I used the built-in Notebook instance option under the AI Platform submenu.

Do I still need to configure a setting somewhere or install a library to continue using/reset my chosen GPU?  I was under the impression that the virtual machine was already loaded with the Nvidia stack and should be plug and play with GPUs.

Thoughts?

EDIT: Here is a full video of the issue as requested --&gt; https://www.youtube.com/watch?v=N5Zx_ZrrtKE&amp;feature=youtu.be
",,,,false,
https://stackoverflow.com/questions/55605853,false,The issue does not meet the criteria for deeper analysis as it does not provide any information or code snippets to determine if it involves an API exhibiting unexpected failures or unpredictable behaviors under specific runtime conditions.,,,,,,,&#39;numpy.dtype&#39; object has no attribute &#39;base_dtype&#39;,,,,,false,
https://stackoverflow.com/questions/51802631,false,The issue does not meet the criteria for deeper analysis as it does not provide any information or code snippets to determine if it involves an API exhibiting unexpected failures or unpredictable behaviors under specific runtime conditions.,,,,,,,Does batch normalization in tensorflow use running averages during training?,,,,,false,
https://stackoverflow.com/questions/50930376,false,The issue does not meet the criteria for deeper analysis as it does not provide any information or code snippets to determine if it involves an API exhibiting unexpected failures or unpredictable behaviors under specific runtime conditions.,,,,,,,TensorFlow :ValueError: None values not supported,,,,,false,
https://stackoverflow.com/questions/49323364,false,The issue does not meet the criteria for deeper analysis as it does not provide any information or code snippets to determine if it involves an API exhibiting unexpected failures or unpredictable behaviors under specific runtime conditions.,,,,,,,Tensorflow Domain Adaptation How can i use two Bazel commands at the same time?,,,,,false,
https://stackoverflow.com/questions/46382866,false,The issue does not meet the criteria for deeper analysis as it does not provide any information or code snippets to determine if it involves an API exhibiting unexpected failures or unpredictable behaviors under specific runtime conditions.,,,,,,,Tensorflow: Different activation values for same image,,,,,false,
https://stackoverflow.com/questions/40381101,false,The issue does not meet the criteria for deeper analysis as it does not provide any information or code snippets to determine if it involves an API exhibiting unexpected failures or unpredictable behaviors under specific runtime conditions.,,,,,,,Keras writing a Recurrent Layer which accepts images,,,,,false,
https://stackoverflow.com/questions/72961365,false,The issue does not meet the criteria for deeper analysis as it does not provide any information or code snippets to determine if it involves an API exhibiting unexpected failures or unpredictable behaviors under specific runtime conditions.,,,,,,,The editor could not be opened due to an unexpected error: Unexpected token ? in JSON at position 0,"I'm trying to open a .ipynb file in vscode and encounter this error:

This is strange because of many .ipynb files in the same folder I have, only this one when opened has this issue. Maybe because of tensorflow? Yesterday everything was fine, the whole folder is stored in an SSD drive with a SATA cable. This morning when after I unplugged the SSD drive from the PC and plugged it into the laptop, this error popped up. I have never seen this error before.
I tried uninstalling and reinstalling Jupyter extension, switching to previous release, opened it in the text editor and saw this:

This file is very important and I don't wanna to lose it. Could you show me a way to solve this?
","You can open this file in text editor (as the error suggests). There you'll see highlighted parts of code and several buttons around them: Accept incoming change, Compare changes and some others. You should press one of these buttons (according to what you'd like to do, for me it was ""Accept incoming change"") and the highlight from this part of code will be removed. When you remove all the highlights you right-click on this file tab and choose ""Reopen editor with""-&gt;""Jupiter Notebook"". Your code will be saved :)
",,,false,
https://stackoverflow.com/questions/72570331,false,The issue does not meet the criteria for deeper analysis as it does not provide any information or code snippets to determine if it involves an API exhibiting unexpected failures or unpredictable behaviors under specific runtime conditions.,,,,,,,GPU memory nearly full after defining tf.distribute.MirroredStrategy?,"I am coming across a strange issue when using TensorFlow (2.9.1). After defining a distributed training strategy, my GPU memory appears to fill.
Steps to reproduce are simple:
import tensorflow as tf
strat = tf.distribute.MirroredStrategy()

After the first line (importing TensorFlow), nvidia-smi outputs:
Fri Jun 10 03:01:47 2022
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 470.103.01   Driver Version: 470.103.01   CUDA Version: 11.4     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  Quadro P6000        Off  | 00000000:04:00.0 Off |                  Off |
| 26%   25C    P8     9W / 250W |      0MiB / 24449MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
|   1  Quadro P6000        Off  | 00000000:06:00.0 Off |                  Off |
| 26%   20C    P8     7W / 250W |      0MiB / 24449MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+

+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|  No running processes found                                                 |
+-----------------------------------------------------------------------------+

After the second line of code, nvidia-smi outputs:
Fri Jun 10 03:02:43 2022
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 470.103.01   Driver Version: 470.103.01   CUDA Version: 11.4     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  Quadro P6000        Off  | 00000000:04:00.0 Off |                  Off |
| 26%   29C    P0    59W / 250W |  23951MiB / 24449MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
|   1  Quadro P6000        Off  | 00000000:06:00.0 Off |                  Off |
| 26%   25C    P0    58W / 250W |  23951MiB / 24449MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+

+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|    0   N/A  N/A   1833720      C   python                          23949MiB |
|    1   N/A  N/A   1833720      C   python                          23949MiB |
+-----------------------------------------------------------------------------+

The GPU memory is almost entirely full? There is also some terminal output:
2022-06-10 03:02:37.442336: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-06-10 03:02:39.136390: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 23678 MB memory:  -&gt; device: 0, name: Quadro P6000, pci bus id: 0000:04:00.0, compute capability: 6.1
2022-06-10 03:02:39.139204: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 23678 MB memory:  -&gt; device: 1, name: Quadro P6000, pci bus id: 0000:06:00.0, compute capability: 6.1
INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1')

Any ideas on why this is occurring would be helpful! Additional details about my configuration:

Python 3.10.4 [GCC 7.5.0] on linux
tensorflow 2.9.1
cuda/11.2.2 cudnn/v8.2.1

","By default, Tensorflow will map almost all of your GPU memory: official guide. This is for performance reasons: by allocating the GPU memory, it reduces latency that memory growth would typically cause.
You can try using tf.config.experimental.set_memory_growth to prevent it from immediately filling up all its memory. There are also some good explanations on this StackOverflow post.
",,,false,
https://stackoverflow.com/questions/69713596,false,The issue does not involve an API exhibiting unexpected failures or unpredictable behaviors under specific runtime conditions.,,,,,,,How to force Python into 64bit mode,"I am having a strange error on my MacBook Air (M1, 2020) where python3.8 is being installed as a 64 bit binary from homebrew brew install python@3.8 and yet, is being run in 32 bit mode, thus causing errors when trying to download packages.
Here is what I have tried:

Reinstalling and confirming that the executable is running in 32 bit

$ brew reinstall python@3.8 #installs to /opt/homebrew/opt/python@3.8/bin/python3
$ python3.8 -c 'import sys;print(""%x"" % sys.maxsize, sys.maxsize &gt; 2**64)'
&gt; 7fffffffffffffff False


Looking at architecture of binary

$ file /opt/homebrew/opt/python@3.8/bin/python3
&gt; /opt/homebrew/opt/python@3.8/bin/python3: Mach-O 64-bit executable arm64

From the above output, one can see that the executable is in fact 64-bit despite being run in 32 bit mode.

Additional installation of Rosetta 2, a mac update supposedly to fix this problem

$ /usr/sbin/softwareupdate --install-rosetta --agree-to-license


Attempt to force binary in 64 bit arch

$ arch -x86_64 /opt/homebrew/opt/python@3.8/bin/python3
&gt; arch: posix_spawnp: /opt/homebrew/opt/python@3.8/bin/python3: Bad CPU type in executable

Here is relevant, but sadly unhelpful for my needs, stack overflow tickets:
How to force using 64 bit python on Mac-os-X 
Installation issue with Tensorflow Package 
How to determine if python is in 32 or 64 bit mode mac 
Bad CPU type in executable Mac Rosetta 2 Instructions 
Any ideas would be helpful, thanks!
","Turns out I had an error in my command:
python3.8 -c 'import sys;print(""%x"" % sys.maxsize, sys.maxsize &gt; 2**64)'
should be:
python3.8 -c 'import sys;print(""%x"" % sys.maxsize, sys.maxsize &gt; 2**32)'
Which returns true when you're running python in 64 bit mode.
This specific problem is likely something else, and as such I am closing this question.
",,,false,
https://stackoverflow.com/questions/68834108,false,The issue does not involve an API exhibiting unexpected failures or unpredictable behaviors under specific runtime conditions.,,,,,,,Tensorflow object detection API loss increases dramatically,"I'm training tensorflow object detection API with my own data but for some reason, all losses seem to skyrocket past some thousands of steps. You can see it on this figure:

At the beginning it goes down, as it should, but somehow at some point it just explodes.
The model I'm using is ssd_resnet50_v1_fpn with pretrained coco checkpoint but this information might be irrelevant since I think this happens with any model (I've tried 2 models a couple of times each and currently the behaviour is the same for all of my tries). I'm training it following this tutorial https://tensorflow-object-detection-api-tutorial.readthedocs.io/en/latest/training.html so I use the model_main_tf2.py script to train the model.
What is very strange is that a colleague of mine does not have this issue. His loss evolution seems rather normal as you can see here:

He provided me with all the necessary files to run the training (so, the tf.records, the labelmap.txt, the pipeline.config file, the ckpt-0.index etc.). He might be lucky and I'm not. But it's very strange that each time he runs it, he has a normal behaviour and I don't... At least the odds at very very low that this is what happens.
Don't know if it matters but I have a GeForce RTX 3060 Ti, Python: 3.9.6, Tensorflow: 2.5.0, Cudnn: 8.1.1 and Cuda: 11.2
while he has a Geforce GTX 1650 Super, Python: 3.9.6, Tensorflow: 2.5.1, Cudnn: 8.1.0 and Cuda: 11.2
Any idea what could cause this?
Edit 1: I've re-run this while doing nothing else on my computer and it ran quite well. It now shows an expected behaviour. I'll re-run it while using it normally (maybe watch a video or something), which is what I did in the last runs that showed a strange behaviour, and see if this strange behaviour occurs again or not. Do you think it could be the cause of this behaviour?
","I'll have the exact same issue. After a varying number of thousands of steps the loss just explodes. I'm also unsure what could be the cause of the issue.
In my opinion it could either be either an GPU issue or something wrong with the data. But at that point I should have gone through the whole set at least once. Let me know if you make any progress on that :)

",,,false,
https://stackoverflow.com/questions/65419103,false,The issue does not involve an API exhibiting unexpected failures or unpredictable behaviors under specific runtime conditions.,,,,,,,Create Version Failed. Bad model detected with error: &quot;Error loading the model&quot; - AI Platform Prediction,"I created a model through AI Platform UI that uses a global endpoint. I am trying to deploy a basic tensorflow 1.15.0 model I exported using the Saved Model builder. When I try to deploy this model I get a Create Version Failed. Bad model detected with error: ""Error loading the model"" error in the UI and the I see the following in the logs:
ERROR:root:Failed to import GA GRPC module. This is OK if the runtime version is 1.x

Failure: Could not reach metadata service: Internal Server Error.

ERROR:root:Command '['/tools/google-cloud-sdk/bin/gsutil', '-o', 'GoogleCompute:service_account=default', 'cp', '-R', 'gs://cml-365057443918-1608667078774578/models/xsqr_global/v6/7349456410861999293/model/*', '/tmp/model/0001']' returned non-zero exit status 1.

ERROR:root:Error loading model: 'generator' object has no attribute 'next'

ERROR:root:Error loading the model



Framework/ML runtime version: Tensorflow 1.15.0
Python: 3.7.3

What is strange is that the gcloud ai-platform local predict works correctly with this exported model, and I can deploy this exact same model on a regional endpoint with no issues. It only gives this error if I try to use a global endpoint model. But I need the global endpoint because I plan on using a custom prediction routine (if I can get this basic model working first).
The logs seem to suggest an issue with copying the model from storage? I've tried giving various IAM roles additional viewer permissions, but I still get the same errors.
Thanks for the help.
","Today I faced the same error (ERROR: (gcloud.ai-platform.versions.create) Create Version failed. Bad model detected with error:  ""Error loading the model"") &amp; for those who wants a summary:
The recommendation is to use n1* machine types (for example: n1-standard-4) via regional endpoints (for example: us-central1) instead of mls1* machines while deploying version. Also I made sure to mention the same region (us-central1) while creating the model itself using the below command, thereby resolving the above mentioned error.

!gcloud ai-platform models create  $model_name 
--region=$REGION

","I think it's the same issue as https://issuetracker.google.com/issues/175316320
The comment in the issue says the fix is now rolling out.
",,false,
https://stackoverflow.com/questions/61463184,false,The issue does not involve an API exhibiting unexpected failures or unpredictable behaviors under specific runtime conditions.,,,,,,,TensorFlow official documentation tutorial error &#39;&#39;numpy.ndarray&#39; object has no attribute &#39;get_shape&#39;,"I was trying to run the exact code on the TensorFlow tutorial here.

When I get to line: 

predictions = model(x_train[:1]).numpy()


I get the error:

'numpy.ndarray' object has no attribute 'get_shape'

I saw here that someone had a similar issue, so I changed the line to:

predictions = model(tf.convert_to_tensor(x_train[:1])).numpy()


And I get the error:

Input 'b' of 'MatMul' Op has type float32 that does not match type float64 of argument 'a'

Could someone tell me the correct code that should be in the tutorial? Or is it an issue on my side?

One thing that is strange is it runs if I run the code in the colab notebook provided, but if I download the jupyter notebook to run locally, that's when I get the error. So I'm wondering if I have a different/wrong version of TensorFlow?

My packages:

tensorflow                1.13.1            
tensorflow-base           1.13.1            
tensorflow-estimator      1.13.0                       
python                    3.7.6                 

","The reason of your first question is that your model() accept tensor type parameters not numpy. And model(x)  called x.get_shape().
In x.get_shape(), x must be tensor. So, your need transform numpy to tensor.  You can use tf.convert_to_tensor(x) or tf.constant(x)

The second error message is Input 'b' of 'MatMul' Op has type float32 that does not match type float64 of argument 'a'. The message means that in a x b a type is float64 and b type is float32. They are different type variables, so they can't be multiplied. You can try use tf.cast(a,tf.float32) to change the type of a or change b. But in your code, you should use x_train[:1].dtype to check the type of x_train[:1]. If it is float32 you can try use x_train[:1].astype(np.float64) to transform it or vice versa. Or use tf.constant(x_train[:1],dtype=tf.float64)
","The tutorial you are referencing is in Tensorflow 2.x (where Eager mode is by default). In Eager mode you can access a tensor with .numpy() method to get numpy value of the tensor.

In your local, you have TensorFlow 1.13.1 which runs the model in graph mode. You can access the numpy value of the graph tensor (not eager tensor) by running a session and evaluating the tensor. Here is one tutorial on TF1.x with mnist data. Overall, in TF1.x, you need to create a graph, create placeholders, then run a Session, feed the data to placeholders, and train the model, then predict.

Other alternative is to install Tensorflow 2.x by running ""pip3 install tensorflow==2.1`. After installing, you can run the example without any issue.
",,false,
https://stackoverflow.com/questions/58141530,false,The issue does not involve an API exhibiting unexpected failures or unpredictable behaviors under specific runtime conditions.,,,,,,,"Inception settings changed: Computed output size would be negative: -4 [input_size: 2, effective_filter_size: 7, stride: 1]","I was working along the image classifier ML.net sample code over at https://learn.microsoft.com/en-US/dotnet/machine-learning/tutorials/image-classification

The classification there uses the following inception settings

private struct InceptionSettings
{
    public const int ImageHeight = 224;
    public const int ImageWidth = 224;
    public const float Mean = 117;
    public const float Scale = 1;
    public const bool ChannelsLast = true;
}


while using the tensorflow inception5h model.
It appears to be working. Unclear for me is however, what breaks when I change Height and Width from 224 to say 64 to reduce the load and precision of the prediction the reuse and tune inception model part nearly instantly crashes with

System.InvalidOperationException: Splitter/consolidator worker encountered exception while consuming source data ---&gt; Microsoft.ML.Transforms.TensorFlow.TFException: Computed output size would be negative: -4 [input_size: 2, effective_filter_size: 7, stride: 1]
     [[{{node avgpool0}}]]
   at Microsoft.ML.Transforms.TensorFlow.TFStatus.CheckMaybeRaise(TFStatus incomingStatus, Boolean last)
   at Microsoft.ML.Transforms.TensorFlow.TFSession.Run(TFOutput[] inputs, TFTensor[] inputValues, TFOutput[] outputs, TFOperation[] targetOpers, TFBuffer runMetadata, TFBuffer runOptions, TFStatus status)
   at Microsoft.ML.Transforms.TensorFlow.TFSession.Runner.Run(TFStatus status)
   at Microsoft.ML.Transforms.TensorFlowTransformer.Mapper.UpdateCacheIfNeeded(Int64 position, ITensorValueGetter[] srcTensorGetters, String[] activeOutputColNames, OutputCache outputCache)
   at Microsoft.ML.Transforms.TensorFlowTransformer.Mapper.&lt;&gt;c__DisplayClass8_0`1.&lt;MakeGetter&gt;b__3(VBuffer`1&amp; dst)
   at Microsoft.ML.Data.DataViewUtils.Splitter.InPipe.Impl`1.Fill()
   at Microsoft.ML.Data.DataViewUtils.Splitter.&lt;&gt;c__DisplayClass5_1.&lt;ConsolidateCore&gt;b__2()
   --- End of inner exception stack trace ---
   at Microsoft.ML.Data.DataViewUtils.Splitter.Batch.SetAll(OutPipe[] pipes)
   at Microsoft.ML.Data.DataViewUtils.Splitter.Cursor.MoveNextCore()
   at Microsoft.ML.Data.RootCursorBase.MoveNext()
   at Microsoft.ML.Trainers.TrainingCursorBase.MoveNext()
   at Microsoft.ML.Trainers.LbfgsTrainerBase`3.TrainCore(IChannel ch, RoleMappedData data)
   at Microsoft.ML.Trainers.LbfgsTrainerBase`3.TrainModelCore(TrainContext context)
   at Microsoft.ML.Trainers.TrainerEstimatorBase`2.TrainTransformer(IDataView trainSet, IDataView validationSet, IPredictor initPredictor)
   at Microsoft.ML.Data.EstimatorChain`1.Fit(IDataView input)
   at D:\My\MLTrainer.Program.ReuseAndTuneInceptionModel(MLContext mlContext, TrainerData trainerData, String dataLocation, String inputModelLocation, String outputModelLocation) in MLTrainer\Program.cs:line 66
   at MLTrainer.Program.Main(String[] args) in D:\My\MLTrainer\Program.cs:line 29


Now I don't get what I can do and where in the details the issue is buried. Is the pre-trained model already fixed to this somewhat strange resolution? The resolution itself seems not to be used somewhere else, nor do I get why the splitter don't like it.

Do I hit some sort of min size condition I am not aware of? If so, what are the boundaries? I tried 1024x1024 for instance, which failed with another error.

Any hints on that are appreciated :)
","You could install Netron to look at your model.

You'll see that on the first layer will stand something like Nx224x224x3.

The model is fixed to this input resolution, because it is trained with a large dataset in exactly this resolution. You could change the input layer with e.g. keras. 
But I never tried to change the input layer in ML.NET.
",,,false,
https://stackoverflow.com/questions/55892554,false,The issue does not involve an API exhibiting unexpected failures or unpredictable behaviors under specific runtime conditions.,,,,,,,Invoke endpoint after model deployment : [Err 104] Connection reset by peer,"I am new to Sagemaker. I have deployed my well trained model in tensorflow  by using Json and Weight file. But it is strange that in my note book, I didn't see it says ""Endpoint successfully built"". Only the below is shown:

--------------------------------------------------------------------------------!


Instead, I found the endpoint number from my console. 

import sagemaker
from sagemaker.tensorflow.model import TensorFlowModel
        predictor=sagemaker.tensorflow.model.TensorFlowPredictor(endpoint_name, sagemaker_session)
data= test_out2
predictor.predict(data)


Then I try to invoke the endpoint by using 2D array:
(1) If my 2D array is in size of (5000, 170), I am getting the error:

ConnectionResetError: [Errno 104] Connection reset by peer


(2) If reducing the array to size of (10,170), error is :

ModelError: An error occurred (ModelError) when calling the InvokeEndpoint operation: Received server error (500) from model with message """". See https://us-east-2.console.aws.amazon.com/cloudwatch/home?region=us-east-2#logEventViewer:group=/aws/sagemaker/Endpoints/sagemaker-tensorflow-2019-04-28-XXXXXXXXX in account 15XXXXXXXX for more information.


Any suggestion please? Found similar case in github, https://github.com/awslabs/amazon-sagemaker-examples/issues/589.

Is it the similar case please?

Thank you very much in advance!
","I had this problem and this post helped me resolve it. There does seem to be a limit to the size of the dataset that the predictor will take. I'm not sure what it is, but in any case I now split my training/test data differently.

I assume there's a limit and the limit is based on raw data volume. In rough terms this would translate to the number of cells in my dataframe, since each cell is probably and integer or a float. 

If I can get a 70%/30% split I use that, but if 30% test data exceeds the maximum number of cells, I split my data to give me the maximum number of rows that will fit into the maximum.

Here's the split code:

# Check that the test data isn't too big for the predictor
max_test_cells = 200000
model_rows, model_cols = model_data.shape
print('model_data.shape=', model_data.shape)
max_test_rows = int(max_test_cells / model_cols)
print('max_test_rows=', max_test_rows)
test_rows = min(int(0.3 * len(model_data)), max_test_rows)
print('actual_test_rows=', test_rows)
training_rows = model_rows - test_rows
print('training_rows=', training_rows)

# Split the data to get the largest test set possible
train_data, test_data = np.split(model_data.sample(frac=1, random_state=1729), [training_rows])
print(train_data.shape, test_data.shape)

",,,false,
https://stackoverflow.com/questions/46890399,false,The issue does not involve an API exhibiting unexpected failures or unpredictable behaviors under specific runtime conditions.,,,,,,,Trouble scaling wide and deep model to train on google cloud ML,,,,,false,
https://stackoverflow.com/questions/46559655,false,The issue does not involve an API exhibiting unexpected failures or unpredictable behaviors under specific runtime conditions.,,,,,,,Queue manager for experiments in neural network training,,,,,false,
https://stackoverflow.com/questions/45809554,false,The issue does not involve an API exhibiting unexpected failures or unpredictable behaviors under specific runtime conditions.,,,,,,,Cannot run Google ML engine locally due to Tensorflow issues,,,,,false,
https://stackoverflow.com/questions/43106054,false,The issue does not involve an API exhibiting unexpected failures or unpredictable behaviors under specific runtime conditions.,,,,,,,Tensorflow. Conditionally trainable variables and stochastic depth neural networks,,,,,false,
https://stackoverflow.com/questions/49103737,true,The issue involves the nn.Embedding layer in PyTorch and the behavior of the model during training under different random seed values.,PyTorch,nn.Embedding,"The issue arises when training a model with a nn.Embedding layer followed by nn.Linear and nn.ReLU layers. The model is expected to learn to predict 1.0 for all inputs, but the training outcome varies depending on the random seed used. In some cases, the model converges and predicts 1.0, while in other cases, it fails to converge and predicts 0.5. The issue is specifically related to the behavior of the model when using different random seed values.",The nn.Embedding layer is typically used to map discrete input indices to continuous embeddings. It is designed to work with a range of input indices and learn meaningful representations.,"The issue is triggered when training the model with different random seed values. The specific conditions under which the model fails to converge or predicts incorrect values are not related to the nn.Embedding layer itself, but rather to the overall training process and the random initialization of the model's parameters.","This issue might be challenging to detect during development and testing because it is not directly related to the nn.Embedding layer. Instead, it involves the interplay between different layers in the model, the training process, and the random seed used for initialization.",PyTorch naive single label classification with embedding layer fails at random,"I am new to PyTorch and I am trying out the Embedding Layer. 

I wrote a naive classification task, where all the inputs are the equal and all the labels are set to 1.0. I hence expect the model to learn quickly to predict 1.0.

The input is always 0, which is fed into a nn.Embedding(1,32) layer, followed by nn.Linear(32,1) and nn.Relu().

However, an unexpected and undesired behavior occurs: training outcome is different for different times I run the code.
For example, 


setting the random seed to 10, model converges: loss decreases and model always predicts 1.0
setting the random seed to 1111, model doesn't converge: loss doesn't decrease and model always predicts 0.5. In those cases the parameters are not updated


Here is the minimal, replicable code:

from torch.nn import BCEWithLogitsLoss
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
from torch.autograd import Variable
from torch.utils.data import Dataset
import torch


class MyModel(nn.Module):

    def __init__(self):
        super(MyModel, self).__init__()
        self.vgg_fc = nn.Linear(32, 1)
        self.relu = nn.ReLU()
        self.embeddings = nn.Embedding(1, 32)

    def forward(self, data):
        emb = self.embeddings(data['index'])
        return self.relu(self.vgg_fc(emb))


class MyDataset(Dataset):

    def __init__(self):
        pass
    def __len__(self):
        return 1000
    def __getitem__(self, idx):
        return {'label': 1.0, 'index': 0}


def train():
    model = MyModel()
    db = MyDataset()
    dataloader = DataLoader(db, batch_size=256, shuffle=True, num_workers=16)

    loss_function = BCEWithLogitsLoss()
    optimizer_rel = optim.SGD(model.parameters(), lr=0.1)

    for epoch in range(50):
        for i_batch, sample_batched in enumerate(dataloader):

            model.zero_grad()
            out = model({'index': Variable(sample_batched['index'])})

            labels = Variable(sample_batched['label'].type(torch.FloatTensor).view(sample_batched['label'].shape[0], 1))

            loss = loss_function(out, labels)
            loss.backward()
            optimizer_rel.step()
            print 'Epoch:', epoch, 'batch', i_batch, 'Tr_Loss:', loss.data[0]
    return model


if __name__ == '__main__':

    # please, try seed 10 (converge) and seed 1111 (fails)
    torch.manual_seed(10)
    train()


Without specifying the random seed, different runs have different outcome. 

Why is, in those cases, the model unable to learn such a easy task?
Is there any mistake in the way I use nn.Embedding layer?

Thank you 
","I found the problem was the final relu layer, before the sigmoid.
As stated here, that layer will:


  throw away information without adding any additional benefit


Removing the layer, the network learned as expected with any seed.
",,,false,
