ISSUE_LINK,AI_VERDICT,REASON,LIBRARY_NAME,API_NAME,ISSUE_DESCRIPTION,NORMAL_CONDITIONS,TRIGGER_CONDITIONS,REASON_FOR_DIFFICULTY_IN_DETECTION,ISSUE_TITLE,ISSUE_BODY,ANSWER_1,ANSWER_2,ANSWER_3,HUMAN_CLASSIFICATION,HUMAN_REASON,ISSUE_SCORE,ISSUE_VIEWS
https://stackoverflow.com/questions/71093471,true,"The issue involves the SpaCy matcher with negation, specifically the use of the '!' operator in the match pattern.",SpaCy,Matcher,"The issue arises when trying to use the '!' operator in a match pattern to indicate negation. However, the '!' operator in SpaCy's Matcher corresponds to a single token in a negative sense, rather than negating the entire pattern. This leads to unexpected behavior when trying to match patterns involving negation.",The Matcher API works as expected when matching patterns without negation.,The issue is triggered when attempting to use the '!' operator in a match pattern to indicate negation.,This issue might be challenging to detect for users who are not familiar with the specific behavior of the '!' operator in SpaCy's Matcher or those who expect it to work as a negation operator for the entire pattern.,Unexpected behavior of SpaCy matcher with negation,"Somehow I have trouble understanding the negation in SpaCy matchers. I tried this code:
import spacy
from spacy.matcher import Matcher
import json



nlp = spacy.load('en_core_web_sm')
    #from spacy.tokenizer import Tokenizer
matcher = Matcher(nlp.vocab)

Sentence = ""The cat is black""

negative_sentence = ""The cat is not black""

test_pattern = '''
[
  [
    {
      ""TEXT"": ""cat""
    },
    {
      ""LEMMA"": ""be""
    },
    {
      ""LOWER"": ""not"",
      ""OP"": ""!""
      
    },
    {
      ""LOWER"": ""black""
    }
  ]
]
''' 

db = json.loads(test_pattern)

matcher.add(""TEST_PATTERNS"", db)


'''*********************Validate matcher on positive sentence******************'''
doc = nlp(Sentence, matcher)

matches = matcher(doc)

if matches != []:
    print('Positive sentence identified')
else:
    print('Nothing found for positive sentence')

'''*********************Validate matcher on negative sentence******************'''
doc = nlp(negative_sentence, matcher)
matches = matcher(doc)

if matches != []:
    print('Negative sentence identified')
else:
    print('Nothing found for negative sentence')
    


The result is:

Nothing found for positive sentence
Nothing found for negative sentence

I would expect that the sentence ""The cat is black"" would be a match. Furthermore, when I replace the ! with any other sign (""*"", ""?"", or ""+"") it works as expected:
import spacy
from spacy.matcher import Matcher
import json



nlp = spacy.load('en_core_web_sm')
    #from spacy.tokenizer import Tokenizer
matcher = Matcher(nlp.vocab)

Sentence = ""The cat is black""

negative_sentence = ""The cat is not black""

test_pattern = '''
[
  [
    {
      ""TEXT"": ""cat""
    },
    {
      ""LEMMA"": ""be""
    },
    {
      ""LOWER"": ""not"",
      ""OP"": ""?""
      
    },
    {
      ""LOWER"": ""black""
    }
  ]
]
''' 

db = json.loads(test_pattern)

matcher.add(""TEST_PATTERNS"", db)


'''*********************Validate matcher on positive sentence******************'''
doc = nlp(Sentence, matcher)

matches = matcher(doc)

if matches != []:
    print('Positive sentence identified')
else:
    print('Nothing found for positive sentence')

'''*********************Validate matcher on negative sentence******************'''
doc = nlp(negative_sentence, matcher)
matches = matcher(doc)

if matches != []:
    print('Negative sentence identified')
else:
    print('Nothing found for negative sentence')


Result:

Positive sentence identified
Negative sentence identified

How can I use the negation and only identify ""The cat is black"" and not ""The cat is not black"".
The reason why like to of the ""OP"" is because there might also other words between ""is"" and ""black"" (e.g., ""The cat is kind and black"" and not ""The cat is not kind and black"" ).
Any help on understanding negation with SpaCy matchers is highly appreciated.
","Each dictionary in your match pattern corresponds to a token by default. With the ! operator it still corresponds to one token, just in a negative sense. With the * operator it corresponds to zero or more tokens, with + it's one or more tokens.
Looking at your original pattern, these are your tokens:

text: cat
lemma: be
text: not, op: !
lower: cat

Given the sentence ""The cat is black"", the match process works like this:

""the"" matches nothing so we skip it.
""cat"" matches your first token.
""is"" matches your second token.
""black"" matches your third token because it is not ""not""
The sentence ends so there is no ""cat"" token, so the match fails.

When debugging patterns it's helpful to step through them like above.
For the other ops... * and ? work because ""not"" matches zero times. I would not expect + to work in the positive case.
The way you are trying to avoid matching negated things is kind of tricky. I would recommend you match all sentences with the relevant words first, ignoring negation, and then check if there is negation using the dependency parse.
",,,false,,,
https://stackoverflow.com/questions/67252812,true,The issue involves the loading of a custom pre-trained model with a custom pipeline in SpaCy 3.0 using the CLI.,SpaCy,nlp.to_disk,The issue arises when trying to load a custom pre-trained model with a custom pipeline from disk as a pipeline in SpaCy 3.0 using the CLI. The error message indicates that the 'exclude' keyword argument is unexpected in the 'from_disk' method.,The 'nlp.to_disk' method is used to save a SpaCy pipeline to disk.,The issue is triggered when trying to load a custom pre-trained model with a custom pipeline from disk as a pipeline in SpaCy 3.0 using the CLI.,This issue might be challenging to detect for users migrating from SpaCy 2.x to SpaCy 3.0 or those who are not familiar with the specific changes in the 'from_disk' method.,Can&#39;t evaluate custom ner in spacy 3.0 using CLI,"I'm trying to load a custom pre-trained model with custom pipeline from disk as a pipeline in spacy 3.0:
The code of the factory is like this:
@CustomEng.factory(""ner-crf"")
def create_my_component(nlp, name):
    crf_extractor = CRFExtractor().from_disk(""path-to-model"")
    return CRFEntityExtractor(nlp, crf_extractor=crf_extractor)

Then I added to 'ner-crf' to my Language class like this:
    nlp = spacy.blank('custom-eng')
    nlp.add_pipe('ner-crf')
    nlp.to_disk('../model')

There's a thing I think may be relevant: When I use to_disk in order to save the nlp object there is no ner-crf package (the pipeline I just added)  in the saved object.
Then I run this CLI command to evaluate the NER pipeline:
python -m spacy evaluate ../model/ ../corpus/dev.spacy --output ../model/metrics.json --gpu-id 0 --code ../../../spacy_utils/custom-eng/__init__.py

But I get this error :
Traceback (most recent call last):
  File ""/home/marzi/anaconda3/envs/spacy-tutorial/lib/python3.8/runpy.py"", line 194, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File ""/home/marzi/anaconda3/envs/spacy-tutorial/lib/python3.8/runpy.py"", line 87, in _run_code
    exec(code, run_globals)
  File ""/home/marzi/anaconda3/envs/spacy-tutorial/lib/python3.8/site-packages/spacy/__main__.py"", line 4, in &lt;module&gt;
    setup_cli()
  File ""/home/marzi/anaconda3/envs/spacy-tutorial/lib/python3.8/site-packages/spacy/cli/_util.py"", line 69, in setup_cli
    command(prog_name=COMMAND)
  File ""/home/marzi/anaconda3/envs/spacy-tutorial/lib/python3.8/site-packages/click/core.py"", line 829, in __call__
    return self.main(*args, **kwargs)
  File ""/home/marzi/anaconda3/envs/spacy-tutorial/lib/python3.8/site-packages/click/core.py"", line 782, in main
    rv = self.invoke(ctx)
  File ""/home/marzi/anaconda3/envs/spacy-tutorial/lib/python3.8/site-packages/click/core.py"", line 1259, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File ""/home/marzi/anaconda3/envs/spacy-tutorial/lib/python3.8/site-packages/click/core.py"", line 1066, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File ""/home/marzi/anaconda3/envs/spacy-tutorial/lib/python3.8/site-packages/click/core.py"", line 610, in invoke
    return callback(*args, **kwargs)
  File ""/home/marzi/anaconda3/envs/spacy-tutorial/lib/python3.8/site-packages/typer/main.py"", line 497, in wrapper
    return callback(**use_params)  # type: ignore
  File ""/home/marzi/anaconda3/envs/spacy-tutorial/lib/python3.8/site-packages/spacy/cli/evaluate.py"", line 42, in evaluate_cli
    evaluate(
  File ""/home/marzi/anaconda3/envs/spacy-tutorial/lib/python3.8/site-packages/spacy/cli/evaluate.py"", line 75, in evaluate
    nlp = util.load_model(model)
  File ""/home/marzi/anaconda3/envs/spacy-tutorial/lib/python3.8/site-packages/spacy/util.py"", line 326, in load_model
    return load_model_from_path(Path(name), **kwargs)
  File ""/home/marzi/anaconda3/envs/spacy-tutorial/lib/python3.8/site-packages/spacy/util.py"", line 392, in load_model_from_path
    return nlp.from_disk(model_path, exclude=exclude)
  File ""/home/marzi/anaconda3/envs/spacy-tutorial/lib/python3.8/site-packages/spacy/language.py"", line 1883, in from_disk
    util.from_disk(path, deserializers, exclude)
  File ""/home/marzi/anaconda3/envs/spacy-tutorial/lib/python3.8/site-packages/spacy/util.py"", line 1176, in from_disk
    reader(path / key)
  File ""/home/marzi/anaconda3/envs/spacy-tutorial/lib/python3.8/site-packages/spacy/language.py"", line 1877, in &lt;lambda&gt;
    deserializers[name] = lambda p, proc=proc: proc.from_disk(
TypeError: from_disk() got an unexpected keyword argument 'exclude'


The custom NER classes that I used belong to spacy-crfsuite library which works fine in spacy 2 but they have no sample code for Spacy 3 yet so I'm trying to make it work in spacy 3.0 myself.
","From spaCy v3.0 onwards, pipeline components are expected to support an exclude keyword on their to_disk method. You can add the exclude keyword to your function, give it a default, and simply not use its value in the function body, and this error should be resolved.
For completeness, here's the migration guide for the transition from v2 to v3, which may include some additional interesting pointers for you: https://spacy.io/usage/v3#migrating
",,,false,,,
https://stackoverflow.com/questions/63693463,true,The issue involves the SpaCy lemmatizer in Hungarian language and the removal of capitalization in the lemmatized output.,SpaCy,token.lemma_,"The issue arises when lemmatizing Hungarian text using the SpaCy lemmatizer. The 'token.lemma_' function works well in terms of lemmatization but returns some sentences without the first letter capitalized. This causes problems for the next function, 'unnest_sentences', which requires first capital letters to identify and break down the text into individual sentences.",The 'token.lemma_' function returns lemmas of tokens without changing the capitalization.,The issue is triggered when lemmatizing Hungarian text using the SpaCy lemmatizer.,This issue might be challenging to detect for users who expect the 'token.lemma_' function to preserve the original capitalization or those who are not familiar with the specific behavior of the lemmatizer in Hungarian language.,SpaCy lemmatizer removes capitalization,"I would like to lemmatize some textual data in Hungarian language and encountered a strange feature in spaCy. The token.lemma_ function works well in terms of lemmatization, however, it returns some of the sentences without first letter capitalization. This is quite annoying, as my next function, unnest_stences (R) requires first capital letters in order to identify and break the text down into individual sentences. 
First I thought the problem was that I used the latest version of spaCy since I had gotten a warning that

UserWarning: [W031] Model  'hu_core_ud_lg' (0.3.1) requires spaCy v2.1
and is incompatible with the current spaCy  version (2.3.2). This may
lead to unexpected results or runtime errors. To resolve this,
download a newer compatible model or retrain your custom model with
the current spaCy  version.

So I went ahead and installed spacy 2.1, but the problem still persists. 
The source of my data are some email messages I cannot share here, but here is a small, artificial example:
# pip install -U spacy==2.1 # takes  9 mins
# pip install hu_core_ud_lg # takes 50 mins

import spacy
from spacy.lemmatizer import Lemmatizer
import hu_core_ud_lg
import pandas as pd
nlp = hu_core_ud_lg.load()

a = ""Tisztelt levélíró!""
b = ""Köszönettel vettük megkeresését.""
df = pd.DataFrame({'text':[a, b]})

output_lemma = []

for i in df.text:
    mondat = """"
    doc = nlp(i)    
    for token in doc:
        mondat = mondat + "" "" + token.lemma_
    output_lemma.append(mondat)

output_lemma

which yields
[' tisztelt levélíró !', ' köszönet vesz megkeresés .']

but I would expect
[' Tisztelt levélíró !', ' Köszönet vesz megkeresés .']

When I pass my original data to the function, it returns some sentences with upercase first letters, others with lowercase letters. For some strange reason I couldn't reproduce that pattern above, but I guess the main point is visible. The function does not work as expected.
Any ideas how I could fix this?
I'm using Jupyter Notebook, Python 2.7, Win 7 and a Toshiba laptop (Portégé Z830-10R i3-2367M).
","Lowercasing is the expected behavior of spaCy's lemmatizer for non-proper-noun tokens.
One workaround is to check if each token is titlecased, and convert to original casing after lemmatizing (only applies to the first character).
import spacy

nlp = spacy.load('en_core_web_sm')

text = 'This is a test sentence.'
doc = nlp(text)
newtext = ' '.join([tok.lemma_.title() if tok.is_title else tok.lemma_ for tok in doc])
print(newtext)
# This be a test sentence .

",,,false,,,
https://stackoverflow.com/questions/51514545,true,The issue involves the performance of the English language model compared to the German language model in SpaCy for identifying German salutation entities.,SpaCy,nlp,"The issue arises when comparing the performance of the English language model and the German language model in SpaCy for identifying German salutation entities. The English model performs better in this specific case, likely due to differences in training data and statistical predictions.",The SpaCy language models are designed to perform well on texts in their respective languages.,The issue is triggered when comparing the performance of the English and German language models in identifying German salutation entities.,This issue might be challenging to detect for users who expect the German language model to consistently outperform the English language model for German texts or those who are not familiar with the specific training data and statistical predictions used by the models.,Spacy - English language model outruns german language model on german text?,"Is it by design that the english language model performs better on german salution entities than the german model?

# pip install spacy
# python -m spacy download en
# python -m spacy download de

nlp = spacy.load('en')
# Uncomment line below to get less good results
# nlp = spacy.load('de')

# Process text
text = (u""Das Auto kauft Herr Müller oder Frau Meier, Frank Muster"")
doc = nlp(text)

# Find named entities
for entity in doc.ents:
    print(entity.text, entity.label_)


expected result if using nlp = spacy.load('en'). All three PERSON is returned

Das Auto ORG
Herr Müller PERSON
Frau Meier PERSON
Frank Muster PERSON


unexpected result if using nlp = spacy.load('de'). Only one of three PERSON is returned

Frank Muster PERSON




Info about spaCy


spaCy version:** 2.0.12
Platform:** Linux-4.17.2-1-ARCH-x86_64-with-arch-Arch-Linux
Python version:** 3.6.5
Models:** en, de

","It's not by design, but it's certainly possible that this is a side-effect of the training data and the statistical predictions. The English model is trained on a larger NER corpus with more entity types, while the German model uses NER data based on Wikipedia. 

In Wikipedia text, full names like ""Frank Muster"" are quite common, whereas things like ""Herr Muster"" are typically avoided. This might explain why the model only labels the full name as a person and not the others. The example sentence also makes it easy for the English model to guess correctly – in English,  capitalization is a much stronger indicator of a named entity than it is in German. This might explain why the model consistently labels all capitalised multi-word spans as entities.

In any case, this is a good example of how subtle language-specific or stylistic conventions end up influencing a model's predictions. It also shows why you almost always want to fine-tune a model with more examples specific to your data. I do think that the German model will likely perform better on German texts overall, but if references like ""Herr Müller"" are common in your texts, you probably want to update the model with more examples of them in different contexts.
",,,false,,,
https://stackoverflow.com/questions/66433304,true,The issue involves the usage of 'nlp()' and 'nlp.pipe()' in SpaCy for processing strings and lists of strings.,SpaCy,"nlp, nlp.pipe","The issue arises when processing a single string using 'nlp()' and a list of strings using 'nlp.pipe()'. The behavior is due to the difference in how these methods handle input data, with 'nlp.pipe()' being more memory efficient as it processes texts in batches.","The 'nlp()' method is used to process a single string, while 'nlp.pipe()' is used to process a list of strings.",The issue is triggered when trying to process a single string using 'nlp()' or a list of strings using 'nlp.pipe()'.,This issue might be challenging to detect for users who are not familiar with the differences between 'nlp()' and 'nlp.pipe()' in terms of memory efficiency and batch processing.,spacy- why nlp() works for single string while nlp.pipe() works fine for a list of strings?,"I recently ran into a strange behavior while using spacy,
which is when I process string,
in case the string is a single string object,
I have to use nlp(string),
while I have to use nlp.pipe(a list) for a list made of strings elements.
The example is as below.
string='this is a string to be process by nlp'

doc =['this','is','a','string','list','to','be','processed','by','spacy']

stringprocess= list(nlp(string))

listprocess = list(nlp.pipe(doc))

listprocess

stringprocess

Why is this?
I assume this must be something to do with nlp.pipe() behavior which is generator.
What is the reason?
Thank you.
","Spacy does this because generators are more efficient. Since generators are consumed only once they are more memory efficient than a list.
According to their documentation instead of processing texts one-by-one and applying nlp pipeline it processes texts in batches.
Furthermore, you can configure batch size in nlp.pipe to optimize performance according to your system

Process the texts as a stream using nlp.pipe and buffer them in
batches, instead of one-by-one. This is usually much more efficient.

If your goal is to process large streams of data using nlp.pipe it would be much more efficient to write a streamer/generator to produce results as you need them from database/filesystem than loading everything in memory and then processing them one by one.
spacy pipe
",,,false,,,
https://stackoverflow.com/questions/62057053,true,The issue involves accessing files stored in the /tmp directory within a Docker container and encountering file not found errors. This issue is relevant for deeper analysis as it meets the criteria of exhibiting unexpected behavior under specific runtime conditions.,Docker,Docker volume,"When running a Django + Celery setup in a Docker container, accessing files stored in the /tmp directory leads to file not found errors. The issue arises because the files and symlinks created in the /tmp directory within the container are not persistent unless the parent directory is in a Docker volume.",The Docker volume is correctly configured to include the parent directory of the /tmp directory.,The issue is triggered when files are stored in the /tmp directory within the Docker container and accessed immediately after.,This issue might be challenging to detect during development and testing as it involves the interaction between Docker containerization and file access within the container.,Dockerized Python Scripts Having Issues Accessing Files Stored to /tmp,"I want to apologize in advance that I don't have specific code examples to share (though I have included my docker-compose file below that may or may not be helpful). I am having a strange issue I can't seem to trace and I am not 100% sure what to share.

I have a django + celery setup running in Docker (based off of cookiecutter-django). Everything seems to be working great at first. I have extensively tested and used this setup outside of Docker, and the Dockerized Celery tasks generally behave as I'd expect (i.e., as they did when they were not Dockerized).

Here's where things get weird, though. For various reasons, I need to load some data files and create temp files at runtime that I can't just put in my docker file. One example is Using NamedTemporaryFile. Another is installing the data files from Spacy. In both cases, my scripts are storing data to /tmp (I know the easy answer here is to put them in my docker file, but I can't predict which files I need ahead of time, sadly). When my celery task worker tries to access data files it supposedly created, downloaded, and/or stored to /tmp, I keep getting file not found errors. Weird thing is I don't get any errors in my logs that file creation or downloads failed...

Yet, when I /bin/bash into my celeryworker container and cd to the /tmp directory, sure enough, no files are there... If, using the python console in my container, I run the same code, I have no issues and the files do show up in /tmp. I'm not getting a consistent error message and the behavior is manifesting itself differently in different scripts, so it's hard to give you a specific error message / stack trace. The common element appears to stem from some issue that arises when my celery task tries to dynamically store data in /tmp and then access it immediately after that. I'm a Docker newb and am at a loss as to what to do next. I suspect this may be a permissioning issue, but I've tried chmodding /tmp to 777 and it didn't fix anything. I also thought it might be a volume issue, but, if that's the case, I wasn't sure why I can make everything work if I use bash inside my container. 

Anyone have any advice here? Spent a couple of days trying to trace the source of the issue at this point and am at a dead end. My docker file and configs are pretty much plain vanilla copies of Django Cookiecutter defaults.

version: '3'

volumes:
  local_postgres_data: {}
  local_postgres_data_backups: {}

services:
  django: &amp;django
    build:
      context: .
      dockerfile: ./compose/local/django/Dockerfile
    image: gremlin_gplv3_local_django
    container_name: django
    depends_on:
      - postgres
      - tika
      - redis
    volumes:
      - .:/app
    env_file:
      - ./.envs/.local/.django
      - ./.envs/.local/.postgres
    ports:
      - ""8000:8000""
    command: /start

  postgres:
    build:
      context: .
      dockerfile: ./compose/production/postgres/Dockerfile
    image: gremlin_gplv3_production_postgres
    container_name: postgres
    volumes:
      - local_postgres_data:/var/lib/postgresql/data
      - local_postgres_data_backups:/backups
    env_file:
      - ./.envs/.local/.postgres

  redis:
    image: redis:5.0
    container_name: redis

  celeryworker:
    &lt;&lt;: *django
    image: gremlin_gplv3_local_celeryworker
    container_name: celeryworker
    depends_on:
      - redis
      - postgres

    ports: []
    command: /start-celeryworker

  celerybeat:
    &lt;&lt;: *django
    image: gremlin_gplv3_local_celerybeat
    container_name: celerybeat
    depends_on:
      - redis
      - postgres

    ports: []
    command: /start-celerybeat

  flower:
    &lt;&lt;: *django
    image: gremlin_gplv3_local_flower
    container_name: flower
    ports:
      - ""5555:5555""
    command: /start-flower

  tika:
    image: lexpredict/tika-server
    command: /start-tika

","I figured this out... well, mostly. The problem is Spacy (and other, similar libraries and tools that download data files) put them into local directories on their file system and possibly create symlinks to them). In a docker container, however, these files and symlinks are not persistent, however, unless the parent directory is in a docker volume.
What I ended up doing is creating docker volumes for the folder that Spacy (or whatever library) uses to store data files / libraries. In my case, Spacy is always called from via Celery which has its own docker image in my docker-compose stack, so I needed to attach volumes for each of my Spacy data directories to my celeryworker like so:
version: '3'

volumes:
  local_postgres_data: {}
  local_postgres_data_backups: {}
  worker_usr: {}
  worker_root: {}
  worker_tmp: {}

services:
  
  [...]

  celeryworker:
    &lt;&lt;: *django
    image: local_django:latest
    container_name: celeryworker
    depends_on:
      - redis
      - postgres
    volumes:
      - worker_usr:/usr
      - worker_tmp:/tmp
      - worker_root:/root
      - .:/app

    ports: []
    command: /start-celeryworker

All of this said, I've noticed there are situations where, at first pass, the installation of a data file (like a Spacy model) in my worker container throws an error that the data file is still not accessible, yet, when this happens (and it's not all the time), I can just run the install again and 99% of the time, this appears to fix the issue. I have not had time to try to troubleshoot that. Perhaps someone else can figure that part out.
",,,false,,,
https://stackoverflow.com/questions/46222462,false,The issue is related to logical behavior in pandas rather than an API exhibiting unexpected failures or unpredictable behaviors.,,,,,,,Rest Template unable to parse json rest api response properly,"I am trying to extract Named Entities from text using Spacy's NER for German text. I have exposed the service as a REST POST request which takes source text as input and returns a dictionary(Map) of list of named entities (person, location, organization). These services are exposed using Flask Restplus hosted on a linux server. 

Consider for a sample text, I get following response using POST request at REST API exposed via Swagger UI:

{
  ""ner_locations"": [
    ""Deutschland"",
    ""Niederlanden""
  ],
  ""ner_organizations"": [
    ""Miele &amp; Cie. KG"",
    ""Bayer CropScience AG""
  ],
  ""ner_persons"": [
    ""Sebastian Krause"",
    ""Alex Schröder""
  ]
}


When I use Spring's RestTemplate to POST request at the API hosted at Linux server from Spring boot application (on Windows OS in Eclipse). The json parsing is done correctly. I have added following line for using UTF-8 encoding.

restTemplate.getMessageConverters().add(0, new StringHttpMessageConverter(Charset.forName(""UTF-8"")));


But When I deploy this spring boot application on linux machine and POST request to API for NER tagging, the ner_persons are not parsed correctly. While remotely debugging, I get following response

{
  ""ner_locations"": [
    ""Deutschland"",
    ""Niederlanden""
  ],
  ""ner_organizations"": [
    ""Miele &amp; Cie. KG"",
    ""Bayer CropScience AG""
  ],
  ""ner_persons"": [
    ""Sebastian "",
    ""Krause"",
    ""Alex "",
    ""Schröder""
  ]
}


I am not able to understand why this strange behavior occurs in case of persons but not organizations.
","Being new to python, it took me 2 days of debugging to understand the real problem and to find a workaround fix.

The reason was that the names (e.g., ""Sebastian Krause"") were separated by \xa0 i.e., non-breaking space character (e.g., ""Sebastian\xa0Krause"") instead of a whitespace. So Spacy was failing to detect them as a single NamedEntity.

Browsing through SO, I found following solution from here:

import unicodedata 
norm_text = unicodedata.normalize(""NFKD"", source_text)


This also normalizes other unicode characters such as \u2013,\u2026, etc. 
",,,false,,,
https://stackoverflow.com/questions/22591174/pandas-multiple-conditions-while-indexing-data-frame-unexpected-behavior,false,The issue is related to a misunderstanding of logical operators and conditions in pandas rather than an API exhibiting unexpected failures or unpredictable behaviors.,,,,,,,,,,,,false,,,
https://stackoverflow.com/questions/63366054,true,The issue involves a strange character ('ﬁ') appearing when translating a PDF file to text using pdfminer. This issue is relevant for deeper analysis as it meets the criteria of exhibiting unexpected behavior under specific runtime conditions.,pdfminer,re.sub,"When translating a PDF file to text using pdfminer, the character 'ﬁ' is incorrectly replacing 'fi'. The issue is resolved by using the re.sub function to replace the character. This behavior is likely due to a PDF font problem.",The translation of a PDF file to text using pdfminer does not result in the replacement of 'fi' with 'ﬁ'.,The issue is triggered when translating specific PDF files that contain the character 'ﬁ'.,"This issue might be challenging to detect during development and testing, as it requires inspecting the translated text for unexpected characters.",Strange character when I translate a pdf file to text using pdfminer,"I translated a pdf file using pdfminer and I realize that in several situations I found a strange non-ascii 'ﬁ' replacing 'fi'.
An easy way to correct this problem seems to be
 content=re.sub('ﬁ','fi',content)

However, I only could correct the problem because I noticed it and It is worth mentioning that it is very difficult to note it. I only note because I was writing a report in latex about a mistake my code was doing due to an incorrect classification that Spacy was providing to the 'fortified' (with this character). In this moment, I realize that the dvi file (output of the latex laguage) was failing. When I checked it I realized that these two characters 'fi' were replaced by something else.
This seems to be probably a kind of pdf font problem.
Is there a list of problems like this that I can predict and automatically solve before any nlp activity? Or maybe a way to use Spacy to check if a given word is unknown (I believe that this word 'fortified' with the strange replacement was unknown for spacy)? Or yet to look for non-ascii characters in the translated text?
Which of these solutions work?
","In the end, I have now replaced automatically all ligatures:
        if(isinstance(content, str)):
            content=re.sub(r'\uA732', 'AA', content)
            content=re.sub(r'\uA733', 'aa', content)
    
            content=re.sub(r'\u00C6', 'AE', content)
            content=re.sub(r'\u00E6', 'ae', content)
    
            content=re.sub(r'\uA734', 'AO', content)
            content=re.sub(r'\uA735', 'ao', content)
    
            content=re.sub(r'\uA736', 'AU', content)
            content=re.sub(r'\uA737', 'au', content)
            
            content=re.sub(r'\uA738', 'AV', content)
            content=re.sub(r'\uA739', 'av', content)
    
            content=re.sub(r'\uA73A', 'AV', content)
            content=re.sub(r'\uA73B', 'av', content)
    
            content=re.sub(r'\uA73C', 'AY', content)
            content=re.sub(r'\uA73D', 'ay', content)
            
            content=re.sub(r'\u1F670', 'et', content)        
    
            content=re.sub(r'\uFB00', 'ff', content)
            content=re.sub(r'\uFB03', 'ffi', content)
            content=re.sub(r'\uFB04', 'ffl', content)
            content=re.sub(r'\uFB01', 'fi', content)
            content=re.sub(r'\uFB02', 'fl', content)
    
            content=re.sub(r'\u01F6', 'Hv', content)
            content=re.sub(r'\u0195', 'hv', content)
    
            content=re.sub(r'\u2114', 'lb', content)
            
            content=re.sub(r'\u1EFA', 'lL', content)
            content=re.sub(r'\u1EFB', 'll', content)
    
            content=re.sub(r'\u0152', 'OE', content)
            content=re.sub(r'\u0153', 'oe', content)
    
            content=re.sub(r'\uA74E', 'OO', content)
            content=re.sub(r'\uA74F', 'oo', content)
            
            content=re.sub(r'\uFB06', 'st', content)
            
            content=re.sub(r'\uFB05', 'ft', content)        
            
            content=re.sub(r'\uA728', 'TZ', content)
            content=re.sub(r'\uA729', 'tz', content)
            
            content=re.sub(r'\u1D6B', 'ue', content)
            content=re.sub(r'\uAB63', 'uo', content)        
            
             content=re.sub(r'\uA760', 'VY', content)
             content=re.sub(r'\uA761', 'vy', content)        

",,,false,,,
https://stackoverflow.com/questions/49703099,false,The issue is related to the behavior of spaCy similarity scores and does not involve an API exhibiting unexpected failures or unpredictable behaviors.,,,,,,,Unable to create a custom entity type/label using Matcher in Spacy 2,"I am trying to create a custom entity label called FRUIT using the rule-based Matcher (i.e. adding on_match rules), following the spaCy guide. I'm using spaCy 2.0.11, so I believe the steps to do so have changed compared to spaCy 1.X

Example: doc = nlp('Tom wants to eat some apples at the United Nations')
Expected text and entity outputs:

Tom PERSON
apples FRUIT
the United Nations ORG


However, I seem to get the following error: [E084] Error assigning label ID 7429577500961755728 to span: not in StringStore. I have included my code below. When I change nlp.vocab.strings['FRUIT'] to nlp.vocab.strings['EVENT'], strangely it works but apples would be assigned the entity label EVENT. Anyone else encountering this issue? 

doc = nlp('Tom wants to eat some apples at the United Nations')

FRUIT = nlp.vocab.strings['FRUIT']

def add_ent(matcher, doc, i, matches):
    # Get the current match and create tuple of entity label, start and end.
    # Append entity to the doc's entity. (Don't overwrite doc.ents!)
    match_id, start, end = matches[i]    
    doc.ents += ((FRUIT, start, end),)

matcher = Matcher(nlp.vocab)
pattern = [{'LOWER': 'apples'}]
matcher.add('AddApple', add_ent, pattern)

matches = matcher(doc)

for ent in doc.ents:
    print(ent.text, ent.label_)

","Oh okay, I think I found a solution. The label has to be added to nlp.vocab.strings if it is not there:

nlp.vocab.strings.add('FRUIT') 

",,,false,,,
https://stackoverflow.com/questions/60656880,false,The issue is related to accessing spaCy BERT dictionary and does not involve an API exhibiting unexpected failures or unpredictable behaviors.,,,,,,,spaCy BERT dictionary,"I am trying to access spaCy BERT dictionary, but I receive strange output from the model. For instance for en_core_web_lg model I can extract ~1.3 million tokens like this

nlp = spacy.load(""en_core_web_lg"") 
tokens = [t for t in nlp.vocab]


When I do the same for en_trf_bertbaseuncased_lg model I only get 478 tokens, 

nlp = spacy.load(""en_trf_bertbaseuncased_lg"") 
tokens = [t for t in nlp.vocab]


while there should be ~30k tokens according to BERT paper. Is there a way I can access them via nlp.vocab or via custom component attributes?
","I had a similar issue with standard models like ""en"". Interestingly enough the length is exactly the same.

import spacy
nlp = spacy.load(""en"")
len([t for t in nlp.vocab])
478


Turned out that models like ""en"" does not contain the real vectors (see https://github.com/explosion/spaCy/issues/1520 ). So I guess this is also the case for Bert. 
",,,false,,,
https://stackoverflow.com/questions/50752266,false,The issue is related to tokenizing a quoted string in spacy and does not involve an API exhibiting unexpected failures or unpredictable behaviors.,,,,,,,Spacy - Tokenize quoted string,"I am using spacy 2.0 and using a quoted string as input.  

Example string

""The quoted text 'AA XX' should be tokenized""


and expecting to extract 

[The, quoted, text, 'AA XX', should, be, tokenized]


I however get some strange results while experimenting.  Noun chunks and ents looses one of the quote.  

import spacy
nlp = spacy.load('en')
s = ""The quoted text 'AA XX' should be tokenized""
doc = nlp(s)
print([t for t in doc])
print([t for t in doc.noun_chunks])
print([t for t in doc.ents])


Result

[The, quoted, text, ', AA, XX, ', should, be, tokenized]
[The quoted text 'AA XX]
[AA XX']


What is the best way to address what I need
","While you could modify the tokenizer and add your own custom prefix, suffix and infix rules that exclude quotes, I'm not sure this is the best solution here.

For your use case, it might make more sense to add a component to your pipeline that merges (certain) quoted strings into one token before the tagger, parser and entity recognizer are called. To accomplish this, you can use the rule-based Matcher and find combinations of tokens surrounded by '. The following pattern looks for one or more alphanumeric characters:

pattern = [{'ORTH': ""'""}, {'IS_ALPHA': True, 'OP': '+'}, {'ORTH': ""'""}]


Here's a visual example of the pattern in the interactive matcher demo. To do the merging, you can then set up the Matcher, add the pattern and write a function that takes a Doc object, extracts the matched spans and merges them into one token by calling their .merge method.

import spacy
from spacy.matcher import Matcher

nlp = spacy.load('en')
matcher = Matcher(nlp.vocab)
matcher.add('QUOTED', None, [{'ORTH': ""'""}, {'IS_ALPHA': True, 'OP': '+'}, {'ORTH': ""'""}])

def quote_merger(doc):
    # this will be called on the Doc object in the pipeline
    matched_spans = []
    matches = matcher(doc)
    for match_id, start, end in matches:
        span = doc[start:end]
        matched_spans.append(span)
    for span in matched_spans:  # merge into one token after collecting all matches
        span.merge()
    return doc

nlp.add_pipe(quote_merger, first=True)  # add it right after the tokenizer
doc = nlp(""The quoted text 'AA XX' should be tokenized"")
print([token.text for token in doc])
# ['The', 'quoted', 'text', ""'AA XX'"", 'should', 'be', 'tokenized']


For a more elegant solution, you can also refactor the component as a reusable class that sets up the matcher in its __init__ method (see the docs for examples).

If you add the component first in the pipeline, all other components like the tagger, parser and entity recognizer will only get to see  the retokenized Doc. That's also why you might want to write more specific patterns that only merge certain quoted strings you care about. In your example, the new token boundaries improve the predictions – but I can also think of many other cases where they don't, especially if the quoted string is longer and contains a significant part of the sentence.
",,,false,,,
https://stackoverflow.com/questions/50078741,false,The issue does not meet the criteria for deeper analysis as it does not involve an API exhibiting unexpected failures or unpredictable behaviors under specific runtime conditions.,,,,,,,strange similarity result in spacy,"I am playing around with the similarity function in Spacy and observed something that I dont understand:

import spacy
nlp = spacy.load('en_core_web_sm')  
doc1 = nlp(""Honda Civic Toyota"")
doc2 = nlp(""Honda Civic Toyota car Christian God"")

for token in doc1:
    print (token.text, doc1[0].similarity(token))
for token in doc2:
    print (token.text, doc2[0].similarity(token))


Output:

Honda 1.0
Civic 0.6631208
Toyota 0.4700994
Honda 1.0
Civic 0.6806056
Toyota 0.54713947
car 0.22469836
Christian 0.5016042
God 0.4778438


The word Honda is being compared to all the other words in doc1 and doc2 and it can be observed that when Honda is compared with Civic and Toyota, the similarity is different in doc1 and doc2.
My understanding is that the similarity is computed from the consine similarity of the Glove vector of the words, which is loaded from 'en_core_web_sm'. If that is the case, shouldn't the similarity between the same pair of words the same regardless of its context?

I think clearly I am misunderstanding something, would appreciate it if someone could clarify on it.
","As mentioned in Word Vectors and Semantic  Similarity Doc:


  Similarity is determined by comparing word vectors or ""word embeddings"", multi-dimensional meaning representations of a word.To make them compact and fast, spaCy's small models (all packages that
  end in sm) don't ship with word vectors, and only include
  context-sensitive tensors. This means you can still use the
  similarity() methods to compare documents, spans and tokens – but the
  result won't be as good, and individual tokens won't have any vectors
  assigned.So in order to use real word vectors, you need to download a larger model


so small models don't use word vectors for similarity.
",,,false,,,
https://stackoverflow.com/questions/75173490,true,The issue meets the criteria for deeper analysis as it involves the spacy.similarity function and the need to compare texts based on their meaning instead of just the occurrence of words.,spaCy,spacy.similarity,"The user wants to compare two different texts based on their meaning rather than just the occurrence of words. They are currently using the spacy.similarity function, but it only considers word embeddings and does not capture the semantic meaning of the texts.","The spacy.similarity function compares texts based on word embeddings, which may not accurately capture the semantic meaning.",The issue is triggered when the user wants to compare texts based on their meaning rather than just the occurrence of words.,This issue might be challenging to detect for users who are not familiar with the limitations of the spacy.similarity function or those who assume that it captures the full semantic meaning of texts.,How can I check similarity in meaning and not just having same words between two texts with spacy,"I'm trying to compare two different texts—one coming from a Curriculum Vitae (CV) and the other from a job announcement.
After cleaning the texts, I'm trying to compare them to detect if a job announcement is more linked to a specific CV.
I am trying to do this using similarity matching in spaCy via the following code:
similarity = pdf_text.similarity(final_text_from_annonce)

This works well, but I'm getting strange results from two different CVs for the same job announcement. Specifically, I get the same similarity score (~0.6), however, one should clearly be higher than the other.
I checked on spaCy website and I found this very important sentence:

Vector averaging means that the vector of multiple tokens is insensitive to the order of the words. Two documents expressing the same meaning with dissimilar wording will return a lower similarity score than two documents that happen to contain the same words while expressing different meanings.

So, what do I need to use or code to make spaCy compare my two texts based on their meaning instead of the occurrence of words?
I am expecting a parameter for the similarity function of spaCy, or another function that will compare my both texts and calculate a similarity score based on the meaning of the texts and not if the same words are used.
","The spaCy library by default will use the average of the word embeddings of words in a sentence to determine semantic similarity. This can be thought of as a naive sentence embedding approach. Such an approach could work, but if you were to use it is recommended that you first filter non-meaningful words (e.g. common words) to prevent them from undesirably influencing the final sentence embeddings.
The alternative (and more reliable) solution is to use a different pipeline within spaCy that has been designed to use sentence embeddings created specifically with a dedicated sentence encoder (e.g. the Universal Sentence Encoder (USE) [1] by Cer et al.). Martino Mensio created a package called spacy-universal-sentence-encoder that makes use of this model. Install it via the following command in your command prompt:
pip install spacy-universal-sentence-encoder

Then you can compute the semantic similarity between sentences as follows:
import spacy_universal_sentence_encoder

# Load one of the models: ['en_use_md', 'en_use_lg', 'xx_use_md', 'xx_use_lg']
nlp = spacy_universal_sentence_encoder.load_model('en_use_lg')

# Create two documents
doc_1 = nlp('Hi there, how are you?')
doc_2 = nlp('Hello there, how are you doing today?')

# Use the similarity method to compare the full documents (i.e. sentences)
print(doc_1.similarity(doc_2))  # Output: 0.9356049733134972
# Or make the comparison using a predefined span of the second document 
print(doc_1.similarity(doc_2[0:7])) # Output: 0.9739387861159459

As a side note, when you run the nlp = spacy_universal_sentence_encoder.load_model('en_use_lg') command for the first time, you may have to do so with administrator rights to allow TensorFlow to create the models folder in C:\Program Files\Python310\Lib\site-packages\spacy_universal_sentence_encoder and download the appropriate model. If you don't, it is possible that there will be a PermissionDeniedError and the code will not run.
References
[1] Cer, D., Yang, Y., Kong, S.Y., Hua, N., Limtiaco, N., John, R.S., Constant, N., Guajardo-Cespedes, M., Yuan, S., Tar, C. and Sung, Y.H., 2018. Universal sentence encoder. arXiv preprint arXiv:1803.11175.
",,,false,,,
https://stackoverflow.com/questions/62735456,true,The issue meets the criteria for deeper analysis as it involves understanding and using the Coreference resolution feature of the Stanford NLP tool.,Stanford NLP,Coreference resolution,The user is trying to understand and use the Coreference resolution feature of the Stanford NLP tool. They have provided code and are encountering an error with the deterministic algorithm.,The Coreference resolution feature of the Stanford NLP tool is used to resolve coreferences in text.,The issue is triggered when the user tries to use the Coreference resolution feature and encounters an error with the deterministic algorithm.,This issue might be challenging to detect for users who are not familiar with the specific requirements and limitations of the Coreference resolution feature in the Stanford NLP tool.,Understanding and using Coreference resolution Stanford NLP tool (in Python 3.7),"I am trying to understand the Coreference NLP Stanford tools.
This is my code and it is working:
import os
os.environ[""CORENLP_HOME""] = ""/home/daniel/StanfordCoreNLP/stanford-corenlp-4.0.0""

from stanza.server import CoreNLPClient

text = 'When he came from Brazil, Daniel was fortiﬁed with letters from Conan but otherwise did not know a soul except Herbert. Yet this giant man from the Northeast, who had never worn an overcoat or experienced a change of seasons, did not seem surprised by his past.'

with CoreNLPClient(annotators=['tokenize','ssplit','pos','lemma','ner', 'parse', 'depparse','coref'],
               properties={'annotators': 'coref', 'coref.algorithm' : 'neural'},timeout=30000, memory='16G') as client:

    ann = client.annotate(text)

chains = ann.corefChain
chain_dict=dict()
for index_chain,chain in enumerate(chains):
    chain_dict[index_chain]={}
    chain_dict[index_chain]['ref']=''
    chain_dict[index_chain]['mentions']=[{'mentionID':mention.mentionID,
                                          'mentionType':mention.mentionType,
                                          'number':mention.number,
                                          'gender':mention.gender,
                                          'animacy':mention.animacy,
                                          'beginIndex':mention.beginIndex,
                                          'endIndex':mention.endIndex,
                                          'headIndex':mention.headIndex,
                                          'sentenceIndex':mention.sentenceIndex,
                                          'position':mention.position,
                                          'ref':'',
                                          } for mention in chain.mention ]


for k,v in chain_dict.items():
    print('key',k)
    mentions=v['mentions']
    for mention in mentions:
        words_list = ann.sentence[mention['sentenceIndex']].token[mention['beginIndex']:mention['endIndex']]
        mention['ref']=' '.join(t.word for t in words_list)
        print(mention['ref'])
    

I tried three algorithms:

statistical (as in the code above). Results:


he
this giant man from the Northeast , who had never worn an overcoat or experienced a change of seasons
Daniel
his



neural


this giant man from the Northeast , who had never worn an overcoat or experienced a change of seasons ,
his



deterministic (I got the error below)
 &gt; Starting server with command: java -Xmx16G -cp
 &gt; /home/daniel/StanfordCoreNLP/stanford-corenlp-4.0.0/*
 &gt; edu.stanford.nlp.pipeline.StanfordCoreNLPServer -port 9000 -timeout
 &gt; 30000 -threads 5 -maxCharLength 100000 -quiet True -serverProperties
 &gt; corenlp_server-9fedd1e9dfb14c9e.props -preload
 &gt; tokenize,ssplit,pos,lemma,ner,parse,depparse,coref Traceback (most
 &gt; recent call last):
 &gt; 
 &gt;   File ""&lt;ipython-input-58-0f665f07fd4d&gt;"", line 1, in &lt;module&gt;
 &gt;     runfile('/home/daniel/Documentos/Working Papers/Leader traits/Code/20200704 - Modeling
 &gt; Organizing/understanding_coreference.py',
 &gt; wdir='/home/daniel/Documentos/Working Papers/Leader
 &gt; traits/Code/20200704 - Modeling Organizing')
 &gt; 
 &gt;   File
 &gt; ""/home/daniel/anaconda3/lib/python3.7/site-packages/spyder_kernels/customize/spydercustomize.py"",
 &gt; line 827, in runfile
 &gt;     execfile(filename, namespace)
 &gt; 
 &gt;   File
 &gt; ""/home/daniel/anaconda3/lib/python3.7/site-packages/spyder_kernels/customize/spydercustomize.py"",
 &gt; line 110, in execfile
 &gt;     exec(compile(f.read(), filename, 'exec'), namespace)
 &gt; 
 &gt;   File ""/home/daniel/Documentos/Working Papers/Leader
 &gt; traits/Code/20200704 - Modeling
 &gt; Organizing/understanding_coreference.py"", line 21, in &lt;module&gt;
 &gt;     ann = client.annotate(text)
 &gt; 
 &gt;   File
 &gt; ""/home/daniel/anaconda3/lib/python3.7/site-packages/stanza/server/client.py"",
 &gt; line 470, in annotate
 &gt;     r = self._request(text.encode('utf-8'), request_properties, **kwargs)
 &gt; 
 &gt;   File
 &gt; ""/home/daniel/anaconda3/lib/python3.7/site-packages/stanza/server/client.py"",
 &gt; line 404, in _request
 &gt;     raise AnnotationException(r.text)
 &gt; 
 &gt; AnnotationException: java.lang.RuntimeException:
 &gt; java.lang.IllegalArgumentException: No enum constant
 &gt; edu.stanford.nlp.coref.CorefProperties.CorefAlgorithmType.DETERMINISTIC



Questions:

Why am I getting this error with the deterministic?

Any piece of code using the NLP Stanford in Python seems to be much slower than the codes related with Spacy or NLTK. I know that there is no coreference in these other libraries. But for instance when I use import nltk.parse.stanford import StanfordDependencyParser for dependence parse it is much faster then this StanfordNLP library. Is there any way to acelerate this CoreNLPClient in Python?

I will use this library to work with long texts. Is it better to work with smaller pieces with the entire text? Long texts can cause wrong results for coreference resolution (I have found very strange results for this coreference library when I am using long texts)? Is there an optimal size?

Results:


The results from the statistical algorithm seems to be better. I expected that the best result would come from the neural algorithm. Do you agree with me? There are 4 valid mention in the statistical algorithm while only 2 when I am using the neural algorithm.
Am I missing something?
","
You may find the list of supported algorithms in Java documentation: link

You might want to start the server and then just use it, something like
# Here's the slowest part—models are being loaded
client = CoreNLPClient(...)

ann = client.annotate(text)

...

client.stop()



But I cannot give you any clue regarding 3 and 4.
",,,false,,,
https://stackoverflow.com/questions/61118213,false,The issue does not meet the criteria for deeper analysis as it does not involve an API exhibiting unexpected failures or unpredictable behaviors under specific runtime conditions.,,,,,,,spacy lemmatizing inconsistency with lemma_lookup table,"There seems to be an inconsistency when iterating over a spacy document and lemmatizing the tokens compared to looking up the lemma of the word in the Vocab lemma_lookup table.

nlp = spacy.load(""en_core_web_lg"")
doc = nlp(""I'm running faster"")
for tok in doc: 
  print(tok.lemma_)


This prints out ""faster"" as lemma for the token ""faster"" instead of ""fast"". However the token does exist in the lemma_lookup table.

nlp.vocab.lookups.get_table(""lemma_lookup"")[""faster""]


which outputs ""fast""

Am I doing something wrong? Or is there another reason why these two are different? Maybe my definitions are not correct and I'm comparing apples with oranges?

I'm using the following versions on Ubuntu Linux:
spacy==2.2.4
spacy-lookups-data==0.1.0
","With a model like en_core_web_lg that includes a tagger and rules for a rule-based lemmatizer, it provides the rule-based lemmas rather than the lookup lemmas when POS tags are available to use with the rules. The lookup lemmas aren't great overall and are only used as a backup if the model/pipeline doesn't have enough information to provide the rule-based lemmas.

With faster, the POS tag is ADV, which is left as-is by the rules. If it had been tagged as ADJ, the lemma would be fast with the current rules.

The lemmatizer tries to provide the best lemmas it can without requiring the user to manage any settings, but it's also not very configurable right now (v2.2). If you want to run the tagger but have lookup lemmas, you'll have to replace the lemmas after running the tagger.
","aab wrote, that:

The lookup lemmas aren't great overall and are only used as a backup
if the model/pipeline doesn't have enough information to provide the
rule-based lemmas.

This is also how I understood it from the spaCy code, but since I wanted to add my own dictionaries to improve the lemmatization of the pretrained models, I decided to try out the following, which worked:
#load model
nlp = spacy.load('es_core_news_lg')
#define dictionary, where key = lemma, value = token to be lemmatized - not case-sensitive
corr_es = {
    ""decir"":[""dixo"", ""decia"", ""Dixo"", ""Decia""],
    ""ir"":[""iba"", ""Iba""],
    ""pacerer"":[""parecia"", ""Parecia""],
    ""poder"":[""podia"", ""Podia""],
    ""ser"":[""fuesse"", ""Fuesse""],
    ""haber"":[""habia"", ""havia"", ""Habia"", ""Havia""],
    ""ahora"" : [""aora"", ""Aora""],
    ""estar"" : [""estàn"", ""Estàn""],
    ""lujo"" : [""luxo"",""luxar"", ""Luxo"",""Luxar""],
    ""razón"" : [""razon"", ""razòn"", ""Razon"", ""Razòn""],
    ""caballero"" : [""cavallero"", ""Cavallero""],
    ""mujer"" : [""muger"", ""mugeres"", ""Muger"", ""Mugeres""],
    ""vez"" : [""vèz"", ""Vèz""],
    ""jamás"" : [""jamas"", ""Jamas""],
    ""demás"" : [""demas"", ""demàs"", ""Demas"", ""Demàs""],
    ""cuidar"" : [""cuydado"", ""Cuydado""],
    ""posible"" : [""possible"", ""Possible""],
    ""comedia"":[""comediar"", ""Comedias""],
    ""poeta"":[""poetas"", ""Poetas""],
    ""mano"":[""manir"", ""Manir""],
    ""barba"":[""barbar"", ""Barbar""],
    ""idea"":[""ideo"", ""Ideo""]
}
#replace lemma with key in lookup table
for key, value in corr_es.items():
    for token in value:
        correct = key
        wrong = token
        nlp.vocab.lookups.get_table(""lemma_lookup"")[token] = key
#process the text
nlp(text) 

Hopefully this could help.
",,false,,,
https://stackoverflow.com/questions/55746174,false,The issue does not meet the criteria for deeper analysis as it does not involve an API exhibiting unexpected failures or unpredictable behaviors under specific runtime conditions.,,,,,,,"Why is the currency symbol sometimes, but not always included in spacy MONEY entities?","In the sentence ""I saved $6 hundred."" spacy NER recognizes ""$6 hundred"" as MONEY entity.
So far, so good. 

But in the sentence ""I saved $600."" it recognizes ""600"" as MONEY - without the dollar sign!
This seems inconsistent. Am I missing something?  

Here's the code to reproduce:

import en_core_web_sm
nlp = en_core_web_sm.load()

def print_entities(txt):
    print(""Entities for input: '{}'"".format(txt))
    doc = nlp(txt)
    for entity in doc.ents:
        print('\t', entity.text, entity.label_)


print_entities(""I saved $6 hundred."")
print_entities(""I saved $600."")


This gives the output:

Entities for input: 'I saved $6 hundred.'
     $6 hundred MONEY
Entities for input: 'I saved $600.'
     600 MONEY


I'm doing this in a fresh python 3.6.8 virtual environment with spacy version 2.1.3 and en_core_web_sm version 2.1.0
","Another possible work-around until a more satisfying solution is developed: 

def money_merger(doc):
    with doc.retokenize() as retokenizer:
        for money in [e for e in doc.ents if e.label_ == 'MONEY']:
            if doc[money.start - 1].is_currency:
                retokenizer.merge(doc[
                money.start-1:money.end])
return doc

nlp.add_pipe(money_merger, after='ner')

","I confirm that the issue (1) is still here, (2) doesn't depend on the currency symbol, (3) only occurs when the entity is just a symbol + number (i.e., no literal expression), and (4) the symbols are not included in the span (you can list it, it doesn't appear) or the entity.start_char and entity.end_char.

A quick-and-dirty hack consists in testing whether the text starts AND ends up with a figure, in which case you get the currency symbol before or after. For example, if source is the initial document and entity the money entity:

def complete_text(source, entity):
    t=entity.text
    try:
        float( t[0] + t[len(t)-1] )
    except:
        return t
    if entity[0].nbor(-1).is_currency:
        return entity[0].nbor(-1).text + t
    else:
        return t + entity[0].nbor(1).text


Hope there will be some neater suggestions!
",,false,,,
https://stackoverflow.com/questions/67100601,false,The issue does not meet the criteria for deeper analysis as it does not involve an API exhibiting unexpected failures or unpredictable behaviors under specific runtime conditions.,,,,,,,Python 3.7 spaCy Help Needed- Environment Inconsistency Issue?,"I am facing an issue when trying to call spaCy into my Jupyter notebook. When I run
import spacy I get the below:

I have used spaCy before many times with no issue, but I noticed this problem began after I was trying to also install from neuralcoref import Coref and am not sure if that has caused this.
When I go into the terminal and run conda list spacy it looks like spaCy is available:

I do not really understand what the errors are suggesting, but I tried to reinstall murmurhash using conda install -c anaconda murmurhash after which I got this. This is just a screenshot of the first few but there are MANY packages that are allegedly causing the inconsistency:

Following the list of packages causing inconsistencies, I get this:

For reference, I am using MacOS and python 3.7. How can I fix this?
","spacy&gt;=3.0 and neuralcoref are currently not compatible - the Cython API of spaCy's v3 has changed too much. This might be causing conflicts in your environment?
",,,false,,,
https://stackoverflow.com/questions/62936489,true,"The issue involves the use of the LSTM layer in Keras, specifically the incorrect usage of the input_shape parameter in the model.add() method.",Keras,model.add(LSTM),The error occurs because the input_shape parameter is not a valid argument for the model.add() method. It should be passed as an argument to the LSTM layer itself.,The LSTM layer is added to the model with the correct input_shape parameter.,The issue is triggered when the input_shape parameter is mistakenly passed to the model.add() method instead of the LSTM layer.,This issue might be challenging to detect for users who are not familiar with the correct usage of the input_shape parameter in Keras or those who assume that it can be used directly in the model.add() method.,Siamese LSTM for document similarity using keras giving input error,"I have written this code to use the Siamese method to calculate the similarity of two documents. I want to embed my vectorize layer (embedding is performed using Google News Dataset) of two separate documents using vectorization approach and then feed it to LSTM and output of LSTM goes into Cosine function to measure the similarity of two documents.
#importing libraries
from __future__ import print_function
import gensim
import numpy as np
import nltk
from nltk.tokenize import sent_tokenize, word_tokenize
import csv
import re
import pandas as pd
from pandas import DataFrame
import pandas as pd
nltk.download('punkt')

from tensorflow import keras
from keras.preprocessing import sequence
from keras.models import Sequential
from keras.layers import Dense, Dropout, Embedding, LSTM, Bidirectional, TimeDistributed
from tensorflow.keras import layers

#Loading pre=trained word2vec model

from gensim.models.keyedvectors import KeyedVectors

# You need to dowload google pre-trained model using below link
# https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit
#Change the path according to your directory

model_path = 'D:\GoogleNews_vectors_negative300\GoogleNews_vectors_negative300.bin'   
w2v_model = KeyedVectors.load_word2vec_format(model_path, binary=True)

#Setting Parameters for model

class DocSim(object):
    def __init__(self, w2v_model , stopwords=[]):
        self.w2v_model = w2v_model
        self.stopwords = stopwords
        
    def vectorize(self, doc):
        """"""Identify the vector values for each word in the given document""""""
        doc = doc.lower()
        words = [w for w in doc.split("" "") if w not in self.stopwords]
        word_vecs = []
        for word in words:
            try:
                vec = self.w2v_model[word]
                word_vecs.append(vec)
            except KeyError:
                # Ignore, if the word doesn't exist in the vocabulary
                pass

        # Assuming that document vector is the mean of all the word vectors

        vector = np.mean(word_vecs, axis=0)
        return vector
        
        
    def Siamese_cosine_sim(self, vectorA, vectorB):
        model = Sequential()
        model.add(LSTM(20, return_sequences=True),input_shape=[vectorA,vectorB])
        model.compile(loss='binary_crossentropy', optimizer='adam')
        outputs = layers.Dense(1, activation=""sigmoid"")(left_doc,right_doc)

        """"""Find the cosine similarity distance between two vectors.""""""
        csim = np.dot(left_doc, right_doc) / (np.linalg.norm(left_doc) * np.linalg.norm(right_doc))
        if np.isnan(np.sum(csim)):
            return 0
        return csim
 

    def calculate_similarity(self, withdigits_source_rules, withdigits_target_rules=[], threshold=0.8):
            """"""Calculates &amp; returns similarity scores between given source rules &amp; all
            the target rules""""""
            if isinstance(withdigits_target_rules, str):
                withdigits_target_rules = [withdigits_target_rules]


            source_vec = self.vectorize(withdigits_source_rules)
            results = []

            for rule in withdigits_target_rules:
                target_vec = self.vectorize(rule)
                sim_score = self.Siamese_cosine_sim (source_vec, target_vec)
                if sim_score &gt; threshold:
                    results.append({
                        'Siamese Sim Score':sim_score,
                        'Target Rule':rule
                    })


                # Sort results by score in desc order
                results.sort(key=lambda k : k['Siamese Sim Score'] , reverse=True)

            return results

ds = DocSim(w2v_model)

#Two documents data
withdigits_source_rules =set([""2.1 Separation of trains"",""2.3.1.2 Level crossing obstruction"",""2.2.1.1 Safety is compromised if a train proceeds without a movement autority"",""Principle: The method of signalling must maintain a space interval between trains that is safe."",""2.1.1 Context""])

#Calculate the similarity score between a source rule &amp; a target rule.


if isinstance(withdigits_source_rules, str):
    withdigits_source_rules = [withdigits_source_rules]
   

# This will return one target rules text with a similarity score

for rule in withdigits_source_rules:
    sim_scores= ds.calculate_similarity(rule, withdigits_target_rules)

    

    
    # Printing the output in text file
    
    print(""Source rule: {} \n\nSimilarity with Target Rule is \n\n {}\n"".format(rule, sim_scores) , file=open(""output.txt"", ""a""))
    print(""\n"")
    
    
    # Printing output in Jupyter
    
    print(""Source rule: {} \n\nSimilarity with Target Rule is \n\n {}\n"".format(rule, sim_scores) )
    print(""\n"")


I am getting following error if someone can help me to solve this issue along with LSTM input function?
TypeError: add() got an unexpected keyword argument 'input_shape'

","Refer to the documentation here for adding layers to a Sequential model. The add method only accepts one parameter - layer. If the passed argument is not a layer instance, it raises TypeError, which is precisely the error it threw. I guess, you wanted to pass the input_shape parameter to the LSTM layer (line after creating Sequential model). Just move it inside the LSTM layer and it should work fine.
",,,false,,,
https://stackoverflow.com/questions/42376652,true,The issue involves the incorrect interpretation of the alpha parameter in the Gensim LDA implementation.,Gensim,LdaModel,"The issue arises from a misunderstanding of the alpha parameter in the LDA model. The user expects the sum of topic probabilities to be equal to the number of topics, but this is not the intended behavior.","The alpha parameter is set to a specific value or one of the available options (e.g., 'symmetric', 'asymmetric', 'auto') as per the user's requirement.",The issue is triggered when the user expects the sum of topic probabilities to be equal to the number of topics.,This issue might be challenging to detect for users who are not familiar with the intended behavior of the alpha parameter in the Gensim LDA implementation.,Gensim LDA alpha-parameter,"I tried the three default-options for alpha in gensim's lda implementation and now wonder about the result:
The sum of topic-probabilities over all documents is smaller than the number of documents in the corpus (see below). For example alpha = 'symmetric' yields about 9357 as sum of topic-probabilities, however, the number of topics is 9459. Could one tell me the reason for this unexpected result?

alpha = symmetric
nr_of_docs = 9459
sum_of_topic_probs = 9357.12285605

alpha = asymmetric
nr_of_docs = 9459
sum_of_topic_probs = 9375.29253851

alpha = auto
nr_of_docs = 9459
sum_of_topic_probs = 9396.40123459

","I tried to replicate your problem but in my case (using a very small corpus), I could not find any difference between the three sums. 
I will still share the paths I tried in the case anybody else wants to replicate the problem ;-)

I use some small example from gensim's website and train the three different LDA models:

from gensim import corpora, models
texts = [['human', 'interface', 'computer'],
         ['survey', 'user', 'computer', 'system', 'response', 'time'],
         ['eps', 'user', 'interface', 'system'],
         ['system', 'human', 'system', 'eps'],
         ['user', 'response', 'time'],
         ['trees'],
         ['graph', 'trees'],
         ['graph', 'minors', 'trees'],
         ['graph', 'minors', 'survey']]

dictionary = corpora.Dictionary(texts)

corpus = [dictionary.doc2bow(text) for text in texts]

lda_sym = models.ldamodel.LdaModel(corpus=corpus, id2word=dictionary, num_topics=10, update_every=1,
                                      chunksize =100000, passes=1, alpha='symmetric')
lda_asym = models.ldamodel.LdaModel(corpus=corpus, id2word=dictionary, num_topics=10, update_every=1,
                                      chunksize =100000, passes=1, alpha='asymmetric')
lda_auto = models.ldamodel.LdaModel(corpus=corpus, id2word=dictionary, num_topics=10, update_every=1,
                                      chunksize =100000, passes=1, alpha='auto')


Now I sum over the topic probabilities for all documents (9 documents in total)

counts = {}
for model in [lda_sym, lda_asym, lda_auto]:
    s = 0
    for doc_n in range(len(corpus)):
        s += pd.DataFrame(lda_sym[corpus[doc_n]])[1].sum()
        if s &lt; 1:
            print('Sum smaller than 1 for')
            print(model, doc_n)
    counts[model] = s


And indeed the sums are always 9:

counts = {&lt;gensim.models.ldamodel.LdaModel at 0x7ff3cd1f3908&gt;: 9.0,
          &lt;gensim.models.ldamodel.LdaModel at 0x7ff3cd1f3048&gt;: 9.0,
          &lt;gensim.models.ldamodel.LdaModel at 0x7ff3cd1f3b70&gt;: 9.0}


Of course that's not a representative example since it's so small. So if you could, maybe provide some more details about your corpus. 

In general I would assume that this should always be the case. My first intuition was that maybe empty documents would change the sum, but that is also not the case, since empty documents just yield a topic distribution identical to alpha (which makes sense):

pd.DataFrame(lda_asym[[]])[1]


returns 

0    0.203498
1    0.154607
2    0.124657
3    0.104428
4    0.089848
5    0.078840
6    0.070235
7    0.063324
8    0.057651
9    0.052911


which is identical to

lda_asym.alpha

array([ 0.20349777,  0.1546068 ,  0.12465746,  0.10442834,  0.08984802,
    0.0788403 ,  0.07023542,  0.06332404,  0.057651  ,  0.05291085])


which also sums to 1.

From a theoretical point of view, choosing different alphas will yield to completely different LDA models. 

Alpha is the hyper parameter for the Dirichlet prior. The Dirichlet prior is the distribution from which we draw theta. And theta becomes the parameter that decides what shape the topic distribution is. So essentially, alpha influences how we draw topic distributions. That is why choosing different alphas will also give you slightly different results for

lda.show_topics()


But I do not see why the sum over document probabilities should differ from 1 for any LDA model or any kind of document.
","I think the problem is as default setting, the minimum_probability is set to 0.01 not 0.00. 

You can check out the LDA model code here:

Therefore if you are training your model with the default setting, it might not return a sum of 1.00 when adding up the prob across topics for a specific document.

Since the minimum_probability is passed in here, you can always change it by something like this to reset it:

your_lda_model_name.minimum_probability = 0.0
",,false,,,
https://stackoverflow.com/questions/51791964,true,The issue involves the deprecation of the most_similar method in the Word2Vec implementation of Gensim.,Gensim,Word2Vec.most_similar,The deprecation warning indicates that the most_similar method will be removed in future versions of Gensim. Users are advised to use self.wv.most_similar instead.,The most_similar method is called on a Word2Vec model instance.,The issue is triggered when the most_similar method is used instead of self.wv.most_similar.,This issue might be challenging to detect for users who are not aware of the deprecation of the most_similar method or those who rely on outdated examples or documentation.,DeprecationWarning in Gensim `most_similar`?,"While implementating Word2Vec in Python 3.7, I am facing an unexpected scenario related to depreciation. My question is what exactly is the depreciation warning with respect to 'most_similar' in word2vec gensim python?
Currently, I am getting the following issue.
DeprecationWarning: Call to deprecated most_similar (Method will be removed in 4.0.0, use self.wv.most_similar() instead).
model.most_similar('hamlet')
FutureWarning: Conversion of the second argument of issubdtype from int to np.signedinteger is deprecated. In future, it will be treated as np.int32 == np.dtype(int).type.
if np.issubdtype(vec.dtype, np.int):
Please help to curb this issue? Any help is appreciated.
The code what, I have tried is as follows.
import re
from gensim.models import Word2Vec
from nltk.corpus import gutenberg

sentences = list(gutenberg.sents('shakespeare-hamlet.txt'))   
print('Type of corpus: ', type(sentences))
print('Length of corpus: ', len(sentences))

for i in range(len(sentences)):
    sentences[i] = [word.lower() for word in sentences[i] if re.match('^[a-zA-Z]+', word)]
print(sentences[0])    # title, author, and year
print(sentences[1])
print(sentences[10])
model = Word2Vec(sentences=sentences, size = 100, sg = 1, window = 3, min_count = 1, iter = 10, workers = 4)
model.init_sims(replace = True)
model.save('word2vec_model')
model = Word2Vec.load('word2vec_model')
model.most_similar('hamlet')

","It's a warning  which that it's about to become obsolete and non-functional.


  Usually things are deprecated for a few versions giving anyone using them enough time to move to the new method before they are removed.


They've moved most_similar to wv

So most_simliar() should look something like:

model.wv.most_similar('hamlet')


src ref

Hope this helps

Edit : using wv.most_similar()

import re
from gensim.models import Word2Vec
from nltk.corpus import gutenberg

sentences = list(gutenberg.sents('shakespeare-hamlet.txt'))   
print('Type of corpus: ', type(sentences))
print('Length of corpus: ', len(sentences))

for i in range(len(sentences)):
    sentences[i] = [word.lower() for word in sentences[i] if re.match('^[a-zA-Z]+', word)]
print(sentences[0])    # title, author, and year
print(sentences[1])
print(sentences[10])
model = Word2Vec(sentences=sentences, size = 100, sg = 1, window = 3, min_count = 1, iter = 10, workers = 4)
model.init_sims(replace = True)
model.save('word2vec_model')
model = Word2Vec.load('word2vec_model')
similarities = model.wv.most_similar('hamlet')
for word , score in similarities:
    print(word , score)

","A deprecation warning is a warning to indicate the use of things that may or may not exist in future versions of Python, often replaced by other things. (tells what they are)

It appears that the errors originate inside of Word2Vec, and not your code. Removing these errors would entail going into that library and changing its code.

Try doing what it tells you to do.

Change your model.most_similar('hamlet') to model.wv.most_similar('hamlet')

I am unfamiliar with this package, so adjust to how it would work for your use. 
","After the update to 4.0.0 version, the function model.most_similar() will be removed. So what you can do is to modify the function to model.wv.most_similar(). The same goes for the function model.similarity(). You have to change it to model.wv.similarity(). 
",false,,,
https://stackoverflow.com/questions/46731926,false,"The issue does not involve an API exhibiting unexpected failures or unpredictable behaviors under specific runtime conditions. It appears to be a problem related to the corpus size and quality, as well as the training parameters and data preparation.",,,,,,,Gensim: quality of word2vec model seems not to correlate with num of iterations in training,"I'm playing with gensim's wordvec and try to build a model using the terms from a large medical thesaurus as sentences. There are about 1 million terms (most of the multiword terms which I treat as sentences) and the hope is, that if word2vec sees terms like ""breast cancer"" and ""breast tumor"" etc. it will be able to conclude that ""cancer"" and ""tumor"" are somewhat similar. 

I run experiments in which I track how similar terms like that are when using different numbers of iterations but it seems that the results don't correlate. I'd expect that when considering word pairs like (wound, lesion), (thorax, lung), (cancer, tumor) etc, when going from 5 to 100 iterations there'd be a tendency (even if small) that the one word in the pair is ""more similar"" to the the other as the number of iterations grows. But no, results appear pretty random or even getting worse.  

Specifically: I loop with 1,5,10,20,50,100 iterations and train a w2v model and then for my word pairs above check the rank of the 2nd word in the list (say ""lung"") of similar words (as returned by w2v) for the first word (say ""thorax""), then sum up and build the average. And the average rank is growing (!) not decreasing, meaning as training proceeds, the vectors for ""lung"" and ""thorax"" move further and further away from each other. 

I didn't expect gensim to detect the clean synonyms and also perhaps 'only' 1 million terms (sentences) is not enough, but still I am puzzled by this effect. 

Does anyone have a suspicion? 

====================================================

Added after comments and feedback came in: 

Thanks for the detailed feedback, gojomo. I had checked many of these issues before: 


yes, the thesaurus terms (""sentences"") come in the right format, e.g. ['breast', 'cancer'] 
yes, of the ~1mio terms more than 850.000 are multiword. Clear that 1-word terms won't provide any context. But there should be ample evidence from the multiword terms
the examples I gave ('clinic', 'cancer', 'lung', ...) occur in many hundreds of terms, often many thousands. This is what I find odd: That not even for words this frequent really good similar words are suggested. 
you ask for the code: Here it is https://www.dropbox.com/s/fo3fazl6frj99ut/w2vexperiment.py?dl=0 It expects to be called (python3) with the name of the model and then the SKOS-XML files of a large thesaurus like Snomed

python w2vexperiment.py snomed-w2v.model SKOS/*.skos
I the code you see that I create a new model with each new experiment (with a different number of iterations) So there should be no effect that one run pollutes the other (wrong learning rate etc...) 
I have set min_count to 10 


Still: the models don't get better but often worse as number of iterations grows. And even the better ones (5 or 10 iterations) give me strange results for my test words... 
","I suspect there's something wrong with your corpus prep, or training – usually word2vec can rank such similarities well.  

Are you supplying the terms alone (eg ['breast, 'tumor'] or ['prophylaxis'] as very tiny sentences), or the terms plus definitions/synonyms as somewhat longer sentences?  

The latter would be better. 

If the former, then 1-word 'sentences' achieve nothing: there's no neighboring 'context' for word2vec to learn anything, and they're essentially skipped. 

And mere 2-word sentences might get some effect, but don't necessarily provide the kind of diverse contexts helpful for training to induce the useful vector arrangements. 

Also if it's 1-million 'sentences' of just 1-4 words each, it's kind of a small dataset, and individual words might not be appearing often enough, in sufficiently slightly-varied contexts, for them to get good vectors. You should check the words/tokens of interest, in the model.wv.vocab dict, for a count value that indicates there were enough examples to induce a good vector - ideally 10+ occurrences each (and more is better). 

So: more data, and more diverse usages from the relevant domain, are always a good idea. A thesaurus with synonyms in each 'sentence', that are many words (5 to dozens), might be enough.

You don't show your code or training-parameters, but people tweaking the defaults, or following outdated online examples, can often sabotage the algorithm's effectiveness. 

For example, it's distressingly common to see people who call train() multiple times, in their own iteration loop, to mismanage the learning-rate alpha such that some iterations run with a negative alpha – meaning every backpropagation serves to drive the context-vectors towards lower target-word predictiveness, the exact opposite of what should be happening. (It's best to either supply the corpus &amp; iter on Word2Vec initialization, or call train() just once. Only advanced tinkerers should need to call train() multiple times.)

Similarly, while naive intuition is often ""keeping more words/info must be better"", and thus people lower min_count to 1 or 0, such low-frequency words can't get good vectors with just 1 (or a few) occurences, but since they are very numerous (in total), they can interfere with other words' meaningful training. (The surviving, more-frequent words get better vectors when low-frequency words are discarded.) 

Good luck!
",,,false,,,
https://stackoverflow.com/questions/50352777,false,The issue does not meet the criteria for deeper analysis as it does not involve an API exhibiting unexpected failures or unpredictable behaviors under specific runtime conditions.,,,,,,,Shape ValueError in LSTM network using Tensorflow,"I want to train a LSTM model with Tensorflow. I have a text data as input and I get doc2vec of each paragraph of the text and pass it to the lstm layers but I get ValueError because of inconsistency of shape rank.
I've searched through Stackoverflow for similar questions and some tutorials, but I couldn't solve this error. Do you have any idea what should I do?
Here is the error:


  Traceback (most recent call last):
    File ""writeRNN.py"", line 97, in 
      outputs, final_state = tf.nn.dynamic_rnn(cell, embed, initial_state=initial_state)
    File ""myven/lib/python3.5/site-packages/tensorflow/python/ops/rnn.py"", line 627, in dynamic_rnn
      dtype=dtype)
    File ""myven/lib/python3.5/site-packages/tensorflow/python/ops/rnn.py"", line 690, in _dynamic_rnn_loop
      for input_ in flat_input)
    File ""myven/lib/python3.5/site-packages/tensorflow/python/ops/rnn.py"", line 690, in 
      for input_ in flat_input)
    File ""myven/lib/python3.5/site-packages/tensorflow/python/framework/tensor_shape.py"", line 761, in with_rank_at_least
      raise ValueError(""Shape %s must have rank at least %d"" % (self, rank))
  ValueError: Shape (?, ?) must have rank at least 3


And below is the code:

lstm_size = 128
lstm_layers = 1
batch_size = 50
learning_rate = 0.001

# Create the graph object
graph = tf.Graph()
# Add nodes to the graph
with graph.as_default():
    inputs_ = tf.placeholder(tf.int32, [None, None], name='inputs')
    labels_ = tf.placeholder(tf.int32, [None, None], name='labels')
    keep_prob = tf.placeholder(tf.float32, name='keep_prob')
with graph.as_default():
    lstm = tf.contrib.rnn.BasicLSTMCell(lstm_size)
    drop = tf.contrib.rnn.DropoutWrapper(lstm, output_keep_prob=keep_prob)
    cell = tf.contrib.rnn.MultiRNNCell([drop] * lstm_layers)
    initial_state = cell.zero_state(batch_size, tf.float32)

with graph.as_default():
    outputs, final_state = tf.nn.dynamic_rnn(cell, inputs_, initial_state=initial_state)


with graph.as_default():
    predictions = tf.contrib.layers.fully_connected(outputs[:, -1], 1, activation_fn=tf.sigmoid)
    cost = tf.losses.mean_squared_error(labels_, predictions)
    optimizer = tf.train.AdamOptimizer(learning_rate).minimize(cost)


with graph.as_default():
    correct_pred = tf.equal(tf.cast(tf.round(predictions), tf.int32), labels_)
    accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))
with tf.Session(graph=graph) as sess:
    sess.run(tf.global_variables_initializer())
    for e in range(epochs):
        state = sess.run(initial_state)
        for ii, (x, y) in enumerate(get_batches(train_x, train_y, batch_size), 1):
            feed = {inputs_: x, labels_: y[:, None], keep_prob: 0.5, initial_state: state}
            loss, state, _ = sess.run([cost, final_state, optimizer], feed_dict=feed)       


I got error on outputs, final_state = tf.nn.dynamic_rnn(cell, inputs_, initial_state=initial_state) Line as the error I described.
doc2vec model is trained on gensim and converts each sentence into a vector with 100 value.
I tried to change inputs_ shape and labels_ shape but also I get same error!
I really don't know what should I do?!

I really thank if you could answer my question.
","If you look at the documentation for the dynamic_rnn function it says that the inputs should have three dimensions. The first one is for the batch size, the second is for the sequence length, and the third is the data embedding size. Your input only has two dimensions, which is why there is an error.
",,,false,,,
https://stackoverflow.com/questions/36263594,false,The issue does not meet the criteria for deeper analysis as it is related to inconsistent results in gensim word2vec embeddings rather than an API exhibiting unexpected failures or unpredictable behaviors.,,,,,,,gensim word2vec giving inconsistent results,"i am using gensim in ubuntu. version is 0.12.4. my word2vec model is not consistent. every time i build the model based on the same exact sentences and same parameter it still have different presentations of the words.

here is the code (that i stole from the initial post)

&gt;&gt;&gt; from nltk.corpus import brown
&gt;&gt;&gt; from gensim.models import Word2Vec
&gt;&gt;&gt; sentences = brown.sents()[:100]
&gt;&gt;&gt; model = Word2Vec(sentences, size=10, window=5, min_count=5, workers=4)
&gt;&gt;&gt; model[sentences[0][0]]
array([ 0.04913874,  0.04574081, -0.07402877, -0.03270053,  0.06598952,
        0.04157289,  0.05075986,  0.01770534, -0.03796235,  0.04594197], dtype=float32)
&gt;&gt;&gt; model = Word2Vec(sentences, size=10, window=5, min_count=5, workers=4)
&gt;&gt;&gt; model[sentences[0][0]]
array([ 0.04907205,  0.04569579, -0.07379777, -0.03273782,  0.06579078,
        0.04167712,  0.05083019,  0.01780009, -0.0378389 ,  0.04578455], dtype=float32)
&gt;&gt;&gt; model = Word2Vec(sentences, size=10, window=5, min_count=5, workers=4)
&gt;&gt;&gt; model[sentences[0][0]]
array([ 0.04906179,  0.04569826, -0.07382379, -0.03274316,  0.06583244,
        0.04166647,  0.0508585 ,  0.01777468, -0.03784611,  0.04578935], dtype=float32)


I have also tried to set seed to some fixed int but this didnt seem to help. i also tried to reinstall gensim which also didnt help.

Any idea how to stabilize my model??
","Try to set PYTHONHASHSEED environment variable  as stated here
https://github.com/gojomo/gensim/blob/develop/gensim/models/doc2vec.py#L566
",,,false,,,
https://stackoverflow.com/questions/62543491,false,"The issue does not meet the criteria for deeper analysis as it is related to inconsistent results when training a gensim model with gensim.downloader versus manual loading, rather than an API exhibiting unexpected failures or unpredictable behaviors.",,,,,,,Inconsistent results when training gensim model with gensim.downloader vs manual loading,"I am trying to understand what is going wrong in the following example.
To train on the 'text8' dataset as described in the docs, one only has to do the following:
import gensim.downloader as api
from gensim.models import Word2Vec

dataset = api.load('text8')
model = Word2Vec(dataset)

doing this gives very good embedding vectors, as verified by evaluating on a word-similarity task.
However, when loading the same textfile which is used above manually, as in
text_path = '~/gensim-data/text8/text'
text = []
with open(text_path) as file:
    for line in file:
        text.extend(line.split())
text = [text]

model = Word2Vec(test)

The model still says it's training for the same number of epochs as above (5), but training is much faster, and the resulting vectors have a very, very bad performance on the similarity task.
What is happening here? I suppose it could have to do with the number of 'sentences', but the text8 file seems to have only a single line, so does gensim.downloader split the text8 file into sentences? If yes, of which length?
","In your second example, you've created a training dataset with just a single text with the entire contents of the file. That's about 1.1 million word tokens, in a single list.
Word2Vec (&amp; other related algorithms) in gensim have an internal implementation limitation, in their optimized paths, of 10,000 tokens per text item. All additional tokens are ignored.
So, in your 2nd case, 99% of your data is being discarded. Training may seem instant, but very little actual training will have occurred. (Word-vectors for words that only appear past the 1st 10,000 tokens won't have been trained at all, having only their initial randomly-set values.) If you enable logging at the INFO level, you'll see more details about each step of the process, and discrepancies like this may be easier to identify.
Yes, the api.load() variant takes extra steps to break the single-line-file into 10,000-token chunks. I believe it's using the LineSentence utility class for this purpose, whose source can be examined here:
https://github.com/RaRe-Technologies/gensim/blob/e859c11f6f57bf3c883a718a9ab7067ac0c2d4cf/gensim/models/word2vec.py#L1209
However, I recommend avoiding the api.load() functionality entirely. It doesn't just download data; it also downloads a shim of additional outside-of-version-control Python code for prepping that data for extra operations. Such code is harder to browse &amp; less well-reviewed than official gensim release code as packaged for PyPI/etc, which also presents a security risk. Each load target (by name like 'text8') might do something different, leaving you with a different object type as the return value.
It's much better for understanding to directly download precisely the data files you need, to known local paths, and do the IO/prep yourself, from those paths, so you know what steps have been applied, and the only code you're running is the officially versioned &amp; released code.
",,,false,,,
https://stackoverflow.com/questions/69786001,false,"The issue does not meet the criteria for deeper analysis as it is related to handling token ids between a pre-trained gensim word2vec model and a tokenizer, rather than an API exhibiting unexpected failures or unpredictable behaviors.",,,,,,,How to know that the token ids in a gensim pre-trained word2vec will match the ids of a tokenizer&#39;s vocabulary,"I am building a pytorch BiLSTM that utilizes pre-trained gensim word2vec. I first used a nn.Embedding layer that was trained with the model from scratch but, i decided to use a pre-trained word2vec embeddings to improve accuracy.
My model architecture follows a simple BiLSTM architecture, where the first layer is the embedding layer followed by a BiLSTM layer(s), and lastly two feed forward layers.
import torch
import gensim

import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F

word2vec = gensim.models.Word2Vec.load('path_to_word2vec/wikipedia_cbow_100')
weights = torch.FloatTensor(word2vec.wv.vectors)

class BiLSTM_model(torch.nn.Module) :
    def __init__(self, max_features, embedding_dim, hidden_dim, num_layers, lstm_dropout) :
        # max_features is the vocabulary size (num of tokens/words).
        super().__init__()
        # self.embeddings = nn.Embedding(max_features, embedding_dim, padding_idx=0)
        self.embeddings = nn.Embedding.from_pretrained(weights)
        self.lstm = nn.LSTM(word2vec.wv.vector_size,
                            hidden_dim,
                            batch_first=True,
                            bidirectional=True,
                            num_layers = num_layers,
                            dropout=lstm_dropout)
        self.relu=nn.ReLU()
        self.fc1 = nn.Linear(hidden_dim * 2, 64)
        self.dropout = nn.Dropout(0.2)
        self.fc2 = nn.Linear(64, config['num_classes'])

    def forward(self, input):
        embeddings_out = self.embeddings(input)
        lstm_out, (hidden, cell) = self.lstm(embeddings_out)
        hidden = torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1)
        rel = self.relu(hidden)
        dense1 = self.fc1(rel)
        drop = self.dropout(dense1)
        final_out = self.fc2(drop)

        return final_out

i use a keras tokenizer to tokenize the text and obtain the token ids.
from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences

## Tokenize the sentences
tokenizer = Tokenizer(num_words=config['max_features'])
tokenizer.fit_on_texts(list(train_X))
train_X = tokenizer.texts_to_sequences(train_X)
test_X = tokenizer.texts_to_sequences(test_X)

finally i use a standard training loop with an optimizer and a loss function. The code runs fine but there are no performance gains from using the pre-trained embeddings.
I suspect that it has to do with token ids not matching between the keras.preprocessing.text tokenizer and the gensim pre-trained embeddings for the words. My question is, how do i confirm (or deny) this inconsistency and ,if it is the case, how do i handle the issue?
Note: i am using a custom word2vec embeddings for the Arabic language. You can find the embeddings here.
","After looking into jhso's comment. It seems that the solution for this problem is to use word2vec.wv.index2word which will return the vocabulary (words) as a list sorted in an order which reflects a word's embedding.
for example, the following code:
pretrained_embedding = gensim.models.Word2Vec.load('path/to/embedding')
word_vectors= pretrained_embedding.wv
for i in range (0,3):
  print(f""{i}: '{word_vectors.index2word[i]}'"")

will print:
0: 'this'
1: 'is'
2: 'an'
3: 'example'

where this token will have the id 0 and so on.
You then use word2vec.wv.index2word as input to the keras.preprocessing.text.Tokenizer object's .fit_on_texts() method as following:
vocabulary = pretrained_embeddings.index2word
tokenizer = Tokenizer(num_words=config['max_features'])
tokenizer.fit_on_texts(vocabulary)

this should preserve the token ids between the gensim word2vec model and the keras tokenizer.
",,,false,,,
https://stackoverflow.com/questions/39582787,false,"The issue does not meet the criteria for deeper analysis as it is related to extracting links from a website using BeautifulSoup, rather than an API exhibiting unexpected failures or unpredictable behaviors.",,,,,,,Unable to find all links with BeautifulSoup to extract links from a website (Link identification),"I’m using this code found here ( retrieve links from web page using python and BeautifulSoup) to extract all links from a website using.

import httplib2
from BeautifulSoup import BeautifulSoup, SoupStrainer

http = httplib2.Http()
status, response = http.request('http://www.bestwestern.com.au')

for link in BeautifulSoup(response, parseOnlyThese=SoupStrainer('a')):
    if link.has_attr('href'):
        print link['href']


I’m using this site http://www.bestwestern.com.au  as test.
Unfortunately, I notice that the code is not extracting some links for example this one http://www.bestwestern.com.au/about-us/careers/ . I don’t know why. 
In the code of the page this is what I found. 

&lt;li&gt;&lt;a href=""http://www.bestwestern.com.au/about-us/careers/""&gt;Careers&lt;/a&gt;&lt;/li&gt;


I think the extractor should normally identify it. 
On the BeautifulSoup documentation I can read:  “The most common type of unexpected behavior is that you can’t find a tag that you know is in the document. You saw it going in, but find_all() returns [] or find() returns None. This is another common problem with Python’s built-in HTML parser, which sometimes skips tags it doesn’t understand. Again, the solution is to install lxml or html5lib.” 
So I installed html5lib. But I still have the same behavior. 

Thank you for your help
","Ok so this is a old question but I stumbled upon it in my search and it seems like it should be relatively simple to accomplish. I did switch from httplib2 to requests.

import requests
from bs4 import BeautifulSoup, SoupStrainer
baseurl = 'http://www.bestwestern.com.au'

SEEN_URLS = []
def get_links(url):
    response = requests.get(url)
    for link in BeautifulSoup(response.content, 'html.parser', parse_only=SoupStrainer('a', href=True)):
        print(link['href'])
        SEEN_URLS.append(link['href'])
        if baseurl in link['href'] and link['href'] not in SEEN_URLS:
            get_links(link['href'])

if __name__ == '__main__':
    get_links(baseurl)

","One problem is - you are using BeautifulSoup version 3 which is not being maintained anymore. You need to upgrade to BeautifulSoup version 4:

pip install beautifulsoup4


Another problem is that there is no ""careers"" link on the main page, but there is one on the ""sitemap"" page - request it and parse with the default html.parser  parser - you'll see ""careers"" link printed among others:

import requests
from bs4 import BeautifulSoup, SoupStrainer

response = requests.get('http://www.bestwestern.com.au/sitemap/')

for link in BeautifulSoup(response.content, ""html.parser"", parse_only=SoupStrainer('a', href=True)):
    print(link['href'])


Note how I've moved the ""has to have href"" rule to the soup strainer.
",,false,,,
https://stackoverflow.com/questions/38246620,false,"The issue does not meet the criteria for deeper analysis as it is related to an XML parser removing a processing instruction close delimiter's question mark, rather than an API exhibiting unexpected failures or unpredictable behaviors.",,,,,,,XML parser removes processing instruction close delimiter&#39;s question mark,"I've encountered the following unexpected behaviour with the xml parser used by Python 3.4's BeautifulSoup 4. When parsing an xml stylesheet, the question mark in the closing delimiter disappears:

Input:

BeautifulSoup('&lt;?xml version=""1.0"" encoding=""UTF-8""?&gt;&lt;?xml-stylesheet href=""myStyleSheet.xsl"" type=""text/xsl""?&gt;','xml')


Output:

&lt;?xml version=""1.0"" encoding=""utf-8""?&gt;
&lt;?xml-stylesheet href=""myStyleSheet.xsl"" type=""text/xsl""&gt;


Notice how type=""text/xsl""?&gt; changed to type=""text/xsl""&gt;.

Also, the example found here breaks in the same way.

Is this intentional? Should I report a bug? Is there any way to prevent this other than inserting a question mark in the correct place afterwards?
","From the changelog for version 4.5.0 of BeautifulSoup released 7/19/2016:

""* Corrected handling of XML processing instructions. [bug=1504393]""
",,,false,,,
https://stackoverflow.com/questions/50560526,true,"The issue involves extracting a subclass from a class using BeautifulSoup, which exhibits unexpected behavior. Further analysis is required.",,,,,,,Extract subclass from class using beautifulsoup,"I'm working with the following HTML snippet from a page on Goodreads using Python 3.6.3:

&lt;div class=""quoteText""&gt;
      “Don't cry because it's over, smile because it happened.”
  &lt;br/&gt;  ―
    &lt;a class=""authorOrTitle"" href=""/author/show/61105.Dr_Seuss""&gt;Dr. Seuss&lt;/a&gt;
&lt;/div&gt;, &lt;div class=""quoteText""&gt;


I used BeautifulSoup to scrape the HTML and isolate just the ""quoteText"" class seen in the snippet above. Now, I want to save the quote and author name as separate strings. I was able to get the author name using

(quote_tag.find(class_=""quoteText"")).text


I'm not sure how to do the same for the quote. I'm guessing I need a way to remove the  subclass from my output and tried using the extract method.

quote.extract(class_=""authorOrTitle"")


but I got an error saying extract got an unexpected keyword argument 'class_'
Is there any other way to do what I'm trying to do?

This is my first time posting on here so I apologize if the post doesn't meet particular specificity/formatting/other standards.
","
  PageElement.extract() removes a tag or string from the tree. It
  returns the tag or string that was extracted


from bs4 import BeautifulSoup
a='''&lt;div class=""quoteText""&gt;
      “Don't cry because it's over, smile because it happened.”
  &lt;br/&gt;  -
    &lt;a class=""authorOrTitle"" href=""/author/show/61105.Dr_Seuss""&gt;Dr. Seuss&lt;/a&gt;
&lt;/div&gt;, &lt;div class=""quoteText""&gt;'''
s=BeautifulSoup(a,'lxml')
s.find(class_=""authorOrTitle"").extract()
print(s.text)

",,,false,,,
https://stackoverflow.com/questions/20205455,true,"The issue stems from attempting to parse UTF-8 encoded HTML to Unicode strings using BeautifulSoup. The unexpected encoding of the extracted text is caused by invalid UTF-8 characters in the HTML content. This issue is related to the handling of encoding and decoding, rather than a specific API behavior.",BeautifulSoup,N/A,"When extracting text from UTF-8 encoded HTML using BeautifulSoup, the resulting text is written in an unexpected encoding due to the presence of invalid UTF-8 characters in the HTML content.",The BeautifulSoup library is used to parse HTML and extract text from specific elements.,"The issue occurs when the HTML content contains invalid UTF-8 characters, causing the extracted text to be written in an unexpected encoding.",This issue might be challenging to detect for users who are not familiar with the specific encoding requirements of BeautifulSoup or those who assume that the extracted text will match the original encoding of the HTML content.,How to correctly parse UTF-8 encoded HTML to Unicode strings with BeautifulSoup?,"I'm running a Python program which fetches a UTF-8-encoded web page, and I extract some text from the HTML using BeautifulSoup.

However, when I write this text to a file (or print it on the console), it gets written in an unexpected encoding.

Sample program:

import urllib2
from BeautifulSoup import BeautifulSoup

# Fetch URL
url = 'http://www.voxnow.de/'
request = urllib2.Request(url)
request.add_header('Accept-Encoding', 'utf-8')

# Response has UTF-8 charset header,
# and HTML body which is UTF-8 encoded
response = urllib2.urlopen(request)

# Parse with BeautifulSoup
soup = BeautifulSoup(response)

# Print title attribute of a &lt;div&gt; which uses umlauts (e.g. können)
print repr(soup.find('div', id='navbutton_account')['title'])


Running this gives the result:

# u'Hier k\u0102\u015bnnen Sie sich kostenlos registrieren und / oder einloggen!'


But I would expect a Python Unicode string to render ö in the word können as \xf6:

# u'Hier k\xf6bnnen Sie sich kostenlos registrieren und / oder einloggen!'


I've tried passing the 'fromEncoding' parameter to BeautifulSoup, and trying to read() and decode() the response object, but it either makes no difference, or throws an error.

With the command curl www.voxnow.de | hexdump -C, I can see that the web page is indeed UTF-8 encoded (i.e. it contains 0xc3 0xb6) for the ö character:

      20 74 69 74 6c 65 3d 22  48 69 65 72 20 6b c3 b6  | title=""Hier k..|
      6e 6e 65 6e 20 53 69 65  20 73 69 63 68 20 6b 6f  |nnen Sie sich ko|
      73 74 65 6e 6c 6f 73 20  72 65 67 69 73 74 72 69  |stenlos registri|


I'm beyond the limit of my Python abilities, so I'm at a loss as to how to debug this further. Any advice?
","As justhalf points out above, my question here is essentially a duplicate of this question.

The HTML content reported itself as UTF-8 encoded and, for the most part it was, except for one or two rogue invalid UTF-8 characters.

This apparently confuses BeautifulSoup about which encoding is in use, and when trying to first decode as UTF-8 when passing the content to BeautifulSoup like 
this:

soup = BeautifulSoup(response.read().decode('utf-8'))


I would get the error:

UnicodeDecodeError: 'utf8' codec can't decode bytes in position 186812-186813: 
                    invalid continuation byte


Looking more closely at the output, there was an instance of the character Ü which was wrongly encoded as the invalid byte sequence 0xe3 0x9c, rather than the correct 0xc3 0x9c.

As the currently highest-rated answer on that question suggests, the invalid UTF-8 characters can be removed while parsing, so that only valid data is passed to BeautifulSoup:

soup = BeautifulSoup(response.read().decode('utf-8', 'ignore'))

","Encoding the result to utf-8 seems to work for me:

print (soup.find('div', id='navbutton_account')['title']).encode('utf-8')


It yields:

Hier kÃ¶nnen Sie sich kostenlos registrieren und / oder einloggen!

",,false,,,
https://stackoverflow.com/questions/59373948,true,"The issue involves scraping JSON served by a script and converting it to a dataframe. The unexpected behavior is caused by an error in parsing the JSON data using the pd.read_json() function, which results in a ValueError. This issue is related to the handling of JSON parsing and dataframe conversion, rather than a specific API behavior.",pandas,pd.read_json,"When trying to convert JSON data obtained from a script to a dataframe using pd.read_json, a ValueError is raised due to an unexpected character found when decoding the object value.",The pd.read_json function is used to convert JSON data to a dataframe.,The issue is triggered when the JSON data contains an unexpected character that causes a decoding error during the conversion process.,This issue might be challenging to detect for users who are not familiar with the specific requirements and limitations of the pd.read_json function or those who assume that it can handle any valid JSON data without issues.,Scraping JSON served by script and converting to dataframe,"I am trying to obtain the details from this job post (script json to df) but am unable to make progress. 

import requests 
from bs4 import BeautifulSoup 
import pandas as pd
from pandas.io.json import json_normalize
import time
import re
import json
from pandas.compat import StringIO


URLS=['https://www.iimjobs.com/j/specialist-manager-operations-student-services-elearning-education-management-organization-4-6-yrs-774235.html?ref=cl','https://www.iimjobs.com/j/specialist-manager-operations-student-services-elearning-education-management-organization-4-6-yrs-774235.html?ref=cl']


i=0
for URL in URLS:
#     time.sleep(5)   
    r = requests.get(URL) 
    soup = BeautifulSoup(r.content, 'html5lib') 
    # print(soup.prettify()) 
    table=soup.find(""script"" , type='application/ld+json').text
    data = json.loads(json.dumps(table))
    if i == 0:
        df = pd.read_json(data)  
    if i != 0:
        dfnew=pd.read_json(data)
        df=df.append(dfnew)     
    i=i+1
df.to_csv('jobs.csv', index=False)
print(df)


Can someone help me with this please? the detailed error is as follows:

ValueError                                Traceback (most recent call last)
&lt;ipython-input-9-5f9199aa28fd&gt; in &lt;module&gt;
     30     data = json.loads(json.dumps(table))
     31     if i == 0:
---&gt; 32         df = pd.read_json(data)
     33     if i != 0:
     34         print(URL)

C:\ProgramData\Anaconda3\lib\site-packages\pandas\io\json\json.py in read_json(path_or_buf, orient, typ, dtype, convert_axes, convert_dates, keep_default_dates, numpy, precise_float, date_unit, encoding, lines, chunksize, compression)
    425         return json_reader
    426 
--&gt; 427     result = json_reader.read()
    428     if should_close:
    429         try:

C:\ProgramData\Anaconda3\lib\site-packages\pandas\io\json\json.py in read(self)
    535             )
    536         else:
--&gt; 537             obj = self._get_object_parser(self.data)
    538         self.close()
    539         return obj

C:\ProgramData\Anaconda3\lib\site-packages\pandas\io\json\json.py in _get_object_parser(self, json)
    554         obj = None
    555         if typ == 'frame':
--&gt; 556             obj = FrameParser(json, **kwargs).parse()
    557 
    558         if typ == 'series' or obj is None:

C:\ProgramData\Anaconda3\lib\site-packages\pandas\io\json\json.py in parse(self)
    650 
    651         else:
--&gt; 652             self._parse_no_numpy()
    653 
    654         if self.obj is None:

C:\ProgramData\Anaconda3\lib\site-packages\pandas\io\json\json.py in _parse_no_numpy(self)
    869         if orient == ""columns"":
    870             self.obj = DataFrame(
--&gt; 871                 loads(json, precise_float=self.precise_float), dtype=None)
    872         elif orient == ""split"":
    873             decoded = {str(k): v for k, v in compat.iteritems(

ValueError: Unexpected character found when decoding object value


I am essentially trying to scrape job details like title, description, skills, jobLocation etc.However the JSON served for this particular URL seems to fail and I have not been able to figure this one out yet
","The issue is that some of the values in the script json contains double quotes for html (ie: class=""""). So it's treating that as the end and start of a new string, and does not have a comma, or a key:value that is valid.

So if you take care of that, it should work:

import requests 
from bs4 import BeautifulSoup 
import pandas as pd
from pandas.io.json import json_normalize
import time
import re
import json
from pandas.compat import StringIO


URLS=['https://www.iimjobs.com/j/specialist-manager-operations-student-services-elearning-education-management-organization-4-6-yrs-774235.html?ref=cl','https://www.iimjobs.com/j/specialist-manager-operations-student-services-elearning-education-management-organization-4-6-yrs-774235.html?ref=cl']


i=0
for URL in URLS:
#     time.sleep(5)   
    r = requests.get(URL) 
    soup = BeautifulSoup(r.content, 'html5lib') 
    # print(soup.prettify()) 
    table=soup.find(""script"" , type='application/ld+json').text
    data = json.loads(json.dumps(table))
    if i == 0:
        try:
            df = pd.read_json(data)  
        except:
            data = data.replace('=""""', '=')
            df = pd.read_json(data)  
    if i != 0:
        try:
            dfnew=pd.read_json(data)
            df=df.append(dfnew) 

        except:
            data = data.replace('=""""', '=')
            dfnew=pd.read_json(data)
            df=df.append(dfnew) 

    i=i+1
df.to_csv('jobs.csv', index=False)
print(df)

",,,false,,,
https://stackoverflow.com/questions/1697774,false,The issue does not involve unexpected failures or unpredictable behaviors of an API. It is related to using regular expressions to match tags in BeautifulSoup and does not meet the criteria for deeper analysis.,,,,,,,Matching tags in BeautifulSoup,"I'm trying to count the number of tags in the 'soup' from a beautifulsoup result. I'd like to use a regular expression but am having trouble.
The code Ive tried is as follows:

reg_exp_tag = re.compile(""&lt;[^&gt;*&gt;"")
tags = re.findall(reg_exp_tag, soup(cast as a string))


but re will not allow reg_exp_tag, giving an unexpected end of regular expression error.

Any help would be much appreciated!

Thanks
","If you've already parsed the HTML with BeautifulSoup, why parse it again?  Try this:

num_tags = len(soup.findAll())

","Shouldn't that be ""&lt;[^&gt;]*&gt;"" instead of ""&lt;[^&gt;*&gt;""?

(the class needs to be closed with a ])
",,false,,,
https://stackoverflow.com/questions/71253524,false,The issue does not involve unexpected failures or unpredictable behaviors of an API. It is related to counting the number of tags in BeautifulSoup and does not meet the criteria for deeper analysis.,,,,,,,HTML parsing not working as expected using BeautifulSoup,"I'm using Python 3 and the BeautifulSoup module, version 4.9.3. I'm trying to use this package to practice parsing some simple HTML.
The string I have is the following:
text = '''&lt;li&gt;&lt;p&gt;Some text&lt;/p&gt;is put here&lt;/li&gt;&lt;li&gt;&lt;p&gt;And other text is put here&lt;/p&gt;&lt;/li&gt;'''
I use BeautifulSoup as follows:
x = BeautifulSoup(text, ""html.parser"")
I then experiment with Beautiful Soup's functionality with the following script:
for li in x.find_all('li'):
    print(li)
    print(li.string)
    print(li.next_element)
    print(li.next_element)
    print(li.next_element.string)
    print(""\n"")

The results (at least for the first iteration) are unexpected:
&lt;li&gt;&lt;p&gt;Some text&lt;/p&gt;is put here&lt;/li&gt;
None
&lt;p&gt;Some text&lt;/p&gt;
Some text


&lt;li&gt;&lt;p&gt;And other text is here&lt;/p&gt;&lt;/li&gt;
And other text is here
&lt;p&gt;And other text is here&lt;/p&gt;
And other text is here

Why is the string attribute of the first li tag None, whereas the string attribute of the inner p tag is not None?
Similarly, if I do:
x.find_all('li', string=re.compile('text'))
I only get one result (the 2nd tag).
But if I do:
for li in x.find_all('li'):
    print(li.find_all(string=re.compile('text')))

I get 2 results (both tags).
","Paraphrasing the doc:


If a tag has only one child, and that child is a NavigableString, the child is made available as .string.
If a tag’s only child is another tag, and that tag has a .string, then the parent tag is considered to have the same .string as its child.
If a tag contains more than one thing, then it’s not clear what .string should refer to, so .string is defined to be None.


Let's apply these rules to your question:

Why is the string attribute of the first li tag None, whereas the string attribute of the inner p tag is not None?

The inner p tag satisfies rule #1; it has exactly one child, and that child is a NavigableString, so .string returns that child.
The first li satisfies rule #3; it has more than one child, so .string would be ambiguous.

Considering your second question, let's consult the doc for the string= argument to .find_all()

With string you can search for strings instead of tags. ... Although string is for finding strings, you can combine it with arguments that find tags: Beautiful Soup will find all tags whose .string matches your value for string.

Your first example:
x.find_all('li', string=re.compile('text'))
# [&lt;li&gt;&lt;p&gt;And other text is put here&lt;/p&gt;&lt;/li&gt;]

That searches for all of the li tags whose .string matches the regular expression. But we have already seen that the first li's .string is None, so it doesn't match.
Your second example:
for li in x.find_all('li'):
    print(li.find_all(string=re.compile('text')))
# ['Some text']
# ['And other text is put here']

This searches for all of the strings contained anywhere in each of the li trees. For the first tree, li.p.string exists and matches, even if li.string doesn't.
",,,false,,,
https://stackoverflow.com/questions/50508759,true,The issue involves finding all links in HTML parsed by BeautifulSoup. The unexpected results are caused by using a regular expression pattern that matches strings that are not valid URLs. This issue is related to the usage of regular expressions and does not indicate a problem with BeautifulSoup's API behavior.,BeautifulSoup,N/A,"The regular expression pattern used to find links in the HTML matches strings that are not valid URLs, resulting in unexpected results.",BeautifulSoup is used to parse HTML and provides methods to extract specific elements or attributes.,"The issue occurs when using a regular expression pattern that does not accurately match valid URLs, leading to unexpected results.",This issue might be challenging to detect for users who are not familiar with regular expressions or those who assume that a general regular expression pattern can accurately match all valid URLs.,find all links in html parsed beautiful soup,"I am using beautifulsoup with python. In scrapping pages links are not enclosed in &lt;a href&gt; tags. 

I want to get all links starting with http/https using soup operation. I have tried some regex given here but they are giving unexpected results for me. 
so i thought if anything is possible using soup?

Example responses from which i want to get links:

&lt;html&gt;\n&lt;head&gt;\n&lt;/head&gt;\n&lt;link href=""https://fonts.googleapis.com/css?family=Open+Sans:600"" rel=""stylesheet""/&gt;\n&lt;style&gt;\n    html, body {\n    height: 100%;\n    width: 100%;\n    }\n\n    body {\n    background: #F5F6F8;\n    font-size: 16px;\n    font-family: \'Open Sans\', sans-serif;\n    color: #2C3E51;\n    }\n    .main {\n    display: flex;\n    align-items: center;\n    justify-content: center;\n    height: 100vh;\n    }\n    .main &gt; div &gt; div,\n    .main &gt; div &gt; span {\n    text-align: center;\n    }\n    .main span {\n    display: block;\n    padding: 80px 0 170px;\n    font-size: 3rem;\n    }\n    .main .app img {\n    width: 400px;\n    }\n  &lt;/style&gt;\n&lt;script type=""text/javascript""&gt;\n      var fallback_url = ""null"";\n      var store_link = ""itms-apps://itunes.apple.com/GB/app/id1032680895?ls=1&amp;mt=8"";\n      var web_store_link = ""https://itunes.apple.com/GB/app/id1032680895?mt=8"";\n      var loc = window.location;\n      function redirect_to_web_store(loc) {\n        loc.href = web_store_link;\n      }\n      function redirect(loc) {\n        loc.href = store_link;\n        if (fallback_url.startsWith(""http"")) {\n          setTimeout(function() {\n            loc.href = fallback_url;\n          },5000);\n        }\n      }\n  &lt;/script&gt;\n&lt;body onload=""redirect(loc)""&gt;\n&lt;div class=""main""&gt;\n&lt;div class=""workarea""&gt;\n&lt;div class=""logo""&gt;\n&lt;img onclick=""redirect_to_web_store(loc)"" src=""https://cdnappicons.appsflyer.com/app|id1032680895.png"" style=""width:200px;height:200px;border-radius:20px;""/&gt;\n&lt;/div&gt;\n&lt;span&gt;BetBull: Sports Betting &amp;amp; Tips&lt;/span&gt;\n&lt;div class=""app""&gt;\n&lt;img onclick=""redirect_to_web_store(loc)"" src=""https://cdn.appsflyer.com/af-statics/images/rta/app_store_badge.png""/&gt;\n&lt;/div&gt;\n&lt;/div&gt;\n&lt;/div&gt;\n&lt;/body&gt;\n&lt;/html&gt;


Tried:

regex_pattern_to_find_all_links = r'(?:(?:https?|ftp):\/\/)?[\w/\-?=%.]+\.[\w/\-?=%.]+'
soup = BeautifulSoup(resp.read(), 'html.parser')
urls = re.findall(regex_pattern_to_find_all_links, str(soup))


Result:

['https://fonts.googleapis.com/css?family=Open', '//itunes.apple.com/GB/app/id1032680895?ls=1', 'https://itunes.apple.com/GB/app/id1032680895?mt=8', 'window.location', 'loc.href', 'loc.href', 'fallback_url.startsWith', 'loc.href', 'https://cdnappicons.appsflyer.com/app', 'id1032680895.png', 'https://cdn.appsflyer.com/af-statics/images/rta/app_store_badge.png']


As you can see above, I am not sure why regex is matching things which are not even urls.

What I have tried.
Most upvoted and accepted answer here is not able to detect links at all!!
I am not sure what i am doing wrong,
","The problem is with protocol that you made optional and engine isn't forced to match it if it is satisfied with the rest of patterns. Try this instead:

(?:(?:https?|ftp):\/\/|\bwww\.)[^\s""']+


Not bulletproof but much better. It matches strings starting with https? or ftp or those with no protocols but www.

See live demo here
",,,false,,,
https://stackoverflow.com/questions/34370455,false,The issue does not involve unexpected failures or unpredictable behaviors of an API. It is related to extracting the main body of HTML using BeautifulSoup and does not meet the criteria for deeper analysis.,,,,,,,BeautifulSoup for extracting text,"I'm not familiar to HTML and trying to extract the main body of a HTML. Firstly I have to filter all elements of HTML but remain text merely.
I receive some unexpected results as below when using method get_text() of BeautifulSoup.

var suffix = device.type === ""pc"" ? "".pc"" : "".mobile"";requirejs.config({
paths: {
    ""F"": ""http://y0.ifengimg.com/base/origin/F-amd-1.2.0.min"",
    ""FM"":  ""http://y0.ifengimg.com/commonpage/1130/F-amd-mobile-1.1.0.min"",
    ""debug"": ""http://y0.ifengimg.com/commonpage/1130/F-amd-mobile-1.1.0.min"",


Of course the text is included, but I don't want the function or other elements of HTML. After checking the code of HTML, it seems that these kinds of functions or scripts are between 2 elements &lt;script&gt; and &lt;/script&gt;

I wonder whether I should use re module or BeautifulSoup can deal with my problem.

Have already done via method extract()...
But received another error. That looks like...
 &lt;img src***=""1""/&gt; 

Still it remains in soup.get_text(). Don't know why it, as a tag, isn't extracted. Surely I can remove it manually, but that seems not elegant for a programmer.
","Hmm...looks like we can simply extract them (remove them from the BeautifulSoup Object, your HTML file):

&gt;&gt;&gt; soup = BeautifulSoup('&lt;p&gt;Hello&lt;/p&gt;&lt;script&gt;console.log(""A test!"")&lt;/script&gt;')
&gt;&gt;&gt; soup.get_text()
'Helloconsole.log(""A test!"")'

&gt;&gt;&gt; soup
&lt;p&gt;Hello&lt;/p&gt;&lt;script&gt;console.log(""A test!"")&lt;/script&gt;

&gt;&gt;&gt; soup.find('script')
&lt;script&gt;console.log(""A test!"")&lt;/script&gt;

&gt;&gt;&gt; soup.find('script').extract()
&lt;script&gt;console.log(""A test!"")&lt;/script&gt;

&gt;&gt;&gt; soup
&lt;p&gt;Hello&lt;/p&gt;

&gt;&gt;&gt; soup.get_text()
'Hello'
&gt;&gt;&gt; 


However if you have more script tags in your HTML file, use soup.find_all() instead like this:

for tag in soup.find_all('script'):
    tag.extract()

print(soup.get_text())

",,,false,,,
https://stackoverflow.com/questions/68641383,true,The issue involves the behavior of the Selenium and BeautifulSoup APIs when used together. The user is facing two major issues: unable to launch Chrome in headless mode and encountering an error related to the 'html.parser'. Both issues are API-related and meet the criteria for deeper analysis.,Selenium and BeautifulSoup,webdriver.Chrome and BeautifulSoup,The issue with launching Chrome in headless mode is likely due to incorrect usage of the 'options' parameter in the 'webdriver.Chrome' function. The issue with the 'html.parser' error is caused by passing a string ('html.parser') instead of the actual parser object to the BeautifulSoup constructor.,"In normal conditions, Chrome can be launched in headless mode using the 'options.headless = True' setting. The 'html.parser' can be used as the parser for BeautifulSoup by passing the parser object instead of a string.",The issue with launching Chrome in headless mode is triggered by incorrect usage of the 'options' parameter. The 'html.parser' error is triggered by passing a string instead of a parser object to the BeautifulSoup constructor.,These issues might be challenging to detect for users who are not familiar with the specific requirements and usage of the Selenium and BeautifulSoup APIs.,Headless chrome and html parser string,"I'm currently using selenium and BeautifulSoup to scrape a website but I'm running into two major issues, first of all, I can't get Chrome to launch in headless mode and it says there are multiple unexpected ends of inputs (photo of said errors). The other problem I have is that I keep getting an error on the line that contains ""html.parser"" saying that a 'str' is not a callable object. Any advice on these issues would be greatly appreciated thank you.
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
import urllib.request
import lxml
import html5lib
import time
from bs4 import BeautifulSoup

#config options
options = Options()
options.headless = True

# Set the URL you want to webscrape from
url = 'https://tokcount.com/?user=mrsam993'

# Connect to the URL
browser = webdriver.Chrome(options=options, executable_path='D:\chromedriver') #chrome_options=options
browser.get(url)

# Parse HTML and save to BeautifulSoup object
soup = BeautifulSoup(browser.page_source(), ""html.parser"")
browser.quit()

# for i in range(10):
links = soup.findAll('span', class_= 'odometer-value')
print(links)

","In order to launch to launch chrome in headless mode,and parse the content to html with BeautifulSoup4, this is what you can do:
#Importing necessary packages
from selenium import webdriver 
from selenium.webdriver.chrome.service import Service as ChromeService 
from webdriver_manager.chrome import ChromeDriverManager 

url = 'https://tokcount.com/?user=mrsam993' 

options = webdriver.ChromeOptions()  
options.headless = True 

with webdriver.Chrome(service=ChromeService(ChromeDriverManager().install()), options=options) as driver: #modified 
    driver.get(url)
    
    print(""Page URL: "", driver.current_url)
    print(""Page title: "", driver.title)

    #Get the source page
    html = driver.page_source

ParsedContent = soup(html, 'html.parser')
ParsedContent
 

Make sure you have the following packages: Selenium, webdriver manager.
pip install selenium
pip install webdriver_manager

","As for the headless you need to call this way:
from selenium import webdriver

options = webdriver.ChromeOptions()
...

the page_source is not a method. So you need to remove the brackets:
browser.page_source

",,false,,,
https://stackoverflow.com/questions/32605694,true,The issue involves the behavior of the BeautifulSoup API when using the 'find_all' function after calling the 'extract' or 'decompose' methods on a tag. The issue is API-related and meets the criteria for deeper analysis.,BeautifulSoup,"find_all, extract, decompose","After calling the 'extract' or 'decompose' methods on a tag, the 'find_all' function does not correctly find all the requested elements. This is caused by a bug in BeautifulSoup 4.4.0, where the previous element's 'next_sibling' attribute is not adjusted correctly, resulting in missed results during subsequent 'find_all' calls.","In normal conditions, the 'find_all' function correctly finds all requested elements in the BeautifulSoup object.",The issue is triggered when the 'extract' or 'decompose' methods are called on a tag before calling the 'find_all' function.,This issue might be challenging to detect for users who are not familiar with the specific behavior of the 'extract' and 'decompose' methods and the bug in BeautifulSoup 4.4.0.,BeautifulSoup4 find_all() behaves strange after extract() or decompose(),"I have observed a behavior that I found strange when using BeautifulSoup4. 
I have the following XML (filename: fake_product.xml):

&lt;product acronym=""ACRO1""&gt;
&lt;formats&gt;
    &lt;format id=""format1""&gt;
    &lt;/format&gt;
    &lt;format id=""format2""&gt;
    &lt;/format&gt;
    &lt;format id=""format3""&gt;
    &lt;/format&gt;
    &lt;format id=""format4""&gt;
    &lt;/format&gt;
    &lt;format id=""format5""&gt;
    &lt;/format&gt;
    &lt;format id=""format6""&gt;
    &lt;/format&gt;
&lt;/formats&gt;
&lt;/product&gt;


This TestCase fails:

import unittest
from bs4 import BeautifulSoup


class Test(unittest.TestCase):

    def setUp(self):
        with open('fake_product.xml') as f:
            self.soup = BeautifulSoup(f, 'xml')

    def test_product_removal(self):
        output = len(self.soup.find_all('format'))
        expected = 6
        self.assertEqual(output, expected)

        format_to_delete = self.soup.find(id='format2')
        format_to_delete.extract()
        #self.soup = BeautifulSoup(self.soup.prettify(), 'xml')
        output = len(self.soup.find_all('format'))
        expected -= 1
        self.assertEqual(output, expected)


The reason is that the find_all() cannot find all the formats anymore. If I do e.g. print self.soup.prettify() everything looks fine to me.
If I uncomment the commented line in the TestCase and create a new BeautifulSoup object after the extract(), the find_all() seems to work fine again and the TestCase succeeds. 

Can somebody explain this behavior to me?
","This is a bug introduced in 4.4.0, see the BeautifulSoup 4 project bug tracker:


  In some situations, it seems calling extract() does not correctly adjust the next_sibling attribute of the previous element. This leaves the extracted element in the descendant generator. When later calling find(...) or find_all(...), the search then terminates at the extracted element, causing results to be missed.


This bug is related as well and contains a potential fix:


  Lines 265, 267, 274, 277 need != changing to is not
  
  Line 290 needs == changing to is


I can confirm that it fixes your specific test.

If you are not comfortable with editing your BeautifulSoup source code, then the work-around is to rebuild the tree as you did, or to downgrade to 4.3.2 until such time that a fix comes out.
",,,false,,,
https://stackoverflow.com/questions/13376417,true,The issue involves the behavior of the BeautifulSoup API when converting special characters from HTML. The issue is API-related and meets the criteria for deeper analysis.,BeautifulSoup,BeautifulSoup,"When converting HTML-encoded text to plain text using BeautifulSoup, the conversion of special characters results in incorrect characters on the production server. This is likely caused by differences in character encoding or handling of special characters between the test server and the production server.","In normal conditions, the conversion of HTML-encoded text to plain text using BeautifulSoup should result in the correct characters.",The issue is triggered when converting HTML-encoded text to plain text using BeautifulSoup on the production server.,This issue might be challenging to detect for users who are not familiar with the specific differences in character encoding or handling of special characters between different environments.,Strange behaviour with BeautifulSoup and converting HTML entities,"I have a strange problem with converting special characters from HTML. I have a Django project where text is stored HTML-encoded in a MySQL database. This is necessary, because I don't want to lose any formatting of the text.

In a preliminary step I must do operational things on the text like calculating positions, so I need to convert it first and clear it from all HTML-Tags. This is done by BeautifulSoup:

convertedText = str(BeautifulSoup(text.text, convertEntities=BeautifulSoup.HTML_ENTITIES))
convertedText = ''.join(BeautifulSoup(convertedText).findAll(text=True))


By working on my Django-default test-server everything works fine, but when I run it on my production server there are strange behaviors when converting special characters.

An example:

Test server

MySQL-Query gives me: &lt;p&gt;bassverst&amp;auml;rker&lt;/p&gt;

is correctly converted to: bassverstärker

Production server

MySQL-Query gives me: &lt;p&gt;bassverst&amp;auml;rker&lt;/p&gt;

This is is wrongly converted to: bassverst\ucc44rker

Somehow the &amp;auml; is converted into \ucc44 and this results in a wrong character.

My configuration:

Test server:


Django build-in solution (python manage.py runserver)
BeautifulSoup 3.2.1
Python 2.6.5
Ubuntu 2.6.32-43-generic


Production server:


Cherokee 1.2.101
BeautifulSoup 3.2.1
python 2.7.3
Ubuntu 3.2.0-32-generic


Because I don't know at which level the error occurs, I would like to ask if anybody can help me with this. Many thanks in advance.
","I found a way to fix this. I didn't know that BeautifulSoup has the builtin method getText(). When converting HTML through:

convertedText = BeautifulSoup(text.text, convertEntities=BeautifulSoup.HTML_ENTITIES).getText()


eveything works fine on both servers. Although this works, it would be interesting to know why both servers are behaving differently when working with the example in the question.

However, thanks to all.
",,,false,,,
https://stackoverflow.com/questions/49338402,true,The issue involves the behavior of the BeautifulSoup API when using the 'find_all' function with a string parameter. The issue is API-related and meets the criteria for deeper analysis.,BeautifulSoup,find_all,"The 'find_all' function does not find all the requested elements when using a string parameter. This is caused by the 'string' property of a tag only considering the text content and not tags within the tag. To overcome this, a lambda function can be used to check for the presence of the desired string in the tag's text.","In normal conditions, the 'find_all' function correctly finds all the requested elements when using a string parameter.",The issue is triggered when using the 'find_all' function with a string parameter that should match elements containing the desired string.,This issue might be challenging to detect for users who are not familiar with the specific behavior of the 'string' property and the need to use a lambda function for more complex matching.,BeautifulSoup find_all() Doesn&#39;t Find All Requested Elements,"I am seeing some strange behavior with BeautifulSoup as demonstrated in the example below.

import re
from bs4 import BeautifulSoup
html = """"""&lt;p style='color: red;'&gt;This has a &lt;b&gt;color&lt;/b&gt; of red. Because it likes the color red&lt;/p&gt;
&lt;p class='blue'&gt;This paragraph has a color of blue.&lt;/p&gt;
&lt;p&gt;This paragraph does not have a color.&lt;/p&gt;""""""
soup = BeautifulSoup(html, 'html.parser')
pattern = re.compile('color', flags=re.UNICODE+re.IGNORECASE)
paras = soup.find_all('p', string=pattern)
print(len(paras)) # expected to find 3 paragraphs with word ""color"" in it
  2
print(paras[0].prettify())
  &lt;p class=""blue""&gt;
    This paragraph as a color of blue.
  &lt;/p&gt;

print(paras[1].prettify())
  &lt;p&gt;
    This paragraph does not have a color.
  &lt;/p&gt;


As you can see for some reason the first paragraph of &lt;p style='color: red;'&gt;This has a &lt;b&gt;color&lt;/b&gt; of red. Because it likes the color red&lt;/p&gt; is not being picked up by find_all(...) and I cannot figure out why not.
","The string property expects the tag to contain only text and not tags. If you try printing .string for the first p tag, it'll return None, since, it has tags in it.
Or, to explain it better, the documentation says:

If a tag has only one child, and that child is a NavigableString, the child is made available as .string
If a tag contains more than one thing, then it’s not clear what .string should refer to, so .string is defined to be None

The way to overcome this, is to use a lambda function.
html = """"""&lt;p style='color: red;'&gt;This has a &lt;b&gt;color&lt;/b&gt; of red. Because it likes the color red&lt;/p&gt;
&lt;p class='blue'&gt;This paragraph has a color of blue.&lt;/p&gt;
&lt;p&gt;This paragraph does not have a color.&lt;/p&gt;""""""
soup = BeautifulSoup(html, 'html.parser')

first_p = soup.find('p')
print(first_p)
# &lt;p style=""color: red;""&gt;This has a &lt;b&gt;color&lt;/b&gt; of red. Because it likes the color red&lt;/p&gt;
print(first_p.string)
# None
print(first_p.text)
# This has a color of red. Because it likes the color red

paras = soup.find_all(lambda tag: tag.name == 'p' and 'color' in tag.text.lower())
print(paras)
# [&lt;p style=""color: red;""&gt;This has a &lt;b&gt;color&lt;/b&gt; of red. Because it likes the color red&lt;/p&gt;, &lt;p class=""blue""&gt;This paragraph has a color of blue.&lt;/p&gt;, &lt;p&gt;This paragraph does not have a color.&lt;/p&gt;]

","I haven't actually figured out why specifying the string (or text for older versions of BeautifulSoup) parameter of find_all(...) doesn't give me what I want but, the following does give me a generalized solution.

pattern = re.compile('color', flags=re.UNICODE+re.IGNORECASE)
desired_tags = [tag for tag in soup.find_all('p') if pattern.search(tag.text) is not None]

","If you want to grap the 'p' you can just do:

import re
from bs4 import BeautifulSoup
html = """"""&lt;p style='color: red;'&gt;This has a &lt;b&gt;color&lt;/b&gt; of red. Because it likes the color red&lt;/p&gt;
&lt;p class='blue'&gt;This paragraph has a color of blue.&lt;/p&gt;
&lt;p&gt;This paragraph does not have a color.&lt;/p&gt;""""""
soup = BeautifulSoup(html, 'html.parser')

paras = soup.find_all('p')
for p in paras:
  print (p.get_text())

",false,,,
https://stackoverflow.com/questions/17137870,true,The issue involves the behavior of the BeautifulSoup API when using the lxml parser. The issue is API-related and meets the criteria for deeper analysis.,BeautifulSoup,BeautifulSoup,"When using the BeautifulSoup API with the lxml parser, the output of the 'prettify' function is initially correct but then changes to an incomplete and incorrect output. This is likely caused by a bug in the lxml parser when handling HTML documents with a non-UTF-8 charset declaration.","In normal conditions, the 'prettify' function correctly formats the BeautifulSoup object's HTML output using the lxml parser.",The issue is triggered when using the lxml parser with BeautifulSoup on HTML documents that include a non-UTF-8 charset declaration.,This issue might be challenging to detect for users who are not familiar with the specific behavior of the lxml parser and the impact of the charset declaration on the output of the 'prettify' function.,Problems with BeautifulSoup and lxml parser,"I noticed a strange behavior when scraping some webpages using BeautifulSoup 4.1.0 and the lxml parser. The built-in html.parser didn't work for the webpage I was trying to scrape and I decided to use a lxml parser. 

The result of the print on my Eclipse console looks good for a fraction of a second and then, it automatically switches to an incomplete, useless and not-so-good-looking output with spaces between all the characters:

                           ! - -   S w i t c h   - - &amp;gt;                

                     / d i v &amp;gt; 


The doc-type of the page is:

&lt;!DOCTYPE html PUBLIC ""-//W3C//DTD XHTML 1.0 Transitional//EN"" ""http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd""&gt;


I was wondering if any of you guys encountered a similar problem and how to fix it. Thanks in advance.
","To help people with this problem I wrote ""Extra spaces in documents parsed by Beautiful Soup: the definitive guide"". Basically, the problem is caused by a bug in lxml's HTML parser, triggered by HTML documents that include a  tag that defines the charset as other than UTF-8.

Please let me know if the suggestions in that document don't work for you.
","I used:

newsoup = str(soup).replace("" "", """")


and managed to pull out the info I needed with lxml. Using the html5lib parser also solve the problem and gave me a clean print.
",,false,,,
https://stackoverflow.com/questions/20906416,false,The behavior observed with the BeautifulSoup API and the lxml parser is likely due to differences in the HTML documents being parsed and the encoding used. It does not meet the criteria for deeper analysis as it is not an API-specific issue.,,,,,,,BeautifulSoup soup.prettify() gives strange output,"I'm trying to parse a web site and I'm going to use it later in my Django project. To do that, I'm using urllib2 and BeautifulSoup4. However, I couldn't get what I want. The output of BeautifulSoup object is weird. I tried different pages, it worked (output is normal). I thought it is because of the page. Then, when my friend tried to do the same thing, he got normal output. I couldn't manage to figure out problem. 

This is the website I'm going to parse.

This is an example of the weird output after the command ""soup.prettify()"":

t   d       B   G   C   O   L   O   R   =   ""   #   9   9   0   4   0   4   ""       w   i   d   t   h   =   ""   3   ""   &amp;gt;   i   m   g       S   R   C   =   ""   1   p   .   g   i   f   ""       A   L   T       B   O   R   D   E   R   =   ""   0   ""       h   e   i   g   h   t   =   ""   1   ""       w   i   d   t   h   =   ""   3   ""   &amp;gt;   /   t   d   &amp;gt;   \n           /   t   r   &amp;gt;   \n           t   r   &amp;gt;   \n                   t   d       c   o   l   s   p   a   n   =   ""   3   ""       B   G   C   O   L   O   R   =   ""   #   9   9   0   4   0   4   ""       w   i   d   t   h   =   ""   6   0   0   ""       h   e   i   g   h   t   =   ""   3   ""   &amp;gt;   i   m   g       s   r   c   =   ""   1   p   .   g   i   f   ""       w   i   d   t   h   =   ""   6   0   0   ""   \n                   h   e   i   g   h   t   =   ""   1   ""   &amp;gt;   /   t   d   &amp;gt;   \n           /   t   r   &amp;gt;   \n   /   t   a   b   l   e   &amp;gt;   \n   /   c   e   n   t   e   r   &amp;gt;   /   d   i   v   &amp;gt;   \n   \n   p   &amp;gt;   &amp;amp;n   b   s   p   ;   &amp;amp;n   b   s   p   ;   &amp;amp;n   b   s   p   ;   &amp;amp;n   b   s   p   ;   /   p   &amp;gt;   \n   /   b   o   d   y   &amp;gt;   \n   /   h   t   m   l   &amp;gt;\n  &lt;/p&gt;\n &lt;/body&gt;\n&lt;/html&gt;'

","Here is a minimal example that does work for me, including the snippet of html that you have a problem with. It's hard to tell without your code, but my guess is you did something like ' '.join(A.split()) somewhere.

import urllib2, bs4

url = ""http://kafemud.bilkent.edu.tr/monu_tr.html""
req = urllib2.urlopen(url)
raw = req.read()
soup = bs4.BeautifulSoup(raw)

print soup.prettify().encode('utf-8')


Giving:

....
&lt;td bgcolor=""#990404"" width=""3""&gt;
       &lt;img alt="""" border=""0"" src=""1p.gif"" width=""3""/&gt;
      &lt;/td&gt;
      &lt;td bgcolor=""#FFFFFF"" valign=""TOP""&gt;
       &lt;div align=""left""&gt;
        &lt;table align=""left"" border=""0"" cellpadding=""10"" cellspacing=""0"" valign=""TOP"" width=""594""&gt;
         &lt;tr&gt;
          &lt;td align=""left"" valign=""top""&gt;
           &lt;table align=""left"" border=""0"" cellpadding=""0"" cellspacing=""0"" class=""icerik"" width=""574""&gt;
....

","This looks like you have your XML coming in with an encoding that beautifulsoup isn't expecting. My guess is that your XML is in UTF-16 and beautifulsoup is reading it as UTF-8. Python offers the .encode and .decode functions for switching between different encodings. Something like 

myXmlStr.encode(""utf-16"").decode(""utf-8"")


Would probably solve your problem if the issue is your incoming XML encoding. I'm new to beautiful soup myself, but a quick google suggests that if the  problem is the encoding of the output, prettify accepts an encoding parameter:

soup.prettify(""utf-16"")


Without more information I can't give you a clearer answer - but hopefully this points you in a helpful direction.
","Possibly you and your friend use different parsers. BeautifulSoup will use the parser it considers ""best"", and thus prefer lxml for speed reasons (if installed).  If using recent versions of Python (and the last version of the included parser), there are cases which are handled better by BeautifulSoup(text, 'html.parser'); this is the case e.g. when there are unmasked &lt; characters (instead of &amp;lt;) in text content.
",false,,,
https://stackoverflow.com/questions/39077890,false,The issue does not meet the criteria for deeper analysis as it does not involve an API exhibiting unexpected failures or unpredictable behaviors under specific runtime conditions.,,,,,,,Unable to crawl some href in a webpage using python and beautifulsoup,"I am currently crawling a web page using Python 3.4 and bs4 in order to collect the match results played by Serbia in Rio2016. So the url here contains links to all the match results she played, for example this.

Then I found that the link is located in the html source like this:

&lt;a href=""/en/volleyball/women/7168-serbia-italy/post"" ng-href=""/en/volleyball/women/7168-serbia-italy/post""&gt;
    &lt;span class=""score ng-binding""&gt;3 - 0&lt;/span&gt;
&lt;/a&gt;


But after several trials, this href=""/en/volleyball/women/7168-serbia-italy/post"" never show up. Then I tried to run the following code to get all the href from the url:

from bs4 import BeautifulSoup
import requests

Countryr = requests.get('http://rio2016.fivb.com/en/volleyball/women/teams/srb-serbia#wcbody_0_wcgridpadgridpad1_1_wcmenucontent_3_Schedule')
countrySoup = BeautifulSoup(Countryr.text)

for link in countrySoup.find_all('a'):
    print(link.get('href'))


Then a strange thing happened. The href=""/en/volleyball/women/7168-serbia-italy/post"" is not included in the output at all.

I found that this href is located in one of the tab pages href=""#scheduldedOver""  in side this url, and it is controlled by the following HTML code:

&lt;nav class=""tabnav""&gt;
    &lt;a href=""#schedulded"" ng-class=""{selected: chosenStatus == 'Pre' }"" ng-click=""setStatus('Pre')"" ng-href=""#schedulded""&gt;Scheduled&lt;/a&gt;
    &lt;a href=""#scheduldedLive"" ng-class=""{selected: chosenStatus == 'Live' }"" ng-click=""setStatus('Live')"" ng-href=""#scheduldedLive""&gt;Live&lt;/a&gt;
    &lt;a href=""#scheduldedOver"" class=""selected"" ng-class=""{selected: chosenStatus == 'Over' }"" ng-click=""setStatus('Over')"" ng-href=""#scheduldedOver""&gt;Complete&lt;/a&gt;
&lt;/nav&gt;


Then how should I get the href using BeautifulSoup inside a tab page?
","Thanks for all your help now I can get the correct url. It is a good learning experience for me. Thanks a lot :)

from bs4 import BeautifulSoup
import requests

Countryr = requests.get('http://rio2016.fivb.com/en/volleyball/women/teams/srb-serbia#wcbody_0_wcgridpadgridpad1_1_wcmenucontent_3_Schedule')
countrySoup = BeautifulSoup(Countryr.text)

for link in countrySoup.find_all('div', {'id': 'AngularPanel'}):
    linkUrl = link.get('data-serviceteammatches')

json = requests.get('http://rio2016.fivb.com' + linkUrl).json()

for item in json:
    print(item.get('Url'))


output:

/en/volleyball/women/7168-serbia-italy/post
/en/volleyball/women/7172-serbia-puerto rico/post
/en/volleyball/women/7177-usa-serbia/post
/en/volleyball/women/7181-china-serbia/post
/en/volleyball/women/7187-serbia-netherlands/post
/en/volleyball/women/7195-russia-serbia/post
/en/volleyball/women/7198-serbia-usa/post
/en/volleyball/women/7200-china-serbia/post

","The data is created dynamically, if you look at the actual source you can see Angularjs templating.

You can still get all the info in json format by mimicking an ajax call, in the source yuuuuou can also see a div like:

&lt;div id=""AngularPanel"" class=""main-wrapper"" ng-app=""fivb""
data-servicematchcenterbar=""/en/api/volley/matches/341/en/user/lives""
data-serviceteammatches=""/en/api/volley/matches/WOG2016/en/user/team/3017""
data-servicelabels=""/en/api/labels/Volley/en"" 
data-servicelive=""/en/api/volley/matches/WOG2016/en/user/live/""&gt;


Using the data-servicematchcenterbar href will give you all the info:

from bs4 import BeautifulSoup
import requests
from urlparse import urljoin

r = requests.get('http://rio2016.fivb.com/en/volleyball/women/teams/srb-serbia#wcbody_0_wcgridpadgridpad1_1_wcmenucontent_3_Schedule')
soup = BeautifulSoup(r.content)

base = ""http://rio2016.fivb.com/""

json = requests.get(urljoin(base, soup.select_one(""#AngularPanel"")[""data-serviceteammatches""])).json()


In  json you will see output like:

{""Id"": 7168, ""MatchNumber"": ""006"", ""TournamentCode"": ""WOG2016"", ""TournamentName"": ""Women's Olympic Games 2016"",
        ""TournamentGroupName"": """", ""Gender"": """", ""LocalDateTime"": ""2016-08-06T22:35:00"",
        ""UtcDateTime"": ""2016-08-07T01:35:00+00:00"", ""CalculatedMatchDate"": ""2016-08-07T03:35:00+02:00"",
        ""CalculatedMatchDateType"": ""user"", ""LocalDateTimeText"": ""August 06 2016"",
        ""Pool"": {""Code"": ""B"", ""Name"": ""Pool B"", ""Url"": ""/en/volleyball/women/results and ranking/round1#anchorB""},
        ""Round"": 68,
        ""Location"": {""Arena"": ""Maracanãzinho"", ""City"": ""Maracanãzinho"", ""CityUrl"": """", ""Country"": ""Brazil""},
        ""TeamA"": {""Code"": ""SRB"", ""Name"": ""Serbia"", ""Url"": ""/en/volleyball/women/teams/srb-serbia"",
                  ""FlagUrl"": ""/~/media/flags/flag_SRB.png?h=60&amp;w=60""},
        ""TeamB"": {""Code"": ""ITA"", ""Name"": ""Italy"", ""Url"": ""/en/volleyball/women/teams/ita-italy"",
                  ""FlagUrl"": ""/~/media/flags/flag_ITA.png?h=60&amp;w=60""},
        ""Url"": ""/en/volleyball/women/7168-serbia-italy/post"", ""TicketUrl"": """", ""Status"": ""Over"", ""MatchPointsA"": 3,
        ""MatchPointsB"": 0, ""Sets"": [{""Number"": 1, ""PointsA"": 27, ""PointsB"": 25, ""Hours"": 0, ""Minutes"": ""28""},
                                    {""Number"": 2, ""PointsA"": 25, ""PointsB"": 20, ""Hours"": 0, ""Minutes"": ""25""},
                                    {""Number"": 3, ""PointsA"": 25, ""PointsB"": 23, ""Hours"": 0, ""Minutes"": ""27""}],
        ""PoolRoundName"": ""Preliminary Round"", ""DayInfo"": ""Weekend Day"",
        ""WeekInfo"": {""Number"": 31, ""Start"": 7, ""End"": 13}, ""LiveStreamUri"": """"},


You can parse whatever you need from those.
",,false,,,
https://stackoverflow.com/questions/73010716,false,The issue does not meet the criteria for deeper analysis as it does not involve an API exhibiting unexpected failures or unpredictable behaviors under specific runtime conditions.,,,,,,,BeautifulSoup find_all returns only first 50 tags,"I am trying to parse html data from this website with BeautifulSoup, but strangely enough, it returns only the first 50 tags that it finds.
When I search the html code through Google DevTools I get 115 matches for the class name that I am looking for.
url='https://wolt.com/az/aze/baku/restaurant/mcdonalds-nnrimanov-ms'
html=urllib.request.urlopen(url).read()
soup = BeautifulSoup(html,'html.parser')

modules=soup.find_all('div',attrs={'class':'MenuItem-module__content___mNrbB'})
print(len(modules))

Output:
&gt;&gt;&gt; 50

I have tried parsing other pages on this website, and still get only 50 results back.
I have also used the answer from Beautiful Soup findAll doesn't find them all to tweak my code.
Any help is appreciated!
","The data is loaded from external URL in Json format. To get all 115 items you can use next example:
import json
import requests

url = ""https://restaurant-api.wolt.com/v4/venues/slug/mcdonalds-nnrimanov-ms/menu?unit_prices=true&amp;show_weighted_items=true""

data = requests.get(url).json()

# ucomment this to print all data:
# print(json.dumps(data, indent=4))

for i, item in enumerate(data[""items""], 1):
    print(""{:&lt;4} {}"".format(i, item[""name""]))

Prints:
...

106  Latte (200 ml)
107  Milk Chocolate (300 ml)
108  Milk Chocolate (200 ml)
109  Ketchup
110  Mayonnaise
111  Barbecue Sauce
112  Sweet Sauce
113  Mustard Sauce
114  1000 Island Sauce
115  Sweet Chilli

","You can't use BeautifulSoup for this task. You don't even need to grab the content of this page.
The link is https://wolt.com/en/aze/baku/restaurant/mcdonalds-nnrimanov-ms and the real link is https://restaurant-api.wolt.com/v4/venues/slug/mcdonalds-nnrimanov-ms/menu?unit_prices=true&amp;show_weighted_items=true
So in fact you can use this piece of code for grabbing the json file.
import urllib
import json
url='https://wolt.com/az/aze/baku/restaurant/mcdonalds-nnrimanov-ms'
link='https://restaurant-api.wolt.com/v4/venues/slug/'+url.replace('/', ' ').strip().split(' ')[-1]+'/menu?unit_prices=true&amp;show_weighted_items=true'
jsn=urllib.request.urlopen(link).read()
dic=json.loads(jsn)
print(dic['items'])
'''
Useful properties:
'alcohol_percentage': percentage of alcohol
'baseprice': price with no discount
'description' and 'name': as it says
'no_contact_delivery_allowed': something special in COVID times
'times': available times(a list)
each element in 'times': 'available_days_of_week': a list, as it means
'''

","When you first access the url, the page will load the first 50 elements with class MenuItem-module__content___mNrbB by default. The rest of them, along with some other content is being loaded dynamically by some scripts which execute in page. Requests is not able to execute Javascript (and BeautifulSoup is just the html parser).
You can test this by disabling Javascript in browser, and reloading the page: you will only find 50 elements in Dev Tools, corresponding to class MenuItem-module__content___mNrbB.
If you want to get all 119 elements corresponding to that class, you will need to inspect the network calls made by javascript - you can see those calls in Network tab, Dev tools, and you can try and scrape those endpoints (this was already clarified in another response).
I hope this is addressing your concerns as to why BeautifulSoup can only find 50 elements, and you understand that particular url/page behaviour now.
",false,,,
https://stackoverflow.com/questions/10402454,false,The issue does not meet the criteria for deeper analysis as it does not involve an API exhibiting unexpected failures or unpredictable behaviors under specific runtime conditions.,,,,,,,BeautifulSoup - Blank line in the output &#39;cause a strange useless &lt;tr&gt;&lt;/tr&gt;,"Little problem for you :-)

I am using BeautifulSoup to parse the content of a table in a HTML page. The problem is that between every line (CSV/EXCEL) of my output file, it pulls a blank line... 
This is an exemple of the HTML Table (which is very big)

&lt;tr&gt;&lt;td class=""normaltext"" valign=""TOP""&gt;Tesco - United Kingdom&amp;nbsp;&amp;nbsp;&lt;/td&gt;
&lt;td class=""normaltext""  valign=""TOP""&gt;CO&lt;/td&gt;
&lt;td class=""normaltext""  valign=""TOP""&gt;Unknown&amp;nbsp;&amp;nbsp;&lt;/td&gt;
&lt;td class=""normaltext""  align=""center"" valign=""top""&gt;lol&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td colspan=""5""&gt;&lt;hr&gt;&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td class=""normaltext"" valign=""TOP""&gt;Tesco - United Kingdom&amp;nbsp;&amp;nbsp;&lt;/td&gt;
&lt;td class=""normaltext""  valign=""TOP""&gt;CO&lt;/td&gt;
&lt;td class=""normaltext""  valign=""TOP""&gt;Unknown&amp;nbsp;&amp;nbsp;&lt;/td&gt;
&lt;td class=""normaltext""  align=""center"" valign=""top""&gt;lol&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td colspan=""5""&gt;&lt;hr&gt;&lt;/td&gt;&lt;/tr&gt;


Every &lt; tr&gt; you have this : &lt; tr&gt;&lt; td colspan=""5""&gt;&lt; hr&gt;&lt; /td&gt;&lt; /tr&gt; So It put a blank line in my CSV/Excel Sheet. I want to pull in the Excel Sheet all the information but without a blank line between every line ... 

Here is the script I use : 

rows = tableau[3].findAll('tr')
for tr in rows:
    cols = tr.findAll('td', attrs={'class' : 'normaltext'})
    y = 0
    x = x + 1
    for td in cols:
        texte_bu = td.text
        texte_bu = texte_bu.encode('utf-8')
        texte_bu = texte_bu.strip()
        ws.write(x,y,td.text)
        y = y + 1


BIG THANKS to the one who can give me the tip to get rib of this * blank useless line between every line of my output file :)
","The solution: when you find an empty row, then skip the loop and read in the next row. This avoids your writing an empty line to the workbook. :)

This is a working simulation. I have added a cosmetic adjustment in order to also avoid the top empty row being sent out. Hope this rids you of empty-line peskiness :)

from BeautifulSoup import BeautifulSoup
import xlwt

text = '''&lt;table&gt;&lt;tr&gt;&lt;td class=""normaltext"" valign=""TOP""&gt;Tesco - United Kingdom&amp;nbsp;&amp;nbsp;&lt;/td&gt;
&lt;td class=""normaltext""  valign=""TOP""&gt;CO&lt;/td&gt;
&lt;td class=""normaltext""  valign=""TOP""&gt;Unknown&amp;nbsp;&amp;nbsp;&lt;/td&gt;
&lt;td class=""normaltext""  align=""center"" valign=""top""&gt;BULATS&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td colspan=""5""&gt;&lt;hr&gt;&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td class=""normaltext"" valign=""TOP""&gt;Tesco - United Kingdom&amp;nbsp;&amp;nbsp;&lt;/td&gt;
&lt;td class=""normaltext""  valign=""TOP""&gt;CO&lt;/td&gt;
&lt;td class=""normaltext""  valign=""TOP""&gt;Unknown&amp;nbsp;&amp;nbsp;&lt;/td&gt;
&lt;td class=""normaltext""  align=""center"" valign=""top""&gt;BULATS&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td colspan=""5""&gt;&lt;hr&gt;&lt;/td&gt;&lt;/tr&gt;&lt;table&gt;'''

wb = xlwt.Workbook()
ws = wb.add_sheet('a test sheet')

soup = BeautifulSoup(text)
table = soup.find('table')
rows = table.findAll('tr')
x = 0
for tr in rows:
    cols = tr.findAll('td', attrs={'class' : 'normaltext'})
    if not cols: 
        # when we hit an empty row, we should not print anything to the workbook
        continue
    y = 0
    for td in cols:
        texte_bu = td.text
        texte_bu = texte_bu.encode('utf-8')
        texte_bu = texte_bu.strip()
        ws.write(x, y, td.text)
        print(x, y, td.text)
        y = y + 1
    # update the row pointer AFTER a row has been printed
    # this avoids the blank row at the top of your table
    x = x + 1

wb.save('example.xls')

",,,false,,,
https://stackoverflow.com/questions/6488706,false,The issue does not meet the criteria for deeper analysis as it does not involve an API exhibiting unexpected failures or unpredictable behaviors under specific runtime conditions.,,,,,,,How to translate/convert unicode escaped &lt; and &gt; in a read HTML doc?,"When I read some (but not all) HTML files in python using a urllib2 opener, on some files I'm getting text filled with lots of backslashes and the unicode 003c strings.  I'm sending this text into BeautifulSoup and am having trouble finding what I'm looking for with findAll(), and I'm now thinking it's due to all these unicode strings.

What's going on with this, and how do I get rid of it?

Approaches like soup.prettify() have no effect.

Here's some example code (this comes from a Facebook profile)

\\u003cdiv class=\\""pas status fcg\\""&gt;Loading...\\u003c\\/div&gt;
\\u003c\\/div&gt;\\u003cdiv class=\\""uiTypeaheadView fbChatBuddyListTypeaheadView dark hidden_elem\\"" id=\\""u971289_14\\""&gt;\\u003c\\/div&gt;
\\u003c\\/div&gt;\\u003c\\/div&gt;\\u003cdiv class=\\""fbNubFlyoutFooter\\""&gt;
\\u003cdiv class=\\""uiTypeahead uiClearableTypeahead fbChatTypeahead\\"" id=\\""u971289_15\\""&gt;
\\u003cdiv class=\\""wrap\\""&gt;\\u003clabel class=\\""clear uiCloseButton\\"" for=\\""u971291_21\\""&gt;


This same HTML page looks fine and normal in a 'view source' window.

EDIT: Here's the code that's producing that text.  What's strange is that I don't get this kind of output from other HTML pages.  Note that I've replaced the username and password with USERNAME and PASSWORD for here.  You could try this on your own FB profile if you replace those two.

fbusername = ""USERNAME@gmail.com""
fbpassword = ""PASSWORD""
cookiefile = ""facebook.cookies""

cj = cookielib.MozillaCookieJar(cookiefile)
if os.access(cookiefile, os.F_OK):
    cf.load()

opener = urllib2.build_opener(
    urllib2.HTTPRedirectHandler(),
    urllib2.HTTPHandler(debuglevel=0),
    urllib2.HTTPSHandler(debuglevel=0),
    urllib2.HTTPCookieProcessor(cj)
)

opener.addheaders = [('User-agent','Mozilla/5.0 (Macintosh; U; Intel Mac OS X 10_6_7; en-us) AppleWebKit/533.21.1 (KHTML, like Gecko) Version/5.0.5 Safari/533.21.1'),('Referer','http://www.facebook.com/')]

def facebooklogin():
    logindata = urllib.urlencode({
        'email' : fbusername,
        'pass' : fbpassword,
    })

    response = opener.open(""https://login.facebook.com/login.php"",logindata)
    return ''.join(response.readlines())


print ""Logging in to Facebook...\n""
facebooklogin()
facebooklogin()
print ""Successful.\n""

fetchURL = 'http://www.facebook.com/USERNAME?ref=profile&amp;v=info'

f = opener.open(fetchURL)
fba = f.read()
f.close()
soup = BeautifulSoup(fba)
print soup

","The u"""""" construct is for Python 2. You omit the u for Python 3.

&gt;&gt;&gt; a=u""""""\\u003cdiv class=\\""pas status fcg\\""&gt;Loading...\\u003c\\/div&gt;
... \\u003c\\/div&gt;\\u003cdiv class=\\""uiTypeaheadView fbChatBuddyListTypeaheadView dark hidden_elem\\"" id=\\""u971289_14\\""&gt;\\u003c\\/div&gt;
... \\u003c\\/div&gt;\\u003c\\/div&gt;\\u003cdiv class=\\""fbNubFlyoutFooter\\""&gt;
... \\u003cdiv class=\\""uiTypeahead uiClearableTypeahead fbChatTypeahead\\"" id=\\""u971289_15\\""&gt;
... \\u003cdiv class=\\""wrap\\""&gt;\\u003clabel class=\\""clear uiCloseButton\\"" for=\\""u971291_21\\""&gt;
... """"""
&gt;&gt;&gt; print(a.decode('unicode_escape')).replace('\\/', '/')
&lt;div class=""pas status fcg""&gt;Loading...&lt;\/div&gt;
&lt;\/div&gt;&lt;div class=""uiTypeaheadView fbChatBuddyListTypeaheadView dark hidden_elem"" id=""u971289_14""&gt;&lt;\/div&gt;
&lt;\/div&gt;&lt;\/div&gt;&lt;div class=""fbNubFlyoutFooter""&gt;
&lt;div class=""uiTypeahead uiClearableTypeahead fbChatTypeahead"" id=""u971289_15""&gt;
&lt;div class=""wrap""&gt;&lt;label class=""clear uiCloseButton"" for=""u971291_21""&gt;


I hope this helps. If not, please improve the information you give in your question.

EDIT: suggested answer now changes \/ to / too.
",,,false,,,
https://stackoverflow.com/questions/61062341,false,The issue does not involve an API exhibiting unexpected failures or unpredictable behaviors under specific runtime conditions.,,,,,,,Scraping coronavirus-related data from a page which is dynamically loaded from a Tableau canvas (I think...),"I will be more than happy to find out this question is a duplicate, but if so - I can't find that Q&amp;A.

There is this mysterious page from the New York State Department of Health containing ""Fatalities by County and Age Group"". As the title implies, it contains two tables (""By County""/""By Age Group""). 

For some strange reason, the data on this page is super-secured. It can't be selected, the page can't be saved and it can't be printed. The data isn't on the page source. I also tried (and failed) to inspect xhr calls for the data.

Obviously, requests and beautifulsoup can't handle it. I tried the usual Selenium incantations (so, unless I'm told otherwise, I won't clutter this question with ""what I tried"" snippets). 

Desire output: the data from those two tables, in any conceivable format.

The only thing I can think of is to take a screenshot and try to ocr the image...

I don't know if it's Selenium, Tableau, the NYS Dep't of Health or just me, but it's time to call in the heavy artillery...
","Let me explain for you the scenario:


Website is generating a session id behind that parameter X-Session-Id which is dynamically generated once you visit the main page page index. So i called it via GET request and I've picked it up from the headers response.
I've figured out an POST request which is automatically generated before you hit your desired url which is actually using the session id which we collected before. here is it https://covid19tracker.health.ny.gov/vizql/w/NYS-COVID19-Tracker/v/NYSDOHCOVID-19Tracker-Fatalities/clear/sessions/{session id}
Now we can call your target which is https://covid19tracker.health.ny.gov/views/NYS-COVID19-Tracker/NYSDOHCOVID-19Tracker-Fatalities?%3Aembed=yes&amp;%3Atoolbar=no&amp;%3Atabs=n. 
Now I noticed another XHR request to the back-end API. But before we do the call, We will parse the HTML content for picking up the time object which is responsible on generating the data freshly from the API so we will get an instant data (consider it like a live chat actually). in our case it's behind lastUpdatedAt inside the HTML
I noticed as well that we will need to pickup the recent X-Session-Id generated from our previous POST request.
Now we will make the call using our picked up session to https://covid19tracker.health.ny.gov/vizql/w/NYS-COVID19-Tracker/v/NYSDOHCOVID-19Tracker-Fatalities/bootstrapSession/sessions/{session}


Now we have received the full response. you can parse it or do whatever you want.

import requests
import re


data = {
    'worksheetPortSize': '{""w"":1536,""h"":1250}',
    'dashboardPortSize': '{""w"":1536,""h"":1250}',
    'clientDimension': '{""w"":1536,""h"":349}',
    'renderMapsClientSide': 'true',
    'isBrowserRendering': 'true',
    'browserRenderingThreshold': '100',
    'formatDataValueLocally': 'false',
    'clientNum': '',
    'navType': 'Reload',
    'navSrc': 'Top',
    'devicePixelRatio': '2.5',
    'clientRenderPixelLimit': '25000000',
    'allowAutogenWorksheetPhoneLayouts': 'true',
    'sheet_id': 'NYSDOH%20COVID-19%20Tracker%20-%20Fatalities',
    'showParams': '{""checkpoint"":false,""refresh"":false,""refreshUnmodified"":false}',
    'filterTileSize': '200',
    'locale': 'en_US',
    'language': 'en',
    'verboseMode': 'false',
    ':session_feature_flags': '{}',
    'keychain_version': '1'
}


def main(url):
    with requests.Session() as req:
        r = req.post(url)
        sid = r.headers.get(""X-Session-Id"")

        r = req.post(
            f""https://covid19tracker.health.ny.gov/vizql/w/NYS-COVID19-Tracker/v/NYSDOHCOVID-19Tracker-Fatalities/clear/sessions/{sid}"")

        r = req.get(
            ""https://covid19tracker.health.ny.gov/views/NYS-COVID19-Tracker/NYSDOHCOVID-19Tracker-Fatalities?%3Aembed=yes&amp;%3Atoolbar=no&amp;%3Atabs=n"")

        match = re.search(r""lastUpdatedAt.+?(\d+),"", r.text).group(1)

        time = '{""featureFlags"":""{\""MetricsAuthoringBeta\"":false}"",""isAuthoring"":false,""isOfflineMode"":false,""lastUpdatedAt"":xxx,""workbookId"":9}'.replace(
            'xxx', f""{match}"")

        data['stickySessionKey'] = time
        nid = r.headers.get(""X-Session-Id"")

        r = req.post(
            f""https://covid19tracker.health.ny.gov/vizql/w/NYS-COVID19-Tracker/v/NYSDOHCOVID-19Tracker-Fatalities/bootstrapSession/sessions/{nid}"", data=data)

        print(r.text)


main(""https://covid19tracker.health.ny.gov"")

","I've made a tableau scraper library to extract the data from Tableau worksheets
You can get all data in pandas dataframe for each worksheet with the following code:
from tableauscraper import TableauScraper as TS

url = ""https://covid19tracker.health.ny.gov/views/NYS-COVID19-Tracker/NYSDOHCOVID-19Tracker-Fatalities""

ts = TS()
ts.loads(url)
dashboard = ts.getWorkbook()

for t in dashboard.worksheets:
    # show worksheet name
    print(f""WORKSHEET NAME : {t.name}"")
    # show dataframe for this worksheet
    print(t.data)

Try this on repl.it
",,false,,,
https://stackoverflow.com/questions/43148784,false,The issue does not involve an API exhibiting unexpected failures or unpredictable behaviors under specific runtime conditions.,,,,,,,Local HTML File Scraping Urllib and BeautifulSoup,"I am very new to python and have been working from scratch on the following code for two weeks to scrape local files.  Probably nearly a hundred hours learning as much as I can about Python, versionality, importing packages such as lxml, bs4, requests, urllib, os, glob and more. 

I'm hopelessly stuck on the first part on getting 12,000 HTML files with strange names all in one directory to load and parse with BeautifulSoup.  I want to get all this data into a csv file or just to output so I can copy it to a file using clipboard.  

import bs4
from urllib.request import urlopen as uReq
from bs4 import BeautifulSoup as soup

#THIS LOCAL FILE WORKS PERFECTLY. I HAVE 12,000 HTML FILES IN THIS DIRECTORY TO PROCESS.  HOW?
#my_url = 'file://127.0.0.1/C:\\My Web Sites\\BioFachURLS\\www.organic-bio.com\\en\\company\\1-SUNRISE-FARMS.html'
my_url = 'http://www.organic-bio.com/en/company/23694-MARTHOMI-ALLERGY-FREE-FOODS-GMBH'

uClient = uReq(my_url)
page_html = uClient.read()
uClient.close()

# html parsing
page_soup = soup(page_html, ""html.parser"")

# grabs each field
contactname = page_soup.findAll(""td"", {""itemprop"": ""name""})
contactstreetaddress = page_soup.findAll(""td"", {""itemprop"": ""streetAddress""})
contactpostalcode = page_soup.findAll(""td"", {""itemprop"": ""postalCode""})
contactaddressregion = page_soup.findAll(""td"", {""itemprop"": ""addressRegion""})
contactaddresscountry = page_soup.findAll(""td"", {""itemprop"": ""addressCountry""})
contactfax = page_soup.findAll(""td"", {""itemprop"": ""faxNumber""})
contactemail = page_soup.findAll(""td"", {""itemprop"": ""email""})
contactphone = page_soup.findAll(""td"", {""itemprop"": ""telephone""})
contacturl = page_soup.findAll(""a"", {""itemprop"": ""url""})

#Outputs as text without tags
Company = contactname[0].text
Address = contactstreetaddress[0].text
Zip = contactpostalcode[0].text
Region = contactaddressregion[0].text
Country = contactaddresscountry[0].text
Fax = contactfax[0].text
Email = contactemail[0].text
Phone = contactphone[0].text
URL = contacturl[0].text

#Prints with comma delimiters

print(Company + ', ' + Address + ', ' + Zip + ', ' + Region + ', ' + Country + ', ' + Fax + ', ' + Email + ', ' + URL)

","I have worked with running through folders with bunches of file before, so I could suggest a little help.
We will start with for loop to files from folder
import os
from bs4 import BeautifulSoup as page_soup

phone = [] # A list to store all the phone
path = 'yourpath' # This is your folder name which stores all your html 
#be careful that you might need to put a full path such as C:\Users\Niche\Desktop\htmlfolder 
for filename in os.listdir(path): #Read files from your path

    #Here we are trying to find the full pathname
    for x in filename: #We will have A-H stored as path
        subpath = os.path.join(path, filename) 
        for filename in os.listdir(subpath):
        #Getting the full path of a particular html file
            fullpath = os.path.join(subpath, filename)
            #If we have html tag, then read it
            if fullpath.endswith('.html'): continue
            #Then we will run beautifulsoup to extract the contents
            soup = page_soup(open(fullpath), 'html.parser')
            #Then run your code
            # grabs each field
            contactname = page_soup.findAll(""td"", {""itemprop"": ""name""})
            contactstreetaddress = page_soup.findAll(""td"", {""itemprop"": ""streetAddress""})
            contactpostalcode = page_soup.findAll(""td"", {""itemprop"": ""postalCode""})
            contactaddressregion = page_soup.findAll(""td"", {""itemprop"": ""addressRegion""})
            contactaddresscountry = page_soup.findAll(""td"", {""itemprop"": ""addressCountry""})
            contactfax = page_soup.findAll(""td"", {""itemprop"": ""faxNumber""})
            contactemail = page_soup.findAll(""td"", {""itemprop"": ""email""})
            contactphone = page_soup.findAll(""td"", {""itemprop"": ""telephone""})
            contacturl = page_soup.findAll(""a"", {""itemprop"": ""url""})

            #Outputs as text without tags
            Company = contactname[0].text
            Address = contactstreetaddress[0].text
            Zip = contactpostalcode[0].text
            Region = contactaddressregion[0].text
            Country = contactaddresscountry[0].text
            Fax = contactfax[0].text
            Email = contactemail[0].text
            Phone = contactphone[0].text
            URL = contacturl[0].text
            #Here you might want to consider using dictionary or a list
            #For example append Phone to list call phone
            phone.append(Phone)

The code is a bit messy but it ran through all the possible folders (Even you have other folders inside your main folder) then try to find html tags, the open it.
I would suggest using dictionary with company as a key in which I presume that the company's name are different. A bunch of lists would also be great because your value will be sorted accordingly. I am not good with dictionary so I can't advise you more than this. I hope I answer your question.
P.S sorry for a messy code.
Edit : Fixing replace lxml with html.parser
",,,false,,,
https://stackoverflow.com/questions/51003621,false,The issue does not involve an API exhibiting unexpected failures or unpredictable behaviors under specific runtime conditions.,,,,,,,Parsing Arabic text in a local html file,"I am trying to extract some data from a local html file using python with BeautifulSoup, the file has some Arabic text data like titles. My problem is that when i try to print out this Arabic text, i get a strange string. An example of code with its output is provided along with the head section of the file, can anybody help me ?
","Open the file with utf-8 encoding


  open(""body.htm"", encoding=""utf-8"")


or target the file to use utf-8

#!/usr/bin/env python
# -*- coding: utf-8 -*-

",,,false,,,
https://stackoverflow.com/questions/6032457,false,The issue does not involve an API exhibiting unexpected failures or unpredictable behaviors under specific runtime conditions.,,,,,,,Remove a bad tag completely with html5lib.sanitizer,"I'm trying to use html5lib.sanitizer to clean user-input as suggested in the docs

The problem is I want to remove bad tags completely and not just escape them (which seems like a bad idea anyway).

The workaround suggested in the patch here doesn't work as expected (it keeps inner content of a &lt;tag&gt;content&lt;/tag&gt;).

Specifically, I want to do something like this:

Input:

&lt;script&gt;bad_thing();&lt;/script&gt;
&lt;style&gt;* { background: #000; }&lt;/style&gt;
&lt;h1&gt;Hello world&lt;/h1&gt;
Lorem ipsum


Output:

&lt;h1&gt;Hello world&lt;/h1&gt;
Lorem ipsum


Any ideas on how to achieve it? I've tried BeautifulSoup, but it doesn't seem to work well, and lxml inserts &lt;p&gt;&lt;/p&gt; tags in very strange places (e.g. around src attrs). So far, html5lib seems to be the best thing for the purpose, if I could just get it to remove tags instead of escaping them.
","The challenge is to also strip unwanted nested tags. It isn't pretty but it's a step in the right direction:

from lxml.html import fromstring
from lxml import etree

html = '''
&lt;script&gt;bad_thing();&lt;/script&gt;
&lt;style&gt;* { background: #000; }&lt;/style&gt;
&lt;h1&gt;Hello world&lt;script&gt;bad_thing();&lt;/script&gt;&lt;/h1&gt;
Lorem ipsum
&lt;script&gt;bad_thing();&lt;/script&gt;
&lt;b&gt;Bold Text&lt;/b&gt;
'''

l = []
doc = fromstring(html)
for el in doc.xpath("".//h1|.//b""):
    i = etree.Element(el.tag)
    i.text, i.tail = el.text, el.tail
    l.append(etree.tostring(i))

print ''.join(l)


Which outputs:

&lt;h1&gt;Hello world&lt;/h1&gt;
Lorem ipsum
&lt;b&gt;Bold Text&lt;/b&gt;

",,,false,,,
https://stackoverflow.com/questions/6837687,false,The issue does not involve an API exhibiting unexpected failures or unpredictable behaviors under specific runtime conditions.,,,,,,,Python - Error Parsing HTML w/ BeautifulSoup,,,,,false,,,
https://stackoverflow.com/questions/17859832,false,The issue does not involve an API exhibiting unexpected failures or unpredictable behaviors under specific runtime conditions.,,,,,,,BeautifulSoup return unexpected extra spaces,"I am trying to grab some text from html documents with BeautifulSoup. In a very relavant case for me, it originates a strange and interesting result: after a certain point, the soup is full of extra spaces within the text (a space separates every letter from the following one). I tried to search the web in order to find a reason for that, but I met only some news about the opposite bug (no spaces at all).

Do you have some suggestion or hint on why it happens, and how to solve this problem?.

This is the very basic code that i created:

from bs4 import BeautifulSoup

import urllib2
html = urllib2.urlopen(""http://www.beppegrillo.it"")
prova = html.read()
soup = BeautifulSoup(prova)
print soup


And this is a line taken from the results, the line where this problem start to appear:


  value=\""Giuseppe labbate ogm? non vorremmo nuovi uccelli chiamati lontre\""&gt;&lt;input onmouseover=\""Tip('&lt;cen t e r   c l a s s = \ \ ' t i t l e _ v i d e o \ \ ' &gt; &lt; b &gt; G i u s e p p e   l a b b a t e   o g m ?   n o n   v o r r e m m o   n u o v i   u c c e l l i   c h i a m a t i   l o n t r e &lt; 

","I believe this is a bug with Lxml's HTML parser. 
Try:

from bs4 import BeautifulSoup

import urllib2
html = urllib2.urlopen (""http://www.beppegrillo.it"")
prova = html.read()
soup = BeautifulSoup(prova.replace('ISO-8859-1', 'utf-8'))
print soup


Which is a workaround for the problem. 
I believe the issue was fixed in lxml 3.0 alpha 2 and lxml 2.3.6, so it could be worth checking whether you need to upgrade to a newer version. 

If you want more info on the bug it was initially filed here:

https://bugs.launchpad.net/beautifulsoup/+bug/972466

Hope this helps,

Hayden
","You can specify the parser as html.parser:

soup = BeautifulSoup(prova, 'html.parser')


Also you can specify the html5 parser:

soup = BeautifulSoup(prova, 'html5')


Haven't installed the html5 parser yet? Install it from terminal:

sudo apt-get install python-html5lib


The xml parser may be used (soup = BeautifulSoup(prova, 'xml')) but you may see some differences in multi-valued attributes like class=""foo bar"".
","I encountered the same issue, it works after change the encoding
                with open(src, ""r"", encoding=""UTF-16"") as file:
                    html = file.read()
                    html = BeautifulSoup(html)

",false,,,
https://stackoverflow.com/questions/629999,false,The issue is not relevant for deeper analysis as it does not involve an API exhibiting unexpected failures or unpredictable behaviors under specific runtime conditions.,,,,,,,"BeautifulSoup gives me unicode+html symbols, rather than straight up unicode. Is this a bug or misunderstanding?","I'm using BeautifulSoup to scrape a website. The website's page renders fine in my browser: 


  Oxfam International’s report entitled “Offside!
  http://www.coopamerica.org/programs/responsibleshopper/company.cfm?id=271


In particular, the single and double quotes look fine. They look html symbols rather than ascii, though strangely when I view source in FF3 they appear to be normal ascii.

Unfortunately, when I scrape I get something like this


  u'Oxfam International\xe2€™s report
  entitled \xe2€œOffside!


oops, I mean this:

u'Oxfam International\xe2€™s report entitled \xe2€œOffside!


The page's meta data indicates 'iso-88959-1' encoding. I've tried different encodings, played with unicode-&gt;ascii and html-&gt;ascii third party functions, and looked at the MS/iso-8859-1 discrepancy, but the fact of the matter is that ™ has nothing to do with a single quote, and I can't seem to turn the unicode+htmlsymbol combo into the right ascii or html symbol--in my limited knowledge, which is why I'm seeking help.

I'd be happy with an ascii double quote, "" or ""

The problem the following is that I'm concerned there are other funny symbols decoded incorrectly. 

\xe2€™


Below is some python to reproduce what I'm seeing, followed by the things I've tried.

import twill
from twill import get_browser
from twill.commands import go

from BeautifulSoup import BeautifulSoup as BSoup

url = 'http://www.coopamerica.org/programs/responsibleshopper/company.cfm?id=271'
twill.commands.go(url)
soup = BSoup(twill.commands.get_browser().get_html())
ps = soup.body(""p"")
p = ps[52]

&gt;&gt;&gt; p         
Traceback (most recent call last):
  File ""&lt;stdin&gt;"", line 1, in &lt;module&gt;
UnicodeEncodeError: 'ascii' codec can't encode character u'\xe2' in position 22: ordinal not in range(128)

&gt;&gt;&gt; p.string
u'Oxfam International\xe2€™s report entitled \xe2€œOffside!&lt;elided&gt;\r\n'


http://groups.google.com/group/comp.lang.python/browse_frm/thread/9b7bb3f621b4b8e4/3b00a890cf3a5e46?q=htmlentitydefs&amp;rnum=3&amp;hl=en#3b00a890cf3a5e46

http://www.fourmilab.ch/webtools/demoroniser/

http://www.crummy.com/software/BeautifulSoup/documentation.html

http://www.cs.tut.fi/~jkorpela/www/windows-chars.html

&gt;&gt;&gt; AsciiDammit.asciiDammit(p.decode())
u'&lt;p&gt;Oxfam International\xe2€™s report entitled \xe2€œOffside!

&gt;&gt;&gt; handle_html_entities(p.decode())
u'&lt;p&gt;Oxfam International\xe2\u20ac\u2122s report entitled \xe2\u20ac\u0153Offside! 

&gt;&gt;&gt; unicodedata.normalize('NFKC', p.decode()).encode('ascii','ignore')
'&lt;p&gt;Oxfam International€™s report entitled €œOffside!

&gt;&gt;&gt; htmlStripEscapes(p.string)
u'Oxfam International\xe2TMs report entitled \xe2Offside!


EDIT:

I've tried using a different BS parser:

import html5lib
bsoup_parser = html5lib.HTMLParser(tree=html5lib.treebuilders.getTreeBuilder(""beautifulsoup""))
soup = bsoup_parser.parse(twill.commands.get_browser().get_html())
ps = soup.body(""p"")
ps[55].decode()


which gives me this

u'&lt;p&gt;Oxfam International\xe2\u20ac\u2122s report entitled \xe2\u20ac\u0153Offside!


the best case decode seems to give me the same results:

unicodedata.normalize('NFKC', p.decode()).encode('ascii','ignore')
'&lt;p&gt;Oxfam InternationalTMs report entitled Offside! 


EDIT 2:

I am running Mac OS X 4 with FF 3.0.7 and Firebug

Python 2.5 (wow, can't believe I didn't state this from the beginning)
",,,,false,,,
https://stackoverflow.com/questions/58276115,false,The issue is not relevant for deeper analysis as it does not involve an API exhibiting unexpected failures or unpredictable behaviors under specific runtime conditions.,,,,,,,how to properly extract utf8 text (japanese symbols) from a webpage with BeautifulSoup4,"i downloaded webpages using wget. now i am trying to extract some data i need from those pages. the problem is with the Japanese words contained in this data. the English words extraction was perfect.
when i try to extract the Japanese words and use them in another app they appear gibberish. during testing diffrent methods there was one solution that fixed only half the japanese words.
what i tried: i tried
from_encoding=""utf-8"" 

which had no effect. also i tried multiple ways to extract the text from the html code like
section.get_text(strip=True) 
section.text.strip()

and others, also i tried to encode the generated text using URLencoding which did not work, also i tried using every code i could find on stackoverflow
one of the methods that strangely worked (but not completely) was saving the string in a dictionary then saving it into a JSON then calling the JSON from ANOTHER script. just using the dictionary, as it is, would not work. i have to use JSON as a middle man between two scripts. strange. (not all the words worked)
my question may seem like duplicates of anther question. but that other question is scraping from the internet. and what i am trying to do is extract from an offline source.
here is a simple script explaining the main problem
from bs4 import BeautifulSoup

page = BeautifulSoup(open(""page1.html""), 'html.parser', from_encoding=""utf-8"")
word = page.find('span', {'class' : ""radical-icon""})
wordtxt = word.get_text(strip=True)
  
#then save the word to a file
    
with open(""text.txt"", ""w"", encoding=""utf8"") as text_file:
    text_file.write(wordtxt)

when i open the file i get gibberish characters
here is the part of the html that BeautifulSoup searchs:
&lt;span class=""radical-icon"" lang=""ja""&gt;亠&lt;/span&gt;

the expected results is to get the symbols inside the text file. or to save them properly in anyway.
is there a better web scraper to use to properly get the utf8?
PS: sorry for bad english
","i think i found an answer, just uninstall beautifulsoup4. i dont need it.

python has a builtin way to search for strings, i tried something like this:

import codecs
import re

with codecs.open(""page1.html"", 'r', 'utf-8') as myfile:
    for line in myfile:
        if line.find('&lt;span class=""radical-icon""') &gt; -1:
            result = re.search('&lt;span class=""radical-icon"" lang=""ja""&gt;(.*)&lt;/span&gt;', line)
            s = result.group(1)

with codecs.open(""text.txt"", 'w', 'utf-8') as textfile:
    textfile.write(s)



which is the over complicated and non-pythonic way of doing it. but what works works.
",,,false,,,
https://stackoverflow.com/questions/9819141,false,The issue is not relevant for deeper analysis as it does not involve an API exhibiting unexpected failures or unpredictable behaviors under specific runtime conditions.,,,,,,,Python BeautifulSoup - Different results with find and findAll,"I am trying to parse some text embedded in HTML using BeautifulSoup, using the ""text"" attribute.
Playing around with ""find"" and ""findAll"", I notice something strange and cannot get my head around why it behaves in this way...

import re
import BeautifulSoup

doc = ""&lt;html&gt;&lt;head&gt;&lt;title&gt;Page title&lt;/title&gt;&lt;title&gt;Author name&lt;/title&gt;&lt;/head&gt;""
soup = BeautifulSoup(doc)

# find test
test1 = soup1.find('title',text=re.compile(""Page"")) 
print test1 # Returns ""Page title""

# findAll test
test2 = soup1.findAll('title',text=re.compile(""Page"")) 
print test2 # Returns ""[&lt;title&gt;Page title&lt;/title&gt;, &lt;title&gt;Author name&lt;/title&gt;]""


In the second test, shouldn't the parser return the same results as in the first example?
The second test should be returning all 'title' tags whose text contains ""Page"", and yet it returns the second title tag too.

Is this expected or am I missing something?
","I got the results for BeautifulSoup version 4.7 in python3.7

&gt;&gt;&gt; soup.find('title', text=re.compile('Page'))
    &lt;title&gt;Page title&lt;/title&gt;
&gt;&gt;&gt; soup.findAll('title', text=re.compile('Page'))
   [&lt;title&gt;Page title&lt;/title&gt;]

",,,false,,,
https://stackoverflow.com/questions/30788850,false,The issue is not relevant for deeper analysis as it does not involve an API exhibiting unexpected failures or unpredictable behaviors under specific runtime conditions.,,,,,,,How should I show results of BeautifulSoup parsing in Django?,"I'm trying to scrape a web page using BeautifulSoup and Django. Here's my views.py which do this task:

def detail(request, article_id):
    article = get_object_or_404(Article, pk=article_id)
    html = urllib2.urlopen(""...url..."")
    soup = BeautifulSoup(html)
    title = soup.title

    return render(request, 'detail.html', {'article': article, 'title':title})


But when I use {{ title }} in django template files, it doesn't show anything. I've test it and it works in shell. I've added a line to this function:

print soup.title


and it prints it every time I reload the page, but it doesn't show up in templates.

The content also displayed very strange for some other commands like: find_all(""a"") or prettify method. Could anyone tell me how can I print the result of beautifulsoup correctly in django templates?
","what's the result that you print out?

have you try to use this?

soup.title.string


if you have to send html in to templates try:

{% autoescape off %}{{ title }}{% endautoescape %}

","Change the following -

title = soup.title


to - 

title = soup.title.text

",,false,,,
https://stackoverflow.com/questions/61094791,false,The issue is not relevant for deeper analysis as it does not involve an API exhibiting unexpected failures or unpredictable behaviors under specific runtime conditions.,,,,,,,Decode base64 encoded urls using selenium,"I would like to scrape some images from a website. I checked the website and everything seemed pretty easy so I started with plain beautifulsoup. Then I noticed, that images are in strange format, probably base64 related, so I tried to decode it but nothing came out of it. I made a little research and I found suggestions to use selenium, because the image urls may be rendered via javascript. So I tried it with selenium with no success. 

I am trying to get the image url this way:

img = self.browser.execute_script(f""return document.querySelectorAll('picture &gt; img')[{num}]"").get_attribute('src')

There are 24 images on page so I iterate through them (via num). If I debug line by line, several urls render correctly, however, if I just let the code go with no breakpoints I get all urls like this: 

data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7.

I tried to base64 decode it, but it makes no sense to me. And it is also too short to be actual image. Correctly rendered urls show that the images are actually not gifs but jpgs.

I also tried to find the element by css selector (using both pure beautifulsoup and selenium) but the result was the same.

I found this discussion: How to extract img src from web page via lxml in beautifulsoup using python? but it did not help me either. I have not found any dynamic key (although there are similarities - there are multiple sizes of the pictures) and the base64 code is too short to be an actual image preview as mentioned above.

If I inspect element in browser I see correct url. Is there a way I can do the same using some beautiful soup or selenium (or other python framework for scraping)? What is the actual data encoded in base64?
","If you look at the source code of the website ,The images links you are trying to scrape exists in another tag noscript .
you can get them using requests and Beautifulsoup as follows :
import requests
from bs4 import BeautifulSoup as bs
url = 'https://eshop.nobilis.cz/aromaterapie/'
res = requests.get(url,headers={'User-Agent': 'Mozilla/5.0'})

soup = bs(res.content, 'html.parser')

images = soup.select('noscript img')
for img in images:
        img_link = img.get('src')
        img_alt  = img.get('alt')
        print(img_alt , '==&gt;' , img_link)

Output:
Obrázek kategorie Aromaterapie ==&gt; https://cdn.nobilis.cz/image/custom-w1920-h480-crop/content/aromaterapie_3840x960-bb98d24ff24a2c55.jpg
Keramický difuzér ==&gt; https://cdn.nobilis.cz/image/custom-w225-h250/data/persistent/products/4/6/33/keramicky-difuzer__S8Ru.jpg
Keramická destička ==&gt; https://cdn.nobilis.cz/image/custom-w225-h250/data/persistent/products/4/5/31/n1700-kopie__nQwF.jpg
Aroma difuzér ==&gt; https://cdn.nobilis.cz/image/custom-w225-h250/data/persistent/products/4/6/57/t0328-aroma-difuzer__JYKy.jpg
MINI difuzér ==&gt; https://cdn.nobilis.cz/image/custom-w225-h250/data/persistent/products/5/6/86/01-t0330-mini-difuzer__9RjF.jpg
Zen difuzér ==&gt; https://cdn.nobilis.cz/image/custom-w225-h250/data/persistent/products/5/3/20/t0329-zen-difuzer__IBcR.jpg
Náplně do MINI difuzéru 10 ks ==&gt; https://cdn.nobilis.cz/image/custom-w225-h250/data/persistent/products/4/5/7/t0331s__IqbM.jpg
Aromaterapie na cesty ==&gt; https://cdn.nobilis.cz/image/custom-w225-h250/data/persistent/products/4/5/55/s0103-aromaterapie-na-cesty__0hat.jpg
Keramická amforka ==&gt; https://cdn.nobilis.cz/image/custom-w225-h250/data/persistent/products/5/7/13/keramicka-amforka-kopie__bpFN.jpg
Prostorový difuzér éterických olejů ==&gt; https://cdn.nobilis.cz/image/custom-w225-h250/data/persistent/products/4/7/59/t0320__egh5.jpg
Směs éterických olejů Inspirace ==&gt; https://cdn.nobilis.cz/image/custom-w225-h250/data/persistent/products/7/0/57/e1081b-smes-eterickych-oleju-inspirace__YAb1.jpg
Směs éterických olejů Tantra ==&gt; https://cdn.nobilis.cz/image/custom-w225-h250/data/persistent/products/6/4/63/e2006b-smes-eterickych-oleju-tantra__KeIG.jpg
Éterický olej bio Citron ==&gt; https://cdn.nobilis.cz/image/custom-w225-h250/data/persistent/products/7/4/59/b0015b-bio-citron__KvPJ.jpg
Éterický olej Meduňka ==&gt; https://cdn.nobilis.cz/image/custom-w225-h250/data/persistent/products/6/6/94/e1027-medunka-1-ml__svsg.jpg
Éterický olej Bergamot ==&gt; https://cdn.nobilis.cz/image/custom-w225-h250/data/persistent/products/7/0/27/e0008b-etericky-olej-bergamot__gab2.jpg
Éterický olej Grapefruit ==&gt; https://cdn.nobilis.cz/image/custom-w225-h250/data/persistent/products/6/8/71/e0024b-etericky-olej-grapefruit__J85r.jpg
Éterický olej bio Rozmarýn ==&gt; https://cdn.nobilis.cz/image/custom-w225-h250/data/persistent/products/5/8/12/b0016b-bio-rozmaryn__POvK.jpg
Směs éterických olejů Druhý dech ==&gt; https://cdn.nobilis.cz/image/custom-w225-h250/data/persistent/products/7/3/27/e2002b-smes-eterickych-oleju-druhy-dech__dPzL.jpg
Éterický olej Šalvěj muškátová ==&gt; https://cdn.nobilis.cz/image/custom-w225-h250/data/persistent/products/5/8/93/e0045b-etericky-olej-salvej-muskatova__wAFx.jpg
Éterický olej Cypřiš ==&gt; https://cdn.nobilis.cz/image/custom-w225-h250/data/persistent/products/6/3/55/e0017b-etericky-olej-cypris__RxDS.jpg
Éterický olej Skořice, kůra ==&gt; https://cdn.nobilis.cz/image/custom-w225-h250/data/persistent/products/7/0/60/e0074b-etericky-olej-skorice-kura__tK0h.jpg
Éterický olej Geranium ==&gt; https://cdn.nobilis.cz/image/custom-w225-h250/data/persistent/products/6/2/71/e1057b-etericky-olej-geranium__dCRQ.jpg
Éterický olej Konopí ==&gt; https://cdn.nobilis.cz/image/custom-w225-h250/data/persistent/products/5/7/67/e0154h-konopi-1-ml__b2oW.jpg
Růže v jojobovém oleji ==&gt; https://cdn.nobilis.cz/image/custom-w225-h250/data/persistent/products/6/7/24/n1010c-ruze-v-jojobe-20-ml__jzLM.jpg
Éterický olej bio Tymián linalol ==&gt; https://cdn.nobilis.cz/image/custom-w225-h250/data/persistent/products/6/3/82/b0005a-bio-tymian-linalol__8IFa.jpg

",,,false,,,
https://stackoverflow.com/questions/52593716,false,The issue is not relevant for deeper analysis as it does not involve an API exhibiting unexpected failures or unpredictable behaviors under specific runtime conditions.,,,,,,,"BeautifulSoup can&#39;t get all source code of the page, only a little lines","from urllib.request import urlopen
from bs4 import BeautifulSoup

page_origin = urlopen(""https://stackoverflow.com"")
page_html = page_origin.read()
page_origin.close()
print(page_html)



  The result is the full html code of https://stackoverflow.com. It works fine. Because it's too long enough, I don't paste it on.


The problem is with BeautifulSoup. I add two lines' code to use BeautifulSoup to analyze the html. Strange things happened. It didn't work at all.

from urllib.request import urlopen
from bs4 import BeautifulSoup

page_origin = urlopen(""https://stackoverflow.com"")
page_html = page_origin.read()
page_origin.close()
# print(page_html)

page_soup = BeautifulSoup(page_html, features=""lxml"", from_encoding=""gbk"")
print(page_soup)


The result is very very simple.

&lt;!DOCTYPE html&gt;
&lt;html class=""html__responsive""&gt;
 &lt;head&gt;
  &lt;title&gt;
   Stack Overflow - Where Developers Learn, Share, &amp;amp; Build Careers
  &lt;/title&gt;
  &lt;link href=""https://cdn.sstatic.net/Sites/stackoverflow/img/favicon.ico?v=4f32ecc8f43d"" rel=""shortcut icon""/&gt;
  &lt;link href=""https://cdn.sstatic.net/Sites/stackoverflow/img/apple-touch-icon.png?v=c78bd457575a"" rel=""apple-touch-icon image_src""/&gt;
  &lt;link href=""/opensearch.xml"" rel=""search"" title=""Stack Overflow"" type=""application/opensearchdescription+xml""/&gt;
 &lt;/head&gt;
&lt;/html&gt;


It's not the full code of html and I can't analyze it at all.

Please help me, I debug it for too much time. Thanks.
","This gives the full source code for me:

import requests

from bs4 import BeautifulSoup

r = requests.get('https://stackoverflow.com/')

soup = BeautifulSoup(r.text, 'lxml')
print(soup)

",,,false,,,
https://stackoverflow.com/questions/43502002,false,The issue is not relevant for deeper analysis as it does not involve an API exhibiting unexpected failures or unpredictable behaviors under specific runtime conditions.,,,,,,,Python Beautifulsoup strange results,"I was trying to pull the heading of all the products on an amazon search.
It works, but the results are just giving me back the header and footer amazon links.

If I inspect the elements on amazon's source it lookslike the product titles are '' tags wrapped in anchors. However, this doesn't appear to be the case when trying to scrape the data.

import requests
import re
from bs4 import BeautifulSoup

def adverts_trade(max_pages):
    page = 1
    while page &lt;= max_pages:
        url = 'https://www.amazon.co.uk/s/ref=sr_pg_2?rh=n%3A560798%2Cn%3A560834%2Ck%3Acanon+lenses&amp;page=' + str(page) + '&amp;keywords=canon+lenses&amp;ie=UTF8'
        source_code = requests.get(url)
        plain_text = source_code.text
        soup = BeautifulSoup(plain_text, ""html.parser"")
        for link in soup.findAll('a'):
            #href = link.find('h2').get_text()
            print(link)
        page += 1

adverts_trade(10)

","Amazon doesn't like you scraping data from them.  If you add this line to your code:

print(plain_text)


You'll see the following:

&gt;     &lt;!--
&gt;             To discuss automated access to Amazon data please contact api-services-support@amazon.com.
&gt;             For information about migrating to our APIs refer to our Marketplace APIs at
&gt; https://developer.amazonservices.co.uk/ref=rm_5_sv, or our Product
&gt; Advertising API at
&gt; https://affiliate-program.amazon.co.uk/gp/advertising/api/detail/main.html/ref=rm_5_ac
&gt; for advertising use cases.
&gt;     --&gt;


Don't expect for most sites to be able to simply use requests and bs4 to scrape data.   Either use their API or consider Selenium or some other scraping tool that can drive an actual browser.
","Are you trying to get the title from an  tag's title attribute? Or are you trying to get the title from the H2 child of a  tag? 

If you are going the first way then try print(link['title']) instead print the whole tag. In beautifulSoup you can access the attributes of a catched anchor as a normal dictionary.
",,false,,,
https://stackoverflow.com/questions/10224287,false,The issue does not involve an API exhibiting unexpected failures or unpredictable behaviors under specific runtime conditions.,,,,,,,Parse the content of a HTML table - But iframe is the source of problems,,,,,false,,,
https://stackoverflow.com/questions/47026542,true,The issue involves the Mailchimp API and the behavior of setting campaign content HTML. The issue is triggered when attempting to update the HTML content of a campaign while using a template. The API requires the campaign to be set to custom HTML instead of a template to update the HTML content.,,,,,,,Setting Mailchimp campaign content html not working,"I tried to update my campaign html content using mailchimp api:

/campaigns/{campaign_id}/content


You can find more information about this api here: https://developer.mailchimp.com/documentation/mailchimp/reference/campaigns/content/#

Before sending a campaign, I tried to get campaign content html, modified it and then set campaign content html using above api. I just simply use BeautifulSoup to append a new tag to content body:

content.body.append(BeautifulSoup('&lt;p&gt;Mailchimp is freaking shittttt&lt;/p&gt;'))


Then, some interesting things happen, the first campaign I created, it works fine, the tag added appears in my email. But, then the sub-sequence campaigns not working anymore, the tag added not appearing.

I observed something strange on my mailchimp campaign site, even though I set campaign html content, only Plain-Text Email gets changed (HTML Source still the old version) for both working and not working campaign.



Anyone got this issue before?
","I had a similar issue and I had to take a slightly different approach to solve it. According to this answer by Joel H., ""MailChimp doesn't allow updating the campaign's HTML content because the campaign type is based on a template. In order to update the HTML content, the campaign has to be set to custom HTML instead of a template.""

That solution didn't suit me but it led me to another solution: creating a template, creating editable content areas within that template, and then using the API to retrieve and edit the text in those content areas.

Here is an attempt at adapting my code to solve your problem. I'm using Python 3 and the mailchimp3 client.

default_footer_content = client.templates.default_content.all(template_id=TEMPLATE_ID)['sections']['SECTION_NAME']
new_footer_content = default_footer_content.replace(PLACEHOLDER, 'Mailchimp is freaking shittttt')
client.campaigns.content.update(campaign_id=CAMPAIGN_ID, data={'template': {'id': TEMPLATE_ID, 'sections': {'SECTION_NAME': new_footer_contennt}}})


Some pointers on the above code:


You can find TEMPLATE_ID with the API or simply by copying the numbers at the end of the URL when editing the template in the web interface
You define SECTION_NAME by placing 'mc:edit=""SECTION NAME""' in the appropriate place in the template
I've used .replace() rather than .append() so you will need to put PLACEHOLDER or similar at the appropriate place in the template


I hope that helps, happy to modify my answer if it needs more clarification. This is my first answer on Stack Overflow so constructive criticism appreciated :) 
",,,false,,,
https://stackoverflow.com/questions/26895813,true,The issue involves PhantomJS and its failure to navigate to the next page after clicking a link. This issue is specific to PhantomJS and does not occur when using other drivers like Firefox.,,,,,,,PhantomJS does not navigate to the next page after clicking a link,"I am having a strange issue with PhantomJS or may be I am newbie. I am trying to login on NewEgg.com via Selenium by using PhantomJS. I am using Python for it. Issue is, when I use Firefox as a driver it works well but as soon as I set PhantomJS as a driver it does not go to next page hence give message:

Exception Message: u'{""errorMessage"":""Unable to find element with id \'UserName\'"",""request"":{""headers"":{""Accept"":""application/json"",""Accept-Encoding"":""identity"",""Connection"":""close"",""Content-Length"":""89"",""Content-Type"":""application/json;charset=UTF-8"",""Host"":""127.0.0.1:55372"",""User-Agent"":""Python-urllib/2.7""},""httpVersion"":""1.1"",""method"":""POST"",""post"":""{\\""using\\"": \\""id\\"", \\""sessionId\\"": \\""aaff4c40-6aaa-11e4-9cb1-7b8841e74090\\"", \\""value\\"": \\""UserName\\""}"",""url"":""/element"",""urlParsed"":{""anchor"":"""",""query"":"""",""file"":""element"",""directory"":""/"",""path"":""/element"",""relative"":""/element"",""port"":"""",""host"":"""",""password"":"""",""user"":"""",""userInfo"":"""",""authority"":"""",""protocol"":"""",""source"":""/element"",""queryKey"":{},""chunks"":[""element""]},""urlOriginal"":""/session/aaff4c40-6aaa-11e4-9cb1-7b8841e74090/element""}}' ; Screenshot: available via screen 


The reason I found after taking screenshot that phantom could not navigate the page and script got finished. How do I sort this out? Code Snippet I tried given below:

import requests
from bs4 import BeautifulSoup
from time import sleep
from selenium import webdriver
import datetime

my_username = ""user@mail.com""
my_password = ""password""

driver = webdriver.PhantomJS('/Setups/phantomjs-1.9.7-macosx/bin/phantomjs')
firefox_profile = webdriver.FirefoxProfile()
#firefox_profile.set_preference('permissions.default.stylesheet', 2)
firefox_profile.set_preference('permissions.default.image', 2)
firefox_profile.set_preference('dom.ipc.plugins.enabled.libflashplayer.so', 'false')
#driver = webdriver.Firefox(firefox_profile)
driver.set_window_size(1120, 550)
driver.get('http://newegg.com')
driver.find_element_by_link_text('Log in or Register').click()
driver.save_screenshot('screen.png')


I even put sleep but it is not making any difference.
","I experienced this with PhantomJS when the content type of the second page is not correct. A normal browser would just interpret the content dynamically, but Phantom just dies, silently.
",,,false,,,
https://stackoverflow.com/questions/54638750,false,The issue does not involve an API exhibiting unexpected failures or unpredictable behaviors under specific runtime conditions.,,,,,,,A website that I am trying to scrape is changing tags/IDs based on if it detects a crawler. Is there a way to avoid this?,"I am trying to write a basic web scraper that looks through a forum, goes into each post, then checks to see if the post has any github links, storing those links. I am doing this as a part of my research to see how people use and implement Smart Device routines. 

I'm fairly new to web scraping, and have been using BeautifulSoup, but I've run into a strange issue. First, my program:

from bs4 import BeautifulSoup
import requests
from user_agent import generate_user_agent

url = 'https://community.smartthings.com/c/projects-stories'

headers = {'User-Agent': generate_user_agent(device_type=""desktop"", os=('linux'))}
page_response = requests.get(url, timeout=5, headers=headers)

page = requests.get(url, timeout = 5)
#print(page.content)
if page.status_code == 200:
    print('URL: ', url, '\nRequest Successful!')
content = BeautifulSoup(page.content, 'html.parser')
print(content.prettify())

project_url = []
for i in content:
    project_url += content.find_all(""/div"", class_=""a href"")
print(project_url)


What I'm trying to do right now is simply collect all the url links to each individual post on the website. When I try to do this, it returns an empty list. After some experimentation in trying to pick out a specific url based on it's ID, I found that while the ID of each post does not seem to change every time the page is reloaded, it DOES change if the website detects that a scraper is being used. I believe this considering that when the contents of the webpage is printed to the console, at the end of the HTML data, there is a section that reads:

  &lt;!-- include_crawler_content? --&gt;
  &lt;/div&gt;
  &lt;footer class=""container""&gt;
   &lt;nav class=""crawler-nav"" itemscope="""" itemtype=""http://schema.org/SiteNavigationElement""&gt;
    &lt;a href=""/""&gt;
     Home
    &lt;/a&gt;
    &lt;a href=""/categories""&gt;
     Categories
    &lt;/a&gt;
    &lt;a href=""/guidelines""&gt;
     FAQ/Guidelines
    &lt;/a&gt;
    &lt;a href=""/tos""&gt;
     Terms of Service
    &lt;/a&gt;
    &lt;a href=""/privacy""&gt;
     Privacy Policy
    &lt;/a&gt;
   &lt;/nav&gt;


The website seems to detect the crawler and change the navigation based on that. I've tried generating a new user_agent to trick it, but I've had no luck.

Any ideas?
","You could potentially start by using 

content.findChildren('a')


and then go from there, sorting through the results for the links you want.
",,,false,,,
https://stackoverflow.com/questions/15174510,true,"The issue involves Beautiful Soup 3 and its inconsistent behavior when parsing HTML. The issue is caused by the presence of JavaScript in the HTML, which confuses Beautiful Soup 3. Switching to Beautiful Soup 4 or removing the JavaScript resolves the issue.",,,,,,,a strange issue when trying to analysis HTML with beautifulsoup,"i'm trying to write some python codes to gather music charts data from official websites, but i get in trouble when gathering billboard's data. i choose beautifulsoup to handle the HTML

my ENV:
python-2.7
beautifulsoup-3.2.0

first i analysis the HTML 

&gt;&gt;&gt; import BeautifulSoup, urllib2, re
&gt;&gt;&gt; html = urllib2.urlopen('http://www.billboard.com/charts/hot-100?page=1').read()
&gt;&gt;&gt; soup = BeautifulSoup.BeautifulSoup(html)


then i try to gather data what i want, e.g., the artist name

HTML:

&lt;div class=""listing chart_listing""&gt;

&lt;article id=""node-1491420"" class=""song_review no_category chart_albumTrack_detail no_divider""&gt;
  &lt;header&gt;
    &lt;span class=""chart_position position-down""&gt;11&lt;/span&gt;
            &lt;h1&gt;Ho Hey&lt;/h1&gt;
        &lt;p class=""chart_info""&gt;
      &lt;a href=""/artist/418560/lumineers""&gt;The Lumineers&lt;/a&gt;            &lt;br&gt;
      The Lumineers          &lt;/p&gt;


artist name is The Lumineers

&gt;&gt;&gt; print str(soup.find(""div"", {""class"" : re.compile(r'\bchart_listing')})\
... .find(""p"", {""class"":""chart_info""}).a.string)
Traceback (most recent call last):
  File ""&lt;stdin&gt;"", line 1, in &lt;module&gt;
AttributeError: 'NoneType' object has no attribute 'find'


NoneType! seems it can't grep the data what i want, maybe my rule is wrong, so i try to grep  some basic tag instead.

&gt;&gt;&gt; print str(soup.find(""div""))
None
&gt;&gt;&gt; print str(soup.find(""a""))
None
&gt;&gt;&gt; print str(soup.find(""title""))
&lt;title&gt;The Hot 100 : Page 2  | Billboard&lt;/title&gt;
&gt;&gt;&gt; print str(soup)
......entire HTML.....


i'm confusing, why can't it grep the basic tag like div, a? they indeed there. what's wrong with my codes? there is nothing wrong when i try to analysis other chart with these.
","This seems to be a Beautifulsoup 3 issue. If you prettify() the output:

from BeautifulSoup import BeautifulSoup as soup3
import urllib2, re

html = urllib2.urlopen('http://www.billboard.com/charts/hot-100?page=1').read()
soup = soup3(html)
print soup.prettify()


you can see at the end of the output:

        &lt;script type=""text/javascript"" src=""//assets.pinterest.com/js/pinit.js""&gt;&lt;/script&gt;
&lt;/body&gt;
&lt;/html&gt;
  &lt;/script&gt;
 &lt;/head&gt;
&lt;/html&gt;


With two html end tags, it looks like BeautifulSoup3 is confused by the Javascript stuff in this data.

If you use:

from bs4 import BeautifulSoup as soup4
import urllib2, re

html = urllib2.urlopen('http://www.billboard.com/charts/hot-100?page=1').read()
soup = soup4(html)
print str(soup.find(""div"", {""class"" : re.compile(r'\bchart_listing')}).find(""p"", {""class"":""chart_info""}).a.string)


You get 'The Lumineers' as output.

If you cannot switch to bs4, I suggest you write out the html variable to a file out.txt, then change the script to read in in.txt and copy the output to the input and cutting away chunks.

from BeautifulSoup import BeautifulSoup as soup3
import re

html = open('in.txt').read()
soup = soup3(html)
print str(soup.find(""div"", {""class"" : re.compile(r'\bchart_listing')}).find(""p"", {""class"":""chart_info""}).a.string)


My first guess was to remove the &lt;head&gt; ... &lt;/head&gt; and that worked wonders.

After that you can solve that programmatically:

from BeautifulSoup import BeautifulSoup as soup3
import urllib2, re

htmlorg = urllib2.urlopen('http://www.billboard.com/charts/hot-100?page=1').read()
head_start = htmlorg.index('&lt;head')
head_end = htmlorg.rindex('&lt;/head&gt;')
head_end = htmlorg.index('&gt;', head_end)
html = htmlorg[:head_start] + htmlorg[head_end+1:]
soup = soup3(html)
print str(soup.find(""div"", {""class"" : re.compile(r'\bchart_listing')}).find(""p"", {""class"":""chart_info""}).a.string)

",,,false,,,
https://stackoverflow.com/questions/32644408,true,"The issue involves Beautiful Soup and its behavior differences between different parsers. By explicitly specifying the parser to use (e.g., ""html.parser"", ""html5lib"", or ""lxml""), consistent behavior can be achieved across different environments.",,,,,,,beautifulSoup inconsistent behavior,"I am completely puzzled by the behavior of the following HTML-scraping code that I wrote in two different environments and need help finding the root cause of this discrepancy.

import sys
import bs4
import md5
import logging
from urllib2 import urlopen
from platform import platform

# Log particulars of the environment
logging.warning(""OS platform is %s"" %platform())
logging.warning(""Python version is %s"" %sys.version)
logging.warning(""BeautifulSoup is at %s and its version is %s"" %(bs4.__file__, bs4.__version__))

# Open web-page and read HTML
url = 'http://www.ncbi.nlm.nih.gov/Traces/wgs/?val=JXIG&amp;size=all'
response = urlopen(url)
html = response.read()

# Calculate MD5 to ensure that the same string was downloaded
print ""MD5 sum for html string downloaded is %s"" %md5.new(html).hexdigest()

# Make beautiful soup
soup = bs4.BeautifulSoup(html, 'html')
contigsTable = soup.find(""table"", {""class"" : ""zebra""})
contigs = []

# Parse table in soup to find all records
for row in contigsTable.findAll('tr'):
    column = row.findAll('td')
    if len(column) &gt; 2:
        contigs.append(column[1])

# Expect identical results on any machine that this is run
print ""Number of contigs identified is %s"" %len(contigs)


On machine 1, this runs to return:

WARNING:root:OS platform is Linux-3.10.10-031010-generic-x86_64-with-Ubuntu-12.04-precise   
WARNING:root:Python version is 2.7.3 (default, Jun 22 2015, 19:33:41)  
[GCC 4.6.3]  
WARNING:root:BeautifulSoup is at /usr/local/lib/python2.7/dist-packages/bs4/__init__.pyc and its version is 4.3.2  
MD5 sum for html string downloaded is ca76b381df706a2d6443dd76c9d27adf  

Number of contigs identified is 630  


On machine 2, this very identical code runs to return:

WARNING:root:OS platform is Linux-2.6.32-431.46.2.el6.nersc.x86_64-x86_64-with-debian-6.0.6
WARNING:root:Python version is 2.7.4 (default, Apr 17 2013, 10:26:13) 
[GCC 4.6.3]
WARNING:root:BeautifulSoup is at /global/homes/i/img/.local/lib/python2.7/site-packages/bs4/__init__.pyc and its version is 4.3.2
MD5 sum for html string downloaded is ca76b381df706a2d6443dd76c9d27adf

Number of contigs identified is 462




The number of contigs calculated is different.  Please note that the same code parses an HTML table to yield different results on two different environments that are not strikingly different from each other and unfortunately leading to this production nightmare. Manual inspection confirms that the results returned on Machine 2 are incorrect, but has so far been impossible to explain.

Does anyone have similar experience? Do you notice anything wrong with this code or should I stop trusting BeautifulSoup altogether?
","You are experiencing the differences between parsers that BeaufitulSoup chooses automatically for the ""html"" markup type you've specified. Which parser is picked up depends on what modules are available in the current Python environment:


  If you don’t specify anything, you’ll get the best HTML parser that’s
  installed. Beautiful Soup ranks lxml’s parser as being the best, then
  html5lib’s, then Python’s built-in parser.


To have a consistent behavior across the platforms, be explicit:

soup = BeautifulSoup(html, ""html.parser"")
soup = BeautifulSoup(html, ""html5lib"")
soup = BeautifulSoup(html, ""lxml"")


See also: Installing a parser.
",,,false,,,
https://stackoverflow.com/questions/49739591,false,The issue does not involve an API exhibiting unexpected failures or unpredictable behaviors under specific runtime conditions.,,,,,,,Python BeautifulSoup with &quot;lxml&quot; parser breaks long strings into characters,"I noticed a bizarre inconsistency between how Python [3.6.5] BeautifulSoup [4.6.0] with ""lxml"" [4.2.1] parser handles long bytes objects vs long strings. (Apparently, ""long"" is &gt;16,384=2**14 characters or bytes.) 

For example, I download the text of Othello from the MIT website and feed it to BS both in the raw (bytes) form and after decoding to a string. Both objects have the same length, because there are no multibyte characters in the document.

from bs4 import BeautifulSoup 
import urllib

url = ""http://shakespeare.mit.edu/othello/full.html""
html_raw = urllib.request.urlopen(url).read()
html_str = urllib.request.urlopen(url).read().decode(""iso-8859-1"")

type(html_raw), len(html_raw)
#(&lt;class 'bytes'&gt;, 304769)
type(html_str), len(html_str)
#(&lt;class 'str'&gt;, 304769)


The resulting soup is the same for shorter strings/bytes but differs for longer strings/bytes. Namely, the soup produced from a string abruptly starts treating words as separate characters, while the soup produced from bytes correctly handles the whole file:

BeautifulSoup(html_raw[:16410], ""lxml"")
#... &lt;i&gt;Enter OTHELLO, IAGO, and Attendants with torches&lt;/i&gt;
#&lt;/blockquote&gt;
#&lt;a&gt;&lt;/a&gt;&lt;/body&gt;&lt;/html&gt;
BeautifulSoup(html_str[:16410], ""lxml"")
#... &lt;i&gt;Enter OTHELLO, IAGO, and Attendants with torch   e   s   /   i   &amp;gt;   
#   /   b   l   o   c   k   q   u   o   t   e   &amp;gt;      
#
#   A   &lt;/i&gt;&lt;/blockquote&gt;&lt;/body&gt;&lt;/html&gt;


This holds both for the subset of the document (above) and for the whole document:

BeautifulSoup(html_raw, ""lxml"")
#...
#&lt;p&gt;&lt;i&gt;Exeunt&lt;/i&gt;&lt;/p&gt;
#&lt;/blockquote&gt;&lt;/body&gt;
#&lt;/html&gt;

BeautifulSoup(html_str, ""lxml"")
#...
#   p   &amp;gt;   i   &amp;gt;   E   x   e   u   n   t   /   i   &amp;gt;   /   p   &amp;gt;   
#   /   h   t   m   l   &amp;gt;   
#   
#   
#   &lt;/i&gt;&lt;/blockquote&gt;&lt;/body&gt;&lt;/html&gt;


There is no difference between the outputs when I use ""html.parser"". 

Is this a bug in the implementation of BS? Or am I violating some undocumented (or documented?) assumptions?
","Not because file size, The problem maybe only happen in Linux because in Windows it work fine. it is because the html has character set of windows-1252, adding .encode() will solve the problem

soup_raw = BeautifulSoup(html_raw, ""lxml"").encode(""iso-8859-1"")

soup_str = BeautifulSoup(html_str.encode(""iso-8859-1""), ""lxml"")

",,,false,,,
https://stackoverflow.com/questions/27694436,false,The issue does not meet the criteria for deeper analysis as it does not involve an API exhibiting unexpected failures or unpredictable behaviors under specific runtime conditions.,,,,,,,Python: BeautifulSoup returning garbage,"I am building a basic data crawler in python using BeautifulSoup, for Batoto, the manga host. For some the reason, the URL works sometimes and other times it doesn't. For example:

from bs4 import BeautifulSoup
from urllib2 import urlopen

x= urlopen(*manga url here*)
y = BeautifulSoup(x)

print y


The result should be a tag soup of the page but instead I get a big wall of this

´ºŸ{›æP™oRhtüs2å÷%ëmßñ6Y›þ�GDŸ0ËÂ­Í‡ì¼®Yé)–ÀØÅð&amp;amp;ô]½f³ÓÞ€Þþ)ú$÷á�üv…úzW¿¾úà†lªÀí¥ï«·_    OTL_ˆêsÁÿƒÁÖ&amp;lt;Ø?°Þ›Â+WLç¥àEh&amp;gt;rýÜ&amp;gt;x    ˆ‡eÇžù»èå»–Ùý e:›§`L_.‹¦úoÓ‘®e=‰ìÓ4Wëo’]~Ãõ¬À8&amp;gt;x:²âœ2¸ Á|&amp;amp;0ÍVpMLÎñ»v¥Ín÷-ÅÃ‰–T§`Ì.SÔsóë„œ¡×[˜·P6»�ùè�&amp;gt;Ô¾È]Œ—·ú£âÊgí%Ø¶kwýÃ=ÜÏ¸2cïÑfÙ_�×]Õê“ž?„UÖ* m³/­`ñ§ÿL0³dµ·jªÅ}õ/õOXß×;«]®’Ï¯w‹·þ¡ÿ|Gýª`I{µœ}œí�ë–¼yÖÇ'�Wç�ëµÅþþ*ýœd{ÿDv:Ð íHzqÿÆ­÷æélG-èÈâpÇßQé´^ÐO´®Xÿ�ýö(‹šëñþ""4!SÃõ2{òÿÜ´»ûE&lt;/kî?x´&amp;ý˜`Ù)uÂï¹ã[ÏŠ²yÂ°kÆpù}¢&gt;&lt;/uŒ¸kpž¼cìâˆ¬ƒcubÆ¡¢=en2‚påÓb9®`áï|z…p""i6pvif¨þõ“âŸ’&gt;&lt;/t`$ò-e&gt;&lt;/cé”r)$�ˆ)ìªÜrd&amp;mÉÊ*ßdÒuÄ.Æ-hx@9[s=m�Ýfd2o1ˆ]‡[Ôádœtë¤qâxæ°‹qËÁ×,½ŠmÊ‡ê‡¢ùÅýl&gt;&lt;/sí°çù¡h?‡ÌÜœbá‰æÆý¡sd~¬&gt;&lt;/zz¡ózwÎ[à!n‰Àš5¤…¸‘Ý¹Ž&gt;&lt;/sÃ:›3Ìæ&gt;&lt;/lÑggu�».Ð‘@4õë\ÃñÆ:¸5ÔwÛ·…)~ÛacÑ,d­³båÖ6&gt;&lt;/tg9y+wÎ‰í%r8ƒ·}n`¼ÁÆ8˜”é²êÞ½°¶Ï&gt;&lt;/sÖ-di¨a±j9³4&gt;&lt;/ss„*w(ßibðïj*¶„)pâýÌ”a§%va{‰ò¦m mi&gt;&lt;/o³o˜Ÿ?¿Ñu-}{cÜ›a~:k²Ì&gt;&lt;/r+=ÅÌk˜c&gt;&lt;/wÓ¹âßŠž‡ëf7vÑ�akÆ4ƒ‚&gt;&lt;/szŽµiÞêzâšÒ¬ú¢“âÀ#�-&gt;&lt;/qebndÎ‘g*cxgsÆ€Ùüe¡³-ŠngÁ:�3ænæ5ï0`coäÏÖ9œ1Ða¯,æ—ªìàãÉÂð&gt;&lt;/j›h¶`à;)òiÖ š+&gt;&lt;/o”64ˆÎº9°��u—Úd¿ý¥pÎÖ‰0¢s:c�yÆ§³t=ÕŸ“Ý‹41%}*,e³Ô¥ó&gt;&lt;/hiræe—';&gt;&lt;/v�fÞ«Ë¥n§Ð·¡kaììë\�`ùsõ©¸pv¦‘&gt;&lt;/bñ¼ut«w)Ø'¹ú@{)n0¡Žan¶Ë5èsª�–u–&gt;&lt;/y_x.mÅd:g}ëÕðhçðÂ«õõ8ŠcËÕÌvž­v™-šêÙ`b¹˜ùÃÎ“çË¤ÔÙtx¹�ßïÇ¶Îgþ°r‹$ò†aÆ–š?ì&lt;y«Ëñõo{%×‡o{ú¥Á»æ]‡&gt;&lt;/u´¬Ø¸eÖïÝtßÚ'è3®nh±ûk4È@l«s]–Åec¹ÑtmÓl|ë£Þ¼~zôéõûwêÓÑñÉÆw\soøÊiyjvØÖ$¯ÈoºÙoyã]æ5]-t^[“¡aÑ{²Å¸6¦ðtŒçm¼ÂÎz´&gt;&lt;/wà™´»äõ@©õ&gt;&lt;/mÏu:=¼þ·'�qwúËö«m„l^ˆær¥30q±ÒšŸëù&gt;&lt;/lî™¯(„7¼=xi’?¤;ö$ØË4ßoóiòyoµxÉøþ¨—«g³Ãíß{|&gt;&lt;/body&gt;&lt;/html&gt;


wrapped in html and body tags. 

Sometimes I will keep trying and it works, but it is so inconsistent, I can't figure out the reason for it.

Any help would be appreciated.
","It seems to be urlopen having issues with encoding, requests works fine:

x = requests.get(""http://bato.to/comic/_/comics/rakudai-kishi-no-eiyuutan-r11615"")
y = BeautifulSoup(x.content)    
print y


&lt;!DOCTYPE html&gt;
&lt;html lang=""en"" xmlns:fb=""http://www.facebook.com/2008/fbml""&gt;
&lt;head&gt;
&lt;meta charset=""utf-8""/&gt;
&lt;title&gt;Rakudai Kishi no Eiyuutan - Scanlations - Comic - Comic Directory - Batoto -    Batoto&lt;/title&gt;
.................


Using urlopen we get the following:

x = urlopen(""http://bato.to/comic/_/comics/rakudai-kishi-no-eiyuutan-r11615"")    
print x.read()


���������s+I���2���l��9C&lt;�� ^�����쾯�dw�xzNT%��,T��A^�ݫ���9��a��E�C���W!�����ڡϳ��f7���s2�Px$���}I�*�'��;'3O&gt;���'g?�u®{����e.�ڇ�e{�u���jf:aث
�����DS��%��X�Zͮ���������9�:�Dx�����\-�
�*tBW������t�I���GQ�=�c��\:����u���S�V(�&gt;&lt;y�C��ã�*:�ۜ?D��a�g�o�sPD�m�""�,�Ɲ&lt;;v[��s���=��V2�fX��ì�Cj̇�В~�
-~����+;V���m�|kv���:V!�hP��D�K�/`oԣ|�k�5���B�{�0�wa�-���iS
�&gt;�œ��gǿ�o�OE3jçCV&lt;`���Q!��5�B��N��Ynd����?~��q���� _G����;T�S'�@΀��t��Ha�.;J�61'`Й�@���&gt;&gt;`��Z�ˠ�x�@� J*u��'���-����]p�9{&gt;����������#�&lt;-~�K""[AQh0HjP
0^��R�]�{N@��
 ...................


So as you can see it is a problem with urlopen not BeautifulSoup.
","The server is returning gzipped bytes. So to download the content using urllib2:

import sys
import urllib2
import gzip
import io
url = ""http://bato.to/comic/_/comics/rakudai-kishi-no-eiyuutan-r11615""
response = urllib2.urlopen(url)

# print(response.headers)
content = response.read()
if response.headers['Content-Encoding'] == 'gzip':
    g = gzip.GzipFile(fileobj=io.BytesIO(content))
    content = g.read()

encoding = response.info().getparam('charset')
content = content.decode(encoding)


This checks the content is the same as the page.text returned by requests:

import requests
page = requests.get(url)
# print(page.headers)

assert content == page.text


Since requests handles the gunzipping and decoding for you -- and more robustly too -- using requests is highly recommended. 
",,false,,,
https://stackoverflow.com/questions/68657996,true,"The issue involves waiting for an HTML element to change its style attribute, indicating that the bookings data on the page has been updated. This behavior is related to the usage of Selenium and BeautifulSoup libraries to scrape data from a website.",,,,,,,Selenium Wait Until HTML Element Changes,"I am trying to scrape bookings data from a website. Upon entering the site, I must:

Change the default date range (at first, the site displays bookings data for the default date range) to a date range I want to search for
Click the 'Refresh' button
Wait a bit for the website to update the displayed results on the page.

Using Selenium, Python, and BeautifulSoup, when I execute Selenium commands to update the fields for the date range, and click the 'Refresh' button, by the time the code reaches the line where I extract HTML data using BeautifulSoup, the default bookings data for the default website date range is extracted, and not the bookings data for the date range I filled in.
Upon observation, it appears the site is moving by too quickly with Selenium to extract the updated data which takes some time to load. I tried to solve this using driver.implicitly_wait(5) but this returns highly inconsistent results.
I wanted to detect an HTML element that allows for stable extraction with no errors. I realized that when I click the 'Refresh' button, this particular element ...
&lt;div id=""textArea"" style=""visibility: visible; display: block;""&gt;

... changes to ...
&lt;div id=""textArea"" style=""visibility: hidden; display: block;""&gt;

... and changes back to ...
&lt;div id=""textArea"" style=""visibility: visible; display: block;""&gt;

... when the bookings data on the page is updated.
Is there a Selenium command that can detect this change prior to moving onto the next lines in my Python code? For instance, after clicking the 'Refresh' button, a ""wait until style=visibility is visible"" command?
","while True:
    a = driver.find_element_by_id('textArea').get_attribute('style')
    if a == r""visibility: hidden; display: block;""
        break
    else:
        time.sleep(.05)
while True:
    a = driver.find_element_by_id('textArea').get_attribute('style')
    if a == r""visibility: visible; display: block;""
        break
    else:
        time.sleep(.05)

This will wait until it goes invisible, then to visible before your program does anything
",,,false,,,
https://stackoverflow.com/questions/47422969,false,The issue does not meet the criteria for deeper analysis as it does not involve an API exhibiting unexpected failures or unpredictable behaviors under specific runtime conditions.,,,,,,,Python BeautifulSoup inconsistent result,"I have been trying to learn a bit of python, and I tried to create a small program that asks the user for subreddit and then prints all the front page headlines and links to the articles, here is the code

import requests
from bs4 import BeautifulSoup

subreddit = input('Type de subreddit you want to see : ')
link_visit = f'https://www.reddit.com/r/{subreddit}/'
print(link_visit)

base_url = link_visit
r = requests.get(base_url)
soup = BeautifulSoup(r.text, 'html.parser')

for article in soup.find_all('div', class_='top-matter'):

   headline = article.find('p', class_='title')
   print('HeadLine : ' , headline.text )

   a = headline.find('a', href=True)
   link = a['href'].split('/domain')
   print('Link : ' , link[0])


My problem is that sometimes it prints the desired result, other times it does nothing, only asks the user for the subrredit and prints the link to said subreddit.

Can someone explain why is this happening?
","Your request is being rejected by reddit in order to conserve their resources. 

When you detect the failing case, print out the HTML. I think you'll see something like this:

    &lt;h1&gt;whoa there, pardner!&lt;/h1&gt;



&lt;p&gt;we're sorry, but you appear to be a bot and we've seen too many requests
from you lately. we enforce a hard speed limit on requests that appear to come
from bots to prevent abuse.&lt;/p&gt;

&lt;p&gt;if you are not a bot but are spoofing one via your browser's user agent
string: please change your user agent string to avoid seeing this message
again.&lt;/p&gt;

&lt;p&gt;please wait 3 second(s) and try again.&lt;/p&gt;

    &lt;p&gt;as a reminder to developers, we recommend that clients make no
    more than &lt;a href=""http://github.com/reddit/reddit/wiki/API""&gt;one
    request every two seconds&lt;/a&gt; to avoid seeing this message.&lt;/p&gt;

",,,false,,,
https://stackoverflow.com/questions/65843310,false,The issue does not meet the criteria for deeper analysis as it does not involve an API exhibiting unexpected failures or unpredictable behaviors under specific runtime conditions.,,,,,,,Completion in IPython (jupyter) does now work (unexpected keyword argument &#39;column&#39;),"I'm using jupyter notebook, and it works fine, but when I press TAB, the auto-completion doesn't work.
I already checked all the similar cases in StackOverflow,  but none of the solutions worked for me.
I have also tried to do ""pip upgrade"" to: IPython, IPYKernel, Jedi, and Tornado, the upgrade works fine but the problem is still there.
I tried in Firefox, Chrome and Edge.
When I press TAB I can see those errors in the terminal:
[IPKernelApp] ERROR | Exception in message handler:
Traceback (most recent call last):
  File ""c:\users\tomer\appdata\local\programs\python\python39\lib\site-packages\ipykernel\kernelbase.py"", line 265, in dispatch_shell
    yield gen.maybe_future(handler(stream, idents, msg))
  File ""c:\users\tomer\appdata\local\programs\python\python39\lib\site-packages\tornado\gen.py"", line 762, in run
    value = future.result()
  File ""c:\users\tomer\appdata\local\programs\python\python39\lib\site-packages\tornado\gen.py"", line 234, in wrapper
    yielded = ctx_run(next, result)
  File ""c:\users\tomer\appdata\local\programs\python\python39\lib\site-packages\ipykernel\kernelbase.py"", line 580, in complete_request
    matches = yield gen.maybe_future(self.do_complete(code, cursor_pos))
  File ""c:\users\tomer\appdata\local\programs\python\python39\lib\site-packages\ipykernel\ipkernel.py"", line 356, in do_complete
    return self._experimental_do_complete(code, cursor_pos)
  File ""c:\users\tomer\appdata\local\programs\python\python39\lib\site-packages\ipykernel\ipkernel.py"", line 381, in _experimental_do_complete
    completions = list(_rectify_completions(code, raw_completions))
  File ""c:\users\tomer\appdata\local\programs\python\python39\lib\site-packages\IPython\core\completer.py"", line 484, in rectify_completions
    completions = list(completions)
  File ""c:\users\tomer\appdata\local\programs\python\python39\lib\site-packages\IPython\core\completer.py"", line 1818, in completions
    for c in self._completions(text, offset, _timeout=self.jedi_compute_type_timeout/1000):
  File ""c:\users\tomer\appdata\local\programs\python\python39\lib\site-packages\IPython\core\completer.py"", line 1861, in _completions
    matched_text, matches, matches_origin, jedi_matches = self._complete(
  File ""c:\users\tomer\appdata\local\programs\python\python39\lib\site-packages\IPython\core\completer.py"", line 2029, in _complete
    completions = self._jedi_matches(
  File ""c:\users\tomer\appdata\local\programs\python\python39\lib\site-packages\IPython\core\completer.py"", line 1373, in _jedi_matches
    interpreter = jedi.Interpreter(
  File ""c:\users\tomer\appdata\local\programs\python\python39\lib\site-packages\jedi\api\__init__.py"", line 725, in __init__
    super().__init__(code, environment=environment,
TypeError: __init__() got an unexpected keyword argument 'column'

I'll be glad if someone can help me with this case
","The solution from @techno1731 is sub-optimal because it just disables jedi rather than fixing the underlying issue.
The latest jedi (0.18) release is incompatible with IPython 7.19 see this discussion. IPython: 7.20 (released Feb 1st 2020) and 8.0 (not yet released) have a compatibility fix.
The correct workaround at this time is to upgrade IPython:
pip install -U ""ipython&gt;=7.20""

In future you can search for the final two lines of the trackback after removing all path fragments specific to your installation, this is searching for:
     super().__init__(code, environment=environment,
TypeError: __init__() got an unexpected keyword argument 'column'

This will give you the relevant issues on GitHub in the first two Google result as for today.

Note: this is a copy of my answer from Giant IPKernelApp Error Using Hydrogen in Atom question which indeed can appear unrelated given the Hydrogen/Atom setting. I will now vote to close all other questions on the topic as duplicate of this one.

","I encontered the same issue some time ago with Jupyterlab when working locally on my machine with virtual environments.
This is a problem with Jedi being too slow (or rather taking forever) to load the completion, what worked for me was to add the follwing line at the top of the notebook (for example where you typically do the imports):
# Jedi not working
%config Completer.use_jedi = False

This should do the trick.
","I'm using a previous version of jedi instead:
pip install jedi==0.17

that works for me.
",false,,,
https://stackoverflow.com/questions/48090119,false,The issue does not meet the criteria for deeper analysis as it does not involve an API exhibiting unexpected failures or unpredictable behaviors under specific runtime conditions.,,,,,,,Jupyter notebook: TypeError: __init__() got an unexpected keyword argument &#39;io_loop&#39;,"I recently installed jupyter notebooks on my macbook pro.
When I create a new notebook, I see the following exception coming continuously on the terminal where I started the notebook.

Monideeps-MacBook-Pro:PythonNotebooks monideepde$ jupyter-notebook 
[I 12:18:43.675 NotebookApp] Serving notebooks from local directory: /Users/monideepde/Documents/PythonNotebooks
[I 12:18:43.675 NotebookApp] 0 active kernels
[I 12:18:43.676 NotebookApp] The Jupyter Notebook is running at:
[I 12:18:43.676 NotebookApp] http://localhost:8888/?token=dcb1990694d91ded77f4287a588886ea567b5907ac8aeafa
[I 12:18:43.676 NotebookApp] Use Control-C to stop this server and shut down all kernels (twice to skip confirmation).
[C 12:18:43.677 NotebookApp] 

    Copy/paste this URL into your browser when you connect for the first time,
    to login with a token:
        http://localhost:8888/?token=dcb1990694d91ded77f4287a588886ea567b5907ac8aeafa
[I 12:18:43.896 NotebookApp] Accepting one-time-token-authenticated connection from ::1
[W 12:18:44.778 NotebookApp] 404 GET /static/components/moment/locale/en-gb.js?v=20180104121843 (::1) 21.10ms referer=http://localhost:8888/tree
[I 12:18:54.840 NotebookApp] Creating new notebook in 
[W 12:18:55.716 NotebookApp] 404 GET /static/components/moment/locale/en-gb.js?v=20180104121843 (::1) 3.06ms referer=http://localhost:8888/notebooks/Untitled.ipynb?kernel_name=python2
[I 12:18:55.920 NotebookApp] Kernel started: 5e16fa4b-3e35-4265-89b0-ab36bb0573f5
[W 12:18:55.941 NotebookApp] 404 GET /nbextensions/widgets/notebook/js/extension.js?v=20180104121843 (::1) 5.57ms referer=http://localhost:8888/notebooks/Untitled.ipynb?kernel_name=python2
[I 12:18:56.998 NotebookApp] Adapting to protocol v5.1 for kernel 5e16fa4b-3e35-4265-89b0-ab36bb0573f5
[E 12:18:57.001 NotebookApp] Uncaught exception in /api/kernels/5e16fa4b-3e35-4265-89b0-ab36bb0573f5/channels
    Traceback (most recent call last):
      File ""/Library/Python/2.7/site-packages/tornado-5.0a1-py2.7-macosx-10.13-intel.egg/tornado/websocket.py"", line 494, in _run_callback
        result = callback(*args, **kwargs)
      File ""/Library/Python/2.7/site-packages/notebook-5.2.2-py2.7.egg/notebook/services/kernels/handlers.py"", line 258, in open
        super(ZMQChannelsHandler, self).open()
      File ""/Library/Python/2.7/site-packages/notebook-5.2.2-py2.7.egg/notebook/base/zmqhandlers.py"", line 168, in open
        self.send_ping, self.ping_interval, io_loop=loop,
    TypeError: __init__() got an unexpected keyword argument 'io_loop'
[I 12:18:58.021 NotebookApp] Adapting to protocol v5.1 for kernel 5e16fa4b-3e35-4265-89b0-ab36bb0573f5


Python version is 2.7.

Any pointers to how I can resolve this?
","Downgrade tornado, this worked for me.

pip install tornado==4.5.3

based on: https://github.com/liftoff/GateOne/issues/689
","I modified the file ""/Library/Python/2.7/site-packages/notebook-5.2.2-py2.7.egg/‌​notebook/base/zmqhan‌​dlers.py"" to remove the io_loop argument.

Based on bkanuka's comment, this is the way to go until Jupyter releases a new version.
","It is a version incompatibility issue. Upgrading Jupyter solves it. Here is what you should try in macOS:

pip install --upgrade jupyter

",false,,,
https://stackoverflow.com/questions/66141235,false,The issue does not meet the criteria for deeper analysis as it does not involve an API exhibiting unexpected failures or unpredictable behaviors under specific runtime conditions.,,,,,,,Jupyter with docker: __init__() got an unexpected keyword argument &#39;column&#39;,"I recently installed TensorFlow with GPU support using docker:
docker pull tensorflow/tensorflow:latest-gpu-jupyter

But sometimes when I start a jupyter notebook server using the command:
docker run --gpus all -it -p 8888:8888 tensorflow/tensorflow:latest-gpu-jupyter jupyter notebook --notebook-dir=/tf --ip 0.0.0.0 --allow-root --NotebookApp.allow_origin='https://colab.research.google.com'

I see the following exception on the terminal:
[IPKernelApp] ERROR | Exception in message handler:
Traceback (most recent call last):
  File ""/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py"", line 272, in dispatch_shell
    yield gen.maybe_future(handler(stream, idents, msg))
  File ""/usr/local/lib/python3.6/dist-packages/tornado/gen.py"", line 762, in run
    value = future.result()
  File ""/usr/local/lib/python3.6/dist-packages/tornado/gen.py"", line 234, in wrapper
    yielded = ctx_run(next, result)
  File ""/usr/local/lib/python3.6/dist-packages/tornado/gen.py"", line 162, in _fake_ctx_run
    return f(*args, **kw)
  File ""/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py"", line 580, in complete_request
    matches = yield gen.maybe_future(self.do_complete(code, cursor_pos))
  File ""/usr/local/lib/python3.6/dist-packages/ipykernel/ipkernel.py"", line 348, in do_complete
    return self._experimental_do_complete(code, cursor_pos)
  File ""/usr/local/lib/python3.6/dist-packages/ipykernel/ipkernel.py"", line 373, in _experimental_do_complete
    completions = list(_rectify_completions(code, raw_completions))
  File ""/usr/local/lib/python3.6/dist-packages/IPython/core/completer.py"", line 484, in rectify_completions
    completions = list(completions)
  File ""/usr/local/lib/python3.6/dist-packages/IPython/core/completer.py"", line 1818, in completions
    for c in self._completions(text, offset, _timeout=self.jedi_compute_type_timeout/1000):
  File ""/usr/local/lib/python3.6/dist-packages/IPython/core/completer.py"", line 1862, in _completions
    full_text=full_text, cursor_line=cursor_line, cursor_pos=cursor_column)
  File ""/usr/local/lib/python3.6/dist-packages/IPython/core/completer.py"", line 2030, in _complete
    cursor_pos, cursor_line, full_text)
  File ""/usr/local/lib/python3.6/dist-packages/IPython/core/completer.py"", line 1374, in _jedi_matches
    text[:offset], namespaces, column=cursor_column, line=cursor_line + 1)
  File ""/usr/local/lib/python3.6/dist-packages/jedi/api/__init__.py"", line 726, in __init__
    project=Project(Path.cwd()), **kwds)
TypeError: __init__() got an unexpected keyword argument 'column'

After that, I have to restart the server or reconnect from google colab.
Any ideas where the error might come from and how to fix it?
","This seems to be an incompatibility between jedi and ipython, see this issue.
The fix would be to pin jedi to 0.17.2, so either run:
pip install jedi==0.17.2

Or if you are using poetry add this to your pyproject.toml:
jedi = ""&lt;=0.17.2""

But since you are using a docker image that image will need to be updated. It seems to be gpu-jupyter.Dockerfile.
I would raise an issue on that project and see if they can pin jedi like they did for nbformat, or you could just fork it. They should probably upgrade python as well, 3.6 is getting a bit long in the tooth.
","I will add more details to @daphtdazz's answer. I had to do the following steps to solve this issue:
1. Download TensorFlow from github:
git clone https://github.com/tensorflow/tensorflow.git

2. Edit the file gpu-jupyter.Dockerfile to add jedi==0.17.2 at the end of line 104:
vim tensorflow/tensorflow/tools/dockerfiles/dockerfiles/gpu-jupyter.Dockerfile 

3. Placed myself inside the dockerfiles folder :
cd tensorflow/tensorflow/tools/dockerfiles/

4. Build the image:
docker build -f ./dockerfiles/gpu-jupyter.Dockerfile -t tf .

",,false,,,
https://stackoverflow.com/questions/46626670,true,"The issue involves the dask dataframe set_index API, which throws an error due to an unexpected keyword argument 'scheme'. Further analysis is required.",,,,,,,dask dataframe set_index throws error,"I have a dask dataframe created from parquet file on HDFS.
When creating setting index using api: set_index, it fails with below error.


  File ""/ebs/d1/agent/conda/envs/py361/lib/python3.6/site-packages/dask/dataframe/shuffle.py"", line 64, in set_index
      divisions, sizes, mins, maxes = base.compute(divisions, sizes, mins, maxes)
    File ""/ebs/d1/agent/conda/envs/py361/lib/python3.6/site-packages/dask/base.py"", line 206, in compute
      results = get(dsk, keys, **kwargs)
    File ""/ebs/d1/agent/conda/envs/py361/lib/python3.6/site-packages/distributed/client.py"", line 1949, in get
      results = self.gather(packed, asynchronous=asynchronous)
    File ""/ebs/d1/agent/conda/envs/py361/lib/python3.6/site-packages/distributed/client.py"", line 1391, in gather
      asynchronous=asynchronous)
    File ""/ebs/d1/agent/conda/envs/py361/lib/python3.6/site-packages/distributed/client.py"", line 561, in sync
      return sync(self.loop, func, *args, **kwargs)
    File ""/ebs/d1/agent/conda/envs/py361/lib/python3.6/site-packages/distributed/utils.py"", line 241, in sync
      six.reraise(*error[0])
    File ""/ebs/d1/agent/conda/envs/py361/lib/python3.6/site-packages/six.py"", line 693, in reraise
      raise value
    File ""/ebs/d1/agent/conda/envs/py361/lib/python3.6/site-packages/distributed/utils.py"", line 229, in f
      result[0] = yield make_coro()
    File ""/ebs/d1/agent/conda/envs/py361/lib/python3.6/site-packages/tornado/gen.py"", line 1055, in run
      value = future.result()
    File ""/ebs/d1/agent/conda/envs/py361/lib/python3.6/site-packages/tornado/concurrent.py"", line 238, in result
      raise_exc_info(self._exc_info)
    File """", line 4, in raise_exc_info
    File ""/ebs/d1/agent/conda/envs/py361/lib/python3.6/site-packages/tornado/gen.py"", line 1063, in run
      yielded = self.gen.throw(*exc_info)
    File ""/ebs/d1/agent/conda/envs/py361/lib/python3.6/site-packages/distributed/client.py"", line 1269, in _gather
      traceback)
    File ""/ebs/d1/agent/conda/envs/py361/lib/python3.6/site-packages/six.py"", line 692, in reraise
      raise value.with_traceback(tb)
    File ""/ebs/d1/agent/conda/envs/py361/lib/python3.6/site-packages/dask/dataframe/io/parquet.py"", line 144, in _read_parquet_row_group
      open=open, assign=views, scheme=scheme)
  TypeError: read_row_group_file() got an unexpected keyword argument 'scheme'


Can some one point me to the reason of this error and how to fix it.
","Solution
Upgrade fastparquet to version 0.1.3.
Details
Dask 0.15.4, used for your example, includes this commit, which adds the argument scheme to read_row_group_file(). This throws an error for fastparquet versions before 0.1.3.
",,,false,,,
https://stackoverflow.com/questions/33353768,false,"The issue is related to a misunderstanding of the argument name 'data' in the fetch method of AsyncHTTPTestCase, rather than an API-related problem.",,,,,,,AsyncHTTPTestCase Post request Passing Data,"Trying to test a POST request using AsyncHTTPTestCase on Python 3.4, Tornado version 4.1.

from tornado.testing import AsyncHTTPTestCase
from myapp  import myclass

class TestFooHandler(AsyncHTTPTestCase):

    def get_app(self):
        return myclass.application

    def test_post_handler(self):
        import urllib.parse
        post_body = urllib.parse.urlencode({""key"":""val""})
        response = self.fetch(""/foo"", method=""POST"", data=post_body)
        self.assertEqual(response.code, 200)


The code fails with message:
unexpected keyword argument 'data'

Here's the complete trace:

Traceback (most recent call last):
  File ""/usr/local/lib/python3.4/dist-packages/tornado/testing.py"", line 120, in __call__
    result = self.orig_method(*args, **kwargs)
  File ""/home/gub/App/unit_tests/test_cors.py"", line 22, in test_post_handler
    response = self.fetch(""/foo"", method=""POST"", data=post_body)
  File ""/usr/local/lib/python3.4/dist-packages/tornado/testing.py"", line 380, in fetch
    self.http_client.fetch(self.get_url(path), self.stop, **kwargs)
  File ""/usr/local/lib/python3.4/dist-packages/tornado/httpclient.py"", line 227, in fetch
    request = HTTPRequest(url=request, **kwargs)
TypeError: __init__() got an unexpected keyword argument 'data'


What is not correct with the above code ?
","The argument name is body, not data. self.fetch(""/foo"", method=""POST"", body=post_body)
",,,false,,,
https://stackoverflow.com/questions/65693469,true,The issue involves the Hydrogen plugin for Atom and the IPython compatibility issue with the jedi library. Further analysis is required.,,,,,,,Giant IPKernelApp Error Using Hydrogen in Atom,"Starting a few days ago, after months without issue, I began having a giant error box keep popping up when editing my code in Atom. I believe it is from the Hydrogen plugin, the weird thing is even with this error the code still runs and does what I want it too.
I created a new conda environment, installing only what I needed (pandas, geopandas, descartes, jupyter) and even when using the new environment in Atom I am getting this issue. I've tried upgrading ipykernel but it is already the most recent version.
Error Message

[IPKernelApp] ERROR | Exception in message handler: Traceback (most recent call last): File ""/home/username/anaconda3/envs/fresh/lib/python3.8/site-packages/ipykernel/kernelbase.py"", line 268, in dispatch_shell yield gen.maybe_future(handler(stream, idents, msg)) File ""/home/username/anaconda3/envs/fresh/lib/python3.8/site-packages/tornado/gen.py"", line 762, in run value = future.result() File ""/home/username/anaconda3/envs/fresh/lib/python3.8/site-packages/tornado/gen.py"", line 234, in wrapper yielded = ctx_run(next, result) File ""/home/username/anaconda3/envs/fresh/lib/python3.8/site-packages/ipykernel/kernelbase.py"", line 583, in complete_request matches = yield gen.maybe_future(self.do_complete(code, cursor_pos)) File ""/home/username/anaconda3/envs/fresh/lib/python3.8/site-packages/ipykernel/ipkernel.py"", line 360, in do_complete return self._experimental_do_complete(code, cursor_pos) File ""/home/username/anaconda3/envs/fresh/lib/python3.8/site-packages/ipykernel/ipkernel.py"", line 385, in _experimental_do_complete completions = list(_rectify_completions(code, raw_completions)) File ""/home/username/anaconda3/envs/fresh/lib/python3.8/site-packages/IPython/core/completer.py"", line 484, in rectify_completions completions = list(completions) File ""/home/username/anaconda3/envs/fresh/lib/python3.8/site-packages/IPython/core/completer.py"", line 1818, in completions for c in self._completions(text, offset, _timeout=self.jedi_compute_type_timeout/1000): File ""/home/username/anaconda3/envs/fresh/lib/python3.8/site-packages/IPython/core/completer.py"", line 1861, in _completions matched_text, matches, matches_origin, jedi_matches = self._complete( File ""/home/username/anaconda3/envs/fresh/lib/python3.8/site-packages/IPython/core/completer.py"", line 2029, in _complete completions = self._jedi_matches( File ""/home/username/anaconda3/envs/fresh/lib/python3.8/site-packages/IPython/core/completer.py"", line 1373, in _jedi_matches interpreter = jedi.Interpreter( File ""/home/username/anaconda3/envs/fresh/lib/python3.8/site-packages/jedi/api/init.py"", line 725, in init super().init(code, environment=environment, TypeError: init() got an unexpected keyword argument 'column'

","The latest jedi (0.18) release is incompatible with IPython 7.19 see this discussion. IPython: 7.20 (released Feb 1st 2020) and 8.0 (not yet released) have a compatibility fix.
The correct workaround at this time is to upgrade IPython:
pip install -U ipython==7.20

In future you can search for the final two lines of the trackback after removing all path fragments specific to your installation, this is searching for:
line 2029, in _complete completions = self._jedi_matches IPython/core/completer.py, line 1373, in _jedi_matches interpreter = jedi.Interpreter( jedi/api/init.py, line 725, in init super().init(code, environment=environment, TypeError: init() got an unexpected keyword argument 'column'

This will give you the relevant issues on GitHub in the first two Google result as for today.
",,,false,,,
https://stackoverflow.com/questions/58449314,false,"The issue is related to a version mismatch between Tornado and Jupyter Notebook, rather than an API-related problem.",,,,,,,"Even python3 environment is activated, JupyterLab cannot be launched properly","When I activate my ipykernel_py3 environment and try to launch Jupyter Lab in terminal, I get the repeated error messages as follows:

Macintosh-8:~ yuenfannie$ source activate ipykernel_py3
(ipykernel_py3) Macintosh-8:~ yuenfannie$ jupyter lab
[I 12:38:19.969 LabApp] JupyterLab alpha preview extension loaded from /Users/yuenfannie/anaconda2/lib/python2.7/site-packages/jupyterlab
JupyterLab v0.27.0
Known labextensions:
[I 12:38:19.971 LabApp] Running the core application with no additional extensions or settings
[I 12:38:19.979 LabApp] Serving notebooks from local directory: /Users/yuenfannie
[I 12:38:19.980 LabApp] 0 active kernels 
[I 12:38:19.980 LabApp] The Jupyter Notebook is running at: http://localhost:8888/?token=81a366305f2a328dfbbc9cfcf757a30e4977d3abab54cb0f
[I 12:38:19.980 LabApp] Use Control-C to stop this server and shut down all kernels (twice to skip confirmation).
[C 12:38:19.981 LabApp] 

    Copy/paste this URL into your browser when you connect for the first time,
    to login with a token:
        http://localhost:8888/?token=81a366305f2a328dfbbc9cfcf757a30e4977d3abab54cb0f
[I 12:38:20.229 LabApp] Accepting one-time-token-authenticated connection from ::1
[I 12:38:22.881 LabApp] Kernel started: cf6886dd-5475-4e1e-972c-8e4614451f0e
[I 12:38:24.074 LabApp] Adapting to protocol v5.0 for kernel cf6886dd-5475-4e1e-972c-8e4614451f0e
[E 12:38:24.078 LabApp] Uncaught exception GET /api/kernels/cf6886dd-5475-4e1e-972c-8e4614451f0e/channels?session_id=5c81adbda104d35cec3acd907d6decc4&amp;token=81a366305f2a328dfbbc9cfcf757a30e4977d3abab54cb0f (::1)
    HTTPServerRequest(protocol='http', host='localhost:8888', method='GET', uri='/api/kernels/cf6886dd-5475-4e1e-972c-8e4614451f0e/channels?session_id=5c81adbda104d35cec3acd907d6decc4&amp;token=81a366305f2a328dfbbc9cfcf757a30e4977d3abab54cb0f', version='HTTP/1.1', remote_ip='::1')
    Traceback (most recent call last):
      File ""/Users/yuenfannie/anaconda2/lib/python2.7/site-packages/tornado/websocket.py"", line 546, in _run_callback
        result = callback(*args, **kwargs)
      File ""/Users/yuenfannie/anaconda2/lib/python2.7/site-packages/notebook/services/kernels/handlers.py"", line 262, in open
        super(ZMQChannelsHandler, self).open()
      File ""/Users/yuenfannie/anaconda2/lib/python2.7/site-packages/notebook/base/zmqhandlers.py"", line 176, in open
        self.send_ping, self.ping_interval, io_loop=loop,
    TypeError: __init__() got an unexpected keyword argument 'io_loop'


The opened JupyterLab Alpha Preview does not work at all. Unfortunately I have no clues why this happens. Any suggestions are welcome!
","Just downgrade your Tornado:
I have the following versions and they are working just fine (downgraded Tornado from 5.1.1 to 4.5.3):
Jupyter: 4.4.0

Tornado: 4.5.3

","This is a version mismatch issue. The io_loop argument was removed from this method in Tornado 5.0. Jupyter notebook was updated for compatibility with Tornado 5 in Notebook version 5.3 (I think). You need to either upgrade to a newer version of notebook or downgrade to an older version of Tornado. 
",,false,,,
https://stackoverflow.com/questions/41854038,false,"The issue is related to webpack configuration and loader setup, rather than an API-related problem.",,,,,,,&quot;Unexpected token &lt;&quot; for VueJS running with Webpack,"Note: Before you mark this as a duplicate, I have looked at a few solutions and they don't work:


[1] https://github.com/webpack/webpack-dev-server/issues/135
[2] https://github.com/gaearon/react-hot-loader/blob/master/docs/Troubleshooting.md#syntax-error-unexpected-token-
Few other Unexpected token &lt; threads here but they are (probably) unrelated.


I am trying to integrate VueJS into an OSS chat application https://github.com/zulip/zulip . I tried to use the standard configuration template from vue-loader which includes single-file-components and hot reload, but when I try to run the server, I get this error:

...
ERROR in ./static/js/components/sidebar.vue
Module parse failed: /srv/zulip/static/js/components/sidebar.vue Line 1: Unexpected token &lt;
You may need an appropriate loader to handle this file type.
| &lt;template&gt;
|     &lt;div&gt;
|         {{ msg }}
 @ ./static/js/src/main.js 3:14-50
...


This is the webpack config:

var webpack = require('webpack')

module.exports = {
    entry: [
        'webpack-dev-server/client?http://0.0.0.0:9991/socket.io',
        './static/js/src/main.js',
    ],
    devtool: 'eval',
    output: {
        publicPath: 'http://0.0.0.0:9991/webpack/',
        path: './static/js',
        filename: 'bundle.js',
    },
    devServer: {
        port: 9994,
        stats: ""errors-only"",
        watchOptions: {
            aggregateTimeout: 300,
            poll: 1000,
        },
    },
    module: {
        rules: [
            {
                test: /\.vue$/,
                loader: 'vue-loader'
            },
            {
                test: /\.(png|jpg|gif|svg)$/,
                loader: 'file-loader',
                options: {
                    name: '[name].[ext]?[hash]'
                }
            }
        ]
    }
};


Info:


The first link suggest adding a explicit public path, but that is already done before me.
There are a few servers running in the code, including django for the main app server and tornado for push events.
The app only exposes port 9991 outside of the development host (vagrant). The webpack-dev-server uses 9994 but is redirected to localhost:9991/webpack/


You can see the changes here: https://github.com/tommyip/zulip/commit/97cf122bda0f7dc09f6c8c3062c55871c315459e
","I missed one of the key information, which is the version of Webpack.

The examples shown in Vue and vue-loader's website uses Webpack 2 API, which is slightly different to Webpack 1:

module: {
    rules: [
        {
            test: /\.vue$/,
            loader: 'vue-loader'
        },


Rules is actually loaders in Webpack 1.
",,,false,,,
https://stackoverflow.com/questions/23123932,true,The issue involves the compatibility of the PhanthomJS Websocket client with the Websocket protocol used by the ROSLIB library. The unexpected response code 426 indicates an unsupported websocket protocol version.,PhanthomJS,Websocket client,PhanthomJS Websocket client encounters an unexpected response code 426 when trying to connect to a Websocket server implemented with the ROSLIB library. This error suggests an incompatibility between the Websocket protocol versions used by PhanthomJS and ROSLIB.,The PhanthomJS Websocket client works as expected when connecting to Websocket servers that use compatible protocol versions.,"The issue is triggered when the PhanthomJS Websocket client tries to connect to a Websocket server that uses an unsupported protocol version, resulting in the unexpected response code 426.",This issue might be challenging to detect during development and testing because it involves the compatibility between two different libraries (PhanthomJS and ROSLIB) and their respective Websocket implementations.,Unexpected response code: 426 with PhanthomJS Websocket client/ROSLIB,"I'm trying to use PhanthomJS (1.9.7) to setup batch test script to drive our robotics software over our Websocket server (RosBridge which is implemented on top of Tornado).

We use ROSLIBJS which is a JS library that uses the standard HTML5 WebSocket API to interact with the server. I would have preferred to use NodeJS instead of PhantomJS, but there are dependencies in the ROSLIBJS that requires browser DOM structures to be available. Someone ported ROSLIBJS for NodeJS. I got it running, but there were critical bugs that prevented it from being usable. Hence, I need PhantomJS.

According to this link, there is some incompatibility with the Websocket protocol.
Unexpected response code 426

Is that a known issue. If so, when would PhantomJS be fixed for this to work? Can I patch PhantomJS myself easily?
","Http 426 may mean that you are trying to connect with an unsupported websocket protocol version. Check if the response has the http header ""sec-websocket-version"" with a list of supported versions.

Which version is your client using?

Also, if you are connecting through a proxy, the proxy may remove the ""upgrade"" header from the request since it is marked as ""connection"" header. Switch to WSS:// to prevent that.
","Funny enough, I was looking at writing integration tests for roslibjs and ran across this question. I'll try phantomjs-2.0 (https://github.com/Vitallium/phantomjs-qt5) and real chrome via chromedriver. I'll most like PR the results against roslibjs.

An alternative option would be to write another bridge that uses socket.io for client communication to achieve maximum portability. 

See this PR: https://github.com/RobotWebTools/roslibjs/pull/83 . It uses chrome to run tests. Alternatively you can allow rosbridge to accept draft76 websockets by changing https://github.com/RobotWebTools/rosbridge_suite/blob/develop/rosbridge_server/src/tornado/websocket.py#L186 to return True;
",,false,,,
https://stackoverflow.com/questions/33379232,true,"The issue involves the different behavior of Tornado's AsyncHTTPClient on different operating systems, indicating a potential platform-specific bug or configuration difference.",Tornado,AsyncHTTPClient,"The Tornado AsyncHTTPClient exhibits different blocking behavior on different operating systems. On Windows and OSX, the code runs non-blocking as expected, but on Ubuntu, Redhat, and the Amazon AMI, there is a noticeable delay between the before and after print statements, indicating blocking behavior.","The Tornado AsyncHTTPClient should execute HTTP requests in a non-blocking manner, allowing other code to run concurrently.","The issue is triggered when using the Tornado AsyncHTTPClient on Ubuntu, Redhat, or the Amazon AMI, resulting in unexpected blocking behavior.",This issue might be challenging to detect during development and testing because it is platform-specific and relies on observing the timing of code execution.,Python Tornado Blocking in AsyncHTTPClient only on Linux. Not on windows or OSX,"I'm getting very strange behavior from my Tornado AsyncHTTPClient client. 

When I run the same code on Windows, OSX, Ubuntu, Redhat, and the Amazon AMI, my code behaves differently. 

Here is the relevant code:

 request = HTTPRequest(self.URL,
                      method=""POST"",
                      auth_username=self.USERNAME,
                      auth_password=self.PASSWORD,
                      headers=self.HEADERS,
                      body=formatted_request
                      )
try:
  print ""before"", datetime.now()
  future = self.HTTP_CLIENT.fetch(request, self.handle_response)
  print ""after"", datetime.now()


On OSX and Winodws, the output of this code is (Non blocking): 

before 2015-10-27 17:51:13.896538
after 2015-10-27 17:51:14.414656
before 2015-10-27 17:51:14.418626
after 2015-10-27 17:51:14.420233
before 2015-10-27 17:51:14.423062
after 2015-10-27 17:51:14.424126
before 2015-10-27 17:51:14.426491
after 2015-10-27 17:51:14.427542
before 2015-10-27 17:51:14.429675
after 2015-10-27 17:51:14.430702
before 2015-10-27 17:51:14.432825
after 2015-10-27 17:51:14.433863


On Ubuntu, Redhat, and the amazon AMI I am getting this (a difference of 2 seconds in between what is supposed to be non blocking code):

before 2015-10-27 21:49:23.644458
after 2015-10-27 21:49:25.541746
before 2015-10-27 21:49:25.542827
after 2015-10-27 21:49:27.428840
before 2015-10-27 21:49:27.429993
after 2015-10-27 21:49:29.326183
before 2015-10-27 21:49:29.327549


I noticed in the tornado code that there is a difference between linux and osx:


  We use epoll (Linux) or kqueue (BSD and Mac OS X) if they
      are available, or else we fall back on select(). If you are
      implementing a system that needs to handle thousands of
      simultaneous connections, you should use a system that supports
      either epoll or kqueue.


But the performance difference between the different platforms seems unlikely to be a epoll / kqueue issue. 

I'm using python 2.7 and tornado 4.2.1. Distribution versions are the EC2 versions downloaded from the AWS instance start page. 

Any ideas would be appreciated! 

Thanks,
Jon
","The difference is probably in the DNS resolution, which is blocking by default. When it's fast, you're getting a cached result, and when it's not you're going out to the original nameservers (and probably talking to a non-optimal resolver if it's taking 2 seconds).

Try installing the futures package and doing tornado.netutil.Resolver.configure(""tornado.netutil.ThreadedResolver"").
",,,false,,,
https://stackoverflow.com/questions/34470662,true,"The issue involves an unexpected logging behavior in a Tornado coroutine, indicating a potential problem with the interaction between Tornado and the logging module.",Tornado,coroutine,"When using the Tornado coroutine in a logging context, the logging messages are not being captured or displayed as expected. The issue is specific to Tornado and the way it interacts with the logging module.","The Tornado coroutine should work seamlessly with the logging module, capturing and displaying logging messages as usual.","The issue is triggered when using the Tornado coroutine in a logging context, resulting in the unexpected behavior of missing or suppressed logging messages.","This issue might be challenging to detect during development and testing because it involves the interaction between Tornado and the logging module, which may require in-depth knowledge of both libraries.",Python&#39;s logging issue in Tornado coroutine,"A couple of days ago I found strange logging issue working with Tornado.

I have a set of files:

main.py:

 1  import logging
 2  import logging.config
 3  import os

 4  from somemodule.mod import ModClass
 5  from tornado import ioloop

 6  if __name__ == ""__main__"":
 7      logging.config.fileConfig(""logging.ini"")
 8      print ioloop.IOLoop.current().run_sync(ModClass.my_coroutine)


logging.ini:

 1  [loggers]
 2  keys=root

 3  [logger_root]
 4  level=NOTSET
 5  handlers=file

 6  [handlers]
 7  keys=file

 8  [handler_file]
 9  level=DEBUG
10  formatter=default
11  class=handlers.TimedRotatingFileHandler
12  args=('/tmp/some-system.log', 'D', 1, 7)

13  [formatters]
14  keys=default

15  [formatter_default]
16  format=%(asctime)s [%(levelname)s] %(name)s@%(lineno)d: %(message)s


somemodule/mod.py:

 1  import logging
 2  from tornado import gen

 3  logger = logging.getLogger(__name__)

 4  class ModClass(object):

 5      @classmethod
 6      @gen.coroutine
 7      def my_coroutine(cls):
 8          # logger = logging.getLogger(__name__)
 9          logger.critical(""SOME MESSAGE"")
10          raise gen.Return(""Some string"")


Also I have an empty __init__.py in somemodule directory.

If I run main.py, I see ""Some string"" in console, and I have a created, but empty file /tmp/some-system.log. I don't know what is wrong with this small system.

To make it work correctly I have to comment line (3) and uncomment line (8) in file somemodule/mod.py.

Does anybody know how to make module logger work without need to declare it in each function in module? And what is the cause of so strange behavior in this simple example?

P.S. My environment:
Python 2.7.6
tornado==3.1.1
","You need to change the call to fileConfig to fileConfig(""logging.ini"", disable_existing_loggers=False) as documented here.
",,,false,,,
https://stackoverflow.com/questions/38664498,true,"The issue involves freezing up of the Neo4j time tree, indicating a potential problem with memory usage or transaction management.",Neo4j,TimeTree,The Neo4j time tree freezes up after rebuilding it and populating it with a large number of records. The freezing behavior suggests a potential problem with memory usage or transaction management during the population process.,The Neo4j time tree should handle the population of records smoothly without freezing or becoming unresponsive.,"The issue is triggered when rebuilding and populating the Neo4j time tree with a large number of records, resulting in freezing or unresponsiveness.","This issue might be challenging to detect during development and testing because it involves the interaction between the Neo4j time tree and the specific data population process, which may require in-depth knowledge of Neo4j and its memory management.",What are reasons why the neo4j time tree would be freezing up?,"I'm having a strange issue in Neo4j. I have been recently working with the GraphAware TimeTree and populating it was going well until yesterday. I had to rebuild the tree because of an error I had made so I left a script running overnight (nohup).

Coming back today, I found that my script only ran for 3 minutes!

$ ps aux | grep timetreepop
root     21840  0.0  0.0 195952  2816 ?        S    Jul28   0:00 sudo nohup python timetreepop.py
root     21841  0.0  0.2 381416 75016 ?        S    Jul28   0:03 python timetreepop.py


I noticed this behavior while I was working, but I figured leaving it overnight while I'm not active would be help. I also turned off my other java server process in case of possible contention. At this point my server would only be running a python tornado server in the background which isn't very hefty, and doesn't get much traffic (couple hits a day).

All in all there is plenty of available RAM in my system, the CPUs are not being utilized elsewhere, and there are no other processes running heavy IO on my machine. Using top / atop show a healthy system with available resources.

Here is a high level of what my script is doing:

neo = neopop.handler()
for i, meta_id in enumerate(meta_gen(ship='KAOU')):
   neo.populate_timetree(record=meta_id)


My handler creates the driver and session in the __init__ constructor:

self.driver = graphdb.driver(config.NEO4J_HTTP_LINK)
self.session = self.driver.session()


My generator provides meta_id values which are unique property values in the nodes in my graph.

The populate_timetree() function creates the following statement:

MATCH (r:record {meta:""KAQP_20120101v20001_0001""}) WITH r 
CALL ga.timetree.events.attach({node: r, time: r.time, relationshipType: ""observedOn"", resolution:""Minute""})
YIELD node RETURN node.meta;


Everything was working fine my first go around. After messing up my time values, I deleted the database, restarted, and tried again. Only this time, my program freezes when I make the call to close the session:

neo.session.close()


Note: I actually call this in my __del__ deconstructor, (which I know is probably considered bad practice but it has been working for me so far and suits my needs.)

I have double checked all my code for rogue readline statements / anything that could cause it to be pausing. Also recompiled my package with all this code in it. I know for a fact it is getting stuck at this session.close() statement.

So I tried playing with the Neo4j-shell tool to see if anything was different.
First a quick responsiveness check:

$ neoc 'schema'
Indexes
  ON :record(meta) ONLINE (for uniqueness constraint) 

Constraints
  ON (record:record) ASSERT record.meta IS UNIQUE


Ok all good. Now I try the timetree call for a single value:

$ ./neo4j-shell -c '
&gt; MATCH (r:record {meta:""KAOU_20110613v20001_0000""}) WITH r 
&gt; CALL ga.timetree.events.attach({node: r, time: r.time, relationshipType: ""observedOn"", resolution:""Minute""})
&gt; YIELD node RETURN node.meta;
&gt; '


BOOM. Neo4j is stuck! To be clear, I know the MATCH statement is not taking forever here, because I only put in about 2 million nodes into my db this time, and calling the match statement alone runs absolutely fine. I even have an index set up for this unique property (see schema call above).

So what is going on here? I figure the initial creation of the tree shouldn't be too much of an issue if I am only inserting a single node. My first try seem to run flawlessly. I'm not sure what I have done different this time except populate my database with 2 million out of the 58 million records I had. (So it should be way faster).

I actually left this command running for a few hours, and it also had only run for a few minutes on my system. I am so confused as to what is happening here. Does anyone have any ideas? Is there anyway I can see what neo4j is actively doing about this command? (Keep in mind I am using community edition)

I am running version 3.0.3 of neo4j and, 3.0.3.39 timetree / graphaware on and CentOS 7 server.

The only idea I have, is that I have been calling cypher statements and cancelling them before they commit so many times, whether it be through python or the cmd line shell tool. Is it possible I am messing with the transaction manager too much by cancelling big transactions before they finish?

For example:

/neo4j-shell -c 'MATCH (r:record) CALL ga.timetree.events.attach(....) ....'


And then hit control-C after it runs for about 2 hours without finishing.



UPDATE:

Ok so I investigated my log files and found some issues. It seems my threads were being blocked due to not having enough memory. I guess my max heap size was being artifically limited and not using the available resources on my machine. (Maybe??). So I manually set dbms.memory.heap.max_size=16000.

The problem seems to go away now! I guess I was confused, since I would expect a Java OOM to appear in the response of the neo4j-shell tool instead of idling as if making progress.
","$ ./neo4j-shell -c '
&gt; MATCH (r:record {meta:""KAOU_20110613v20001_0000""}) WITH r 
&gt; CALL ga.timetree.events.attach({node: r, time: r.time, relationshipType: ""observedOn"", resolution:""Minute""})
&gt; YIELD node RETURN node.meta;
&gt; '


How many nodes is this query expected to attach to the timetree ?

This procedure doesn't use batching so it is limited to what your memory can do in a single transaction (this is now on our backlog).

That said, it generally doesn't make sense to attach manually the events to the tree, this procedure is there for convenience but we recommend to use automatic event attachments : 

https://github.com/graphaware/neo4j-timetree#automatic-event-attachment

In that way, re-attaching the complete db (or only the nodes configured for attachment) will be a matter of re-starting your database and this process will run with batch transactions, which will speed up significantly the time taken.
",,,false,,,
https://stackoverflow.com/questions/25328818,false,"The issue is related to the installation and configuration of Python and PIP on Windows, rather than a specific API exhibiting unexpected failures or behaviors.",,,,,,,python 2.7: cannot pip on windows &quot;bash: pip: command not found&quot;,"I am trying to install the SciPy stack located at https://scipy.org/stackspec.html [I am only allowed 2 links; trying to use them wisely].  I realize that there are much easier ways to do this, but I think there is a lot to be learned by doing it manually.  I am relatively new to a lot of this stuff, so I apologize if I sound ignorant at any point.
I am running  Windows 7 Enterprise - 64 bit.  Here is what I have done so far:

Installed python-2.7.8.msi (32-bit) from https://www.python.org/download/releases/2.7.8/

Installed numpy-1.8.1-win32-superpack-python2.7 from
http://sourceforge.net/projects/numpy/files/
Test: import numpy as np ---&gt; no errors

Installed scipy library,
scipy-0.14.0-win32-superpack-python2.7.exe from
(SCIPY DOT ORG LINK REMOVED)
Test: import scipy as sp ---&gt; no errors

Installed matplotlib: matplotlib-1.3.1.win32-py2.7.exe from
(MATPLOTLIB DOT ORG LINK REMOVED)

Installed PIP by running script here:
https://raw.githubusercontent.com/pypa/pip/master/contrib/get-pip.py
I just copied-pasted script to a new file in IDLE,
saved as C:\Python27\Scripts\pip_install.py and clicked Run&gt;module. No errors reported.


Does the path on which I saved
pip_install.py matter?




HERE IS WHERE I FAIL
Attempted to install matlibplot dependency dateutil: Opened a
Cygwin Shell, and typed
        cd C:\Python27          ! is it necessary to cd to python directtory?
        pip install python-dateutil

This results in the error:
    bash: pip: command not found

I get the same error attempting from cmd.
Any help  is appreciated; the closest I found was bash: pip: command not found.  But the OSX nature of it is just enough to confise me further.

UPDATE:
I added the pip-path per Paul H's suggestion below.  It made the error go away, but strangely, nothing I pip actually installs. For example, in Cygwin, I type:
cbennett2&gt; pip install python-dateutil
cbennett2&gt;                            

You can see that there is no output or feedback from the shell (which I think there should be).  Then when I go to a new python shell:
&gt;&gt;&gt; from dateutil.parser import parse
Traceback (most recent call last):
  File ""&lt;pyshell#12&gt;"", line 1, in &lt;module&gt;
    from dateutil.parser import parse
ImportError: No module named dateutil.parser
&gt;&gt;&gt;&gt;

This happens with all of the modules that I thought I had pip'd ... pandas, tornado, etc.
","On Windows, pip lives in C:\[pythondir]\scripts.

So you'll need to add that to your system path in order to run it from the command prompt. You could alternatively cd into that directory each time, but that's a hassle.

See the top answer here for info on how to do that:
Adding Python Path on Windows 7

Also, that is a terrifying way to install pip. Grab it from Christophe Gohlke. Grab everything else from there for that matter.
http://www.lfd.uci.edu/~gohlke/pythonlibs/ 
","As long as pip lives within the scripts folder you can run 

python -m pip .... 

This will tell python to get pip from inside the scripts folder. This is also a good way to have both python2.7 and pyhton3.5 on you computer and have them in different locations. I currently have both python2 and pyhton3 installed on windows. When I type python it defaults to python2. But if I type python3 I can use python3. (I also had to change the python.exe file for python3 to ""python3.exe"")If I need to install flask for python 2 I can run

python -m pip install flask

and it will be installed in the pyhton2 folder, but if I need flask for python 3 I run:

python3 -m pip install flask

and I now have it in the python3 folder
","
press [win] + Pause
Advanced settings
System variables
Append ;C:\python27\Scripts to the end of Path variable
Restart console

",false,,,
