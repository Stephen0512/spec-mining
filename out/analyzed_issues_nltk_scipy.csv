ISSUE_LINK,ai_verdict,REASON,LIBRARY_NAME,API_NAME,ISSUE_DESCRIPTION,NORMAL_CONDITIONS,TRIGGER_CONDITIONS,REASON_FOR_DIFFICULTY_IN_DETECTION,ISSUE_TITLE,ISSUE_BODY,ANSWER_1,ANSWER_2,ANSWER_3,HUMAN_CLASSIFICATION,HUMAN_REASON
https://stackoverflow.com/questions/8842817,false,The issue does not meet the criteria for deeper analysis as it does not involve an API exhibiting unexpected failures or unpredictable behaviors under specific runtime conditions.,,,,,,,Selecting the most fluent text from a set of possibilities via grammar checking (Python),"Some background

I am a literature student at New College of Florida, currently working on an overly ambitious creative project. The project is geared towards the algorithmic generation of poetry. It's written in Python. My Python knowledge and Natural Language Processing knowledge come only from teaching myself things through the internet. I've been working with this stuff for about a year, so I'm not helpless, but at various points I've had trouble moving forward in this project. Currently, I am entering the final phases of development, and have hit a little roadblock.

I need to implement some form of grammatical normalization, so that the output doesn't come out as un- conjugated/inflected caveman-speak. About a month ago some friendly folks on SO gave me some advice on how I might solve this issue by using an ngram language modeller, basically -- but I'm looking for yet other solutions, as it seems that NLTK's NgramModeler is not fit for my needs. (The possibilities of POS tagging were also mentioned, but my text may be too fragmentary and strange for an implementation of such to come easy, given my amateur-ness.)

Perhaps I need something like AtD, but hopefully less complex

I think need something that works like After the Deadline or Queequeg, but neither of these seem exactly right. Queequeg is probably not a good fit -- it was written in 2003 for Unix and I can't get it working on Windows for the life of me (have tried everything). But I like that all it checks for is proper verb conjugation and number agreement.

On the other hand, AtD is much more rigorous, offering more capabilities than I need. But I can't seem to get the python bindings for it working. (I get 502 errors from the AtD server, which I'm sure are easy to fix, but my application is going to be online, and I'd rather avoid depending on another server. I can't afford to run an AtD server myself, because the number of ""services"" my application is going to require of my web host is already threatening to cause problems in getting this application hosted cheaply.)

Things I'd like to avoid

Building Ngram language models myself doesn't seem right for the task. my application throws a lot of unknown vocabulary, skewing all the results. (Unless I use a corpus that's so large that it runs way too slow for my application -- the application needs to be pretty snappy.)

Strictly checking grammar is neither right for the task. the grammar doesn't need to be perfect, and the sentences don't have to be any more sensible than the kind of English-like jibberish that you can generate using ngrams. Even if it's jibberish, I just need to enforce verb conjugation, number agreement, and do things like remove extra articles.

In fact, I don't even need any kind of suggestions for corrections. I think all I need is for something to tally up how many errors seem to occur in each sentence in a group of possible sentences, so I can sort by their score and pick the one with the least grammatical issues.

A simple solution? Scoring fluency by detecting obvious errors

If a script exists that takes care of all this, I'd be overjoyed (I haven't found one yet). I can write code for what I can't find, of course; I'm looking for advice on how to optimize my approach.

Let's say we have a tiny bit of text already laid out:

existing_text = ""The old river""

Now let's say my script needs to figure out which inflection of the verb ""to bear"" could come next. I'm open to suggestions about this routine. But I need help mostly with step #2, rating fluency by tallying grammatical errors:


Use the Verb Conjugation methods in NodeBox Linguistics to come up with all conjugations of this verb; ['bear', 'bears', 'bearing', 'bore', 'borne'].
Iterate over the possibilities, (shallowly) checking the grammar of the string resulting from existing_text + "" "" + possibility (""The old river bear"", ""The old river bears"", etc). Tally the error count for each construction. In this case the only construction to raise an error, seemingly, would be ""The old river bear"".
Wrapping up should be easy... Of the possibilities with the lowest error count, select randomly.

","Very cool project, first of all. 

I found a java grammar checker. I've never used it but the docs claim it can run as a server. Both java and listening to a port should be supported basically anywhere. 

I'm just getting into NLP with a CS background so I wouldn't mind going into more detail to help you integrate whatever you decide on using. Feel free to ask for more detail.
","The pylinkgrammar link provided above is a bit out of date.  It points to version 0.1.9, and the code samples for that version no longer work.  If you go down this path, be sure to use the latest version which can be found at:

https://pypi.python.org/pypi/pylinkgrammar
","Another approach would be to use what is called an overgenerate and rank approach.  In the first step you have your poetry generator generate multiple candidate generations.  Then using a service like Amazon's Mechanical Turk to collect human judgments of fluency.  I would actually suggest collecting simultaneous judgments for a number of sentences generated from the same seed conditions.  Lastly, you extract features from the generated sentences (presumably using some form of syntactic parser) to train a model to rate or classify question quality.  You could even thrown in the heuristics listed above.

Michael Heilman uses this approach for question generation.  For more details, read these papers:
Good Question! Statistical Ranking for Question Generation and
Rating Computer-Generated Questions with Mechanical Turk.
",false,
https://stackoverflow.com/questions/40480839,true,"The issue involves the NLTK library and the extract_rels function, which is used for relation extraction from text. The issue is related to the behavior of the function when the input has fewer than three named entities (NEs).",NLTK,extract_rels,"The extract_rels function returns nothing when the input has fewer than three NEs. In the provided code, the input sentence 'Gates was born in Seattle on October 28, 1955' has only two NEs, resulting in no relations being extracted.",The extract_rels function is designed to work with inputs that have at least three NEs.,The issue is triggered when the input to extract_rels has fewer than three NEs.,This issue might be challenging to detect for users who are not familiar with the specific behavior of the extract_rels function or those who assume that it can handle inputs with any number of NEs.,NLTK relation extraction returns nothing,"I am recently working on using nltk to extract relation from text. so i build a sample text:"" Tom is the cofounder of Microsoft."" and using following program to test and return nothing. I cannot figure out why.

I'm using NLTK version: 3.2.1, python version: 3.5.2.

Here is my code:

import re
import nltk
from nltk.sem.relextract import extract_rels, rtuple
from nltk.tokenize import sent_tokenize, word_tokenize


def test():
    with open('sample.txt', 'r') as f:
        sample = f.read()   # ""Tom is the cofounder of Microsoft""

    sentences = sent_tokenize(sample)
    tokenized_sentences = [word_tokenize(sentence) for sentence in sentences]
    tagged_sentences = [nltk.tag.pos_tag(sentence) for sentence in tokenized_sentences]

    OF = re.compile(r'.*\bof\b.*')

    for i, sent in enumerate(tagged_sentences):
        sent = nltk.chunk.ne_chunk(sent) # ne_chunk method expects one tagged sentence
        rels = extract_rels('PER', 'GPE', sent, corpus='ace', pattern=OF, window=10) 
        for rel in rels:
            print('{0:&lt;5}{1}'.format(i, rtuple(rel)))

if __name__ == '__main__':
    test()




1.　After some debug, if found that when i changed the input as


  ""Gates was born in Seattle, Washington on October 28, 1955. ""


the nltk.chunk.ne_chunk() output is:


  (S
    (PERSON Gates/NNS)
    was/VBD
    born/VBN
    in/IN
    (GPE Seattle/NNP)
    ,/,
    (GPE Washington/NNP)
    on/IN
    October/NNP
    28/CD
    ,/,
    1955/CD
    ./.)


The test() returns:


  [PER: 'Gates/NNS'] 'was/VBD born/VBN in/IN' [GPE: 'Seattle/NNP']


2. After i changed the input as:


  ""Gates was born in Seattle on October 28, 1955. ""


The test() retuns nothing.

3. I digged into nltk/sem/relextract.py and find this strange

output is caused by function:
semi_rel2reldict(pairs, window=5, trace=False), which returns result only when len(pairs) &gt; 2, and that's why when one sentence with less than three NEs will return None.

Is this a bug or i used NLTK in wrong way?
","Firstly, to chunk NEs with ne_chunk, the idiom would look something like this 

&gt;&gt;&gt; from nltk import ne_chunk, pos_tag, word_tokenize
&gt;&gt;&gt; text = ""Tom is the cofounder of Microsoft""
&gt;&gt;&gt; chunked = ne_chunk(pos_tag(word_tokenize(text)))
&gt;&gt;&gt; chunked
Tree('S', [Tree('PERSON', [('Tom', 'NNP')]), ('is', 'VBZ'), ('the', 'DT'), ('cofounder', 'NN'), ('of', 'IN'), Tree('ORGANIZATION', [('Microsoft', 'NNP')])])


(see also https://stackoverflow.com/a/31838373/610569)

Next let's look at the extract_rels function.

def extract_rels(subjclass, objclass, doc, corpus='ace', pattern=None, window=10):
    """"""
    Filter the output of ``semi_rel2reldict`` according to specified NE classes and a filler pattern.
    The parameters ``subjclass`` and ``objclass`` can be used to restrict the
    Named Entities to particular types (any of 'LOCATION', 'ORGANIZATION',
    'PERSON', 'DURATION', 'DATE', 'CARDINAL', 'PERCENT', 'MONEY', 'MEASURE').
    """"""


When you evoke this function:

extract_rels('PER', 'GPE', sent, corpus='ace', pattern=OF, window=10)


It performs 4 processes sequentially.

1. It checks whether your subjclass and objclassare valid

i.e. https://github.com/nltk/nltk/blob/develop/nltk/sem/relextract.py#L202 :

if subjclass and subjclass not in NE_CLASSES[corpus]:
    if _expand(subjclass) in NE_CLASSES[corpus]:
        subjclass = _expand(subjclass)
    else:
        raise ValueError(""your value for the subject type has not been recognized: %s"" % subjclass)
if objclass and objclass not in NE_CLASSES[corpus]:
    if _expand(objclass) in NE_CLASSES[corpus]:
        objclass = _expand(objclass)
    else:
        raise ValueError(""your value for the object type has not been recognized: %s"" % objclass)


2. It extracts ""pairs"" from your NE tagged inputs:

if corpus == 'ace' or corpus == 'conll2002':
    pairs = tree2semi_rel(doc)
elif corpus == 'ieer':
    pairs = tree2semi_rel(doc.text) + tree2semi_rel(doc.headline)
else:
    raise ValueError(""corpus type not recognized"")


Now let's see given your input sentence Tom is the cofounder of Microsoft, what does tree2semi_rel() returns:

&gt;&gt;&gt; from nltk.sem.relextract import tree2semi_rel, semi_rel2reldict
&gt;&gt;&gt; from nltk import word_tokenize, pos_tag, ne_chunk
&gt;&gt;&gt; text = ""Tom is the cofounder of Microsoft""
&gt;&gt;&gt; chunked = ne_chunk(pos_tag(word_tokenize(text)))
&gt;&gt;&gt; tree2semi_rel(chunked)
[[[], Tree('PERSON', [('Tom', 'NNP')])], [[('is', 'VBZ'), ('the', 'DT'), ('cofounder', 'NN'), ('of', 'IN')], Tree('ORGANIZATION', [('Microsoft', 'NNP')])]]


So it returns a list of 2 lists, the first inner list consist of a blank list and the Tree that contains the ""PERSON"" tag.

[[], Tree('PERSON', [('Tom', 'NNP')])] 


The second list consist of the phrase is the cofounder of and the Tree that contains ""ORGANIZATION"".

Let's move on.

3. extract_rel then tries to change the pairs to some sort of relation dictionary

reldicts = semi_rel2reldict(pairs)


If we look what the semi_rel2reldict function returns with your example sentence, we see that this is where the empty list gets returns:

&gt;&gt;&gt; tree2semi_rel(chunked)
[[[], Tree('PERSON', [('Tom', 'NNP')])], [[('is', 'VBZ'), ('the', 'DT'), ('cofounder', 'NN'), ('of', 'IN')], Tree('ORGANIZATION', [('Microsoft', 'NNP')])]]
&gt;&gt;&gt; semi_rel2reldict(tree2semi_rel(chunked))
[]


So let's look into the code of semi_rel2reldict https://github.com/nltk/nltk/blob/develop/nltk/sem/relextract.py#L144:

def semi_rel2reldict(pairs, window=5, trace=False):
    """"""
    Converts the pairs generated by ``tree2semi_rel`` into a 'reldict': a dictionary which
    stores information about the subject and object NEs plus the filler between them.
    Additionally, a left and right context of length =&lt; window are captured (within
    a given input sentence).
    :param pairs: a pair of list(str) and ``Tree``, as generated by
    :param window: a threshold for the number of items to include in the left and right context
    :type window: int
    :return: 'relation' dictionaries whose keys are 'lcon', 'subjclass', 'subjtext', 'subjsym', 'filler', objclass', objtext', 'objsym' and 'rcon'
    :rtype: list(defaultdict)
    """"""
    result = []
    while len(pairs) &gt; 2:
        reldict = defaultdict(str)
        reldict['lcon'] = _join(pairs[0][0][-window:])
        reldict['subjclass'] = pairs[0][1].label()
        reldict['subjtext'] = _join(pairs[0][1].leaves())
        reldict['subjsym'] = list2sym(pairs[0][1].leaves())
        reldict['filler'] = _join(pairs[1][0])
        reldict['untagged_filler'] = _join(pairs[1][0], untag=True)
        reldict['objclass'] = pairs[1][1].label()
        reldict['objtext'] = _join(pairs[1][1].leaves())
        reldict['objsym'] = list2sym(pairs[1][1].leaves())
        reldict['rcon'] = _join(pairs[2][0][:window])
        if trace:
            print(""(%s(%s, %s)"" % (reldict['untagged_filler'], reldict['subjclass'], reldict['objclass']))
        result.append(reldict)
        pairs = pairs[1:]
    return result


The first thing that semi_rel2reldict() does is to check where there are more than 2 elements the output from tree2semi_rel(), which your example sentence doesn't:

&gt;&gt;&gt; tree2semi_rel(chunked)
[[[], Tree('PERSON', [('Tom', 'NNP')])], [[('is', 'VBZ'), ('the', 'DT'), ('cofounder', 'NN'), ('of', 'IN')], Tree('ORGANIZATION', [('Microsoft', 'NNP')])]]
&gt;&gt;&gt; len(tree2semi_rel(chunked))
2
&gt;&gt;&gt; len(tree2semi_rel(chunked)) &gt; 2
False


Ah ha, that's why the extract_rel is returning nothing.

Now comes the question of how to make extract_rel() return something even with 2 elements from tree2semi_rel()? Is that even possible?

Let's try a different sentence:

&gt;&gt;&gt; text = ""Tom is the cofounder of Microsoft and now he is the founder of Marcohard""
&gt;&gt;&gt; chunked = ne_chunk(pos_tag(word_tokenize(text)))
&gt;&gt;&gt; chunked
Tree('S', [Tree('PERSON', [('Tom', 'NNP')]), ('is', 'VBZ'), ('the', 'DT'), ('cofounder', 'NN'), ('of', 'IN'), Tree('ORGANIZATION', [('Microsoft', 'NNP')]), ('and', 'CC'), ('now', 'RB'), ('he', 'PRP'), ('is', 'VBZ'), ('the', 'DT'), ('founder', 'NN'), ('of', 'IN'), Tree('PERSON', [('Marcohard', 'NNP')])])
&gt;&gt;&gt; tree2semi_rel(chunked)
[[[], Tree('PERSON', [('Tom', 'NNP')])], [[('is', 'VBZ'), ('the', 'DT'), ('cofounder', 'NN'), ('of', 'IN')], Tree('ORGANIZATION', [('Microsoft', 'NNP')])], [[('and', 'CC'), ('now', 'RB'), ('he', 'PRP'), ('is', 'VBZ'), ('the', 'DT'), ('founder', 'NN'), ('of', 'IN')], Tree('PERSON', [('Marcohard', 'NNP')])]]
&gt;&gt;&gt; len(tree2semi_rel(chunked)) &gt; 2
True
&gt;&gt;&gt; semi_rel2reldict(tree2semi_rel(chunked))
[defaultdict(&lt;type 'str'&gt;, {'lcon': '', 'untagged_filler': 'is the cofounder of', 'filler': 'is/VBZ the/DT cofounder/NN of/IN', 'objsym': 'microsoft', 'objclass': 'ORGANIZATION', 'objtext': 'Microsoft/NNP', 'subjsym': 'tom', 'subjclass': 'PERSON', 'rcon': 'and/CC now/RB he/PRP is/VBZ the/DT', 'subjtext': 'Tom/NNP'})]


But that only confirms that extract_rel can't extract when tree2semi_rel returns pairs of &lt; 2. What happens if we remove that condition of while len(pairs) &gt; 2?

Why can't we do while len(pairs) &gt; 1?

If we look closer into the code, we see the last line of populating the reldict, https://github.com/nltk/nltk/blob/develop/nltk/sem/relextract.py#L169:

reldict['rcon'] = _join(pairs[2][0][:window])


It tries to access a 3rd element of the pairs and if the length of the pairs is 2, you'll get an IndexError.

So what happens if we remove that rcon key and simply change it to while len(pairs) &gt;= 2?

To do that we have to override the semi_rel2redict() function:

&gt;&gt;&gt; from nltk.sem.relextract import _join, list2sym
&gt;&gt;&gt; from collections import defaultdict
&gt;&gt;&gt; def semi_rel2reldict(pairs, window=5, trace=False):
...     """"""
...     Converts the pairs generated by ``tree2semi_rel`` into a 'reldict': a dictionary which
...     stores information about the subject and object NEs plus the filler between them.
...     Additionally, a left and right context of length =&lt; window are captured (within
...     a given input sentence).
...     :param pairs: a pair of list(str) and ``Tree``, as generated by
...     :param window: a threshold for the number of items to include in the left and right context
...     :type window: int
...     :return: 'relation' dictionaries whose keys are 'lcon', 'subjclass', 'subjtext', 'subjsym', 'filler', objclass', objtext', 'objsym' and 'rcon'
...     :rtype: list(defaultdict)
...     """"""
...     result = []
...     while len(pairs) &gt;= 2:
...         reldict = defaultdict(str)
...         reldict['lcon'] = _join(pairs[0][0][-window:])
...         reldict['subjclass'] = pairs[0][1].label()
...         reldict['subjtext'] = _join(pairs[0][1].leaves())
...         reldict['subjsym'] = list2sym(pairs[0][1].leaves())
...         reldict['filler'] = _join(pairs[1][0])
...         reldict['untagged_filler'] = _join(pairs[1][0], untag=True)
...         reldict['objclass'] = pairs[1][1].label()
...         reldict['objtext'] = _join(pairs[1][1].leaves())
...         reldict['objsym'] = list2sym(pairs[1][1].leaves())
...         reldict['rcon'] = []
...         if trace:
...             print(""(%s(%s, %s)"" % (reldict['untagged_filler'], reldict['subjclass'], reldict['objclass']))
...         result.append(reldict)
...         pairs = pairs[1:]
...     return result
... 
&gt;&gt;&gt; text = ""Tom is the cofounder of Microsoft""
&gt;&gt;&gt; chunked = ne_chunk(pos_tag(word_tokenize(text)))
&gt;&gt;&gt; tree2semi_rel(chunked)
[[[], Tree('PERSON', [('Tom', 'NNP')])], [[('is', 'VBZ'), ('the', 'DT'), ('cofounder', 'NN'), ('of', 'IN')], Tree('ORGANIZATION', [('Microsoft', 'NNP')])]]
&gt;&gt;&gt; semi_rel2reldict(tree2semi_rel(chunked))
[defaultdict(&lt;type 'str'&gt;, {'lcon': '', 'untagged_filler': 'is the cofounder of', 'filler': 'is/VBZ the/DT cofounder/NN of/IN', 'objsym': 'microsoft', 'objclass': 'ORGANIZATION', 'objtext': 'Microsoft/NNP', 'subjsym': 'tom', 'subjclass': 'PERSON', 'rcon': [], 'subjtext': 'Tom/NNP'})]


Ah! It works but there's still a 4th step in extract_rels().

4. It performs a filter of the reldict given the regex you have provided to the pattern parameter, https://github.com/nltk/nltk/blob/develop/nltk/sem/relextract.py#L222:

relfilter = lambda x: (x['subjclass'] == subjclass and
                       len(x['filler'].split()) &lt;= window and
                       pattern.match(x['filler']) and
                       x['objclass'] == objclass)


Now let's try it with the hacked version of semi_rel2reldict:

&gt;&gt;&gt; text = ""Tom is the cofounder of Microsoft""
&gt;&gt;&gt; chunked = ne_chunk(pos_tag(word_tokenize(text)))
&gt;&gt;&gt; tree2semi_rel(chunked)
[[[], Tree('PERSON', [('Tom', 'NNP')])], [[('is', 'VBZ'), ('the', 'DT'), ('cofounder', 'NN'), ('of', 'IN')], Tree('ORGANIZATION', [('Microsoft', 'NNP')])]]
&gt;&gt;&gt; semi_rel2reldict(tree2semi_rel(chunked))
[defaultdict(&lt;type 'str'&gt;, {'lcon': '', 'untagged_filler': 'is the cofounder of', 'filler': 'is/VBZ the/DT cofounder/NN of/IN', 'objsym': 'microsoft', 'objclass': 'ORGANIZATION', 'objtext': 'Microsoft/NNP', 'subjsym': 'tom', 'subjclass': 'PERSON', 'rcon': [], 'subjtext': 'Tom/NNP'})]
&gt;&gt;&gt; 
&gt;&gt;&gt; pattern = re.compile(r'.*\bof\b.*')
&gt;&gt;&gt; reldicts = semi_rel2reldict(tree2semi_rel(chunked))
&gt;&gt;&gt; relfilter = lambda x: (x['subjclass'] == subjclass and
...                            len(x['filler'].split()) &lt;= window and
...                            pattern.match(x['filler']) and
...                            x['objclass'] == objclass)
&gt;&gt;&gt; relfilter
&lt;function &lt;lambda&gt; at 0x112e591b8&gt;
&gt;&gt;&gt; subjclass = 'PERSON'
&gt;&gt;&gt; objclass = 'ORGANIZATION'
&gt;&gt;&gt; window = 5
&gt;&gt;&gt; list(filter(relfilter, reldicts))
[defaultdict(&lt;type 'str'&gt;, {'lcon': '', 'untagged_filler': 'is the cofounder of', 'filler': 'is/VBZ the/DT cofounder/NN of/IN', 'objsym': 'microsoft', 'objclass': 'ORGANIZATION', 'objtext': 'Microsoft/NNP', 'subjsym': 'tom', 'subjclass': 'PERSON', 'rcon': [], 'subjtext': 'Tom/NNP'})]


It works! Now let's see it in tuple form:

&gt;&gt;&gt; from nltk.sem.relextract import rtuple
&gt;&gt;&gt; rels = list(filter(relfilter, reldicts))
&gt;&gt;&gt; for rel in rels:
...     print rtuple(rel)
... 
[PER: 'Tom/NNP'] 'is/VBZ the/DT cofounder/NN of/IN' [ORG: 'Microsoft/NNP']

",,,false,
https://stackoverflow.com/questions/62735456,true,"The issue involves the TensorFlow API tf.image.rgb_to_grayscale, which exhibits unexpected behavior when applied to an input tensor with a shape that does not end in 3 (i.e., not an RGB image).",TensorFlow,tf.image.rgb_to_grayscale,"When applying tf.image.rgb_to_grayscale on a segmentation mask (which typically has a single channel), TensorFlow raises a ""dimensions must be equal"" error. This is because the API expects an input tensor with three channels (RGB), but the input mask only has one channel.","The tf.image.rgb_to_grayscale is applied to a tensor with a shape that ends in 3 (i.e., an RGB image).","The issue is triggered when tf.image.rgb_to_grayscale is applied to a tensor with a shape that does not end in 3 (i.e., not an RGB image).",This issue might be challenging to detect for users unfamiliar with the specific requirements of the tf.image.rgb_to_grayscale function or those who assume that it can handle inputs with any number of channels.,Understanding and using Coreference resolution Stanford NLP tool (in Python 3.7),"I am trying to understand the Coreference NLP Stanford tools.
This is my code and it is working:
import os
os.environ[""CORENLP_HOME""] = ""/home/daniel/StanfordCoreNLP/stanford-corenlp-4.0.0""

from stanza.server import CoreNLPClient

text = 'When he came from Brazil, Daniel was fortiﬁed with letters from Conan but otherwise did not know a soul except Herbert. Yet this giant man from the Northeast, who had never worn an overcoat or experienced a change of seasons, did not seem surprised by his past.'

with CoreNLPClient(annotators=['tokenize','ssplit','pos','lemma','ner', 'parse', 'depparse','coref'],
               properties={'annotators': 'coref', 'coref.algorithm' : 'neural'},timeout=30000, memory='16G') as client:

    ann = client.annotate(text)

chains = ann.corefChain
chain_dict=dict()
for index_chain,chain in enumerate(chains):
    chain_dict[index_chain]={}
    chain_dict[index_chain]['ref']=''
    chain_dict[index_chain]['mentions']=[{'mentionID':mention.mentionID,
                                          'mentionType':mention.mentionType,
                                          'number':mention.number,
                                          'gender':mention.gender,
                                          'animacy':mention.animacy,
                                          'beginIndex':mention.beginIndex,
                                          'endIndex':mention.endIndex,
                                          'headIndex':mention.headIndex,
                                          'sentenceIndex':mention.sentenceIndex,
                                          'position':mention.position,
                                          'ref':'',
                                          } for mention in chain.mention ]


for k,v in chain_dict.items():
    print('key',k)
    mentions=v['mentions']
    for mention in mentions:
        words_list = ann.sentence[mention['sentenceIndex']].token[mention['beginIndex']:mention['endIndex']]
        mention['ref']=' '.join(t.word for t in words_list)
        print(mention['ref'])
    

I tried three algorithms:

statistical (as in the code above). Results:


he
this giant man from the Northeast , who had never worn an overcoat or experienced a change of seasons
Daniel
his



neural


this giant man from the Northeast , who had never worn an overcoat or experienced a change of seasons ,
his



deterministic (I got the error below)
 &gt; Starting server with command: java -Xmx16G -cp
 &gt; /home/daniel/StanfordCoreNLP/stanford-corenlp-4.0.0/*
 &gt; edu.stanford.nlp.pipeline.StanfordCoreNLPServer -port 9000 -timeout
 &gt; 30000 -threads 5 -maxCharLength 100000 -quiet True -serverProperties
 &gt; corenlp_server-9fedd1e9dfb14c9e.props -preload
 &gt; tokenize,ssplit,pos,lemma,ner,parse,depparse,coref Traceback (most
 &gt; recent call last):
 &gt; 
 &gt;   File ""&lt;ipython-input-58-0f665f07fd4d&gt;"", line 1, in &lt;module&gt;
 &gt;     runfile('/home/daniel/Documentos/Working Papers/Leader traits/Code/20200704 - Modeling
 &gt; Organizing/understanding_coreference.py',
 &gt; wdir='/home/daniel/Documentos/Working Papers/Leader
 &gt; traits/Code/20200704 - Modeling Organizing')
 &gt; 
 &gt;   File
 &gt; ""/home/daniel/anaconda3/lib/python3.7/site-packages/spyder_kernels/customize/spydercustomize.py"",
 &gt; line 827, in runfile
 &gt;     execfile(filename, namespace)
 &gt; 
 &gt;   File
 &gt; ""/home/daniel/anaconda3/lib/python3.7/site-packages/spyder_kernels/customize/spydercustomize.py"",
 &gt; line 110, in execfile
 &gt;     exec(compile(f.read(), filename, 'exec'), namespace)
 &gt; 
 &gt;   File ""/home/daniel/Documentos/Working Papers/Leader
 &gt; traits/Code/20200704 - Modeling
 &gt; Organizing/understanding_coreference.py"", line 21, in &lt;module&gt;
 &gt;     ann = client.annotate(text)
 &gt; 
 &gt;   File
 &gt; ""/home/daniel/anaconda3/lib/python3.7/site-packages/stanza/server/client.py"",
 &gt; line 470, in annotate
 &gt;     r = self._request(text.encode('utf-8'), request_properties, **kwargs)
 &gt; 
 &gt;   File
 &gt; ""/home/daniel/anaconda3/lib/python3.7/site-packages/stanza/server/client.py"",
 &gt; line 404, in _request
 &gt;     raise AnnotationException(r.text)
 &gt; 
 &gt; AnnotationException: java.lang.RuntimeException:
 &gt; java.lang.IllegalArgumentException: No enum constant
 &gt; edu.stanford.nlp.coref.CorefProperties.CorefAlgorithmType.DETERMINISTIC



Questions:

Why am I getting this error with the deterministic?

Any piece of code using the NLP Stanford in Python seems to be much slower than the codes related with Spacy or NLTK. I know that there is no coreference in these other libraries. But for instance when I use import nltk.parse.stanford import StanfordDependencyParser for dependence parse it is much faster then this StanfordNLP library. Is there any way to acelerate this CoreNLPClient in Python?

I will use this library to work with long texts. Is it better to work with smaller pieces with the entire text? Long texts can cause wrong results for coreference resolution (I have found very strange results for this coreference library when I am using long texts)? Is there an optimal size?

Results:


The results from the statistical algorithm seems to be better. I expected that the best result would come from the neural algorithm. Do you agree with me? There are 4 valid mention in the statistical algorithm while only 2 when I am using the neural algorithm.
Am I missing something?
","
You may find the list of supported algorithms in Java documentation: link

You might want to start the server and then just use it, something like
# Here's the slowest part—models are being loaded
client = CoreNLPClient(...)

ann = client.annotate(text)

...

client.stop()



But I cannot give you any clue regarding 3 and 4.
",,,false,
https://stackoverflow.com/questions/22591174/pandas-multiple-conditions-while-indexing-data-frame-unexpected-behavior,false,The behavior is due to a misunderstanding of logical operators and conditions in pandas rather than a problem with the API itself.,,,,,,,,,,,,false,
https://stackoverflow.com/questions/48030920,false,The behavior is not related to an API exhibiting unexpected failures or unpredictable behaviors under specific runtime conditions.,,,,,,,"Tweet Feels: Always returns the same Sentiment Score, regardless tags","I am trying to use this library to generate sentiment score for cryptocurrencies:

https://github.com/uclatommy/tweetfeels/blob/master/README.md

When I use the code from the example trump, it returns a sentiment score of -0.00082536637608123106.

I have changed the tags to the following:

btc_feels = TweetFeels(login, tracking=['bitcoin'])
btc_feels.start(20)
btc_feels.sentiment.value


and it still gives me the same value.

I did notice something strange when I installed the library.

from the instructions:


  If for some reason pip did not install the vader lexicon:
  
  
    python3 -m nltk.downloader vader_lexicon
  


When I ran this, I got: 


  /anaconda/lib/python3.6/runpy.py:125: RuntimeWarning:
  'nltk.downloader' found in sys.modules after import of package 'nltk',
  but prior to execution of 'nltk.downloader'; this may result in
  unpredictable behaviour   warn(RuntimeWarning(msg))


Could this be why it appears not to be working?
","By default, tweetfeels creates a database in your current directory. The next time you start the program, it will continue using the same database, and pick up where it left off. I don't know what tweetfeels does to handle you changing the keyword on it, but this behaviour of tweetfeels could be a problem. The solution would be to use a different database for different keywords, and then pass in the location of your database to the TweetFeels constructor.

I don't know that much about Tweetfeels, it just sounded interesting, so I've downloaded the project, and I have a working script that will perform the sentiment analysis on any keyword I give it. I can add a copy of the script here, if you're still having problems getting TweetFeels to work.



Edit: here the script I am using

I am currently having the following problems with the script.

1) I was getting some error that was different from the one you'd got, but I was able to fix the issue by replacing the tweetfeels library from pip with the latest code in their Github repository.

2) If a sentiment value does not get reported, sometimes tweetfeels fails to come to a complete stop, without forcefully sending a ctrl+c keyboard interrupt.

import os, sys, time
from threading import Thread
from pathlib import Path

from tweetfeels import TweetFeels

consumer_key = 'em...'
consumer_secret = 'aF...'
access_token = '25...'
access_token_secret = 'd3...'
login = [consumer_key, consumer_secret, access_token, access_token_secret]

try:
    kw = sys.argv[1]
except IndexError:
    kw = ""iota""

try:
    secs = int(sys.argv[2])
except IndexError:
    secs = 15

for arg in sys.argv:
    if (arg == ""-h"" or arg == ""--help""):
        print(""Gets sentiment from twitter.\n""
              ""Pass in a search term, and how frequently you would like the sentiment recalculated (defaults to 15 seconds).\n""
              ""The keyword can be a comma seperated list of keywords to look at."")
        sys.exit(0)

db = Path(f""~/tweetfeels/{kw}.sqlite"").expanduser()
if db.exists():
    print(""existing db detected. Continueing from where the last sentiment stream left off"")
else:
    #ensure the parent folder exists, the db will be created inside of this folder
    Path(f""~/tweetfeels"").expanduser().mkdir(exist_ok=True)

feels = TweetFeels(login, tracking=kw.split("",""), db=str(db))

go_on = True
def print_feels(feels, seconds):
    while go_on:
        if feels.sentiment:
            print(f""{feels.sentiment.volume} tweets analyzed from {feels.sentiment.start} to {feels.sentiment.end}"")
            print(f'[{time.ctime()}] Sentiment Score: {feels.sentiment.value}')
            print(flush=True)
        else:
            print(f""The datastream has not reported a sentiment value."")
            print(f""It takes a little bit for the first tweets to be analyzed (max of {feels._stream.retry_time_cap + seconds} seconds)."")
            print(""If this problem persists, there may not be anyone tweeting about the keyword(s) you used"")
            print(flush=True)
        time.sleep(seconds)


t = Thread(target=print_feels, kwargs={""feels"":feels,""seconds"":secs}, daemon=True)
print(f'Twitter posts containing the keyword(s) ""{kw}"" will be streamed, and a new sentiment value will be recalculated every {secs} seconds')
feels.start()
time.sleep(5)
t.start()

try:
    input(""Push enter at any time to stop the feed...\n\n"")
except (Exception, KeyboardInterrupt) as e:
    feels.stop()
    raise e

feels.stop()
go_on = False
print(f""Stopping feed. It may take up to {feels._stream.retry_time_cap} for the feed to shut down.\n"")
#we're waiting on the feels thread to stop

","No, the same sentiment value that you see printed is not related to the warning you've got when downloading the dataset.

The problem with the same sentiment score is coming from these lines:

for s in sentiments:
    pass
return s


I suspect that this unbound variable s remembers the previous value of the sentiment score.

But, the problem itself is that you are printing out the score right after you execute the start() function which starts a multi-threaded program to constantly update data from twitter - you should not expect the sentiment score to arrive right after you started the update. 

Note that the examples in the README are shown from the Python terminal where they wait after the execution of start() function until the Timer completed. Disconnecting now... message appears.
",,false,
https://stackoverflow.com/questions/63508107,false,The behavior is not related to an API exhibiting unexpected failures or unpredictable behaviors under specific runtime conditions.,,,,,,,"What causes the problem: csv, pandas or nltk?","I have a strange problem resulting in wrong output delivered by NLTK collocations. In short, when I pass pandas object created in python envi (PyCharm or Jupyter) to the function I get correct result. When I save this object to csv and upload it to the pandas object, functions returns single letters and/or numbers instead of full words. Must be sth wrong with csv upload through pandas but I have no idea what is wrong...
here is the code.
Function that is applied:
def counts(x):
    trigram_measures = nltk.collocations.BigramAssocMeasures()
    finder = BigramCollocationFinder.from_documents(x)
    finder.nbest(trigram_measures.pmi, 100)

    s = pd.Series(x)
    ngram_list = [pair for row in s for pair in ngrams(row, 3)]
    c = Counter(ngram_list).most_common(3)

    return pd.DataFrame([(x.name, ) + element for element in c], columns=['group', 'Ngram', 'Frequency'])

Here is the object:
d = {'words' : pd.Series((['coffee', 'maker', 'brewing', 'properly', '2', '420', '420', '420'],
    ['galley', 'work', 'table', 'stuck'],
    ['cloth', 'stuck'],
    ['stuck', 'coffee'])),
    'group' : pd.Series([1, 2, 1, 2])}
df_cleaned = pd.DataFrame(d)

Then I apply function from above + some extra functions:
output = df_cleaned.groupby('group', as_index=False).words.apply(counts).reset_index(drop=True)

Result is correct:

But when pandas object is saved and uploaded result is sth like this:
here is a code for saving and uploading:
df.to_csv('test_file.csv', index=False, sep=',')

df = pd.read_csv('path/test_file.csv',
sep=',', usecols=['group','words']) 

I found quotes in uploaded pandas object therefore I had removed them before applying the fucntion""
df = df.replace({'\'': ''}, regex=True)

output = df_cleaned.groupby('group', as_index=False).words.apply(counts).reset_index(drop=True)

Now it returns wrong results.

Do have any suggestions which way shall I go?
","I reproduced what you described in the following steps. I don't see any errors
import pandas as pd
d = {'words' : pd.Series((['coffee', 'maker', 'brewing', 'properly', '2','420', '420', '420'],
    ['galley', 'work', 'table', 'stuck'],
    ['cloth', 'stuck'],
    ['stuck', 'coffee'])),
    'group' : pd.Series([1, 2, 1, 2])}
df_cleaned = pd.DataFrame(d)
df_cleaned

The function you're using is
import nltk
from nltk.util import ngrams
from nltk.collocations import *
from collections import Counter
def counts(x):
    trigram_measures = nltk.collocations.BigramAssocMeasures()
    finder = BigramCollocationFinder.from_documents(x)
    finder.nbest(trigram_measures.pmi, 100)
    s = pd.Series(x)
    ngram_list = [pair for row in s for pair in ngrams(row, 3)]
    c = Counter(ngram_list).most_common(3)
    return pd.DataFrame([(x.name, ) + element for element in c], columns=['group', 'Ngram', 'Frequency'])

You then apply counts to the data
output = df_cleaned.groupby('group', 
            as_index=False).words.apply(counts).reset_index(drop=True)

and save the results to file
output.to_csv('test_file.csv', index=False, sep=',')
df = pd.read_csv('test_file.csv',sep=',')

I don't see any problems
",,,false,
https://stackoverflow.com/questions/39603633,false,The behavior is not related to an API exhibiting unexpected failures or unpredictable behaviors under specific runtime conditions.,,,,,,,nltk semantic word substitution,"I'm trying to find different ways of writing ""events in [city]"" which are semantically similar. I am trying to do this by finding words that are semantically similar to ""events"" so I can substitute them in. 

To find these words I'm using nltk's wordnet corpus, but I'm getting some pretty strange results. For example, using the hyponyms of 'event.n.01', I'm getting ""Miracles in Ottawa"". 

co-hyponyms and hypernyms seem just as bad or worse. I wonder if anyone understands the structure better and can offer a potential solution?

Here's some sample code:

!/usr/bin/python3

import nltk

lemma = 'event.n.01'
synset = nltk.corpus.wordnet.synset(lemma)

print(""%s: %s"" % (synset.name(), synset.definition()))

print(""\nFinding hyponyms..."")
print([s.split('.')[0] for w in synset.hyponyms() for s in w.lemma_names()])

print(""\nFinding hypernym paths..."")
print([s.split('.')[0] for hyprs in synset.hypernym_paths() for hypr in hyprs for s in hypr.lemma_names()])

print(""\nFinding co-hyponyms..."")
for hypers in synset.hypernym_paths():
        for hyper in hypers:
                print(hyper.name())
                for hypos in hyper.hyponyms():
                        print(""\t%s"" % (', '.join(hypos.lemma_names())))

print(synset.similar())

","You can take a deep learning approach. Train a word2vec model and get the most similar vectors to the ""event"" vector.

You can test a model here Word2Vec Demo
","The hyponyms of ""event"" are types of ""event"". One of them is ""miracle"", some others are:

&gt;&gt;&gt; [s for w in synset.hyponyms() for s in w.lemma_names][:7]  # is 7 enough? :)
['zap', 'act', 'deed', 'human_action', 'human_activity', 'happening', 'occurrence']


""Event's"" hypernyms are the oposite. Terms that ""event"" is a type of:

&gt;&gt;&gt; synset.hypernyms()
[Synset('psychological_feature.n.01')]


You can see that ""event"" is one of it's hyponyms:

&gt;&gt;&gt; synset.hypernyms()[0].hyponyms()
[Synset('motivation.n.01'), Synset('cognition.n.01'), Synset('event.n.01')]


Those are not really ""similar"" terms (""Psychological features in Ottawa"" may seem like a correct result to a robot, but not to humans).

Perhaps it is better to go at it from a completely different angle, e.g.

&gt;&gt;&gt; text = nltk.Text(word.lower() for word in nltk.corpus.brown.words())
&gt;&gt;&gt; text.similar('event')
time day man order state way case house one place action night point
situation work year act and area audience


Now, take those and sort them e.g. by path_similarity:

&gt;&gt;&gt; words = 'time day man order state way case house one place action night point'\
...         ' situation work year act and area audience'.split()
&gt;&gt;&gt; 
&gt;&gt;&gt; def get_symilarity(synset, word):
...     return max([synset.path_similarity(synset2)
...                for synset2 in nltk.corpus.wordnet.synsets(word)]+[0])
&gt;&gt;&gt; 
&gt;&gt;&gt; sorted(words, key=lambda w: get_symilarity(synset, w), reverse=True)[:5]
['act', 'case', 'action', 'time', 'way']


Is that a good result? I don't know. I guess it could work: ""Acts in Ottawa"", ""Cases in New York"", ""Action in Rome"", ""Time in Tokyo"", ""Ways in Amsterdam""...
",,false,
https://stackoverflow.com/questions/63508107,false,The behavior is not related to an API exhibiting unexpected failures or unpredictable behaviors under specific runtime conditions.,,,,,,,"What causes the problem: csv, pandas or nltk?","I have a strange problem resulting in wrong output delivered by NLTK collocations. In short, when I pass pandas object created in python envi (PyCharm or Jupyter) to the function I get correct result. When I save this object to csv and upload it to the pandas object, functions returns single letters and/or numbers instead of full words. Must be sth wrong with csv upload through pandas but I have no idea what is wrong...
here is the code.
Function that is applied:
def counts(x):
    trigram_measures = nltk.collocations.BigramAssocMeasures()
    finder = BigramCollocationFinder.from_documents(x)
    finder.nbest(trigram_measures.pmi, 100)

    s = pd.Series(x)
    ngram_list = [pair for row in s for pair in ngrams(row, 3)]
    c = Counter(ngram_list).most_common(3)

    return pd.DataFrame([(x.name, ) + element for element in c], columns=['group', 'Ngram', 'Frequency'])

Here is the object:
d = {'words' : pd.Series((['coffee', 'maker', 'brewing', 'properly', '2', '420', '420', '420'],
    ['galley', 'work', 'table', 'stuck'],
    ['cloth', 'stuck'],
    ['stuck', 'coffee'])),
    'group' : pd.Series([1, 2, 1, 2])}
df_cleaned = pd.DataFrame(d)

Then I apply function from above + some extra functions:
output = df_cleaned.groupby('group', as_index=False).words.apply(counts).reset_index(drop=True)

Result is correct:

But when pandas object is saved and uploaded result is sth like this:
here is a code for saving and uploading:
df.to_csv('test_file.csv', index=False, sep=',')

df = pd.read_csv('path/test_file.csv',
sep=',', usecols=['group','words']) 

I found quotes in uploaded pandas object therefore I had removed them before applying the fucntion""
df = df.replace({'\'': ''}, regex=True)

output = df_cleaned.groupby('group', as_index=False).words.apply(counts).reset_index(drop=True)

Now it returns wrong results.

Do have any suggestions which way shall I go?
","I reproduced what you described in the following steps. I don't see any errors
import pandas as pd
d = {'words' : pd.Series((['coffee', 'maker', 'brewing', 'properly', '2','420', '420', '420'],
    ['galley', 'work', 'table', 'stuck'],
    ['cloth', 'stuck'],
    ['stuck', 'coffee'])),
    'group' : pd.Series([1, 2, 1, 2])}
df_cleaned = pd.DataFrame(d)
df_cleaned

The function you're using is
import nltk
from nltk.util import ngrams
from nltk.collocations import *
from collections import Counter
def counts(x):
    trigram_measures = nltk.collocations.BigramAssocMeasures()
    finder = BigramCollocationFinder.from_documents(x)
    finder.nbest(trigram_measures.pmi, 100)
    s = pd.Series(x)
    ngram_list = [pair for row in s for pair in ngrams(row, 3)]
    c = Counter(ngram_list).most_common(3)
    return pd.DataFrame([(x.name, ) + element for element in c], columns=['group', 'Ngram', 'Frequency'])

You then apply counts to the data
output = df_cleaned.groupby('group', 
            as_index=False).words.apply(counts).reset_index(drop=True)

and save the results to file
output.to_csv('test_file.csv', index=False, sep=',')
df = pd.read_csv('test_file.csv',sep=',')

I don't see any problems
",,,false,
https://stackoverflow.com/questions/34491819,false,"The issue is not related to an API exhibiting unexpected failures or unpredictable behaviors under specific runtime conditions. It is a request for advice on finding clusters of terms related to a single concept, which falls under the category of general guidance on software development processes or methodologies.",,,,,,,Semantic Clustering,"I am looking for advice on how to find clusters of terms that are all related to a single concept. 

The goal is to improve a tag or keyword search for images that describe concepts or processes or situations. An image may describe a brainstorming session, or a particular theme. These images which are meant to be used in PowerPoint or other presentation material have user contributed tags. 

The issue is  our tag based search may bring back completely unrelated images. Our goal is to find the clusters within the tags in order to refine the tags related to a central concept and remove the outliers that are not related to the clusters. 

For example if you have a you had the tags meeting, planning, brainstorming, and round table. Ideally we would want to remove round table from the cluster as it doesn't fit the theme. 

I have worked with WordNet Similarity but the results are quite strange. I was wondering if there are any other tools in python's NLTK that could help me solve this.

Thanks!
","Your question is based in the area called ""topic modeling"" you can use:
gensim
https://radimrehurek.com/gensim/
or lda
https://pypi.python.org/pypi/lda
",,,false,
https://stackoverflow.com/questions/64945076,false,"The issue is not related to an API exhibiting unexpected failures or unpredictable behaviors under specific runtime conditions. It is a question about using Natural Language Tool Kit (NLTK) with Django on Heroku and resolving an error related to the 'nltk.txt' file, which falls under the category of dependency management and configuration issues rather than API-specific problems.",,,,,,,Using Natural Language Tool Kit with Django on Heroku - - Error: &#39;nltk.txt&#39; not found,"I’ve got a basic Django project. One feature I am working on counts the number of most commonly occurring words in a .txt file, such as a large public domain book. I’ve used the Python Natural Language Tool Kit to filter out “stopwords” (in SEO language, that means redundant words such as ‘the’, ‘you’, etc. ).
Anyways, I’m getting this debug traceback when Django serves the template:

Resource [93mstopwords[0m not found. Please use the NLTK Downloader to
obtain the resource: [31m &lt;&lt;&lt; import nltk nltk.download('stopwords')
[0m For more information see: https://www.nltk.org/data.html

So I need to download the library of stopwords. To resolve the issue, I simply open a Python REPL on my remote server and invoke these two straightforward lines:
&lt;&lt;&lt; import nltk
&lt;&lt;&lt; nltk.download('stopwords')

That's covered at length elsewhere on SO. That resolves the issue, but only temporarily. As soon as the REPL session is terminated on my remote server, the error returns because the stopwords file just evaporates.
I noticed something strange when I use git to push my changes up to my remote server on Heroku. Check this:
remote: -----&gt; Python app detected
remote: -----&gt; No change in requirements detected, installing from cache
remote: -----&gt; Installing pip 20.1.1, setuptools 47.1.1 and wheel 0.34.2
remote: -----&gt; Installing SQLite3
remote: -----&gt; Installing requirements with pip
remote: -----&gt; Downloading NLTK corpora…
remote:  !     'nltk.txt' not found, not downloading any corpora
remote:  !     Learn more: https://devcenter.heroku.com/articles/python-nltk 
remote: -----&gt; $ python manage.py collectstatic --noinput
remote:        122 static files copied to '/tmp/build_f2f9d10f/staticfiles', 388 post-processed.

That devcenter link is kind of like a stub, meaning that it’s not very detailed. It’s sparse at best. The article says that to use Python nltk, you need to add an nltk.txt file to the project directory which specifies the list of objects for Heroku to download. So I went ahead and created an nltk text file which contained:

corpora

Here is this active nltk.txt currently located in my project directory. In addition to coprora, I also tried adding various combinations of the following three entries to nltk.txt:

corpus


stoplist


english

I tried adding all four, just two and just one. For example, here is an alternate nltk.txt that I tried verbatim. My feeling is that the main one I really need is just corpora, so that is the only entry in the nltk.txt that I am working with right now. With corpora there, when I push the change and Heroku builds the environment, I see this error and trace-back:
remote: -----&gt; Downloading NLTK corpora…
remote: -----&gt; Downloading NLTK packages: corpora english stopwords corpus
remote: /app/.heroku/python/lib/python3.6/runpy.py:125: RuntimeWarning: 'nltk.downloader' found in sys.modules after import of package 'nltk', but prior to execution of 'nltk.downloader'; this may result in unpredictable behaviour
remote:   warn(RuntimeWarning(msg))
remote: [nltk_data] Error loading corpora: Package 'corpora' not found in
remote: [nltk_data]     index
remote: Error installing package. Retry? [n/y/e]
remote: Traceback (most recent call last):
remote:   File ""/app/.heroku/python/lib/python3.6/runpy.py"", line 193, in _run_module_as_main
remote:     ""__main__"", mod_spec)
remote:   File ""/app/.heroku/python/lib/python3.6/runpy.py"", line 85, in _run_code
remote:     exec(code, run_globals)
remote:   File ""/app/.heroku/python/lib/python3.6/site-packages/nltk/downloader.py"", line 2538, in &lt;module&gt;
remote:     halt_on_error=options.halt_on_error,
remote:   File ""/app/.heroku/python/lib/python3.6/site-packages/nltk/downloader.py"", line 790, in download

I am clearly not using nltk.txt properly because it isn’t finding the corpora package. I can install nltk and have it run without issue in my local dev server but my remaining question is this: how do I make Heroku handle nltk properly remotely in this situation?
User Michael Godshall provides the same answer to more than one Stack Overflow question explaining that you can create a bin directory within the project root and add both a post_compile bash script and a install_nltk_data script. However this is no longer necessary because heroku-buildpack-python upstream maintainer Kenneth Reitz implemented an easy solution. All that is required now is to add an nltk.txt which contains the library you need. But I did that and I am still getting the error above.
The official nltk website documents how to use the library in general and how to install it which isn’t helpful in the case of Heroku because Heroku seems to handle nltk differently.
","Eureka! I got it working. My problem was with the name of the nltk library download. I tried stoplist when the actual name is stopwords. Ha! The contents of my nltk.txt is now simply: stopwords.  When I pushed to Heroku, the build succeeded and my website is now deployed and accessible on the web.
Special thanks goes out to @Darkknight for his patience and insight in the comment section of his answer.
","Yes, you need the nltk.txt file similar to the requirements.txt file properly. refer to the official doc here. if you still facing the same situation post the nltk.txt file here that will give us some way to find the solution
maybe this also will help you
",,false,
https://stackoverflow.com/questions/38523385,false,"The issue is not related to an API exhibiting unexpected failures or unpredictable behaviors under specific runtime conditions. It is a question about installing numpy and nltk python-modules on a Windows machine, which falls under the category of dependency management and installation issues rather than API-specific problems.",,,,,,,Not able to install numpy or nltk python-modules,"I am having a strange issue installing numpy or nltk python-modules in my windows-7 machine. I have successfully installed Python 2.7.12 but I get this error when I type pip install numpy as in this screenshot. I have also included the directory of pip.exe in the PATH. Any help would be appreciated.Thank you :)
","Installing such these things in windows are sometime difficult, specially for someone new to python packages(also for some experts!)

Try to use Anaconda for windows: https://www.continuum.io/downloads#_windows
This install a python for you and many requirement packages(e.g Numpy, Scipy, Scikit and many more)

You can use older version of Anaconda, for python2.x if you want strictly python2.x



An alternative way is to download Numpy from github and then install it as a python package, that contain setup.py file

python setup.py install


Or you can download Numpy wheel package, then install it localy with pip
",,,false,
https://stackoverflow.com/questions/49280404,true,"The issue involves the scipy.ndimage.interpolation.shift API, which exhibits unexpected behavior when shifting a numpy data array along one axis with periodic boundary treatment (mode = 'wrap'). The API forces the first pixel to be identical to the last one instead of the expected behavior of the 'last plus one'. This issue meets the criteria for deeper analysis.",,,,,,,Shift interpolation does not give expected behaviour,"When using scipy.ndimage.interpolation.shift to shift a numpy data array along one axis with periodic boundary treatment (mode = 'wrap'), I get an unexpected behavior. The routine tries to force the first pixel (index 0) to be identical to the last one (index N-1) instead of the ""last plus one (index N)"".

Minimal example:

# module import
import numpy as np
from scipy.ndimage.interpolation import shift
import matplotlib.pyplot as plt

# print scipy.__version__
# 0.18.1

a = range(10)

plt.figure(figsize=(16,12))

for i, shift_pix in enumerate(range(10)):
    # shift the data via spline interpolation
    b = shift(a, shift=shift_pix, mode='wrap')

    # plotting the data
    plt.subplot(5,2,i+1)
    plt.plot(a, marker='o', label='data')
    plt.plot(np.roll(a, shift_pix), marker='o', label='data, roll')
    plt.plot(b, marker='o',label='shifted data')
    if i == 0:
        plt.legend(loc=4,fontsize=12)
    plt.ylim(-1,10)
    ax = plt.gca()
    ax.text(0.10,0.80,'shift %d pix' % i, transform=ax.transAxes)


Blue line: data before the shift
Green line: expected shift behavior
Red line: actual shift output of scipy.ndimage.interpolation.shift

Is there some error in how I call the function or how I understand its behavior with mode = 'wrap'? The current results are in contrast to the mode parameter description from the related scipy tutorial page and from another StackOverflow post. Is there an off-by-one-error in the code?

Scipy version used is 0.18.1, distributed in anaconda-2.2.0


","It is worth noting that this behavior appears to be a bug, as noted in this SciPy issue:
https://github.com/scipy/scipy/issues/2640

The issue appears to effect every extrapolation mode in scipy.ndimage other than mode='mirror'.
","It seems that the behaviour you have observed is intentional.

The cause of the problem lies in the C function map_coordinate which translates the coordinates after shift to ones before shift:

map_coordinate(double in, npy_intp len, int mode)


The function is used as the subroutine in NI_ZoomShift that does the actual shift. Its interesting part looks like this:



Example. Lets see how the output for output = shift(np.arange(10), shift=4, mode='wrap') (from the question) is computed.

NI_ZoomShift computes edge values output[0] and output[9] in some special way, so lets take a look at computation of output[1] (a bit simplified):

# input  =         [0,1,2,3,4,5,6,7,8,9]
# output = [ ,?, , , , , , , , ]          '?' == computed position
# shift  = 4
output_index = 1

in  = output_index - shift    # -3
sz  = 10 - 1                  # 9
in += sz * ((-5 / 9) + 1)
#  +=  9 * ((     0) + 1) == 9
# in == 6

return input[in]  # 6 


It is clear that sz = len - 1 is responsible for the behaviour you have observed. It was changed from sz = len in a suggestively named commit dating back to 2007: Fix off-by-on errors in ndimage boundary routines. Update tests.

I don't know why such change was introduced. One of the possible explanations that come to my mind is as follows:

Function 'shift' uses splines for interpolation.
A knot vector of an uniform spline on interval [0, k] is simply [0,1,2,...,k]. When we say that the spline should wrap, it is natural to require equality on values for knots 0 and k, so that many copies of the spline could be glued together, forming a periodic function:

0--1--2--3-...-k              0--1--2--3-...-k              0--1-- ...
               0--1--2--3-...-k              0--1--2--3-...-k      ...


Maybe shift just treats its input as a list of values for spline's knots?
",,false,
https://stackoverflow.com/questions/40451203,false,The issue does not meet the criteria for deeper analysis as it does not involve an API exhibiting unexpected failures or unpredictable behaviors under specific runtime conditions.,,,,,,,Cython parallel loop problems,"I am using cython to compute a pairwise distance matrix using a custom metric as a faster alternative to scipy.spatial.distance.pdist.

My Motivation

My metric has the form 

def mymetric(u,v,w):
     np.sum(w * (1 - np.abs(np.abs(u - v) / np.pi - 1))**2)


and the pairwise distance using scipy can be computed as

x = sp.spatial.distance.pdist(r, metric=lambda u, v: mymetric(u, v, w))


Here, r is a m-by-n matrix of m vectors with dimension of n and w is a ""weight"" factor with dimmension n.

Since in my problem m is rather high, the computation is really slow. For m = 2000 and n = 10 this takes approx 20 sec.

Initial solution with Cython

I implemented a simple function in cython that computes the pairwise distance and immediately got very promising results -- speedup of over 500x.

import numpy as np
cimport numpy as np
import cython

from libc.math cimport fabs, M_PI

@cython.wraparound(False)
@cython.boundscheck(False)
def pairwise_distance(np.ndarray[np.double_t, ndim=2] r, np.ndarray[np.double_t, ndim=1] w):
    cdef int i, j, k, c, size
    cdef np.ndarray[np.double_t, ndim=1] ans
    size = r.shape[0] * (r.shape[0] - 1) / 2
    ans = np.zeros(size, dtype=r.dtype)
    c = -1
    for i in range(r.shape[0]):
        for j in range(i + 1, r.shape[0]):
            c += 1
            for k in range(r.shape[1]):
                ans[c] += w[k] * (1.0 - fabs(fabs(r[i, k] - r[j, k]) / M_PI - 1.0))**2.0

    return ans


Problems using OpenMP

I wanted to speed up the computation some more using OpenMP, however, the following solution is roughly 3 times slower than the serial version.

import numpy as np
cimport numpy as np

import cython
from cython.parallel import prange, parallel

cimport openmp

from libc.math cimport fabs, M_PI

@cython.wraparound(False)
@cython.boundscheck(False)
def pairwise_distance_omp(np.ndarray[np.double_t, ndim=2] r, np.ndarray[np.double_t, ndim=1] w):
    cdef int i, j, k, c, size, m, n
    cdef np.double_t a
    cdef np.ndarray[np.double_t, ndim=1] ans
    m = r.shape[0]
    n = r.shape[1]
    size = m * (m - 1) / 2
    ans = np.zeros(size, dtype=r.dtype)
    with nogil, parallel(num_threads=8):
        for i in prange(m, schedule='dynamic'):
            for j in range(i + 1, m):
                c = i * (m - 1) - i * (i + 1) / 2 + j - 1
                for k in range(n):
                    ans[c] += w[k] * (1.0 - fabs(fabs(r[i, k] - r[j, k]) / M_PI - 1.0))**2.0

    return ans


I don't know why is it actually slower, but I tried to introduce the following changes. This resulted not only in even slightly worse performance but also, the resulting distance ans is computed correctly only in the beginning of the array, the rest is just zeros. The speedup achieved through this is negligible. 

import numpy as np
cimport numpy as np

import cython
from cython.parallel import prange, parallel

cimport openmp

from libc.math cimport fabs, M_PI
from libc.stdlib cimport malloc, free

@cython.wraparound(False)
@cython.boundscheck(False)
def pairwise_distance_omp_2(np.ndarray[np.double_t, ndim=2] r, np.ndarray[np.double_t, ndim=1] w):
    cdef int k, l, c, m, n
    cdef Py_ssize_t i, j, d
    cdef size_t size
    cdef int *ci, *cj

    cdef np.ndarray[np.double_t, ndim=1, mode=""c""] ans

    cdef np.ndarray[np.double_t, ndim=2, mode=""c""] data
    cdef np.ndarray[np.double_t, ndim=1, mode=""c""] weight

    data = np.ascontiguousarray(r, dtype=np.float64)
    weight = np.ascontiguousarray(w, dtype=np.float64)

    m = r.shape[0]
    n = r.shape[1]
    size = m * (m - 1) / 2
    ans = np.zeros(size, dtype=r.dtype)

    cj = &lt;int*&gt; malloc(size * sizeof(int))
    ci = &lt;int*&gt; malloc(size * sizeof(int))

    c = -1
    for i in range(m):
        for j in range(i + 1, m):
            c += 1
            ci[c] = i
            cj[c] = j

    with nogil, parallel(num_threads=8):
        for d in prange(size, schedule='guided'):
            for k in range(n):
                ans[d] += weight[k] * (1.0 - fabs(fabs(data[ci[d], k] - data[cj[d], k]) / M_PI - 1.0))**2.0

    return ans


For all functions, I am using the following .pyxbld file

def make_ext(modname, pyxfilename):
    from distutils.extension import Extension
    return Extension(name=modname,
                     sources=[pyxfilename],
                     extra_compile_args=['-O3', '-march=native', '-ffast-math', '-fopenmp'],
                     extra_link_args=['-fopenmp'],
                     )


Summary

I have zero experience with cython and know only basics of C. I would appreciate any suggestion of what may be the cause of this unexpected behavior, or even, how to rephrase my question better.



Best serial solution (10 % faster than original serial)

@cython.cdivision(True)
@cython.wraparound(False)
@cython.boundscheck(False)
def pairwise_distance_2(np.ndarray[np.double_t, ndim=2] r, np.ndarray[np.double_t, ndim=1] w):
    cdef int i, j, k, c, size
    cdef np.ndarray[np.double_t, ndim=1] ans
    cdef np.double_t accumulator, tmp
    size = r.shape[0] * (r.shape[0] - 1) / 2
    ans = np.zeros(size, dtype=r.dtype)
    c = -1
    for i in range(r.shape[0]):
        for j in range(i + 1, r.shape[0]):
            c += 1
            accumulator = 0
            for k in range(r.shape[1]):
                tmp = (1.0 - fabs(fabs(r[i, k] - r[j, k]) / M_PI - 1.0))
                accumulator += w[k] * (tmp*tmp)
            ans[c] = accumulator

    return ans


Best parallel solution (1 % faster then original parallel, 6 times faster then best serial using 8 threads)

@cython.cdivision(True)
@cython.wraparound(False)
@cython.boundscheck(False)
def pairwise_distance_omp_2d(np.ndarray[np.double_t, ndim=2] r, np.ndarray[np.double_t, ndim=1] w):
    cdef int i, j, k, c, size, m, n
    cdef np.ndarray[np.double_t, ndim=1] ans
    cdef np.double_t accumulator, tmp
    m = r.shape[0]
    n = r.shape[1]
    size = m * (m - 1) / 2
    ans = np.zeros(size, dtype=r.dtype)
    with nogil, parallel(num_threads=8):
        for i in prange(m, schedule='dynamic'):
            for j in range(i + 1, m):
                c = i * (m - 1) - i * (i + 1) / 2 + j - 1
                accumulator = 0
                for k in range(n):
                    tmp = (1.0 - fabs(fabs(r[i, k] - r[j, k]) / M_PI - 1.0))
                    ans[c] += w[k] * (tmp*tmp)

    return ans




Unsolved issues:

When I try to apply the accumulator solution proposed in the answer, I get the following error:

Error compiling Cython file:
------------------------------------------------------------
...
                c = i * (m - 1) - i * (i + 1) / 2 + j - 1
                accumulator = 0
                for k in range(n):
                    tmp = (1.0 - fabs(fabs(r[i, k] - r[j, k]) / M_PI - 1.0))
                    accumulator += w[k] * (tmp*tmp)
                ans[c] = accumulator
                                   ^
------------------------------------------------------------
pdist.pyx:207:36: Cannot read reduction variable in loop body


Full code:

@cython.cdivision(True)
@cython.wraparound(False)
@cython.boundscheck(False)
def pairwise_distance_omp(np.ndarray[np.double_t, ndim=2] r, np.ndarray[np.double_t, ndim=1] w):
    cdef int i, j, k, c, size, m, n
    cdef np.ndarray[np.double_t, ndim=1] ans
    cdef np.double_t accumulator, tmp
    m = r.shape[0]
    n = r.shape[1]
    size = m * (m - 1) / 2
    ans = np.zeros(size, dtype=r.dtype)
    with nogil, parallel(num_threads=8):
        for i in prange(m, schedule='dynamic'):
            for j in range(i + 1, m):
                c = i * (m - 1) - i * (i + 1) / 2 + j - 1
                accumulator = 0
                for k in range(n):
                    tmp = (1.0 - fabs(fabs(r[i, k] - r[j, k]) / M_PI - 1.0))
                    accumulator += w[k] * (tmp*tmp)
                ans[c] = accumulator

    return ans

","I haven't timed this myself so it's possible this might not help too much, however:

If you run cython -a to get an annotated version of your initial attempt (pairwise_distance_omp) you'll find the ans[c] += ... line is yellow, suggesting it's got Python overhead. A look at that the C corresponding to that line suggests that it's checking for divide by zero. One key part of it starts:

if (unlikely(M_PI == 0)) {


You know this will never be true (and in any case you'd probably live with NaN values rather than an exception if it was). You can avoid this check by adding the following extra decorator to the function:

@cython.cdivision(True)
# other decorators
def pairwise_distance_omp # etc...


This cuts out quite a bit of C code, including bits that have to be run in a single thread. The flip-side is that most of that code should never be run, and the compiler should probably be able to work that out, so it isn't clear how much difference that will make.



Second suggestion:

# at the top
cdef np.double_t accumulator, tmp

    # further down later in the loop:
    c = i * (m - 1) - i * (i + 1) / 2 + j - 1
    accumulator = 0
    for k in range(r.shape[1]):
        tmp = (1.0 - fabs(fabs(r[i, k] - r[j, k]) / M_PI - 1.0))
        accumulator = accumulator + w[k] * (tmp*tmp)
    ans[c] = accumulator


This has two advantages hopefully: 1) tmp*tmp should probably be quicker than floating point exponent to the power of 2. 2) You avoid reading from the ans array, which might be a bit slow because the compiler always has to be careful that some other thread hasn't changed it (even though you know it shouldn't have).
",,,false,
https://stackoverflow.com/questions/45255265,true,"The issue involves unexpected behavior in the Gaussian filtering function of Scipy. The behavior is due to a bug in the gaussian_filter function when both order and cval are nonzero. The incorrect result is observed when the order includes a derivative, and the cval is set to a nonzero value. A suggested workaround is provided to achieve the desired result.",,,,,,,Unexpected behavior of Gaussian filtering with Scipy,"Given that I have an image f(x,y) loaded, for example,



I want to compute the Gaussian derivative ∂/∂x ∂/∂y G*f of the image f, where G is a Gaussian filter and * denotes convolution. This is easily done using Scipy:

from scipy.ndimage.filters import gaussian_filter
imshow(gaussian_filter(g, sigma, order=1))


With sigma=50 this produces the following result:



Now, for applicationary reasons, I need to do the computation with mode='constant':

imshow(gaussian_filter(g, sigma, order=1, mode='constant', cval=0))


Still, the result looks reasonable:



However, note that my image's background's intensity is 1 and not 0. Hence, it should be reasonable to use cval=1:

imshow(gaussian_filter(g, sigma, order=1, mode='constant', cval=1))




Now this is unexpected! This result makes no sense, does it?

For the record, I also checked the partial differentials ∂/∂x G*f and ∂/∂y G*f. Whereas

imshow(gaussian_filter(g, sigma, order=[0, 1], mode='constant', cval=1)


looks reasonable



the other one

imshow(gaussian_filter(g, sigma, order=[1, 0], mode='constant', cval=1)


does not:



Why is that?
","There is a bug in gaussian_filter that manifests itself when both order and cval are nonzero. Specifically, it's here: 

for axis, sigma, order, mode in axes:
    gaussian_filter1d(input, sigma, axis, order, output, mode, cval, truncate)
    input = output


The filter performs repeated 1d convolution, and each time it passes in cval to 1d filter. Problem is, if there were any derivatives taken, then cval should be set to 0 because the derivative of any constant is zero. This is why the result is wrong with order=[1, 0] but not with order=[0, 1]. Without testing (don't have SciPy dev environment), I think the following would be correct: 

for axis, sigma, order, mode in axes:
    gaussian_filter1d(input, sigma, axis, order, output, mode, cval, truncate)
    if order &gt; 0: 
        cval = 0.0
    input = output


Workaround

A nonzero cval can be emulated by subtracting it from the image before filtering (and adding back after filtering only if the order is zero). Example:

import numpy as np
import matplotlib.pyplot as plt
from scipy.ndimage.filters import gaussian_filter

g = np.ones((500, 500))
g[200:300, 200:300] = 2
sigma = 50
cval = 1
gf = gaussian_filter(g-cval, sigma, order=1, mode='constant')
plt.matshow(gf)
plt.show()


returns



which is the expected result. (My original image is a bit different from yours, and I use a different visualization tool.)
",,,false,
https://stackoverflow.com/questions/18155972,true,"The issue involves unexpected behavior in numpy array slicing and views. The behavior observed in the second case is due to the use of the + operator, which creates a new array instead of modifying the view in-place. This is different from the behavior observed in the first case where the += operator is used, which modifies the view directly. The explanation clarifies the difference between the two cases.",,,,,,,unexpected result in numpy array slicing (view vs copy),"I'm trying to reduce the amount of copying in my code and I came across surprising behavior when dealing with numpy array slicing and views, as explained in:
Scipy wiki page on copying numpy arrays
I've stumbled across the following behavior, which is unexpected for me:
Case 1.:
import numpy as np
a = np.ones((3,3))
b = a[:,1:2]
b += 5
print a
print b.base is a

As expected, this outputs:
array([[ 1.,  6.,  1.],
       [ 1.,  6.,  1.],
       [ 1.,  6.,  1.]])
True

Case 2: When performing the slicing and addition in one line, things look different:
import numpy as np
a = np.ones((3,3))
b = a[:,1:2] + 5
print a
print b.base is a

The part that's surprising to me is that a[:,1:2] does not seem to create a view, which is then used as a left hand side argument, so, this outputs:
array([[ 1.,  1.,  1.],
       [ 1.,  1.,  1.],
       [ 1.,  1.,  1.]])
False

Maybe someone can shed some light on why these two cases are different, I think I'm missing something.
Solution: I missed the obvious fact that the ""+"" operator, other than the in-place operator ""+="" will always create a copy, so it's in fact not related but slicing other than how in-place operators are defined for numpy arrays.
To illustrate this, the following generates the same output as Case 2:
import numpy as np
a = np.ones((3,3))
b = a[:,1:2]
b = b + 5
print a
print b.base is a

","The above is no different than:

&gt;&gt;&gt; a=np.arange(5)
&gt;&gt;&gt; b=a
&gt;&gt;&gt; b
array([0, 1, 2, 3, 4])

&gt;&gt;&gt; b+=5
&gt;&gt;&gt; a
array([5, 6, 7, 8, 9])
&gt;&gt;&gt; b
array([5, 6, 7, 8, 9])

&gt;&gt;&gt; b=b+5
&gt;&gt;&gt; b
array([10, 11, 12, 13, 14])
&gt;&gt;&gt; a
array([5, 6, 7, 8, 9])


Which, at least to me, seem like completely expected behavior. The b+=x operator calls __iadd__ which importantly first tries to modify the array in place, so it will update b which is still a view of a. While the b=b+x operator calls __add__ which creates new temporary data and then assigns it to b.

For a[i] +=b the sequence is (in numpy): 

a.__setitem__(i, a.__getitem__(i).__iadd__(b))

","a[:, 1:2] creates a view, but you don't modify the view in the second example. Instead, + creates a new array from its arguments. Suppose you do

a = np.ones((3, 3))
b = a + 5


In that case, you wouldn't expect a change to a, because this isn't an in-place addition. The operator is +, rather than +=. It's the same with the second example.

b = a[:, 1:2] + 5


doesn't modify a[:, 1:2], because this isn't in-place addition.
","The default thing to do when slicing a numpy array is to create b as a view of a, thus when you change b, a changes as well, which is confirmed by your first case. 

The second case is more tricky. You are not telling that b is a slice of a and then adding a number. What you are doing is creating b as something that does not coincide with a, so numpy is forced to copy the data instead of just creating a view. 
",false,
https://stackoverflow.com/questions/52450658,false,The issue does not meet the criteria for deeper analysis as it does not involve an API exhibiting unexpected failures or unpredictable behaviors under specific runtime conditions.,,,,,,,Python - unexpected shape parameter behavior in scipy genextreme fit,"I've been trying to fit the GEV distribution to some annual maximum river discharge using Scipy's stats.genextreme function, but I've found some weird behavior of the fit. Depending on how small your data is (i.e., 1e-5 vs. 1e-1), the shape parameter that is returned can be dramatically different. For example: 

import scipy as scipy
import numpy as np
from scipy.stats import genextreme as gev
from scipy.stats import gumbel_r as gumbel

#Set up arrays of values to fit curve to 
sample=np.random.rand(1,30) #Random set of decimal values 
smallVals = sample*1e-5     #Scale to smaller values 

#If the above is not creating different values, this instance of random numbers has:
bugArr = np.array([[0.25322987, 0.81952358, 0.94497455, 0.36295543, 0.72272746, 0.49482558,0.65674877, 0.40876558, 0.64952248, 0.23171052, 0.24645658, 0.35359126,0.27578928, 0.24820775, 0.69789187, 0.98876361, 0.22104156,0.40019593,0.0756707,  0.12342556, 0.3601186,  0.54137089,0.43477705, 0.44622486,0.75483338, 0.69766687, 0.1508741,  0.75428996, 0.93706003, 0.1191987]])
bugArr_small = bugArr*1e-5

#This array of random numbers gives the same shape parameter regardless 
fineArr = np.array([[0.7449611,  0.82376693, 0.32601009, 0.18544293, 0.56779629, 0.30495415,
        0.04670362, 0.88106521, 0.34013959, 0.84598841, 0.24454428, 0.57981437,
        0.57129427, 0.8857514,  0.96254429, 0.64174078, 0.33048637, 0.17124045,
        0.11512589, 0.31884749, 0.48975204, 0.87988863, 0.86898236, 0.83513966,
        0.05858769, 0.25889509, 0.13591874, 0.89106616, 0.66471263, 0.69786708]])
fineArr_small = fineArr*1e-5

#GEV fit for both arrays - shouldn't dramatically change distribution 
gev_fit      = gev.fit(sample)
gevSmall_fit = gev.fit(smallVals)

gevBug      = gev.fit(bugArr)
gevSmallBug = gev.fit(bugArr_small)

gevFine      = gev.fit(fineArr)
gevSmallFine = gev.fit(fineArr_small)


I get the following output for the GEV parameters estimated for the bugArr/bugArr_small and fineArr/fineArr_small:

Known bug array
Random values:         (0.12118250540401079, 0.36692231766996053, 0.23142400358716353)
Random values scaled:  (-0.8446554391074808, 3.0751769299431084e-06, 2.620390405092363e-06)

Known fine array
Random values:         (0.6745399522587823, 0.47616297212022757, 0.34117425062278584)
Random values scaled:  (0.6745399522587823, 4.761629721202293e-06, 3.411742506227867e-06)


Why would the shape parameter change so dramatically when the only difference in the data is a change in scaling? I would've expected the behavior to be consistent with the FineArr results (no change in shape parameter, and appropriate scaling of location and scale parameters). I've repeated the test in Matlab, but the results there are in line with what I expected (i.e., no change in shape parameter). 
","I think I know why this might be happening. It is possible to pass initial shape parameter estimates when fitting, see the documentation for scipy.stats.rv_continuous.fit where it states ""Starting value(s) for any shape-characterizing arguments (those not provided will be determined by a call to _fitstart(data)). No default value."" Here is some extremely ugly, functional, code using my pyeq3 statistical distribution fitter which internally attempts to use different estimates, fit them, and return the parameters for best nnlf of the different fits. This example code does not show the behavior you observe, and gives the same shape parameters regardless of scaling. You would need to install pyeq3 with ""pip3 install pyeq3"" to run this code. The pyeq3 code is designed for text input from a web interface on zunzun.com, so hold you nose - here is the example code:

import numpy as np

#Set up arrays of values to fit curve to 
sample=np.random.rand(1,30) #Random set of decimal values 
smallVals = sample*1e-5     #Scale to smaller values 

#If the above is not creating different values, this instance of random numbers has:
bugArr = np.array([0.25322987, 0.81952358, 0.94497455, 0.36295543, 0.72272746, 0.49482558,0.65674877, 0.40876558, 0.64952248, 0.23171052, 0.24645658, 0.35359126,0.27578928, 0.24820775, 0.69789187, 0.98876361, 0.22104156,0.40019593,0.0756707,  0.12342556, 0.3601186,  0.54137089,0.43477705, 0.44622486,0.75483338, 0.69766687, 0.1508741,  0.75428996, 0.93706003, 0.1191987])
bugArr_small = bugArr*1e-5

#This array of random numbers gives the same shape parameter regardless 
fineArr = np.array([0.7449611,  0.82376693, 0.32601009, 0.18544293, 0.56779629, 0.30495415,
        0.04670362, 0.88106521, 0.34013959, 0.84598841, 0.24454428, 0.57981437,
        0.57129427, 0.8857514,  0.96254429, 0.64174078, 0.33048637, 0.17124045,
        0.11512589, 0.31884749, 0.48975204, 0.87988863, 0.86898236, 0.83513966,
        0.05858769, 0.25889509, 0.13591874, 0.89106616, 0.66471263, 0.69786708])
fineArr_small = fineArr*1e-5

bugArr_str = ''
for i in range(len(bugArr)):
    bugArr_str += str(bugArr[i]) + '\n'
bugArr_small_str = ''
for i in range(len(bugArr_small)):
    bugArr_small_str += str(bugArr_small[i]) + '\n'
fineArr_str = ''
for i in range(len(fineArr)):
    fineArr_str += str(fineArr[i]) + '\n'
fineArr_small_str = ''
for i in range(len(fineArr_small)):
    fineArr_small_str += str(fineArr_small[i]) + '\n'
import pyeq3

simpleObject_bugArr = pyeq3.IModel.IModel()
simpleObject_bugArr._dimensionality = 1
pyeq3.dataConvertorService().ConvertAndSortColumnarASCII(bugArr_str, simpleObject_bugArr, False)
solver = pyeq3.solverService()
result_bugArr = solver.SolveStatisticalDistribution('genextreme', simpleObject_bugArr.dataCache.allDataCacheDictionary['IndependentData'][0], 'nnlf')
simpleObject_bugArr_small = pyeq3.IModel.IModel()
simpleObject_bugArr_small._dimensionality = 1
pyeq3.dataConvertorService().ConvertAndSortColumnarASCII(bugArr_small_str, simpleObject_bugArr_small, False)
solver = pyeq3.solverService()
result_bugArr_small = solver.SolveStatisticalDistribution('genextreme', simpleObject_bugArr_small.dataCache.allDataCacheDictionary['IndependentData'][0], 'nnlf')

simpleObject_fineArr = pyeq3.IModel.IModel()
simpleObject_fineArr._dimensionality = 1
pyeq3.dataConvertorService().ConvertAndSortColumnarASCII(fineArr_str, simpleObject_fineArr, False)
solver = pyeq3.solverService()
result_fineArr = solver.SolveStatisticalDistribution('genextreme', simpleObject_fineArr.dataCache.allDataCacheDictionary['IndependentData'][0], 'nnlf')

simpleObject_fineArr_small = pyeq3.IModel.IModel()
simpleObject_fineArr_small._dimensionality = 1
pyeq3.dataConvertorService().ConvertAndSortColumnarASCII(fineArr_small_str, simpleObject_fineArr_small, False)
solver = pyeq3.solverService()
result_fineArr_small = solver.SolveStatisticalDistribution('genextreme', simpleObject_fineArr_small.dataCache.allDataCacheDictionary['IndependentData'][0], 'nnlf')

print('ba',result_bugArr[1]['fittedParameters'])
print('ba_s',result_bugArr_small[1]['fittedParameters'])
print()
print('fa',result_fineArr[1]['fittedParameters'])
print('fa_s',result_fineArr_small[1]['fittedParameters'])

",,,false,
https://stackoverflow.com/questions/59281884,true,The issue involves unexpected behavior in the scipy.ndimage.zoom() function when the order parameter is set to 0. This API exhibits different behavior than expected when zooming an array with a whole number zoom factor. Further analysis is required to provide detailed information.,,,,,,,Unexpected behavior of `scipy.ndimage.zoom()` for `order=0`,"I have difficulties understanding the behavior of scipy.ndimage.zoom() when order=0.

Consider the following code:

import numpy as np
import scipy as sp
import scipy.ndimage

arr = np.arange(3) + 1
print(arr)
for order in range(5):
    zoomed = sp.ndimage.zoom(arr.astype(float), 4, order=order)
    print(order, np.round(zoomed, 3))


whose output is:

0 [1. 1. 1. 2. 2. 2. 2. 2. 2. 3. 3. 3.]
1 [1.    1.182 1.364 1.545 1.727 1.909 2.091 2.273 2.455 2.636 2.818 3.   ]
2 [1.    1.044 1.176 1.394 1.636 1.879 2.121 2.364 2.606 2.824 2.956 3.   ]
3 [1.    1.047 1.174 1.365 1.601 1.864 2.136 2.399 2.635 2.826 2.953 3.   ]
4 [1.    1.041 1.162 1.351 1.59  1.86  2.14  2.41  2.649 2.838 2.959 3.   ]


So, when order=0 the values are (expectedly) not interpolated.
However, I was expecting to have:

[1. 1. 1. 1. 2. 2. 2. 2. 3. 3. 3. 3.]


i.e. exactly the same number of elements for each value, since the zoom is a whole number.
Hence, I was expecting to get the same result as np.repeat():

print(np.repeat(arr.astype(float), 4))
[1. 1. 1. 1. 2. 2. 2. 2. 3. 3. 3. 3.]


Why is there a variation in the number of times each element gets repeated?



Note that np.repeat() does not directly work with multi-dimensional arrays and that is the reason why I would like to get the ""correct"" behavior from scipy.ndimage.zoom().



My NumPy and SciPy versions are:

print(np.__version__)
# 1.17.4
print(sp.__version__)
# 1.3.3




I found this:
`scipy.ndimage.zoom` vs `skimage.transform.rescale` with `order=0`
which points toward some unexpected behavior for scipy.ndimage.zoom() but I am not quite sure it is the same effect being observed.
","This is a bin/edge array interpretation issue.
The behavior of scipy.ndimage.zoom() is based on the edge interpretation of the array values, while the behavior that would produce equally-sized blocks for integer zoom factors (mimicking np.repeat()) is based on the bin interpretation.

Let's illustrate with some ""pictures"".

Bin Interpretation

Consider the array [1 2 3], and let's assign each value to a bin.
The edges of each bin would be: 0 and 1 for 1, 1 and 2 for 2, etc.

0 1 2 3
|1|2|3|


Now, let's zoom this array by a factor of 4:

                    1 1 1
0 1 2 3 4 5 6 7 8 9 0 1 2
|   1   |   2   |   3   |


Hence, the values to assign to the bins using the Next-door Neighbor method are:

                    1 1 1
0 1 2 3 4 5 6 7 8 9 0 1 2
|1 1 1 1|2 2 2 2|3 3 3 3|


Edge Interpretation

Consider the same array as before [1 2 3], but now let's assign each value to an edge:

0 1 2
| | |
1 2 3


Now, let's zoom this array by a factor of 4:

                    1 1
0 1 2 3 4 5 6 7 8 9 0 1
| | | | | | | | | | | |
1          2          3



Hence, the values to assign to the edges using the Next-door Neighbor method are:

                    1 1
0 1 2 3 4 5 6 7 8 9 0 1
| | | | | | | | | | | |
1 1 1 2 2 2 2 2 2 3 3 3


and edge 3 is assigned to 2 because 2 has position 5.5 while 1 has position 0 and (5.5 - 3 = 2.5) &lt; (3 - 0 = 3).
Similarly, edge 8 is assigned to 2 because (8 - 5.5 = 2.5) &lt; (11 - 8 = 3).



Comments

In Physics, the ""bin array interpretation"" is generally more useful, because measurements are typically ""the result of some integration over a certain bin in an appropriate domain"" (notably signal of any form -- including images -- collected at a given time interval), hence I was expecting a ""bin interpretation"" for scipy.ndimage.zoom() but I acknowledge that the ""edge interpretation"" is equally valid (although I am not sure which applications benefit the most from it).



(Thanks to @Patol75 for pointing me into the right direction)
","I think that this is the expected behaviour.

Consider your initial list, [1, 2, 3]. You ask scipy to zoom on it 4 times, which thereby creates a 4x3=12 elements list. The first element of the list has to be 1, the last one has to be 3. Then, for 2, well we have an even number of elements, so it would make sense to have 2 as both the 6th and 7th elements. This gives [1, , , , , 2, 2, , , , , 3]. From here, you provided zoom with order=0, which means zoom is going to fill in for the missing values with splines of order 0. First case, zoom needs to fill in for 4 missing values between 1 and 2. This has to be [1, 1, 2, 2]. Second case, 4 missing values between 2 and 3. Same logic, [2, 2, 3, 3]. Final result [1, 1, 1, 2, 2, 2, 2, 2, 2, 3, 3, 3].

Now consider a 5x zoom, which generates a 15 elements array. Same story, except that there is a ""middle"" element, so that only one 2 is initially placed in the new list, at the 8th spot. With six elements to fill in between each pair, we get with the same logic [1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3].

Therefore, you get more 2s than 1s or 3s because 2 is involved in two interpolation operations, opposed to one for both 1 &amp; 3. 
",,false,
https://stackoverflow.com/questions/29306538,true,The issue involves unexpected behavior in the scipy.stats.genextreme.fit() function when fitting the GEV distribution to data with different scaling. The shape parameter returned by the fit can vary dramatically depending on the scaling of the data. Further analysis is required to provide detailed information.,,,,,,,Unexpected behavior in scipy isf,"I am using scipy's stats module to try and determine values of a distribution at which the upper tail probability reaches some small value, but I am getting some very unrealistic results. For example:

I fit a beta distribution to an array of the square of normalized correlation coefficients for a signal matching operation (correlation coefficient is always between -1 and 1 so its square is between 0 and 1). Using 

import scipy, numpy as np
bd=scipy.beta.fit(np.square(data),floc=0,fscale=1) #fitted beta dist


which gives me the beta distribution parameters of 
    (0.42119596435034012, 16939.046996018118, 0, 1) the data array is about 3 million elements long. 

Now when I plot the distribution it is clear that most the area of the distribution is very near 0 on the x axis

import matplotlib.pyplot as plt
x=x=np.linspace(0,1,num=1000000)
plt.plot(x,scipy.stats.beta.pdf(x,betaparams[0],betaparams[1]))
plt.xlim([0,.0001])




Now when I try to find the x value for which some upper tail probability remains I get some unexpected behavior. For example

for expon in [-1,-2,-3,-4,-5,-6,-7,-8,-9,-10]:
    print (expon,scipy.stats.beta.isf(10**expon,betaparams[0],betaparams[1]))


yeilds:

(-1, 6.9580465891063448e-05)
(-2, 0.00018124328968143608)
(-3, 0.00030250611696189104)
(-4, 0.00042796070123291116)
(-5, 0.0005557482540313166)
(-6, 0.00068501413697673774)
(-7, 0.99999966996999767)
(-8, 0.99999996699699967)
(-9, 0.99999999669970008)
(-10, 0.99999999966997)


Clearly scipy is returning poor estimates around 10**-7. My question is why, why it would express this behavior silently, and how to fix it. 

Thanks
","This appears to be a bug in scipy.special.btdtri which is supposed to compute quantiles for the beta distribution.  Maybe you can file a bug report.

&gt;&gt;&gt; from scipy import special
&gt;&gt;&gt; special.btdtri (betaparams[0],betaparams[1], 1-1e-6)
0.00068501413697504238
&gt;&gt;&gt; special.btdtri (betaparams[0],betaparams[1], 1-1e-7)
0.99999966996999767


I can't figure out where btdtri is defined.

EDIT: For the record, here is the SciPy bug report: https://github.com/scipy/scipy/issues/4677
",,,false,
https://stackoverflow.com/questions/54968954,false,The issue does not meet the criteria for deeper analysis as it does not involve an API exhibiting unexpected failures or unpredictable behaviors under specific runtime conditions.,,,,,,,scipy.optimize.curve_fit: Default value of max_nfev broken?,"I am experiencing unexpected behavior when calling scipy.optimize.curve_fit with the max_nfev argument. The documentation states that extra kwargs are passed on to leastsq for method='lm' and to least_squares otherwise. Also, method should default to 'trf' (i.e. not 'lm') if bounds are provided. Finally, least_squares takes the max_nfev argument, which defaults to 100*len(x) if not provided (or explicitly passed as max_nfev=None), x being one of the arrays on which to do the curve fitting.

I have a dataset (and matching function) on which scipy.optimize.curve_fit fails. The time it takes before the routine gives up depends linearly on max_nfev, as expected. However, this time is vastly different between specifying max_nfev=100*len(x) in the call to scipy.optimize.curve_fit and not passing in max_nfev at all, which seems to go against the documented behavior.

Here is a script which demonstrates this:

import time
import numpy as np
import scipy.optimize

x, y = np.loadtxt('data.txt', unpack=True)

# Attempt curve fit
initial_guess = (1, 0)
bounds = ([-np.inf, 0], [np.inf, 15])
for max_nfev in (None, 1*len(x), 10*len(x), 100*len(x)):
    print('\nRunning with max_nfev = {}:'.format(max_nfev))
    t0 = time.time()
    try:
        scipy.optimize.curve_fit(
            lambda x, factor, exponent: factor*x**exponent,
            x,
            y,
            initial_guess,
            bounds=bounds,
            ftol=1e-10,
            maxfev=max_nfev,
        )
        deltat = time.time() - t0
        print('Succeeded after', deltat, 's')
    except RuntimeError:
        deltat = time.time() - t0
        print('Failed after', deltat, 's')


The script needs the dataset in data.txt (24 KB), which you can download here.

On my system, the output of this script is


  Running with max_nfev = None:
  Failed after 0.10752344131469727 s
  
  Running with max_nfev = 441:
  Failed after 0.17525863647460938 s
  
  Running with max_nfev = 4410:
  Failed after 1.732572078704834 s
  
  Running with max_nfev = 44100:
  Failed after 17.796284437179565 s


I would expect the first (max_nfev=None) and last (max_nfev=100*len(x)) call to take roughly the same amount of time to fail. To add to the mystery, it seems that instead of passing max_nfev I might just as well pass maxfev, which is not a valid argument of least_squares, but instead the maxfev-equivalent argument taken by leastsq. 

Have I misunderstood something, or is the documentation or the implementation wrong?

I am experiencing this under both {SciPy 1.1.0, Python 3.6.5} and {SciPy 1.2.0, Python 3.7.1}.
","
  I would expect the first (max_nfev=None) and last (max_nfev=100*len(x)) call to take roughly the same amount of time


Set a breakpoint at line 250 of scipy.optimize._lsq.trf.py:

    if max_nfev is None:
        max_nfev = x0.size * 100


At this point x0 has just two elements, so your call that passed in None could equivalently have passed in 200. x0 came from p0, which defaulted to [1 0].

Based on this, your observed timings do make sense.
",,,false,
https://stackoverflow.com/questions/57477723,false,The issue does not involve an API exhibiting unexpected failures or unpredictable behaviors under specific runtime conditions. It seems to be related to numerical calculations and the expected behavior of the scipy.integrate API.,,,,,,,Unexpected behaviour of scipy.integrate,"I want to use scipy.integrate for some numerical calculations. I just ran a little example to try it and ran across some unexpected behavior. 

I made some clean code to demonstrate the problem. I use a very simple exponential distribution to test. 

Here's my code:

import numpy as np
import sys
import scipy as sc
from scipy import integrate


print(sys.version)
print(np.version.version)
print(sc.version.version)
print()

r1 = integrate.quad(lambda x: sc.exp(-x), 0, 10)
r2 = integrate.quad(lambda x: sc.exp(-x), 0, 100000)
r3 = integrate.quad(lambda x: sc.exp(-x), 0, np.inf)

print(r1)
print(r2)
print(r3)

print()
r4 = integrate.quad(lambda x: sc.exp(-x), 0, 10000)
print(r4)


The output is

3.7.2 (default, Jan  2 2019, 17:07:39) [MSC v.1915 64 bit (AMD64)]
1.15.4
1.1.0

r1 (0.9999546000702375, 2.8326146575791917e-14)
r2 (2.0614532085314573e-45, 4.098798466247153e-45)
r3 (1.0000000000000002, 5.842606996763696e-11)

r4 (1.0, 1.6059202674761255e-14)


I expect all the output to be always approximately one. But in r2 i get an incredibly small value. Strangely, when integrating to infinity (r3), or a very small border (r1), the problem doesn't show up. Also, by decreasing the limit by one order of magnitude (r4) I also get a perfect result.

Does anyone know why this problem appears in scipy? 
I would call this a bug, but maybe I violated some restrictions?
How do I know in advance to prevent wrong results in my applied problems?

Thank you in advance

Output for full_output:

r2 (2.0614532085314573e-45, 4.098798466247153e-45, {'neval': 63, 'last': 2, 'iord': array([      1,       2,       3,       4,       5, 6357060, 6357108,
       4259932, 6357102, 7274595, 6553710, 3342433, 7077980, 6422633,
       7536732, 7602281, 2949221, 6357104, 7012451, 6750305, 7536741,
       7536732, 6881379, 7929968, 7274588, 7602288, 7143529, 7995497,
       6029413, 7209055, 7077998, 3014771, 7340131, 3604531, 7798829,
       7209065, 6357087, 6553709, 3407926, 7340078, 6553721, 3276846,
       5046318, 7209057, 6684777, 7536741,     116, 6619136, 7602291,
             0], dtype=int32), 'alist': array([0.00000000e+000, 5.00000000e+004, 0.00000000e+000, 0.00000000e+000,
       6.88436472e-272, 3.80218509e-136, 2.65902947e-068, 2.20016853e-034,
       1.04474528e-019, 3.09734336e-016, 9.03970673e-019, 8.23342652e-316,
       8.23342968e-316, 8.23343284e-316, 8.23343601e-316, 8.23343917e-316,
       8.23344233e-316, 8.23344549e-316, 8.23344865e-316, 8.23345182e-316,
       8.23345498e-316, 8.23345814e-316, 8.23346130e-316, 8.23346446e-316,
       8.23346763e-316, 8.23347079e-316, 8.23347395e-316, 8.23347711e-316,
       8.23348027e-316, 8.23348344e-316, 8.23348660e-316, 8.23348976e-316,
       8.23349292e-316, 8.23349608e-316, 8.23349925e-316, 8.23350241e-316,
       8.23350557e-316, 8.23350873e-316, 8.23351189e-316, 8.23351506e-316,
       8.23351822e-316, 8.23352138e-316, 8.23352454e-316, 8.23352770e-316,
       8.23353087e-316, 8.23353403e-316, 8.23353719e-316, 8.23354035e-316,
       8.23354351e-316, 8.23354668e-316]), 'blist': array([5.00000000e+004, 1.00000000e+005, 0.00000000e+000, 0.00000000e+000,
       6.88436472e-272, 3.80218509e-136, 2.65902947e-068, 2.20016853e-034,
       1.04474528e-019, 3.09734336e-016, 9.03970673e-019, 1.20736675e+285,
       1.05117823e-153, 1.05132391e-153, 1.05146958e-153, 3.79823888e-258,
       1.61465766e+184, 3.11517960e+161, 4.26137323e+257, 6.01346953e-154,
       6.01366349e-154, 1.19632546e-153, 3.64465882e-086, 1.31100174e-259,
       1.20679441e-153, 1.20679327e-153, 3.24245662e-086, 3.64465882e-086,
       6.01357764e-154, 1.20679441e-153, 5.75105581e+072, 2.20791354e+214,
       1.27734658e-152, 5.29444423e+160, 6.19633416e+223, 2.25563599e-153,
       8.21947530e+223, 6.09892510e-013, 1.06097757e-153, 2.86747940e-110,
       6.06154135e-154, 6.06445477e-154, 6.96312298e-077, 3.00226946e-067,
       6.03810921e-154, 1.30421760e-076, 1.21438942e-067, 4.61448322e-072,
       8.51221910e-053, 3.73237334e+069]), 'rlist': array([2.06145321e-045, 0.00000000e+000, 6.73898103e+149, 3.51023756e+151,
       4.50937881e-292, 9.43293441e-314, 4.65203811e+151, 6.99386802e-283,
       3.53886392e-308, 1.33360313e+241, 1.15420781e+171, 9.30281767e+242,
       1.17364463e+214, 3.12671297e+185, 2.85341794e-313, 8.18432962e-085,
       6.45840689e+170, 4.42638830e-239, 9.78681729e+199, 3.38460675e+125,
       3.11732880e+150, 9.78747303e+199, 2.27948172e-191, 1.04972250e+214,
       4.77402433e+180, 1.12985581e+277, 3.16464606e-307, 1.33360315e+241,
       1.76252970e-310, 1.02318154e-012, 1.15549302e-313, 1.03539814e-308,
       1.33360293e+241, 5.67421675e-311, 5.00120719e-162, 6.46048250e-313,
       1.68400738e-019, 1.10811151e-302, 1.66468912e-312, 1.09403545e-303,
       1.27613271e-303, 7.10020498e-270, 4.99875566e-111, 9.11927054e-304,
       9.11571045e-304, 9.11749048e-304, 9.11571042e-304, 9.60205653e+303,
       5.43239349e-312, 1.79972786e-304]), 'elist': array([4.09879847e-045, 0.00000000e+000, 6.47287707e+170, 5.98178835e-154,
       1.69375668e+190, 4.44389806e+252, 1.12297399e+219, 1.87673453e-152,
       7.20706153e+159, 1.27826731e-152, 2.43812981e-152, 5.52716101e+228,
       6.01346953e-154, 1.57761457e+214, 7.19938459e+252, 3.94357072e+180,
       3.44210870e+175, 3.62478142e+228, 1.23732543e-259, 3.53810655e+155,
       4.81222029e+233, 1.06843264e-258, 9.15000112e+199, 4.26614628e+180,
       3.53387914e+246, 2.35509149e+251, 1.69375944e+190, 1.57762309e+214,
       6.19634286e+223, 8.95533289e-106, 5.98148090e-154, 1.17914189e+195,
       5.42869734e+213, 6.72794695e+199, 5.30383390e+180, 1.02188594e-152,
       2.16452413e+233, 7.50052033e+247, 6.98907523e+096, 7.69843824e+218,
       3.23097122e+174, 9.84214185e-154, 1.36723829e+161, 1.19346501e+243,
       1.94670285e+227, 2.21366476e+214, 8.95533289e-106, 8.75378213e+247,
       1.87673453e-152, 2.50722129e-310])})


´´´
","It is not a bug, it has to do with the numerical precision of the integration, and the fact that you are integrating a function that is (almost) 0 in most of the interval.
From the docs:


  Be aware that pulse shapes and other sharp features as compared to the
  size of the integration interval may not be integrated correctly using
  this method.


Based on your output, the function is using only two (last=2) intervals, evaluating values for rlist=(2.06145321e-045, 0.00000000e+000, ..) on each (see the docs for more detail on the output)

You can add points to the interval to force the routine to use points closer to the left limit.

a = quad(lambda x: np.exp(-x), 0, 1e9, points=np.logspace(-10,3,10))
print(a)
(0.9999999999999997, 2.247900608926337e-09)


Adding to the explanation (thanks to @norok2): Note that points is a sequence of break points in the bounded integration interval where local difficulties of the integrand may occur (e.g., singularities, discontinuities). In this case, I'm not using it to point out discontinuities, but rather to force quad to perform more integration steps near the left boundary, using a log-spaced interval since a I have an exponential function (this is of course arbitrary and for this function, since I know its shape).
","There is no need to convert your integral into one over a (very large) interval. There is a specific integration scheme for integrals of the form

,

namely Gauss-Laguerre quadrature. It's also included in quadpy (a project of mine). Simply try out

import numpy
import quadpy

scheme = quadpy.e1r.gauss_laguerre(1)

val = scheme.integrate(lambda x: numpy.ones(x.shape[1:]))

print(val)


1.0

",,false,
https://stackoverflow.com/questions/61594854,false,The issue does not involve an API exhibiting unexpected failures or unpredictable behaviors under specific runtime conditions. It seems to be related to the configuration and usage of Apache Airflow for packaged DAGs.,,,,,,,Packaged dags - Airflow can&#39;t find module installed,"I'm trying to use apache airlfow with packaged dags (https://airflow.apache.org/docs/stable/concepts.html#packaged-dags).

I've written my code as a python package and obviously my code depends on other popular libraries such as numpy, scipy etc.

EDIT:
This is setup.py of my custom python package:

from setuptools import setup, find_packages
from pathlib import Path
from typing import List

import distutils.text_file

def parse_requirements(filename: str) -&gt; List[str]:
    """"""Return requirements from requirements file.""""""
    # Ref: https://stackoverflow.com/a/42033122/
    return distutils.text_file.TextFile(filename=str(Path(__file__).with_name(filename))).readlines()


setup(name='classify_business',
      version='0.1',
      python_requires=""&gt;=3.6"",
      description='desc',
      url='https://urlgitlab/datascience/classifybusiness',
      author='Marco fumagalli',
      author_email='marco.fumagalli@mycompany.com',
      packages = find_packages(),
      license='MIT',
      install_requires=
      parse_requirements('requirements.txt'),
      zip_safe=False,
      include_package_data=True)


requirements.txt contains packages ( vertica_python, pandas, numpy etc) along with their version needed for my code.

I wrote a litte shell script based on the one provied in the doc:

set -eu -o pipefail

if [ $# == 0 ]; then
    echo ""First param should be /srv/user_name/virtualenvs/name_virtual_env""
    echo ""Second param should be name of temp_directory""
    echo ""Third param directory should be git url""
    echo ""Fourth param should be dag zip name, i.e dag_zip.zip to be copied into AIRFLOW__CORE__DAGS__FOLDER""
    echo ""Fifth param should be package name, i.e classify_business""
fi


venv_path=${1}
dir_tmp=${2}
git_url=${3}
dag_zip=${4}
pkg_name=${5}



python3 -m venv $venv_path
source $venv_path/bin/activate
mkdir $dir_tmp
cd $dir_tmp

python3 -m pip install --prefix=$PWD git+$git_url

zip -r $dag_zip *
cp $dag_zip $AIRFLOW__CORE__DAGS_FOLDER

rm -r $dir_tmp


The shell will install my package along with dependencies directly from gitlab, zip and then move to the dags folder.

This is the content of the folder tmp_dir before being zipped.

bin  
lib  
lib64  
predict_dag.py  
train_dag.py


Airflow doesn't seem to be able to import package installed in lib or lib64.
I'm getting this error


  ModuleNotFoundError: No module named 'vertica_python'


I even tried to move my custom package outside of lib:

bin
my_custom_package
lib  
lib64  
predict_dag.py  
train_dag.py


But still getting same error.

PS: One of the problem I think relies on how to use pip to install package in a specific location.
Airflow example use --install-option=""--install-lib=/path/"" but it's unsupported:


  Location-changing options found in --install-option: ['--install-lib']
  from command line. This configuration may cause unexpected behavior
  and is unsupported. pip 20.2 will remove support for this
  functionality. A possible replacement is using pip-level options like
  --user, --prefix, --root, and --target. You can find discussion regarding this at https://github.com/pypa/pip/issues/7309.


Using --prefix leads to a structure like above, with module not found error.

Using --target leads to every package installed in the directory specified.
In this case I have a pandas related error

C extension: No module named 'pandas._libs.tslibs.conversion' not built


I guess that it's related to dynamic libraries that should be available at a system level?

Any hint?

Thanks
","Ciao Marco,
I know this is an old question, but I had to go through the very same process and what worked for me was to use:
pip install -r ../requirements_dag.txt --target=""$PWD""

The same works for packages hosted on git. The key difference is the use of --target rather than --prefix.
","The Airflow documentation page you're referring to says this about packaged DAGs:


  To allow this you can create a zip file that contains the DAG(s) in the root of the zip file and have the extra modules unpacked in directories.


The way I interpret this is different from yours. I don't think Airflow handles these packaged DAGs as a real python package. It just seems like a custom zip folder that will be added to your DAGs folder. So the lib or lib64 folders you have are probably not real python modules (they don't have a __init__.py file). That's why they say that ""the extra modules should be unpacked in directories"".

Look at the example zip file they give:

my_dag1.py
my_dag2.py
package1/__init__.py
package1/functions.py


package1 has a __init__.py file. So in your case, your vertica_python library should be directly importable like this:

my_custom_package
vertica_python/
predict_dag.py  
train_dag.py


However, I don't think you should do this. I have the impression that the modules that you should add here are your own developed modules, not third party libraries.

So I suggest that you install the libraries you need to run your packaged DAGs beforehand.
",,false,
https://stackoverflow.com/questions/29103540,false,The issue does not involve an API exhibiting unexpected failures or unpredictable behaviors under specific runtime conditions. It seems to be related to the usage of the scipy.sparse API and the expected behavior of the identity matrix multiplication.,,,,,,,Scipy Sparse: Unexpected Identity Behavior,"Following is my attempt to create a sparse matrix that has c as its diagonal. I know there is also alternative methods for this, but I'm rather curious why the following code is not working as expected:

import numpy as np
import scipy.sparse as sparse

c = np.arange(0,5)
&gt;&gt;&gt; np.identity(5)*c
array([[ 0.,  0.,  0.,  0.,  0.],
       [ 0.,  1.,  0.,  0.,  0.],
       [ 0.,  0.,  2.,  0.,  0.],
       [ 0.,  0.,  0.,  3.,  0.],
       [ 0.,  0.,  0.,  0.,  4.]])
&gt;&gt;&gt; sparse.identity(5)*c
array([ 0.,  1.,  2.,  3.,  4.])
#expected output:
&lt;5x5 sparse matrix of type '&lt;type 'numpy.float64'&gt;'
with 5 stored elements (1 diagonals) in DIAgonal format&gt;
# and (sparse.identity(5)*c).todense() == np.identity(5)*c

","There are 2 common types of array multiplication, element-by-element, and matrix.

In MATLAB, * is the matrix version, .* is the element by element one.

In numpy, * is element by element (with broadcasting), 'np.dot' is the basic form of matrix multiplication.  Python developers have approved @ as an operator that could be used for matrix multiplication (eventually).

For numpy matrix subclass, * is the matrix multiplication, np.multiply is used for element-by-element.  (np.multiply also works for ndarray.)  

scipy follows the np.matrix convention.  * is the matrix multiplication.  sparse.identity(5).multiply(c) does the element by element multiplication (though it returns a np.matrix, not a sparse one).

As to why, it comes down to conventions that the developers were used to.  For linear algebra problems, matrix multiplication is common, hence it's use in sparse.  np.matrix copies the MATLAB conventions.  MATLAB was created to give access to FORTRAN matrix libraries.

In physics there's another convention, Einstein notation.  This is a generalized matrix multiplication, extended to more dimensions. np.einsum does this. It can implement element by element multiplication, though at its core it uses a 'sum of products' method.  But it's not been implemented for np.matrix or sparse (and really isn't needed since those are always 2d).

Notice how similar the specifications are for your example

np.einsum('ij,j-&gt;ij',np.identity(5),c) # element by element
np.einsum('ij,j-&gt;i',np.identity(5),c)  # matrix (sum on j)

","In the expression sparse.identity(5)*c, the multiplication operator of the sparse matrix is used, which is the algebraic matrix multiplication (i.e. a matrix times a vector gives a vector).

You can create a sparse diagonal matrix with a given diagonal using scipy.sparse.diags:

In [18]: from scipy import sparse

In [19]: c = np.arange(5)

In [20]: d = sparse.diags(c, 0)

In [21]: d
Out[21]: 
&lt;5x5 sparse matrix of type '&lt;type 'numpy.float64'&gt;'
    with 5 stored elements (1 diagonals) in DIAgonal format&gt;

In [22]: d.A
Out[22]: 
array([[ 0.,  0.,  0.,  0.,  0.],
       [ 0.,  1.,  0.,  0.,  0.],
       [ 0.,  0.,  2.,  0.,  0.],
       [ 0.,  0.,  0.,  3.,  0.],
       [ 0.,  0.,  0.,  0.,  4.]])

",,false,
https://stackoverflow.com/questions/67142952,true,The issue involves unexpected behavior when fitting a sin wave to a large dataset using scipy.optimize.curve_fit. The API does not handle the large dataset properly and fits the data to a horizontal line instead of a sin wave.,,,,,,,scipy weird unexpected behavior curve_fit large data set for sin wave,"For some reason when I am trying to large amount of data to a sin wave it fails and fits it to a horizontal line. Can somebody explain?
Minimal working code:
import numpy as np
import matplotlib.pyplot as plt
from scipy import optimize
# Seed the random number generator for reproducibility
import pandas

np.random.seed(0)

# Here it work as expected
# x_data = np.linspace(-5, 5, num=50)
# y_data = 2.9 * np.sin(1.05 * x_data + 2) + 250 + np.random.normal(size=50)

# With this data it breaks
x_data = np.linspace(0, 2500, num=2500)
y_data = -100 * np.sin(0.01 * x_data + 1) + 250 + np.random.normal(size=2500)

# And plot it

plt.figure(figsize=(6, 4))
plt.scatter(x_data, y_data)


def test_func(x, a, b, c, d):
    return a * np.sin(b * x + c) + d

# Used to fit the correct function
# params, params_covariance = optimize.curve_fit(test_func, x_data, y_data)

# making some guesses
params, params_covariance = optimize.curve_fit(test_func, x_data, y_data,
                                               p0=[-80, 3, 0, 260])

print(params)
plt.figure(figsize=(6, 4))
plt.scatter(x_data, y_data, label='Data')
plt.plot(x_data, test_func(x_data, *params),
         label='Fitted function')

plt.legend(loc='best')

plt.show()




Does anybody know, how to fix this issue. Should I use a different fitting method not least square? Or should I reduce the number of data points?
","Given your data, you can use the more robust lmfit instead of scipy.
In particular, you can use SineModel (see here for details).
SineModel in lmfit is not for ""shifted"" sine waves, but you can easily deal with the shift doing
y_data_offset = y_data.mean()
y_transformed = y_data - y_data_offset
plt.scatter(x_data, y_transformed)
plt.axhline(0, color='r')


Now you can fit to sine wave
from lmfit.models import SineModel

mod = SineModel()

pars = mod.guess(y_transformed, x=x_data)
out = mod.fit(y_transformed, pars, x=x_data)

you can inspect results with print(out.fit_report()) and plot results with
plt.plot(x_data, y_data, lw=7, color='C1')
plt.plot(x_data, out.best_fit+y_data_offset, color='k')
#           we add the offset ^^^^^^^^^^^^^


or with the builtin plot method out.plot_fit(), see here for details.
Note that in SineModel all parameters ""are constrained to be non-negative"", so your defined negative amplitude (-100) will be positive (+100) in the parameters fit results. So the phase too won't be 1 but π+1 (PS: they call shift the phase)
print(out.best_values)

{'amplitude': 99.99631403054289,
 'frequency': 0.010001193681616227,
 'shift': 4.1400215410836605}

",,,false,
https://stackoverflow.com/questions/24242660,false,The issue does not involve an API exhibiting unexpected failures or unpredictable behaviors under specific runtime conditions. The details of the issue are not provided.,,,,,,,pymc3 : Multiple observed values,,,,,false,
https://stackoverflow.com/questions/50831551,false,The issue is not related to an API exhibiting unexpected failures or unpredictable behaviors under specific runtime conditions. It is a question about parallel/multithread differential evolution in Python and does not meet the criteria for deeper analysis.,,,,,,,parallel/multithread differential evolution in python,"I'm trying to model a biochemical process, and I structured my question as an optimization problem, that I solve using  differential_evolution from scipy.
So far, so good, I'm pretty happy with the  implementation of a simplified model with 15-19 parameters.
I expanded the model and now, with 32 parameters, is taking way too long. Not  totally unexpected, but still an issue, hence the question.

I've seen:
- an almost identical question for R  Parallel differential evolution
- and a github issue https://github.com/scipy/scipy/issues/4864 on the topic  

but it would like to stay in python (the model is within a python pipeline), and the pull request did not lead to and officially accepted solution yet, although some options have been suggested.

Also, I can't parallelize the code within the function to be optimised because is a series of sequential calculations each requiring the result of the previous step. The ideal option would be to have something that evaluates some individuals in parallel and return them to the population.

Summing up:
- Is there any option within scipy that allows parallelization of differential_evolution that I dumbly overlooked? (Ideal solution)
- Is there a suggestion for an alternative algorithm in scipy that is either (way) faster in serial or possible to parallelize?
- Is there any other good package that offers parallelized differential evolution funtions? Or other applicable optimization methods?
- Sanity check: am I overloading DE with 32 parameter and I need to radically change approach?

PS
I'm a biologist, formal math/statistics isn't really my strenght, any formula-to-english translation would be hugely appreciated :)

PPS
As an extreme option I could try to migrate to R, but I can't code C/C++ or other languages.
","Scipy differential_evolution can now be used in parallel extremely easily, by specifying the workers:


  workers int or map-like callable, optional
  
  If workers is an int the population is subdivided into workers
  sections and evaluated in parallel (uses multiprocessing.Pool). Supply
  -1 to use all available CPU cores. Alternatively supply a map-like callable, such as multiprocessing.Pool.map for evaluating the
  population in parallel. This evaluation is carried out as
  workers(func, iterable). This option will override the updating
  keyword to updating='deferred' if workers != 1. Requires that func be
  pickleable.
  
  New in version 1.2.0.


scipy.optimize.differential_evolution documentation
","Thanks to @jp2011 for pointing to pygmo

First, worth noting the difference from pygmo 1, since the fist link on google still directs to the older version.

Second, Multiprocessing island are available only for python 3.4+

Third, it works. The processes I started when I first asked the question are still running while I write, the pygmo archipelago running an extensive test of all the 18 possible DE variations present in saDE made in less than 3h. The compiled version using Numba as suggested here https://esa.github.io/pagmo2/docs/python/tutorials/coding_udp_simple.html will probably finish even earlier. Chapeau.

I personally find it a bit less intuitive than the scipy version, given the need to build a new class (vs a signle function in scipy) to define the problem but is probably just a personal preference. Also, the mutation/crossing over parameters are defined less clearly, for someone approaching DE for the first time might be a bit obscure.
But, since serial DE in scipy just isn't cutting it, welcome pygmo(2).

Additionally I found a couple other options claiming to parallelize DE. I didn't test them myself, but might be useful to someone stumbling on this question.  

Platypus, focused on multiobjective evolutionary algorithms 
https://github.com/Project-Platypus/Platypus

Yabox
https://github.com/pablormier/yabox

from Yabox creator a detailed, yet IMHO crystal clear, explaination of DE
https://pablormier.github.io/2017/09/05/a-tutorial-on-differential-evolution-with-python/
","I suggest the batch mode of PyFDE.
https://pythonhosted.org/PyFDE/tutorial.html#batch-mode
In batch mode, the fitness function will be called only once per iteration to evaluate the fitness of all the population.
The example w/o the batch mode:
import pyfde
from math import cos, pi
import time
import numpy

t1=time.time()
def fitness(p):
    x, y = p[0], p[1]
    val = 20 + (x**2 - 10*cos(2*pi*x)) + (y**2 - 10*cos(2*pi*y))
    return -val
    
solver = pyfde.ClassicDE(fitness, n_dim=2, n_pop=40, limits=(-5.12, 5.12))
solver.cr, solver.f = 0.9, 0.45
best, fit = solver.run(n_it=150)
t2=time.time()
print(""Estimates: "",best)
print(""Normal mode elapsed time (s): "",t2-t1)

The batch mode example:
t1=time.time()
def vec_fitness(p,fit):
    x, y = numpy.array(p[:,0]), numpy.array(p[:,1])
    val = 20 + (x**2 - 10*numpy.cos(2*pi*x)) + (y**2 - 10*numpy.cos(2*pi*y))
    fit[:] = -val
    
solver = pyfde.ClassicDE(vec_fitness, n_dim=2, n_pop=40, limits=(-5.12, 5.12), batch=True)
solver.cr, solver.f = 0.9, 0.45
best, fit = solver.run(n_it=150)
t2=time.time()
print(""Estimates: "",best)
print(""Batch mode elapsed time (s): "",t2-t1)

The output is:
Estimates:  [1.31380987e-09 1.12832169e-09]
Normal mode elapsed time (s):  0.015959978103637695
Estimates:  [2.01733383e-10 1.23826873e-10]
Batch mode elapsed time (s):  0.006017446517944336
############################################################
It's 1.5x faster, but only for a simple question. You can see &gt;10x faster for a complex question.
The code runs on a single CPU core (no multi-processing), and the performance improvement comes from the use of vectorization and MIMD (multiple instruction, multiple data). Combining vectorization and parallel/multi-processing will result in a double-improvement.
",false,
https://stackoverflow.com/questions/28321286,false,The issue is not related to an API exhibiting unexpected failures or unpredictable behaviors under specific runtime conditions. It is a question about pandas rolling functions and does not meet the criteria for deeper analysis.,,,,,,,pandas rolling_quantile bug?,"i recently bumped an unexpected issue with pandas rolling funcs. rolling_quantile for example:

&gt;&gt; row = 10
&gt;&gt; col = 5
&gt;&gt; idx = pd.date_range(20100101,periods=row,freq='B')
&gt;&gt; a = pd.DataFrame(np.random.rand(row*col).reshape((row,-1)),index=idx)
&gt;&gt; a
                   0           1           2           3           4
2010-01-01  0.341434    0.497274    0.596341    0.259909    0.872207
2010-01-04  0.222653    0.056723    0.064019    0.936307    0.785647
2010-01-05  0.179067    0.647165    0.931266    0.557698    0.713282
2010-01-06  0.049766    0.259756    0.945736    0.380948    0.282667
2010-01-07  0.385036    0.517609    0.575958    0.050758    0.850735
2010-01-08  0.628169    0.510453    0.325973    0.263361    0.444959
2010-01-11  0.099133    0.976571    0.602235    0.181185    0.506316
2010-01-12  0.987344    0.902289    0.080000    0.254695    0.753325
2010-01-13  0.759198    0.014548    0.139858    0.822900    0.251972
2010-01-14  0.404149    0.349788    0.038714    0.280568    0.197865

&gt;&gt; a.quantile([0.25,0.5,0.75],axis=0)
               0           1           2           3           4
0.25    0.189963    0.282264    0.094964    0.255999    0.323240
0.50    0.363235    0.503864    0.450966    0.271964    0.609799
0.75    0.572164    0.614776    0.600761    0.513510    0.777567

&gt;&gt; np.percentile(a,[25,50,75],axis=0)
[array([ 0.18996316,  0.28226404,  0.09496441,  0.25599853,  0.32323997]),
 array([ 0.36323529,  0.50386356,  0.45096554,  0.27196429,  0.60979881]),
 array([ 0.57216415,  0.61477607,  0.6007611 ,  0.51351021,  0.7775667 ])]

&gt;&gt; pd.rolling_quantile(a,row,0.25).tail(1)
                   0           1       2           3           4
2010-01-14  0.179067    0.259756    0.08    0.254695    0.282667


looks like pandas.DataFrame.quantile member func is consistent with the numpy.percentile func.  however the pandas.rolling_quantile func returns diff results.  reduce the row number to 5, the problem will be gone (all three methods return the same results).  any thoughts?

ps: i also tested rolling_std func which will ""random"" generate error with 10^-7 ~ 10^-8 scales for long (row-wise) pandas.DataFrames

python environment:


python 3.4.2
cython 0.21.1
numpy 1.8.2
scipy 0.14.0
pandas 0.15.1
statsmodels 0.6.0

",,,,false,
https://stackoverflow.com/questions/55428503,true,"The issue involves the scipy.sparse.kron function, which is expected to produce a sparse matrix but is giving non-sparse results. This API is exhibiting unexpected behavior under specific runtime conditions.",,,,,,,Scipy sparse.kron gives non-sparse matrix,"I am getting unexpected non-sparse results when using the kron method of Scipy's sparse module. Specifically, matrix elements that are equal to zero after performing the kronecker product are being kept in the result, and I'd like to understand what I should do to ensure the output is still fully sparse.

Here's an example of what I mean, taking the kronecker product of two copies of the identity:

import scipy.sparse as sp

s = sp.eye(2)

S = sp.kron(s,s)

S 
&lt;4x4 sparse matrix of type '&lt;class 'numpy.float64'&gt;'
with 8 stored elements (blocksize = 2x2) in Block Sparse Row format&gt;

print(S)

(0, 0)  1.0
(0, 1)  0.0
(1, 0)  0.0
(1, 1)  1.0
(2, 2)  1.0
(2, 3)  0.0
(3, 2)  0.0
(3, 3)  1.0


The sparse matrix S should only contain the 4 (diagonal) non-zero entries, but here it also has other entries that are equal to zero. Any pointers on what I am doing wrong would be much appreciated.
","In 

Converting from sparse to dense to sparse again decreases density after constructing sparse matrix

I point out that sparse.kron produces, by default a BSR format matrix.  That's what your display shows.  Those extra zeros are part of the dense blocks.

If you specify another format, kron will not produce  those zeros:

In [672]: sparse.kron(s,s,format='csr')                                         
Out[672]: 
&lt;4x4 sparse matrix of type '&lt;class 'numpy.float64'&gt;'
    with 4 stored elements in Compressed Sparse Row format&gt;
In [673]: _.A                                                                   
Out[673]: 
array([[1., 0., 0., 0.],
       [0., 1., 0., 0.],
       [0., 0., 1., 0.],
       [0., 0., 0., 1.]])

",,,false,
https://stackoverflow.com/questions/50932433,false,The issue is not related to an API exhibiting unexpected failures or unpredictable behaviors under specific runtime conditions. It is a question about the difference in results between Scipy and Sklearn chi2 implementations and does not meet the criteria for deeper analysis.,,,,,,,Scipy and Sklearn chi2 implementations give different results,"I an using sklearn.feature_selection.chi2 for feature selection and found out some unexpected results (check the code). Do anyone knows what is the reason or can point me to some documentation or pull request?

I include a comparison of the results I got and the expected ones obtained by hand and using scipy.stats.chi2_contingency.

The code:

import numpy as np
import pandas as pd
from scipy.stats import chi2_contingency
from sklearn.feature_selection import chi2, SelectKBest

x = np.array([[1, 1, 1, 0, 1], [1, 0, 1, 0, 0], [0, 0, 1, 1, 1], [0, 0, 1, 1, 0], [0, 0, 0, 1, 1], [0, 0, 0, 1, 0]])
y = np.array([1, 1, 2, 2, 3, 3])

scores = []
for i in range(x.shape[1]):
    result = chi2_contingency(pd.crosstab(x[:, i], y))
    scores.append(result[0])

sel = SelectKBest(score_func=chi2, k=3)
sel.fit(x, y)

print(scores)
print(sel.scores_)
print(sel.get_support())


The results are:

[6., 2.4, 6.0, 6.0, 0.0] (Expected)
[4. 2. 2. 2. 0.] (Unexpected)
[ True  True False  True False]


Using scipy, it keeps features 0, 2, 3, while, with sklearn it keeps features 0,1,3.
","Yes, they do give different results. And I think you should trust the results from scipy, and reject the results from sklearn.
But let me provide details of my reasoning, because I could be wrong.
I lately observed a similar effect to what you describe, with a data set of 300 data points: the results of the two chi2 implementations differ indeed. In my case the difference was striking. I described the issue in details in this article , followed by this Cross Validated discussion thread and I also submitted a bug request to sklearn, available for review here.
The added value from my research, if any, seems to be that the results delivered by the scipy implementation seem correct, while the results from sklearn are incorrect. Please see the article for the details. But I only focused on my sample, so the conclusion may not be universally true. Sadly the source code analysis is beyond my capability, but I hope this input can help someone to possibly either improve the code, or disprove my reasoning if wrong.
",,,false,
https://stackoverflow.com/questions/50142269,true,"The issue involves the scipy.special.jn_zeros function, which is not returning the root at zero. This API is exhibiting unexpected behavior under specific runtime conditions.",,,,,,,Why does scipy bessel root finding not return roots at zero?,"I am trying to use code which uses Bessel function zeros for other calculations. I noticed the following piece of code produces results that I consider unexpected.

    import scipy
    from scipy import special

    scipy.special.jn_zeros(1,2)


I would expect the result from this call to be

    array([0., 3.83170597])


instead of

    array([3.83170597, 7.01558667])


Is there a reason a reason why the root at x=0.0 is not being returned?

From what I can see the roots are symmetric along the x-axis except for any found at the origin, but I do not think this would be enough of a reason to leave off the root completely.

The computer I am using has python version 2.7.10 installed and is using scipy version 0.19.0

P.S. the following function is what I am trying to find the zeros of

    scipy.special.j1

",,,,false,
https://stackoverflow.com/questions/46556376,true,The issue involves the cut_tree() function from the scipy.cluster.hierarchy module. The function does not return the requested number of clusters in some cases when given a linkage matrix obtained using the fastcluster package. This indicates a potential problem with the cut_tree() function or a discrepancy between the two packages.,scipy,scipy.cluster.hierarchy.cut_tree,"The cut_tree() function does not return the expected number of clusters when given a linkage matrix obtained using the fastcluster package. In the provided code snippet, the function is called with a linkage matrix (Z_2) obtained from fastcluster.linkage_vector(). However, when requesting 33 clusters, the function returns only 32 clusters.",The cut_tree() function is expected to return the requested number of clusters when given a valid linkage matrix.,The issue is triggered when cut_tree() is called with a linkage matrix obtained using fastcluster.linkage_vector().,This issue might be challenging to detect because it involves the interaction between two different packages (scipy and fastcluster) and the specific implementation of the cut_tree() function.,Scipy&#39;s cut_tree() doesn&#39;t return requested number of clusters and the linkage matrices obtained with scipy and fastcluster do not match,"I'm doing an agglomerative hierarchical clustering (AHC) experiment using the fastcluster package in connection with scipy.cluster.hierarchy module functions, in Python 3, and I found a puzzling behaviour of the cut_tree() function.

I cluster data with no problem and get a linkage matrix, Z, using linkage_vector() with method=ward. Then, I want to cut the dendogram tree to get a fixed number of clusters (e.g. 33) and I do this properly using cut_tree(Z, n_clusters=33). (Remember that AHC is a deterministic method yielding a binary tree connecting all your datapoints, which sit at the leafs of the tree; you can look at this tree at any level to ""see"" the number of clusters you want in the end; all cut_tree() does is to return a set of 'n_cluster' integer labels from 0 to n_clusters - 1, attributed to every point of the dataset.)

I've done this many times in other experiments and I always get the number of clusters I request. The problem is that with this one dataset, when I ask cut_tree() for 33 clusters, it gives me only 32. I don't see why this is the case. Could it be a bug? Are you aware of any bug with cut_tree()? I tried to debug this behaviour and performed the same clustering experiment using scipy's linkage() function. With the resulting linkage matrix as input to cut_tree() I didn't get an unexpected number of clusters as output. I also verified that the linkage matrices output by the two methods are not equal.

The [dataset] I'm using consists of 10680 vectors, each with 20 dimensions. Check the following experiment:

import numpy as np
import fastcluster as fc
import scipy.cluster.hierarchy as hac
from scipy.spatial.distance import pdist

### *Load dataset (10680 vectors, each with 20 dimensions)*
X = np.load('dataset.npy')

### *Hierarchical clustering using traditional scipy method*
dists = pdist(X)
Z_1 = hac.linkage(dists, method='ward')

### *Hierarchical clustering using optimized fastcluster method*
Z_2 = fc.linkage_vector(X, method='ward')

### *Comparissons*

## Are the linkage matrices equal?
print(""Z_1 == Z_2 ? "", np.allclose(Z_1, Z_2))

## Is scipy's cut_tree() returning the requested number of clusters when using Z_2?
print(""Req.\tGot\tequal?"")
for i in range(1,50):
    cut = hac.cut_tree(Z_2, i)
    uniq = len(np.unique(cut))
    print(i,""\t"",uniq,""\t"",i==uniq)

## The same as before, but in condensed form. When requesting cut_tree() for clusters
#  in the range [1,50] does it return wrong results at some point?
print(""Any problem cutting Z_1 for n_clusters in [1,50]? "", not np.all([len(np.unique(
                                      hac.cut_tree(Z_1, i)))==i for i in range(1,50)]))
print(""Any problem cutting Z_2 for n_clusters in [1,50]? "", not np.all([len(np.unique(
                                      hac.cut_tree(Z_2, i)))==i for i in range(1,50)]))

#Output:
#
#Z_1 == Z_2 ?  False
#
#Req.    Got     equal?
#1        1       True
#2        2       True
#3        3       True
#4        4       True
#5        5       True
#6        6       True
#7        7       True
#8        8       True
#9        9       True
#10       10      True
#11       11      True
#12       12      True
#13       13      True
#14       14      True
#15       15      True
#16       16      True
#17       17      True
#18       18      True
#19       19      True
#20       20      True
#21       21      True
#22       22      True
#23       23      True
#24       24      True
#25       25      True
#26       26      True
#27       27      True
#28       28      True
#29       29      True
#30       30      True
#31       31      True
#32       32      True
#33       32      False
#34       33      False
#35       34      False
#36       35      False
#37       36      False
#38       37      False
#39       38      False
#40       39      False
#41       40      False
#42       41      False
#43       42      False
#44       43      False
#45       44      False
#46       45      False
#47       46      False
#48       47      False
#49       48      False
#
#Any problem cutting Z_1 for n_clusters in [1,50]?  False
#Any problem cutting Z_2 for n_clusters in [1,50]?  True


You might have noticed the dataset contains 37 vectors with at least an exact copy, and counting all the copies there is a total 55 vectors with at least a copy in the dataset.

For inspection, I decided to plot the dendrogram tree up to a shallow depth level for the two linkage matrices, which you can see on the image bellow (Z_1 at the top and Z_2 at the bottom). Numbers inside parenthesis indicate the population contained bellow in that branch; numbers without parenthesis are leafs of the tree (the number is the index of the vector in the X matrix). One can see the only difference (at the plotted level) is at the branches marked with the red square, which coalesce at 0 distance as they contain overlapping vectors.



So, I ran the clustering procedures as shown in the previous code again, but this time with only the subset of the data containing the 55 vectors which have at least a copy. I obtained X_subset with:

uniqs, uniqs_indices, uniqs_count = np.unique(X, axis=0, return_index=True, return_counts=True)
duplicate_rows_indices = list( set(range(len(X))) - set(uniqs_indices) )
number_of_duplicate_rows = len(X)-len(uniqs) # 37

all_duplicate_rows = set()
for i in duplicate_rows_indices:
    _rows = set(np.where(X == X[i])[0])
    for j in _rows:
        all_duplicate_rows.add(j)

rows_with_at_least_a_copy = list(all_duplicate_rows)
number_of_rows_with_at_least_a_copy = len(rows_with_at_least_a_copy)  # 55

X_subset = X[rows_with_at_least_a_copy]


and my output this time was:

#Z_1 == Z_2 ?  False
#Req.    Got     equal?
#1        1       True
#2        2       True
#3        2       False
#4        3       False
#5        4       False
#6        5       False
#7        6       False
#8        7       False
#9        8       False
#10       9       False
#11       10      False
#12       11      False
#13       12      False
#14       13      False
#15       14      False
#16       15      False
#17       16      False
#18       17      False
#19       18      False
#20       20      True
#21       21      True
#22       22      True
#23       23      True
#24       24      True
#25       25      True
#26       26      True
#27       27      True
#28       28      True
#29       29      True
#30       30      True
#31       31      True
#32       32      True
#33       33      True
#34       34      True
#35       35      True
#36       36      True
#37       37      True
#38       38      True
#39       39      True
#40       40      True
#41       41      True
#42       42      True
#43       43      True
#44       44      True
#45       45      True
#46       46      True
#47       47      True
#48       48      True
#49       49      True
#Any problem cutting Z_1 for n_clusters in [1,50]?  False
#Any problem cutting Z_2 for n_clusters in [1,50]?  True


Thus, fastcluster and scipy are not returning the same results, and if it is only due to the overlapping points this could be acceptable because of the ambiguity of that clustering situation. But the problem is cut_tree() which sometimes doesn't return the requested number of clusters in these cases when given the linkage matrix obtained by linkage_vector(). How can this be fixed?

Library versions used: scipy '0.19.1', numpy '1.13.3', fastcluster '1.1.24'

Edit: It's also posted here: https://github.com/scipy/scipy/issues/7977.
",,,,false,
https://stackoverflow.com/questions/76011928,true,"The issue involves the np.linalg.eigvalsh function from the numpy library. The function returns negative eigenvalues for a given covariance matrix, which is unexpected behavior for a valid covariance matrix.",numpy,np.linalg.eigvalsh,"The np.linalg.eigvalsh function returns negative eigenvalues for a given covariance matrix, which contradicts the expected behavior for a valid covariance matrix. The provided code snippet demonstrates this issue.","The np.linalg.eigvalsh function is expected to return the eigenvalues of a symmetric matrix, such as a covariance matrix.",The issue is triggered when np.linalg.eigvalsh is called with a specific covariance matrix that results in negative eigenvalues.,"This issue might be challenging to detect because it involves the understanding and interpretation of eigenvalues in the context of covariance matrices, as well as potential rounding or precision errors in the input matrix.",Python: np.linalg.eigvalsh returning negatve eigen values,"I have a 7x7 covariance matrix (represented as an numpy array).
t = np.array(
    [
        [1.4, 0.3, 0.4, 0.8, 0.4, 0.9, 0.3],
        [0.3, 1.3, 0.4, 2.3, 0.4, 2.4, 0.4],
        [0.4, 0.4, 1.3, 2.8, 0.4, 1.0, 0.3],
        [0.8, 2.3, 2.8, 9.5, 1.0, 7.0, 1.0],
        [0.4, 0.4, 0.4, 1.0, 1.1, 1.2, 0.3],
        [0.9, 2.4, 1.0, 7.0, 1.2, 7.7, 1.0],
        [0.3, 0.4, 0.3, 1.0, 0.3, 1.0, 0.5],
    ],
    dtype=np.float64,
)

I have checked this matrix to be symmetric.
np.allclose(t, t.T)
True

And np.linalg.svd returns valid non-negative singular values. However, np.linalg.eigvalsh, is returning a negative eigenvalue.
min(np.linalg.eigvalsh(t))
-0.06473876145336957

This doesnt make too much sense to me as I have checked that the column of the matrix are linearly independent (getting the reduced row echelon form of the matrix).
import sympy
reduced_form, inds = sympy.Matrix(t.values).rref()

I see similar issues where people have reported eigvalsh to return negative eigenvalue for a well behaved matrix but none of the suggestions have helped.
Examples:

Why is scipy's eigh returning unexpected negative eigenvalues?
numpy.cov or numpy.linalg.eigvals gives wrong results

I was wondering if anyone has faced a similar issue with np.linalg.eigvalsh and have any recommendations on how to solve it?
Thank you so much.
","It's totally normal for symmetric matrices to have negative eigenvalues. A matrix being Hermitian only guarantees that its eigenvalues are real. It doesn't guarantee that the eigenvalues are positive.
It's not normal for a covariance matrix to have negative eigenvalues, meaning this isn't actually a covariance matrix, despite what you thought. Perhaps you rounded the entries of an actual covariance matrix?
",,,false,
https://stackoverflow.com/questions/60172680,true,The issue involves the scipy.linalg.ldl function from the scipy library. The decomposition does not return the expected result when attempting to reconstruct the original matrix using the obtained lower triangular matrix and diagonal matrix.,scipy,scipy.linalg.ldl,The scipy.linalg.ldl function does not return the expected result when attempting to reconstruct the original matrix using the obtained lower triangular matrix (l) and diagonal matrix (d). The provided code snippet demonstrates this issue.,"The scipy.linalg.ldl function is expected to perform an LDL decomposition of a symmetric matrix and return the lower triangular matrix (l), diagonal matrix (d), and row permutation matrix (p) such that the original matrix can be reconstructed as x = l @ d @ l.transpose().",The issue is triggered when scipy.linalg.ldl is called with a specific matrix that does not meet the requirements of the LDL decomposition algorithm or when the matrix is not symmetric.,This issue might be challenging to detect because it involves understanding the requirements and limitations of the LDL decomposition algorithm and the specific properties of the input matrix.,Scipy LDL decomposition returning unexpected result,"I have generated a random 5*5 matrix x like so:

&gt;&gt;&gt; x = np.random.randn(5,5)


And decomposed it using the scipy.linalg.ldl decomposition like so:

&gt;&gt;&gt; l, d, p = la.ldl(x)


Using l, d and p I want to return x. I thought I would be able to do the following:

&gt;&gt;&gt; l[p,:] @ d @ l[p,:].transpose() - x


But this does not give me zero as I would have expected. Can anyone explain where I am going wrong?

My aim is to obtain the lower diagonal matrix L such that x = LDL^T without the need for the row permutation matrix p, but I am very confused as to what scipy is giving as output.
","The LDL decomposition algorithm only applies for Hermitian/symmetric matrices. You are passing it a matrix with random values which is very unlikely to be symmetric. In addition, the matrix multiplication should be performed without applying the permutation matrix to the lower triangular matrices.

When a non-symmetric matrix is passed to scipy.linalg.ldl, only the lower or upper triangular part of the matrix is referenced, depending on the value of the lower keyword argument, which defaults to True. We can see the effects of this with np.isclose():

&gt;&gt;&gt; x = np.random.randn(5,5)
&gt;&gt;&gt; l, d, p = la.ldl(x)
&gt;&gt;&gt; np.isclose(l.dot(d).dot(l.T) - x, 0)
[[ True False False False False]
 [ True  True False False False]
 [ True  True  True False False]
 [ True  True  True  True False]
 [ True  True  True  True  True]]


Here we see that the upper triangular part of the matrix has been assumed to be symmetric, and so the algorithm has returned values that would be correct if that were the case.

Below, we pass la.ldl an actual symmetric matrix, and obtain the expected result.

&gt;&gt;&gt; x = np.array([[1, 2, 3],
                  [2, 4, 5],
                  [3, 5, 6]])
&gt;&gt;&gt; l, d, p = la.ldl(x)
&gt;&gt;&gt; print(np.isclose(l.dot(d).dot(l.T) - x, 0))
[[ True  True  True]
 [ True  True  True]
 [ True  True  True]]


If you're looking for a decomposition into LDL^T in general, without having permutations, this reduces the field of matrices even further. Your matrix also needs to be positive definite.

Here is an example with one such matrix:

&gt;&gt;&gt; x = np.array([[2, -1, 0],
                  [-1, 3, -1],
                  [0, -1, 4]])
&gt;&gt;&gt; l, d, p = la.ldl(x)
&gt;&gt;&gt; l
array([[ 1. ,  0. ,  0. ],
       [-0.5,  1. ,  0. ],
       [ 0. , -0.4,  1. ]])
&gt;&gt;&gt; d
array([[2. , 0. , 0. ],
       [0. , 2.5, 0. ],
       [0. , 0. , 3.6]])
&gt;&gt;&gt; p
array([0, 1, 2], dtype=int64)


As you can see, the permutation p is just [0, 1, 2], and l is already lower triangular.
",,,false,
https://stackoverflow.com/questions/58212061,true,The issue involves the scipy.spatial.distance.euclidean function from the scipy library. The function does not return the expected result when calculating the Euclidean distance between two sets of vectors.,scipy,scipy.spatial.distance.euclidean,"The scipy.spatial.distance.euclidean function does not return the expected result when calculating the Euclidean distance between a set of vectors (X) and a fixed vector ([0.2, 2, 40, 40, 0, 25]). The provided code snippet demonstrates this issue.",The scipy.spatial.distance.euclidean function is expected to calculate the Euclidean distance between two sets of vectors.,The issue is triggered when scipy.spatial.distance.euclidean is called with a specific set of vectors (X) and a fixed vector.,"This issue might be challenging to detect because it involves understanding the input data and the expected behavior of the Euclidean distance calculation, as well as potential issues with the iteration over the vectors in the provided code snippet.",calculating euclidean distance using scipy giving unexpected results,"I want to calculate euclidean distance between X and Y, where X is not static. So, X is define by combination of range.:

a= np.arange(0.2,0.41,0.1)
b= np.arange(2,8,1)
c= np.arange(40,61,5)
d= np.arange(40,61,5)
e= np.arange(0,11,5)
f= np.arange(25,71,5)

from itertools import product
X = list(product(a,b,c,d,e,f))
X = np.around(X,2)

from scipy.spatial.distance import euclidean
dis = []
for x in np.nditer(X):
    d = euclidean(x,[0.2,2,40,40,0,25]) #Inset Y here
    dis.append(d)

min(dis)


Since in this case, to test if everything was working as I wanted, I input Y which is one of possible value of X. I expected my minimum distance to be 0, however in this case, it was not so (~47).
","The problem is that you're iterating over every individual number in X, rather than just iterating over the rows. Just remove the call to np.nditer(X) and iterate over X directly

for x in np.nditer(X):


should just be 

for x in X:

",,,false,
https://stackoverflow.com/questions/40050369,false,The issue does not involve an API exhibiting unexpected failures or unpredictable behaviors under specific runtime conditions. It appears to be an empty issue without any description or code snippets.,,,,,,,Different eigenvalues between scipy.sparse.linalg.eigs and numpy/scipy.eig,,,,,false,
https://stackoverflow.com/questions/32920920,false,The issue does not involve an API exhibiting unexpected failures or unpredictable behaviors under specific runtime conditions. It appears to be an empty issue without any description or code snippets.,,,,,,,Stepsize control of dopri5 integrator,,,,,false,
https://stackoverflow.com/questions/77945259,false,"The issue does not involve an API exhibiting unexpected failures or unpredictable behaviors under specific runtime conditions. It appears to be a problem with the execution of the code on the slurm computing system, rather than an issue with a specific API or library.",,,,,,,"slurm didn&#39;t execute my Python code after running a few lines but also didn&#39;t stop, whereas it worked well on my local Linux","my code:
from datasets import load_dataset
MAX_LEN = 512
dataset = load_dataset(""glue"",""mrpc"")
from transformers import AutoTokenizer
from transformers import RobertaTokenizerFast

#tokenizer =AutoTokenizer.from_pretrained(""bert-base-uncased"")
tokenizer = RobertaTokenizerFast.from_pretrained(""/data/home//raw_roberta/Roberta_Tokenizer"", max_length=MAX_LEN, padding='max_length', return_tensors='pt')

print(""mapped_dataset"")

mapped_dataset = dataset.map(lambda x: tokenizer(x[""sentence1""], x[""sentence2""], max_length = MAX_LEN, truncation=True, padding='max_length', return_tensors='pt'), batched=True)

print(""completeed mapped_dataset"")

from transformers import DataCollatorWithPadding
data_collator= DataCollatorWithPadding(tokenizer=tokenizer)

from transformers import AutoModelForSequenceClassification
from transformers import RobertaForMaskedLM

#model = AutoModelForSequenceClassification.from_pretrained(""bert-base-uncased"",num_labels = 2)
#model = AutoModelForSequenceClassification.from_pretrained(""data/home//raw_roberta/Roberta_Model/checkpoint-90000"", num_labels = 2)
#model = AutoModelForSequenceClassification.from_pretrained(""data/home//raw_roberta/Roberta_Model/checkpoint-90000"")
base_model = RobertaForMaskedLM.from_pretrained('/data/home//raw_roberta/Roberta_Model/checkpoint-90000').roberta

from transformers import TrainingArguments
print(base_model.config)

I run above code on my local linux,it only takes about 2 minutes to execute.the log:
The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization.
The tokenizer class you load from this checkpoint is 'BertTokenizer'.
The class this function is called from is 'RobertaTokenizer'.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization.
The tokenizer class you load from this checkpoint is 'BertTokenizer'.
The class this function is called from is 'RobertaTokenizerFast'.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
mapped_dataset
completeed mapped_dataset
RobertaConfig {
  ""_name_or_path"": ""/data/home//raw_roberta/Roberta_Model/checkpoint-90000"",
  ""architectures"": [
    ""RobertaForMaskedLM""
  ],
  ""attention_probs_dropout_prob"": 0.1,
  ""bos_token_id"": 0,
  ""classifier_dropout"": null,
  ""eos_token_id"": 2,
  ""hidden_act"": ""gelu"",
  ""hidden_dropout_prob"": 0.1,
  ""hidden_size"": 768,
  ""initializer_range"": 0.02,
  ""intermediate_size"": 3072,
  ""layer_norm_eps"": 1e-12,
  ""max_position_embeddings"": 514,
  ""model_type"": ""roberta"",
  ""num_attention_heads"": 12,
  ""num_hidden_layers"": 12,
  ""pad_token_id"": 1,
  ""position_embedding_type"": ""absolute"",
  ""torch_dtype"": ""float32"",
  ""transformers_version"": ""4.33.2"",
  ""type_vocab_size"": 1,
  ""use_cache"": true,
  ""vocab_size"": 52000
}


but when I upload my code to slurm, it run for 4 hours,only get these log:
/data/home//anaconda3/envs/py38v1/lib/python3.8/site-packages/scipy/__init__.py:138: UserWarning: A NumPy version &gt;=1.16.5 and &lt;1.23.0 is required for this version of SciPy (detected version 1.24.4)
  warnings.warn(f""A NumPy version &gt;={np_minversion} and &lt;{np_maxversion} is required for this version of ""
The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. 
The tokenizer class you load from this checkpoint is 'BertTokenizer'. 
The class this function is called from is 'RobertaTokenizer'.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. 
The tokenizer class you load from this checkpoint is 'BertTokenizer'. 
The class this function is called from is 'RobertaTokenizerFast'.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
slurmstepd: error: *** JOB xxxxx ON compute-9-0 CANCELLED AT 2024-02-05T08:35:30 DUE TO TIME LIMIT ***

this problem truly confuses me,anyone know how to fix it? thanks!
","check the python, scipy and numpy versions on the local workstation and the computing node(s) on the cluster. If you can create conda envs on the cluster, create a conda environment with just the python + relevant libs. First on the local workstation to check that it works, then replicate it on HPC.
",,,false,
https://stackoverflow.com/questions/76667318,false,The issue does not meet the criteria for deeper analysis as it does not involve an API exhibiting unexpected failures or unpredictable behaviors under specific runtime conditions.,,,,,,,conda environment.yaml package conflict,"When I'm trying to build a git project, the environment,yml won't work properly. It seems like some package conflict, but other user doesn't seem to run into this issue, and I couldn't figure out a solution. My conda version is conda 4.8.2,and down below is the environment.yaml, requirment.txt and error
environment.yml:
name: oneposeplus
channels:
  - pytorch
  - conda-forge
  - defaults
dependencies:
  - python=3.7
  - pytorch=1.8.0
  - torchvision=0.9.1
  - cudatoolkit=10.1
  - ipython
  - tqdm
  - matplotlib
  - pylint
  - conda-forge::jupyterlab
  - conda-forge::h5py=3.1.0
  - conda-forge::loguru=0.5.3
  - conda-forge::scipy
  - conda-forge::numba
  - conda-forge::ipdb
  - conda-forge::albumentations=0.5.1
  - pip
  - pip:
    - -r requirements.txt

requirment.txt:
pytorch-lightning==1.5.10
ray==1.13.0
aiohttp==3.7
aioredis==1.3.1
pydegensac==0.1.2
opencv_python==4.4.0.46
yacs&gt;=0.1.8
pytorch_memlab
joblib
pytorch3d
open3d
einops==0.3.0
kornia==0.4.1
autopep8
pickle5==0.0.11
timm&gt;=0.3.2
hydra-core
omegaconf
pycocotools
wandb
rich
transforms3d
natsort
plyfile
pycolmap==0.3.0

error:
rvl224@eervl224:~/OnePose_Plus_Plus-main$ conda env create -f environment.yaml
Collecting package metadata (repodata.json): done
Solving environment: \ 
Found conflicts! Looking for incompatible packages.
This can take several minutes.  Press CTRL-C to abort.
Examining conflict for pylint numba python matplotlib jupyterlab ipython tqdm scExamining conflict for pylint numba python matplotlib torchvision jupyterlab ipyExamining conflict for pylint numba python matplotlib torchvision jupyterlab ipyExamining conflict for pylint jupyterlab ipython: : 20it [06:56, 20.25s/it]     Examining conflict for albumentations numba matplotlib torchvision h5py scipy pyExamining conflict for albumentations numba matplotlib torchvision h5py scipy pyExamining conflict for albumentations torchvision: : 24it [08:51,  8.89s/it]    Examining conflict for albumentations pytorch torchvision: : 25it [09:21, 37.46sExamining conflict for albumentations pytorch torchvision: : 26it [09:21, 35.40sExamining conflict for albumentations h5py: : 26it [09:35, 35.40s/it]           Examining conflict for albumentations matplotlib torchvision: : 27it [09:41, 28.Examining conflict for albumentations matplotlib torchvision: : 28it [09:41, 21.Examining conflict for albumentations matplotlib: : 28it [10:08, 21.91s/it]     Examining conflict for albumentations numba matplotlib torchvision scipy: : 29itExamining conflict for albumentations numba matplotlib torchvision scipy: : 30itExamining conflict for albumentations numba python matplotlib torchvision h5py cExamining conflict for albumentations numba python matplotlib torchvision h5py cExamining conflict for albumentations pytorch: : 31it [11:39, 25.74s/it]        failed                                                                  

# &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; ERROR REPORT &lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;

    Traceback (most recent call last):
      File ""/home/rvl224/anaconda3/lib/python3.7/site-packages/conda/exceptions.py"", line 1079, in __call__
        return func(*args, **kwargs)
      File ""/home/rvl224/anaconda3/lib/python3.7/site-packages/conda_env/cli/main.py"", line 80, in do_call
        exit_code = getattr(module, func_name)(args, parser)
      File ""/home/rvl224/anaconda3/lib/python3.7/site-packages/conda_env/cli/main_create.py"", line 111, in execute
        result[installer_type] = installer.install(prefix, pkg_specs, args, env)
      File ""/home/rvl224/anaconda3/lib/python3.7/site-packages/conda_env/installers/conda.py"", line 32, in install
        prune=getattr(args, 'prune', False), update_modifier=UpdateModifier.FREEZE_INSTALLED)
      File ""/home/rvl224/anaconda3/lib/python3.7/site-packages/conda/core/solve.py"", line 117, in solve_for_transaction
        should_retry_solve)
      File ""/home/rvl224/anaconda3/lib/python3.7/site-packages/conda/core/solve.py"", line 158, in solve_for_diff
        force_remove, should_retry_solve)
      File ""/home/rvl224/anaconda3/lib/python3.7/site-packages/conda/core/solve.py"", line 281, in solve_final_state
        ssc = self._run_sat(ssc)
      File ""/home/rvl224/anaconda3/lib/python3.7/site-packages/conda/common/io.py"", line 88, in decorated
        return f(*args, **kwds)
      File ""/home/rvl224/anaconda3/lib/python3.7/site-packages/conda/core/solve.py"", line 808, in _run_sat
        should_retry_solve=ssc.should_retry_solve
      File ""/home/rvl224/anaconda3/lib/python3.7/site-packages/conda/common/io.py"", line 88, in decorated
        return f(*args, **kwds)
      File ""/home/rvl224/anaconda3/lib/python3.7/site-packages/conda/resolve.py"", line 1318, in solve
        self.find_conflicts(specs, specs_to_add, history_specs)
      File ""/home/rvl224/anaconda3/lib/python3.7/site-packages/conda/resolve.py"", line 347, in find_conflicts
        bad_deps = self.build_conflict_map(specs, specs_to_add, history_specs)
      File ""/home/rvl224/anaconda3/lib/python3.7/site-packages/conda/resolve.py"", line 507, in build_conflict_map
        root, search_node, dep_graph, num_occurances)
      File ""/home/rvl224/anaconda3/lib/python3.7/site-packages/conda/resolve.py"", line 369, in breadth_first_search_for_dep_graph
        last_spec = MatchSpec.union((path[-1], target_paths[-1][-1]))[0]
      File ""/home/rvl224/anaconda3/lib/python3.7/site-packages/conda/models/match_spec.py"", line 481, in union
        return cls.merge(match_specs, union=True)
      File ""/home/rvl224/anaconda3/lib/python3.7/site-packages/conda/models/match_spec.py"", line 475, in merge
        reduce(lambda x, y: x._merge(y, union), group) if len(group) &gt; 1 else group[0]
      File ""/home/rvl224/anaconda3/lib/python3.7/site-packages/conda/models/match_spec.py"", line 475, in &lt;lambda&gt;
        reduce(lambda x, y: x._merge(y, union), group) if len(group) &gt; 1 else group[0]
      File ""/home/rvl224/anaconda3/lib/python3.7/site-packages/conda/models/match_spec.py"", line 502, in _merge
        final = this_component.union(that_component)
      File ""/home/rvl224/anaconda3/lib/python3.7/site-packages/conda/models/match_spec.py"", line 764, in union
        return '|'.join(options)
    TypeError: sequence item 0: expected str instance, Channel found

`$ /home/rvl224/anaconda3/bin/conda-env create -f environment.yaml`

  environment variables:
                 CIO_TEST=&lt;not set&gt;
        CMAKE_PREFIX_PATH=/opt/ros/noetic
  CONDA_AUTO_UPDATE_CONDA=false
                CONDA_EXE=/home/rvl224/anaconda3/bin/conda
         CONDA_PYTHON_EXE=/home/rvl224/anaconda3/bin/python
               CONDA_ROOT=/home/rvl224/anaconda3
              CONDA_SHLVL=0
            DEFAULTS_PATH=/usr/share/gconf/ubuntu.default.path
          LD_LIBRARY_PATH=/opt/ros/noetic/lib:/opt/ros/noetic/lib/x86_64-linux-
                          gnu:/usr/local/cuda/lib64:
           MANDATORY_PATH=/usr/share/gconf/ubuntu.mandatory.path
                     PATH=/home/rvl224/anaconda3/bin:/home/rvl224/anaconda3/condabin:/opt/ros/no
                          etic/bin:/usr/local/cuda/bin:/home/rvl224/.local/bin:/home/rvl224/.loc
                          al/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/u
                          sr/games:/usr/local/games:/snap/bin
          PKG_CONFIG_PATH=/opt/ros/noetic/lib/pkgconfig:/opt/ros/noetic/lib/x86_64-linux-
                          gnu/pkgconfig
               PYTHONPATH=/opt/ros/noetic/lib/python3/dist-packages
       REQUESTS_CA_BUNDLE=&lt;not set&gt;
         ROS_PACKAGE_PATH=/opt/ros/noetic/share
            SSL_CERT_FILE=&lt;not set&gt;
               WINDOWPATH=2

     active environment : None
            shell level : 0
       user config file : /home/rvl224/.condarc
 populated config files : /home/rvl224/.condarc
          conda version : 4.8.2
    conda-build version : 3.18.11
         python version : 3.7.6.final.0
       virtual packages : __cuda=12.0
                          __glibc=2.31
       base environment : /home/rvl224/anaconda3  (writable)
           channel URLs : https://conda.anaconda.org/conda-forge/linux-64
                          https://conda.anaconda.org/conda-forge/noarch
                          https://repo.anaconda.com/pkgs/main/linux-64
                          https://repo.anaconda.com/pkgs/main/noarch
                          https://repo.anaconda.com/pkgs/r/linux-64
                          https://repo.anaconda.com/pkgs/r/noarch
          package cache : /home/rvl224/anaconda3/pkgs
                          /home/rvl224/.conda/pkgs
       envs directories : /home/rvl224/anaconda3/envs
                          /home/rvl224/.conda/envs
               platform : linux-64
             user-agent : conda/4.8.2 requests/2.22.0 CPython/3.7.6 Linux/5.15.0-76-generic ubuntu/20.04.4 glibc/2.31
                UID:GID : 1000:1000
             netrc file : None
           offline mode : False


An unexpected error has occurred. Conda has prepared the above report.

If submitted, this report will be used by core maintainers to improve
future releases of conda.
Would you like conda to send this report to the core maintainers?

[y/N]: 
Timeout reached. No report sent.

down below are attempt updates to fix the error:
installing micromamba:
## install micromamba
curl micro.mamba.pm/install.sh | bash
export MAMBA_ROOT_PREFIX=/home/rvl224/anaconda3
micromamba update -n base conda

micromamba create -n oneposeplus -f environment.yaml

stuck at:
pkgs/main/linux-64                                            No change
pkgs/r/linux-64                                               No change
pkgs/r/noarch                                                 No change
pkgs/main/noarch                                              No change
pytorch/linux-64                                              No change
pytorch/noarch                                                No change
conda-forge/noarch                                  11.7MB @   2.3MB/s  5.2s
conda-forge/linux-64                                29.2MB @   4.4MB/s  6.9s

install mamba
#install mamba
micromamba install -c conda-forge mamba --root-prefix=/home/rvl224/anaconda3 -n base

mamba create -n oneposeplus -f environment.yaml

error:
Looking for: ['environment.yaml']

warning  libmamba Could not parse mod/etag header
warning  libmamba Could not parse mod/etag header
warning  libmamba Could not parse mod/etag header
warning  libmamba Could not parse mod/etag header
warning  libmamba Could not parse mod/etag header
warning  libmamba Could not parse mod/etag header
pkgs/main/noarch                                   851.4kB @   1.3MB/s  0.7s
pkgs/r/linux-64                                      1.4MB @   1.6MB/s  0.9s
pkgs/r/noarch                                        1.3MB @   1.0MB/s  0.6s
pkgs/main/linux-64                                 @   2.5MB/s  2.4s
conda-forge/noarch                                 @   3.0MB/s  4.6s
conda-forge/linux-64                               @   4.8MB/s  7.4s
Encountered problems while solving:
  - nothing provides requested environment.yaml

tried installing packages one by one in a test environmemt,down below are some error while installing:
mamba install -n test_environment cudatoolkit=10.1 --yes

Encountered problems while solving:
  - nothing provides pytorch 1.10.2 cpu_py37h76afcab_0 needed by pytorch-cpu-1.10.2-cpu_py37h718b53a_0


mamba install -n test_environment ""conda-forge::h5py=3.1.0"" --yes

Encountered problems while solving:
  - package pulseaudio-daemon-16.1-ha8d29e2_3 requires openssl &gt;=3.1.0,&lt;4.0a0, but none of the providers can be installed

mamba install -n test_environment -c conda-forge -c pytorch -c defaults --file requirements.txt --yes


Encountered problems while solving:
  - nothing provides requested ray 1.13.0
  - nothing provides requested pydegensac 0.1.2
  - nothing provides requested opencv_python 4.4.0.46
  - nothing provides requested pytorch_memlab
  - nothing provides requested pytorch3d
  - nothing provides requested open3d
  - nothing provides requested kornia 0.4.1
  - package pycolmap-0.3.0-cpu_py39h5202583_1 requires python &gt;=3.9,&lt;3.10.0a0, but none of the providers can be installed

change into:
cudatoolkit=10.2
h5py (without version)

still figuring what to do with requirement.txt
","Had to manually install every package,idk why environment.yml doesn't work
mamba create -n oneposeplus python=3.7 --yes
conda activate oneposeplus

pip install torch==1.8.0 --no-input
pip install torchvision==0.9.1 --no-input
pip install matplotlib --no-input
pip install ipython --no-input
pip install tqdm --no-input
pip install pylint --no-input
pip install jupyterlab --no-input
pip install ""h5py==3.1.0"" --no-input
pip install loguru==0.5.3 --no-input
pip install scipy --no-input
pip install numba --no-input
pip install ipdb --no-input
pip install ""albumentations==0.5.1"" --no-input

pip install pytorch-lightning==1.5.10 --no-input
pip install ray==1.13.0 --no-input
pip install aiohttp==3.7 --no-input
pip install aioredis==1.3.1 --no-input
pip install pydegensac==0.1.2 --no-input
pip install opencv-python==4.4.0.46 --no-input
pip install ""yacs&gt;=0.1.8"" --no-input
pip install pytorch_memlab --no-input
pip install joblib --no-input
pip install pytorch3d --no-input
pip install open3d --no-input
pip install ""einops==0.3.0"" --no-input
pip install ""kornia==0.4.1"" --no-input
pip install autopep8 --no-input
pip install ""pickle5==0.0.11"" --no-input
pip install ""timm&gt;=0.3.2"" --no-input
pip install hydra-core --no-input
pip install omegaconf --no-input
pip install pycocotools --no-input
pip install wandb --no-input
pip install rich --no-input
pip install transforms3d --no-input
pip install natsort --no-input
pip install plyfile --no-input
pip install ""pycolmap==0.3.0"" --no-input

Thanks for @merv for helping.
",,,false,
https://stackoverflow.com/questions/68080031,true,"The issue involves the usage of numpy functions on scipy.sparse matrices, which can lead to unexpected and incorrect results. The answer suggests using the 'sum' function instead of 'any' on bool arrays to achieve the desired result.",,,,,,,numpy.any(axis=i) for scipy.sparse,"import numpy
a = numpy.array([
    [0, 1, 0, 0],
    [1, 0, 0, 0],
    [0, 0, 1, 0],
    [0, 0, 0, 0],
    [0, 0, 0, 0],
])
numpy.any(a, axis=0)
numpy.any(a, axis=1)

produces
array([ True,  True,  True, False])
array([ True,  True,  True, False, False])

However, after
from scipy import sparse
a = sparse.csr_matrix(a)

the same numpy.any(a, axis) calls produces
&lt;5x4 sparse matrix of type '&lt;class 'numpy.intc'&gt;'
        with 3 stored elements in Compressed Sparse Row format&gt;

and
Traceback (most recent call last):
  File ""&lt;stdin&gt;"", line 1, in &lt;module&gt;
  File ""&lt;__array_function__ internals&gt;"", line 5, in any
  File ""C:\Users\user\.conda\envs\py385\lib\site-packages\numpy\core\fromnumeric.py"", line 2330, in any
    return _wrapreduction(a, np.logical_or, 'any', axis, None, out, keepdims=keepdims)
  File ""C:\Users\user\.conda\envs\py385\lib\site-packages\numpy\core\fromnumeric.py"", line 87, in _wrapreduction
    return ufunc.reduce(obj, axis, dtype, out, **passkwargs)
numpy.AxisError: axis 1 is out of bounds for array of dimension 0

Of course, a is actually so large a sparse matrix that converting to normal numpy array is not an option.  How can I acquire the same (or equivalent) result for a csr_matrix and other scipy.sparse matrices?
ADDED:
According to Usage information in official scipy documentation,

Despite their similarity to NumPy arrays, it is strongly discouraged to use NumPy functions directly on these matrices because NumPy may not properly convert them for computations, leading to unexpected (and incorrect) results. If you do want to apply a NumPy function to these matrices, first check if SciPy has its own implementation for the given sparse matrix class, or convert the sparse matrix to a NumPy array (e.g., using the toarray() method of the class) first before applying the method.

I'm looking for ""its own implementation"" or equivalent.
","you can use sum instead of any on bool arrays
import numpy
a = numpy.array([
    [0, 1, 0, 0],
    [1, 0, 0, 0],
    [0, 0, 1, 0],
    [0, 0, 0, 0],
    [0, 0, 0, 0],
])

from scipy import sparse
a = sparse.csr_matrix(a.astype(bool))
# Use sum instead of any on a bool array
print(a.sum(axis=0).astype(bool))
print(a.sum(axis=1).flatten().astype(bool))

output:
[[ True  True  True False]]
[[ True  True  True False False]]

If you want to do 'all' that would be a little tricky since scipy doesn't appear to have an implementation for 'prod'.
But this post has an answer for that case.
",,,false,
https://stackoverflow.com/questions/66640632,true,The issue involves the inconsistency in results when using different types of CDF inputs for the Kolmogorov-Smirnov (KS) test in scipy.stats. This indicates a potential problem with the API.,scipy.stats,kstest,"The issue arises when using different types of CDF inputs (callable, string, or array) for the KS test in scipy.stats. The results vary depending on the type of input, which is unexpected.",The KS test works as expected when the CDF input is a callable or a string representing a distribution in scipy.stats.,"The issue occurs when the CDF input is an array, which leads to inconsistent and unexpected results.",This issue might be challenging to detect during development and testing because the behavior of the KS test with different types of CDF inputs may not be explicitly documented or intuitive to users.,Python Kolmogorov-Smirnov (KS) Test Inconsistent Results,"I am trying to compute the KS test specifying the CDF as a array, however, I encountered unexpected results. Upon further evaluation, I found different results based on whether I specified the CDF as a callable, string or array. My code is as follows:
import scipy.stats as st

random_variables = st.norm.rvs(loc=1, scale=1,size=1000000)
cdf_data = st.norm.cdf(random_variables, loc=1,scale=1)
params = st.norm.fit(data=random_variables)
display(params)
print('\n')

#test 1
out = kstest(rvs=random_variables,cdf='norm',args=params)
display(out, out[0], out[1])
print('\n')

#test 2
out = kstest(rvs=random_variables,cdf=st.norm.cdf,args=params)
display(out, out[0], out[1])
print('\n')

#test 3
out = kstest(rvs=random_variables,cdf=cdf_data)
display(out, out[0], out[1])

The results from this code are:
(1.0004825310590526, 0.9996641807017618)


KstestResult(statistic=0.0007348981302804924, pvalue=0.6523439724424506)
0.0007348981302804924
0.6523439724424506


KstestResult(statistic=0.0007348981302804924, pvalue=0.6523439724424506)
0.0007348981302804924
0.6523439724424506


KstestResult(statistic=0.500165, pvalue=0.0)
0.500165
0.0

Given the large sample data is compared against its the exact distribution from which the sample was generated, I expect a failure to reject the null hypothesis. This is the case in test 1 and 2, but it is not the case in test 3. I want to be able to replicate this test using an array argument for the ""cdf"" argument. Any help as to what I am doing wrong for test 3 would be very helpful. My numpy is version 1.19.2 and scipy is 1.5.2. Thank you!
","I think there are two things that may be contributing to your confusion.

I don't think you want to be comparing to cdf_data = st.norm.cdf(random_variables, loc=1,scale=1). This is returning the value of the cumulative distribution function at all the x values of random-variables. In a KS test you are comparing two distributions, and your cdf_data and random_variable are two very different distributions, so you would expect to get a p-value of 0. I suggest you replace cdf_data with something like random_variable_2 = st.norm.rvs(loc=1,scale=1, size=size)
Additionally you are performing two different KS tests between your first two (one sample) and third (two sample) test. In the first two you compare your data to a fixed functional form to check if the data is consistent with that functional distribution. Since you have the same data and distribution between cases one and two, you would expect the output to be the same. However in case three you are testing two independent distributions to see if they are consistent with each other. Since the cdf_data should just be other normally distributed data points, you should find that the two distributions are consistent, but it should not necessarily give you the exact same answer as the previous two cases, just KS test statistic and p-value that suggests the two data sets come from the same underlying distribution.

",,,false,
https://stackoverflow.com/questions/65010384,true,"The issue involves differing results between scipy.signal.ss2tf() and scipy.signal.StateSpace().to_tf() functions, indicating a potential problem with the API.",scipy.signal,"ss2tf, StateSpace().to_tf","The issue arises when comparing the results of ss2tf() and StateSpace().to_tf() functions in scipy.signal. The numerator of the transfer function does not correspond between the two functions, leading to unexpected results.",The ss2tf() and StateSpace().to_tf functions work as expected when converting state-space models to transfer functions in scipy.signal.,"The issue occurs when comparing the results of ss2tf() and StateSpace().to_tf functions for certain input matrices, resulting in differing numerators of the transfer function.",This issue might be challenging to detect during development and testing because the difference in results between the two functions may not be immediately apparent or explicitly documented.,Differing results in Scipy vs Matlab transfer functions,"I can't understand why the scipy.signal.ss2tf() and scipy.signal.StateSpace().to_tf() give (the same) unexpected result.
Example:
A=[[0, 1, 0], [0, 0, 1], [-3, -4, -2]]
B=[[0], [0], [1]]
C=[[5, 1, 0]]
D=[[0]]

The result for scipy is
num = array([[0, 0, 0, 4]]),
den = array([1., 2., 4., 3.])

in Matlab the result is
num = [0,0,1,5],
den = [1,2,4,3]

It seems that the denominator is always right, I tried other examples, but the numerator of the transfer function doesn't correspond.
Am I using scipy in an incorrect way?
(another example)
A=[[0, 1, 0], [0, 0, 1], [-8, -14, -7]]
B=[[0], [0], [1]]
C=[[15, 5, 0]]
D=[[0]]

","This is a bug in SciPy that was fixed in SciPy 1.6.0.  When ss2tf creates an array to hold the numerator, it uses the data types of the inputs to determine the data type of the numerator array.  In your case, the values are all integers, so the data type of the numerator is integer.  However, the coefficients for the numerator are the result of a floating point calculation, and therefore are subject to loss of precision.  When the computed values are copied into the numerator array, those values are truncated to integers.  In your example, that results in a large error.  When I reproduce the floating point calculation of the numerator, I get [0.0, 0.0, 0.9999999999999947, 4.999999999999995], and when those values are copied into the integer array, the result is [0, 0, 0, 4].
A work-around is to ensure that your inputs (or at least A) contain floating point values.  For example,
In [33]: A = [[0., 1., 0.], [0., 0., 1.], [-8., -14., -7.]] 
    ...: B = [[0], [0], [1]] 
    ...: C = [[15, 5, 0]] 
    ...: D = [[0]]                                                              

In [34]: num, den = ss2tf(A, B, C, D)                                           

In [35]: num                                                                    
Out[35]: array([[0.00000000e+00, 1.77635684e-15, 5.00000000e+00, 1.50000000e+01]])

",,,false,
https://stackoverflow.com/questions/54766766,true,"The issue involves incorrect results when using OpenCV's filter2d function with a custom Laplacian kernel, indicating a potential problem with the API.",OpenCV,filter2d,The issue arises when filtering an image with a custom Laplacian kernel using OpenCV's filter2d function. The result differs from the expected result obtained using the implementation in SciPy.,The filter2d function in OpenCV works as expected when applying standard filters or kernels to images.,"The issue occurs when using a custom Laplacian kernel with the filter2d function, resulting in an unexpected output that differs from the expected result obtained using SciPy.",This issue might be challenging to detect during development and testing because the difference in results between OpenCV's filter2d and SciPy's implementation may not be immediately apparent or explicitly documented.,OpenCV filter2d gives incorrect result,"I am currently trying to filter an image with a Laplacian kernel that I have constructed myself. However, when filtering an input image with this kernel it gives an unexpected result compared to the implementation in SciPy.

The Laplacian kernel I have constructed should be verified by the following images

 


The code for filtering the image:

im = cv2.imread(""test.png"",0)
im = im.astype(np.float32)

def lkern(t=1.):
    ax = np.arange(np.round(-5*np.sqrt(t),0),np.round(5*np.sqrt(t),0)+1)
    xx, yy = np.meshgrid(ax, ax)

    kernel = -1/(np.sqrt(2*np.pi*t)*t)*np.exp(-(xx**2+yy**2)/(2*t))+
        (xx**2+yy**2)/(np.sqrt(2*np.pi*t)*t**2)*np.exp(-(xx**2+yy**2)/(2*t))


    return kernel.astype(np.float)

t = 25**2/2
l = lkern(t)

L = cv2.filter2D(im/255,-1,l)

plt.figure()
plt.imshow(L,cmap=""gray"")
plt.show()


which results in



Comparing to SciPy's ndimage.gaussian_laplace, the result should have been



which is very different, and I cannot figure out how to do this properly.
","The code in the OP seems to take the equation for a 1D Laplace of Gaussian and use that to construct a 2D radially symmetric function. That is, along any of the diameters of the kernel the function looks like a 1D Laplace of Gaussian. This is not the correct way to create a 2D Laplace of Gaussian.

The Laplace of Gaussian is defined as the sum of the second order derivative of the Gaussian kernel along each of the axes. That is,

LoG = d²/dx² G + d²/dy² G


With G the Gaussian kernel.

With Numpy, you can construct this kernel as follows. I'm using the separability of the Gaussian to reduce the computational complexity.

s = 5;
x = np.arange(np.floor(-4*s),np.ceil(4*s)+1)
g = 1/(np.sqrt(2*np.pi)*s)*np.exp(-x**2/(2*s**2))
d2g = (x**2 - s**2)/(s**4) * g
log = g * d2g[:,None] + g[:,None] * d2g


Tricks here: g and d2g are 1D functions. g[:,None] turns the 1D function on its side, so that the multiplication causes broadcasting, leading to a 2D output.

I have written the kernel this way, rather than expressing the full 2D equation in one go, because this leads to large efficiencies in your code: The convolution of an image f with the kernel log can be written as:

conv(f, log) = conv(f, g * d2g[:,None] + g[:,None] * d2g)
             = conv(conv(f, g), d2g[:,None]) + conv(conv(f, g[:,None]), d2g)


That is, instead of one convolution with a large 2D kernel, we compute 4 convolutions with relatively small 1D kernels. Note that the actual order here does not matter:


One applies a 1D kernel g and on the result a 1D kernel d2g along the other axis. These two operations can be reversed.
Then one repeats this process changing the axes along which each of the operations is applied.
Finally one adds the two results.


(It is OK to use cv2.filter2D where I wrote conv. conv just indicates any convolution function, but a correlation function like filter2D is fine because the kernels are all symmetric.)
",,,false,
https://stackoverflow.com/questions/31040188,false,The issue does not involve an API exhibiting unexpected failures or unpredictable behaviors under specific runtime conditions.,,,,,,,Dot product between 1D numpy array and scipy sparse matrix,,,,,false,
https://stackoverflow.com/questions/77619352,true,"The issue involves unexpected results in the Voronoi diagram generated using scipy.spatial.Voronoi, indicating a potential problem with the API.",scipy.spatial,"Voronoi, voronoi_plot_2d",The issue arises when creating a Voronoi diagram using scipy.spatial.Voronoi and visualizing it with voronoi_plot_2d. The resulting diagram is missing a border and the borders appear slightly off.,The Voronoi and voronoi_plot_2d functions in scipy.spatial work as expected when generating Voronoi diagrams.,"The issue occurs when creating a Voronoi diagram from a specific pandas DataFrame and visualizing it using voronoi_plot_2d, resulting in missing borders and slightly misaligned borders.",This issue might be challenging to detect during development and testing because the inconsistencies in the Voronoi diagram's borders may not be immediately apparent or explicitly documented.,Voronoi diagram gives unexpected results in scipy,"I have the following pandas dataframe:
import pandas as pd
pd.DataFrame({'cl': {0: 'A', 1: 'C', 2: 'H', 3: 'M', 4: 'S'},
 'd': {0: 245.059986986012,
  1: 320.49044143557785,
  2: 239.79023081978914,
  3: 263.38325791238833,
  4: 219.53334398353175},
 'p': {0: 10.971011721360075,
  1: 10.970258360366753,
  2: 13.108487516946218,
  3: 12.93241352743668,
  4: 13.346107628161008}})

    cl  d           p
0   A   245.059987  10.971012
1   C   320.490441  10.970258
2   H   239.790231  13.108488
3   M   263.383258  12.932414
4   S   219.533344  13.346108

I want to create a Voronoi diagram. To do so I am using the package from scipy.
I am using the following code:
import numpy as np
import matplotlib.pyplot as plt
from scipy.spatial import Voronoi, voronoi_plot_2d

centers2 = np.array(
    centers_dt[['d', 'p']]
)

scatter_x = np.array(centers_dt['d'])
scatter_y = np.array(centers_dt['p'])
group = np.array(centers_dt['cl'])
cdict = {'C': 'red', 'A': 'blue', 'H': 'green', 'M': 'yellow', 'S': 'black'}

fig, ax = plt.subplots()
for g in np.unique(group):
    ix = np.where(group == g)
    ax.scatter(scatter_x[ix], scatter_y[ix], c = cdict[g], label = g, s = 100)
ax.legend()

vor = Voronoi(centers2)
fig = voronoi_plot_2d(vor,plt.gca())

plt.show()
plt.close()

But the result I am getting is unexpected:

Since there is a boarder missing plus the boarders seem a bit off.
Any ideas ?
","The code in voronoi_plot_2d draws segments with finite length, even for borders that should extend to infinity.  It is possible that with the default axis limits chosen by voronoi_plot_2d, a border segment doesn't appear in the plot, because the finite length chosen by vorono_plot_2d is too small for it to extend into the displayed window.
Here's the plot that I get from your code if I add plt.ylim(-400, 400).

Note that the dashed line extending upwards from the lowest intersection point should continue upwards to infinity, but the code draws only a finite segment.
Also note that the axis scales are not equal, so the dividing lines between points appear to be not perpendicular to the lines that would connect the points.  Here's the plot with equal axis scales:

",,,false,
https://stackoverflow.com/questions/76564629,true,"The issue involves the import of the 'builder' module from 'google.protobuf.internal' in the Azure Functions environment, which is incompatible with the required version of protobuf for TensorFlow. This indicates a potential conflict between the version of protobuf installed in the Azure Functions host and the version required by TensorFlow.",protobuf,google.protobuf.internal.builder,"When importing the 'builder' module from 'google.protobuf.internal' in the Azure Functions environment, an ImportError occurs, indicating that the 'builder' module cannot be found. This error is caused by a version mismatch between the installed protobuf version in the Azure Functions host and the required version for TensorFlow.",The 'builder' module can be imported successfully in a local development environment with the required version of protobuf.,"The issue is triggered when importing the 'builder' module in the Azure Functions environment, where a different version of protobuf is present.",This issue might be challenging to detect during development and testing because the error only occurs in the Azure Functions environment due to the specific version of protobuf installed in the host.,Cannot import protobuf builder when deploying Azure function,"The Problem
I have an Azure function built using the Python v1 programming model which builds a basic Keras model and trains it using some data from an Azure file share.
Tested locally and everything works.
I'm deploying to Azure using Azure Pipelines. It's invoking correctly, but then failing. Checking the detailed invocation history reveals the following error:
Result: Failure Exception: ImportError: cannot import name 'builder' from 'google.protobuf.internal' (/azure-functions-host/workers/python/3.9/LINUX/X64/google/protobuf/internal/__init__.py). 
Please check the requirements.txt file for the missing module. For more info, please refer the troubleshooting guide: https://aka.ms/functions-modulenotfound
Stack: File ""/azure-functions-host/workers/python/3.9/LINUX/X64/azure_functions_worker/dispatcher.py"", line 380, in _handle__function_load_request func = loader.load_function( 
File ""/azure-functions-host/workers/python/3.9/LINUX/X64/azure_functions_worker/utils/wrappers.py"", line 48, in call raise extend_exception_message(e, message) File ""/azure-functions-host/workers/python/3.9/LINUX/X64/azure_functions_worker/utils/wrappers.py"", line 44, in call return func(*args, **kwargs)
File ""/azure-functions-host/workers/python/3.9/LINUX/X64/azure_functions_worker/loader.py"", line 132, in load_function mod = importlib.import_module(fullmodname)
File ""/usr/local/lib/python3.9/importlib/__init__.py"", line 127, in import_module return _bootstrap._gcd_import(name[level:], package, level) File ""&lt;frozen importlib._bootstrap&gt;"", line 1030, in _gcd_import
File ""&lt;frozen importlib._bootstrap&gt;"", line 1007, in _find_and_load
File ""&lt;frozen importlib._bootstrap&gt;"", line 986, in _find_and_load_unlocked 
File ""&lt;frozen importlib._bootstrap&gt;"", line 680, in _load_unlocked
File ""&lt;frozen importlib._bootstrap_external&gt;"", line 850, in exec_module
File ""&lt;frozen importlib._bootstrap&gt;"", line 228, in _call_with_frames_removed 
File ""/home/site/wwwroot/func-gc-imgorientation-train/__init__.py"", line 20, in &lt;module&gt; from core.data.training_image_filestore import TrainingImageFilestore
File ""/home/site/wwwroot/core/data/training_image_filestore.py"", line 4, in &lt;module&gt; import tensorflow as tf
File ""/home/site/wwwroot/.python_packages/lib/site-packages/tensorflow/__init__.py"", line 37, in &lt;module&gt; from tensorflow.python.tools import module_util as _module_util
File ""/home/site/wwwroot/.python_packages/lib/site-packages/tensorflow/python/__init__.py"", line 37, in &lt;module&gt; from tensorflow.python.eager import context
File ""/home/site/wwwroot/.python_packages/lib/site-packages/tensorflow/python/eager/context.py"", line 28, in &lt;module&gt; from tensorflow.core.framework import function_pb2
File ""/home/site/wwwroot/.python_packages/lib/site-packages/tensorflow/core/framework/function_pb2.py"", line 5, in &lt;module&gt; from google.protobuf.internal import builder as _builder

Well Documented Error with a Solution that Works Outside of Azure
This error is well documented and understood as in the following version:
ImportError: cannot import name 'builder' from 'google.protobuf.internal'
In general, arising from API changes in v3.20:
https://stackoverflow.com/a/71984564/1928761
TF adopted these changes, but there were bugs when using TF with protobuf &gt;= v3.20 which were resolved in the most recent Tensorflow release so protobuf 4.23.3 should work with TF 2.12.0 as here:
https://github.com/tensorflow/tensorflow/issues/59221
Problem is resolved locally
I resolved this issue locally and have tested my function. All works fine. The problem only occurs in Azure.
Suspected Root Cause in Azure
Looking through the error above I noticed that all my custom modules, and Tensorflow, are installed to the same root: /home/site/wwwroot/
For instance, my custom core.data module is installed at /home/site/wwwroot/core/data/
Tensorflow is installed at /home/site/wwwroot/.python_packages/lib/site-packages/tensorflow/
I've confirmed that all modules in my requirements.txt are being installed as expected in /home/site/wwwroot/.python_packages/lib/site-packages/ - including the latest protobuf with the builder module as expected.
However, protobuf is being imported from /azure-functions-host/workers/python/3.9/LINUX/X64/google/protobuf/internal/init.py
Presumably the implication is that TF is using the version of protobuf that's bundled with Azure's python distribution on the host, rather than the version in my site packages.
To test this theory I added the following code to the top of my init.py for the function:
import sys
print(sys.path)
import google.protobuf
print(google.protobuf.__version__)
print(google.protobuf.__path__)

This confirmed that protobuf version 3.19 was being loaded from the following path and not from site packages: /azure-functions-host/workers/python/3.9/LINUX/X64/google/protobuf/
The Question
The question therefore is can I upgrade the version of protobuf that's included in this python bundle?
Alternatively, can I force TF to use the version of protobuf in my site packages rather than the ones in the Azure python bundle?
Attempted Solutions
ADO is building and installing the correct versions of both protobuf and TF, and the builder file is definitely in the installed site-packages. I've confirmed this by downloading the package from my storage account and unzipping it.
To resolve the issue, I've tried the following:

Confirmed that I can import other modules and that this is specific to importing the protobuf builder
Confirmed that I can import google.protobuf.internal.
Tried clearing the protobuf pycache by adding an rm -rvf command to my bash script immediately after the pip install --target....
Using subprocess to run pip install --upgrade protobuf==4.23.3 from within my function and before importing TF (which runs without error, but does not do the job).

At this point, I'm all out of ideas.
requirements.txt
# DO NOT include azure-functions-worker in this file
# The Python Worker is managed by Azure Functions platform
# Manually managing azure-functions-worker may cause unexpected issues

# Protobuf comes first to force the very latest version
protobuf==4.23.3

# Tensorflow, Dotenv, Pillow
absl-py==1.4.0
astunparse==1.6.3
cachetools==5.3.1
certifi==2023.5.7
charset-normalizer==3.1.0
contourpy==1.1.0
cycler==0.11.0
flatbuffers==23.5.26
fonttools==4.40.0
gast==0.4.0
google-auth==2.20.0
google-auth-oauthlib==1.0.0
google-pasta==0.2.0
graphviz==0.20.1
grpcio==1.54.2
h5py==3.8.0
idna==3.4
jax==0.4.12
keras==2.12.0
kiwisolver==1.4.4
libclang==16.0.0
Markdown==3.4.3
MarkupSafe==2.1.3
matplotlib==3.7.1
ml-dtypes==0.2.0
numpy==1.23.5
oauthlib==3.2.2
opt-einsum==3.3.0
packaging==23.1
Pillow==9.5.0
pyasn1==0.5.0
pyasn1-modules==0.3.0
pydot==1.4.2
pyparsing==3.0.9
python-dateutil==2.8.2
python-dotenv
requests==2.31.0
requests-oauthlib==1.3.1
rsa==4.9
scipy==1.10.1
six==1.16.0
tensorboard==2.12.3
tensorboard-data-server==0.7.1
tensorflow==2.12.0
tensorflow-estimator==2.12.0
termcolor==2.3.0
typing_extensions==4.6.3
urllib3==1.26.16
Werkzeug==2.3.6
wrapt==1.14.1

# Azure libraries
azure-functions
azure-identity
azure-keyvault

Deployment pipeline YAML
# Python Function App to Linux on Azure
# Build a Python function app and deploy it to Azure as a Linux function app.
# Add steps that analyze code, save build artifacts, deploy, and more:
# https://docs.microsoft.com/azure/devops/pipelines/languages/python

trigger:
- main

variables:
  # Azure Resource Manager connection created during pipeline creation
  azureSubscription: '4ae24131-0b22-421c-8e3e-6d766e891ece'

  # Function app name
  functionAppName: 'func-xxxx-dev'

  # Agent VM image name
  vmImageName: 'ubuntu-latest'

  # Working Directory
  workingDirectory: '$(System.DefaultWorkingDirectory)'

stages:
- stage: Build
  displayName: Build stage

  jobs:
  - job: Build
    displayName: Build
    pool:
      vmImage: $(vmImageName)

    steps:
    - bash: |
        if [ -f extensions.csproj ]
        then
            dotnet build extensions.csproj --runtime ubuntu.16.04-x64 --output ./bin
        fi
      workingDirectory: $(workingDirectory)
      displayName: 'Build extensions'

    - task: UsePythonVersion@0
      displayName: 'Use Python 3.9'
      inputs:
        versionSpec: 3.9 # Functions V2 supports Python 3.6 as of today

    - bash: |
        python -m pip install --upgrade pip
        pip install --target=""./.python_packages/lib/site-packages"" -r ./requirements.txt
      workingDirectory: $(workingDirectory)
      displayName: 'Install application dependencies'

    - task: ArchiveFiles@2
      displayName: 'Archive files'
      inputs:
        rootFolderOrFile: '$(workingDirectory)'
        includeRootFolder: false
        archiveType: zip
        archiveFile: $(Build.ArtifactStagingDirectory)/$(Build.BuildId).zip
        replaceExistingArchive: true

    - publish: $(Build.ArtifactStagingDirectory)/$(Build.BuildId).zip
      artifact: drop

- stage: Deploy
  displayName: Deploy stage
  dependsOn: Build
  condition: succeeded()

  jobs:
  - deployment: Deploy
    displayName: Deploy
    environment: 'development'
    pool:
      vmImage: $(vmImageName)

    strategy:
      runOnce:
        deploy:

          steps:
          - task: AzureFunctionApp@1
            displayName: 'Azure functions app deploy'
            inputs:
              azureSubscription: '$(azureSubscription)'
              appType: functionAppLinux
              appName: $(functionAppName)
              package: '$(Pipeline.Workspace)/drop/$(Build.BuildId).zip'

Log stream
2023-06-27T11:50:43Z   [Information]   Host Status: {
  ""id"": ""func-xxxx-dev"",
  ""state"": ""Running"",
  ""version"": ""4.21.3.3"",
  ""versionDetails"": ""4.21.3+2e42e3beb40b89d4f5d3dd962f3a5d420d376d71"",
  ""platformVersion"": """",
  ""instanceId"": ""9A4EF22A-638234606489642041"",
  ""computerName"": """",
  ""processUptime"": 2778401,
  ""functionAppContentEditingState"": ""NotAllowed"",
  ""extensionBundle"": {
    ""id"": ""Microsoft.Azure.Functions.ExtensionBundle"",
    ""version"": ""4.5.0""
  }
}
2023-06-27T11:50:43Z   [Information]   Host Status: {
  ""id"": ""func-xxxx-dev"",
  ""state"": ""Running"",
  ""version"": ""4.21.3.3"",
  ""versionDetails"": ""4.21.3+2e42e3beb40b89d4f5d3dd962f3a5d420d376d71"",
  ""platformVersion"": """",
  ""instanceId"": ""9A4EF22A-638234606489642041"",
  ""computerName"": """",
  ""processUptime"": 2778404,
  ""functionAppContentEditingState"": ""NotAllowed"",
  ""extensionBundle"": {
    ""id"": ""Microsoft.Azure.Functions.ExtensionBundle"",
    ""version"": ""4.5.0""
  }
}
2023-06-27T11:50:43Z   [Information]   Host Status: {
  ""id"": ""func-xxxx-dev"",
  ""state"": ""Running"",
  ""version"": ""4.21.3.3"",
  ""versionDetails"": ""4.21.3+2e42e3beb40b89d4f5d3dd962f3a5d420d376d71"",
  ""platformVersion"": """",
  ""instanceId"": ""9A4EF22A-638234606489642041"",
  ""computerName"": """",
  ""processUptime"": 2778801,
  ""functionAppContentEditingState"": ""NotAllowed"",
  ""extensionBundle"": {
    ""id"": ""Microsoft.Azure.Functions.ExtensionBundle"",
    ""version"": ""4.5.0""
  }
}
2023-06-27T11:51:00Z   [Information]   Executing 'Functions.func-xxxx-train' (Reason='Timer fired at 2023-06-27T11:51:00.0016150+00:00', Id=3795600b-379e-423c-b29f-65fec390289a)
2023-06-27T11:51:00Z   [Verbose]   Sending invocation id: '3795600b-379e-423c-b29f-65fec390289a
2023-06-27T11:51:00Z   [Verbose]   Posting invocation id:3795600b-379e-423c-b29f-65fec390289a on workerId:250ae2e1-4416-41ef-b355-0684a59d0a91
2023-06-27T11:51:00Z   [Error]   Executed 'Functions.func-xxxx-train' (Failed, Id=3795600b-379e-423c-b29f-65fec390289a, Duration=2ms)
2023-06-27T11:51:00Z   [Verbose]   Function 'func-xxx-train' updated status: Last='2023-06-27T11:51:00.0015724+00:00', Next='2023-06-27T11:52:00.0000000+00:00', LastUpdated='2023-06-27T11:51:00.0015724+00:00'
2023-06-27T11:51:00Z   [Verbose]   Timer for 'func-gc-imgorientation-train' started with interval '00:00:59.9674482'.
2023-06-27T11:51:05Z   [Information]   Host Status: {
  ""id"": ""func-gc-imgorientation-dev"",
  ""state"": ""Running"",
  ""version"": ""4.21.3.3"",
  ""versionDetails"": ""4.21.3+2e42e3beb40b89d4f5d3dd962f3a5d420d376d71"",
  ""platformVersion"": """",
  ""instanceId"": ""9A4EF22A-638234606489642041"",
  ""computerName"": """",
  ""processUptime"": 2800545,
  ""functionAppContentEditingState"": ""NotAllowed"",
  ""extensionBundle"": {
    ""id"": ""Microsoft.Azure.Functions.ExtensionBundle"",
    ""version"": ""4.5.0""
  }
}

","I had a similar issue and found a different solution. There is an app setting that isolates the application's dependencies from the azure function runtime dependencies:
https://learn.microsoft.com/en-us/azure/azure-functions/functions-app-settings#python_isolate_worker_dependencies
Adding the PYTHON_ISOLATE_WORKER_DEPENDENCIES setting with a value of 1 to my function in azure fixed the issue for me.
","The root cause of the problem, as highlighted by @SiddheshDesai, turned out to be that the Azure Functions host was loading a version of protobuf (3.19.6) into the cache which was earlier than the version that the latest Tensorflow needed (4.23.3).
Even when placing my protobuf/TF imports at the top of my function app module, the same error arose.
I was, however, able to reload protobuf from my site-packages using importlib.
This cause was proven by adding the following code to my function:
import logging
import importlib
import google.protobuf

logging.info(google.protobuf.__version__)
logging.info(google.protobuf.__file__)

importlib.reload(google.protobuf)

logging.info(google.protobuf.__version__)
logging.info(google.protobuf.__file__)

However, the reload then interfered with the Azure Functions library and host causing a heap of other exceptions.
My conclusion therefore is that this is an issue with Azure Functions rather than an issue with my code.
Rather than trying to work around these issues, I've now containerised my function and re-deployed. This has worked as expected.
The answer, therefore, turns out to be - as SiddheshDesai suggested above - that any application needing to use versions of protobuf &gt;= 3.20.0 need to be containerised rather than deployed as code/run from packages.
","If you want to use Protobuf module in Azure Functions, You need to downgrade it to 3.20.* and add it in your requirements.txt.
I added protobuf==3.20.* in my requirements.txt and the Http Trigger got deployed successfully in Azure Functions via DevOps YAML pipeline, Refer below:-
My requirements.txt:-
azure-functions
protobuf==3.20.*

My init.py:-
import logging

import azure.functions as func


def main(req: func.HttpRequest) -&gt; func.HttpResponse:
    logging.info('Python HTTP trigger function processed a request.')

    name = req.params.get('name')
    if not name:
        try:
            req_body = req.get_json()
        except ValueError:
            pass
        else:
            name = req_body.get('name')

    if name:
        return func.HttpResponse(f""Hello, {name}. This HTTP triggered function executed successfully."")
    else:
        return func.HttpResponse(
             ""This HTTP triggered function executed successfully. Pass a name in the query string or in the request body for a personalized response."",
             status_code=200
        )


My YAML pipeline:-
trigger:
- master

variables:
  
  azureSubscription: 'xxxxxxxx-xxxxx-xxx9bbd4354dd'

 
  functionAppName: 'valleyfunc541'

 
  vmImageName: 'ubuntu-latest'

 
  workingDirectory: '$(System.DefaultWorkingDirectory)'

stages:
- stage: Build
  displayName: Build stage

  jobs:
  - job: Build
    displayName: Build
    pool:
      vmImage: $(vmImageName)

    steps:
    - bash: |
        if [ -f extensions.csproj ]
        then
            dotnet build extensions.csproj --runtime ubuntu.16.04-x64 --output ./bin
        fi
      workingDirectory: $(workingDirectory)
      displayName: 'Build extensions'

    - task: UsePythonVersion@0
      displayName: 'Use Python 3.10'
      inputs:
        versionSpec: 3.10 # Functions V2 supports Python 3.6 as of today

    - bash: |
        pip install --target=""./.python_packages/lib/site-packages"" -r ./requirements.txt
      workingDirectory: $(workingDirectory)
      displayName: 'Install application dependencies'

    - task: ArchiveFiles@2
      displayName: 'Archive files'
      inputs:
        rootFolderOrFile: '$(workingDirectory)'
        includeRootFolder: false
        archiveType: zip
        archiveFile: $(Build.ArtifactStagingDirectory)/$(Build.BuildId).zip
        replaceExistingArchive: true

    - publish: $(Build.ArtifactStagingDirectory)/$(Build.BuildId).zip
      artifact: drop

- stage: Deploy
  displayName: Deploy stage
  dependsOn: Build
  condition: succeeded()

  jobs:
  - deployment: Deploy
    displayName: Deploy
    environment: 'development'
    pool:
      vmImage: $(vmImageName)

    strategy:
      runOnce:
        deploy:

          steps:
          - task: AzureFunctionApp@1
            displayName: 'Azure functions app deploy'
            inputs:
              azureSubscription: '$(azureSubscription)'
              appType: functionAppLinux
              appName: $(functionAppName)
              package: '$(Pipeline.Workspace)/drop/$(Build.BuildId).zip'

Output:-

The HTTP Trigger got deployed successfully:-


Reference:- My SO thread answer
",false,
https://stackoverflow.com/questions/75879324,false,The issue does not involve an API exhibiting unexpected failures or unpredictable behaviors under specific runtime conditions.,,,,,,,How to use kwargs in scipy.optimize.curve_fit() to pass a parameter that isn&#39;t being fitted,"The problem originates from fitting a diffraction pattern to data, where the number of slits is known beforehand. I have given a simplified version below that highlights the same issue. The function should fit the values of a and b in the data while passing n to the function. I could use a global n which would solve my issues, however, I would like to do this using **kwargs as shown in the scipy.optimize.curve_fit() reference.
Here is an example of the issue. The code generates the curve of 4sin(2x)+3cos(2x) with some noise as the data:

import numpy as np
import scipy
import matplotlib.pyplot as plt

def curve(x,a,b,**kwargs):
    n = kwargs[""n""]
    return a*np.sin(n*x)+b*np.cos(n*x)

x = np.linspace(-5,5,1000)
y = np.random.normal(loc=curve(x, 4, 3, n=2), scale=0.2, size=None)
result = scipy.optimize.curve_fit(curve, x, y, n = 2)
y2 = curve(x, *result[0], n=2)

plt.plot(x, y2)
plt.plot(x,y)
plt.show()


This returns the error
  File ""C:\Users\HP\OneDrive\Documents\Uni\lab year 2\diffraction\kwargs.py"", line 13, in &lt;module&gt;
    result = scipy.optimize.curve_fit(curve, x, y, n = 2)

  File ""C:\Users\HP\anaconda3\lib\site-packages\scipy\optimize\_minpack_py.py"", line 834, in curve_fit
    res = leastsq(func, p0, Dfun=jac, full_output=1, **kwargs)

TypeError: leastsq() got an unexpected keyword argument 'n'

","You might consider using lmfit for this (main author here). Your example would look like:
import numpy as np
import matplotlib.pyplot as plt
from lmfit import Model

def curve(x, a, b, n=2):
    return a*np.sin(n*x)+b*np.cos(n*x)

x = np.linspace(-5,5,1000)
y = np.random.normal(loc=curve(x, 4, 3, n=2), scale=0.2, size=None)

# turn your curve function into an lmfit Model:
model = Model(curve)

# create parameters for the fit, giving decent initial values
# note that n will use the default function value by default
params = model.make_params(a=3.2, b=2.5)

# fix n so that it is not varied in the fit:
params['n'].vary = False

# do the fit
result = model.fit(y, params, x=x)

# print out fit report
print(result.fit_report())

# plot data and best fit
plt.plot(x, y, label='data')
plt.plot(x, result.best_fit, label='fit')
plt.legend()
plt.show()


which will print a report of
[[Model]]
    Model(curve)
[[Fit Statistics]]
    # fitting method   = leastsq
    # function evals   = 7
    # data points      = 1000
    # variables        = 2
    chi-square         = 39.7903631
    reduced chi-square = 0.03987010
    Akaike info crit   = -3220.13053
    Bayesian info crit = -3210.31502
    R-squared          = 0.99678449
[[Variables]]
    a:  4.00080256 +/- 0.00914251 (0.23%) (init = 3.2)
    b:  3.00787199 +/- 0.00873116 (0.29%) (init = 2.5)
    n:  2 (fixed)

and a plot of

","I had a similar issue with curve_fit not accepting the args argument and after following the suggestion
here:, managed to get something working with a curried function
import numpy as np
import scipy
import matplotlib.pyplot as plt

def curve_curry(n):
    def curve(x, a, b):
        return a * np.sin(n * x) + b * np.cos(n * x)
    
    return curve

n = 2
x = np.linspace(-5, 5, 1000)
y = np.random.normal(loc=curve_curry(n)(x, 4, 3), scale=0.2, size=None)
result = scipy.optimize.curve_fit(curve_curry(n=2), x, y)
y2 = curve_curry(n)(x, *result[0])

plt.plot(x, y, '.', color='lightgrey')
plt.plot(x, y2)
plt.show()

In my case, I had fit parameters [p0, p1, p2], constants c1, c2, c3  and pre-defined functions f1, f2.  My code looked something like this:
def func_curry(c1, c2, c3, f1, f2):
    def func(x, *p):
        
        yinv = f1(c1) + x * c2 * (c3 + p[1] + p[2] * x ) / p[0]
        y = f2(yinv)
        
        return y
    
    return func 


# Curve fit using scipy

args = (c1, c2, c3, f1, f2)
p0 = [p0, p1, p2]
popt, pcov = scipy.optimize.curve_fit(func_curry(*args), xdata, ydata, p0=p0)

",,false,
https://stackoverflow.com/questions/22591174/pandas-multiple-conditions-while-indexing-data-frame-unexpected-behavior,false,"The issue is a misunderstanding of logical operators and conditions in pandas, rather than a problem with an API.",,,,,,,,,,,,false,
https://stackoverflow.com/questions/71634466,false,The issue does not involve an API exhibiting unexpected failures or unpredictable behaviors under specific runtime conditions.,,,,,,,Curve fitting using Scipy leads to unexpected result,"I have a data set which I can fit to a given function. I did it using Desmos graphing calculator and got the expected results. But when I implement it using Matplotlib, I get a totally different curve and I can't figure out the error in my code. Here is my code:
import numpy as np
import matplotlib.pyplot as plt
from scipy.optimize import curve_fit

data = np.loadtxt('data.txt')


def func(x, a, b):
    return a/(np.sin((x-b)/2))**4

x = data[:, 0]
y = data[:, 1]

popt, pcov = curve_fit(func, x, y)

plt.plot(x, y, 'ko', label=""Observed data points"")


plt.plot(x, func(x, *popt), 'r-', label=""Fitted Curve"")


plt.legend()
plt.show()

Here is the expected plot (plotted using Desmos): 
And here is what I got from Matplotlib: 
Any help would be appreciated. This is the data.txt for reference:




X
Y




-0.523598776
0.530580093


-0.436332313
1.016423844


-0.34906585
6.38245854


-0.261799388
18.70139225


-0.174532925
30.7389007


0.174532925
13.59465343


0.261799388
4.011313119


0.34906585
0.805865977


0.436332313
0.50894953


0.523598776
0.253654518



","You have a singularity at x==b (your function tends to infinity). That makes it very difficult for curve_fit to find a value of b that ""crosses"" any of your data points. Since your data is about zero centered, you'll have better luck providing a an initial value such as
popt, pcov = curve_fit(func, x, y, [0.0, 0.0])

",,,false,
https://stackoverflow.com/questions/67284779,false,The issue does not involve an API exhibiting unexpected failures or unpredictable behaviors under specific runtime conditions.,,,,,,,Scipy rv_continuous fit does not check input data bounds,"I am fitting synthetic data to various distributions in scipy, however, I am observing some unexpected results. My data contains negative numbers and I do not get an error when I fit this data to distributions with non negative supports when fixing location and scale. My code is as follows:
import scipy.stats as st
import numpy as np
import pandas as pd

np.random.seed(7)
test_data = pd.Series(0.5 + 0.1*np.sin(np.linspace(0, 3*np.pi, 100)) + 0.5*np.random.normal(0,1,size=100))
print(np.min(test_data))

Which returns:
-0.5900934692403015

Confirming I have generated negative observations. When I fit scipy lognorm, which has a non inclusive non negative support, I get the expected result of an error where the data bounds are violated:
st.lognorm.fit(test_data, floc=0, fscale=1)

---------------------------------------------------------------------------
FitDataError                              Traceback (most recent call last)
&lt;ipython-input-13-fbeaae8f3c2e&gt; in &lt;module&gt;
----&gt; 1 st.lognorm.fit(test_data, floc=0, fscale=1)

~\Miniconda3\lib\site-packages\scipy\stats\_continuous_distns.py in fit(self, data, *args, **kwds)
   5087             data = data - floc
   5088         if np.any(data &lt;= 0):
-&gt; 5089             raise FitDataError(""lognorm"", lower=floc, upper=np.inf)
   5090         lndata = np.log(data)
   5091 

FitDataError: Invalid values in `data`.  Maximum likelihood estimation with 'lognorm' requires that 0.0 &lt; x &lt; inf for each x in `data`.

However, with the following distributions, I am able to fit the data, despite the fact that all of these distributions have non negative data bounds (as defined by their scipy documentation) and fixed location and scale.
st.burr.fit(test_data, floc=0, fscale=1)

st.expon.fit(test_data)

st.chi2.fit(test_data, floc=0, fscale=1)

st.invgauss.fit(test_data, floc=0, fscale=1)

st.invgamma.fit(test_data, floc=0, fscale=1)

Which yield:
(4.435119987970436, 0.32475585134451646, 0, 1)
(-0.5900934692403015, 1.1171187649605647)
(1.349414062500001, 0, 1)
(0.6815429687499996, 0, 1)
(2.301074218750003, 0, 1)

Additionally, the distribution expon without any shape parameters is able parameters was able to execute which was surprising. If someone could explain how these distributions are able to fit to the data despite the fact that their support bounds have been violated I would really appreciate it.
I am running numpy 1.19.2 and scipy 1.5.2
Thank you!
","The fact that those fit didn't throw any error doesn't mean that they have been a good fit or that they can describe your data.
I'm using scipy==1.6.1.
You can check plotting results
x = np.linspace(test_data.min(), test_data.max(), 100)

Burr: no error, bu cannot describe data &lt;0
burr_pars = sps.burr.fit(test_data, floc=0, fscale=1)
y = sps.burr(*burr_pars).pdf(x)
plt.plot(x, y)
plt.hist(test_data, alpha=.5, density=True);


Expon: no error, but very bad fit
expon_pars = sps.expon.fit(test_data)
y = sps.expon(*expon_pars).pdf(x)
plt.plot(x, y)
plt.hist(test_data, alpha=.5, density=True);


Chi2: no error but very bad fit and cannot describe data &lt;0
chi2_pars = sps.chi2.fit(test_data, floc=0, fscale=1)
y = sps.chi2(*chi2_pars).pdf(x)
plt.plot(x, y)
plt.hist(test_data, alpha=.5, density=True);


Invgauss: error
invgauss_pars = sps.invgauss.fit(test_data, floc=0, fscale=1)
FitDataError: Invalid values in `data`.  Maximum likelihood estimation with 'invgauss' requires that 0 &lt; (x - loc)/scale  &lt; inf for each x in `data`.

If you don't set loc and scale, works best for x&gt;=0, but given the formula of its PDF there is no reason why it should throw an error for x&lt;0
invgauss_pars = sps.invgauss.fit(test_data)
y = sps.invgauss(*invgauss_pars).pdf(x)
plt.plot(x, y)
plt.hist(test_data, alpha=.5, density=True);


Invgamma: a warning, bad fit and cannot describe for x&lt;0
invagamm_pars = sps.invgamma.fit(test_data, floc=0, fscale=1)
y = sps.invgauss(*invagamm_pars).pdf(x)
plt.plot(x, y)
plt.hist(test_data, alpha=.5, density=True);
RuntimeWarning: invalid value encountered in double_scalars
  Lhat = muhat - Shat*mu


EDIT
From https://github.com/scipy/scipy/blob/v1.6.3/scipy/stats/_continuous_distns.py you see that FitDataError is called only by beta, expon (but if floc is None then floc = data_min), gamma,  invgauss (but only np.any(data - floc &lt; 0)), lognorm, pareto, rayleigh, uniform.
For other distributions FitDataError is not implemented.
",,,false,
https://stackoverflow.com/questions/65152041,false,The issue does not involve an API exhibiting unexpected failures or unpredictable behaviors under specific runtime conditions.,,,,,,,Using sp.ndimage.label on Xarray DataArray with apply_ufunc,"Lets say I have the simple array:
data = [1,1,0,0,1,1,1]

I can apply labeling to this data with the scipy ndimage module with:
groups, _ = sp.ndimage.label(data)

Resulting in
In [68]: print(groups)
[1 1 0 0 2 2 2]

Now, I would like to do the same labeling function on a  xarray DataArray.
xr_data = xr.DataArray([1,1,0,0,1,1,1], coords = [(""x"", [0,1,2,3,4,5,6])])

I know I could call the same function as before on the xr_data, but the output of doing this call is a numpy array, which in my actual dataset, is too large to fit in memory.
It seems like the xr.apply_ufunc function is what I need. However, I am having trouble getting it to work.
def xr_label(arr):
    return xr.apply_ufunc(sp.ndimage.label, arr)

xr_groups, _ = xr_label(xr_data)

This results in:
""ValueError: applied function returned data with unexpected number of dimensions. Received 0 dimension(s) but expected 1 dimensions with names: ('x',)""
I'm finding the documentation on the apply_ufunc method difficult to interpret. Can someone help me out with this?
","You have to define input_core_dims and output_core_dims as parameters to apply_ufunc. See the documentation at: http://xarray.pydata.org/en/stable/generated/xarray.apply_ufunc.html
In your case I think this will be:
xr.apply_ufunc(sp.ndimage.label, arr, input_core_dims=[['x']], output_core_dims=[['x']])

I also recently struggled with understanding apply_ufunc (to be fair, I still don't have a full understanding), however the example at http://xarray.pydata.org/en/stable/examples/apply_ufunc_vectorize_1d.html helped me a lot.
",,,false,
https://stackoverflow.com/questions/50960016,false,The issue does not involve an API exhibiting unexpected failures or unpredictable behaviors under specific runtime conditions.,,,,,,,What is the cosine distance of something and itself?,"I'm playing with scipy's cosine distance. From what I've gathered, the closer a cosine distance is to 1, the more similar the vectors are. I got some unexpected results in a text mining project, so I decided to investigate the simplest case.

import numpy as np
import scipy.spatial
arr1 = np.array([1,1])
arr2 = np.array([1,1])
print scipy.spatial.distance.cosine(arr1, arr2)


My program prints 0.0.

Shouldn't the result be 1.0? Why or why not?
","It is the cosine distance, not the cosine similarity.  A basic requirement for a function d(u, v) to be a distance is that d(u, u) = 0.

See the definition of the formula in the docstring of scipy.spatial.distance.cosine, and notice that the formula begins 1 - (...).  Your expectation of the function is probably based on the quantity in (...), but that expression is the cosine similarity.
",,,false,
https://stackoverflow.com/questions/48675322,true,"The issue involves unexpected results from the scipy.interpolate.griddata function, which suggests an API exhibiting unexpected behavior under specific runtime conditions.",,,,,,,How to make all elements in many columns of a sparse csc matrix 0,"my_csr_matrix


&lt;338232x1783504 sparse matrix of type ''
    with 15740456 stored elements in Compressed Sparse Column format&gt;

my_csr_matrix[:,736225:1783504] = 0

Traceback (most recent call last):
  File ""C:\Users\abhatia\AppData\Local\Continuum\anaconda3\lib\site-packages\IPython\core\interactiveshell.py"", line 2862, in run_code
    exec(code_obj, self.user_global_ns, self.user_ns)
  File ""&lt;ipython-input-135-b0e125d5d27e&gt;"", line 1, in &lt;module&gt;
    my_csr_matrix[:,736225:1783504] = 0
  File ""C:\Users\abhatia\AppData\Local\Continuum\anaconda3\lib\site-packages\scipy\sparse\compressed.py"", line 695, in __setitem__
    i, j = self._swap((i.ravel(), j.ravel()))
MemoryError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:\Users\abhatia\AppData\Local\Continuum\anaconda3\lib\site-packages\IPython\core\interactiveshell.py"", line 2802, in run_ast_nodes
    if self.run_code(code, result):
  File ""C:\Users\abhatia\AppData\Local\Continuum\anaconda3\lib\site-packages\IPython\core\interactiveshell.py"", line 2879, in run_code
    self.showtraceback(running_compiled_code=True)
TypeError: showtraceback() got an unexpected keyword argument 'running_compiled_code'

",,,,false,
https://stackoverflow.com/questions/45036419,false,The issue does not involve an API exhibiting unexpected failures or unpredictable behaviors under specific runtime conditions.,,,,,,,Solver (fsolve in python.scipy) not working,"I have been trying to solve the following system of equations using a scipy solver:

    from scipy.optimize import fsolve
    import math
    import numpy as np

    S0 = 1000
    u  = 1
    d  = 1

    delta_t = 1
    r = 0
    psi_r = (np.exp(-r*delta_t))**(-1)
    fi_r = np.exp(-r*delta_t)
    fi_2r = np.exp(-2*r*delta_t)

    def equations(p):
        p_u,p_m,p_d = p
        return (p_u+p_m+p_d - 1, fi_r*(p_u*(S0+u) + p_m*S0 + p_u*(S0-d)) -S0,fi_2r*
(p_u*p_u*(S0+2*u) + 2*p_m*p_u*(S0+u) + (2*p_u*p_d+p_m*p_m)*S0 + 2*p_m*p_d*(S0-d) + p_d*p_d*(S0-2*d)) - S0)

    p_u,p_m,p_d = fsolve(equations,(0.3,0.5,0.4))

    print(equations((p_u,p_m,p_d)))


The problem is that, despite the first equation stating that the sum of my unknowns should be 1, it never gives a result that would satisfy this. What I get is unexpected numbers on the order of 10 to the -12, or sometimes even negative numbers, which I know cannot be the correct solution.

I know I have to try several initial guesses, but what concerns me is that none of the guesses so far has given me probabilities which sum up to 1. 
",,,,false,
https://stackoverflow.com/questions/40377810,false,The issue does not involve an API exhibiting unexpected failures or unpredictable behaviors under specific runtime conditions.,,,,,,,"interpolate.griddata shifts data northwards, is it a bug?","I observe unexpected results from scipy.interpolate.griddata. I am trying to visualize a set of irregularly spaced points using matplotlib.basemap and scipy.interpolate.griddata.

The data is given as three lists: latitudes, longitudes and values. To get them on the map I interpolate the data onto a regular grid and visualize it using Basemap's imshow function.

I observe that the interpolated data is shifted northwards from true positions.

Here is an example. Here I want to highlight a cell formed by two meridians and two parallels. I expect to get something like this:



However what I get is something like this:



You can see that the red rectangle is visibly shifted northwards.

I have tried to vary the grid resolution and the number of points, however this does not seem to have any effect on this observed shift.

Here is an IPython notebook that illustrates the issue.

Also below is the complete code:

import numpy as np
from numpy import random
from scipy import interpolate
import matplotlib.pyplot as plt
from mpl_toolkits.basemap import Basemap

# defining the region of interest
r = {'lon':[83.0, 95.5], 'lat':[48.5,55.5]}
# initializing Basemap
m = Basemap(projection='merc', 
            llcrnrlon=r['lon'][0],
            llcrnrlat=r['lat'][0],
            urcrnrlon=r['lon'][1],
            urcrnrlat=r['lat'][1],
            lon_0=r['lon'][0], 
            ellps='WGS84',
            fix_aspect=True,
            resolution='h')
# defining the highlighted block
block = {'lon':[89,91],'lat':[50.5,52.5]}
# generating the data
npixels = 100000
lat_range = r['lat'][1] - r['lat'][0]
lats = lat_range * random.random(npixels) + r['lat'][0]
lon_range = r['lon'][1] - r['lon'][0]
lons = lon_range * random.random(npixels) + r['lon'][0]
values = np.zeros(npixels)
for p in range(npixels):
    if block['lat'][0] &lt; lats[p] &lt; block['lat'][1] \
    and block['lon'][0] &lt; lons[p] &lt; block['lon'][1]:
        values[p] = 1.0 
# plotting the original data without interpolation
plt.figure(figsize=(5, 5))
m.drawparallels(np.arange(r['lat'][0], r['lat'][1] + 0.25, 2.0),
                    labels=[True,False,True,False])
m.drawmeridians(np.arange(r['lon'][0], r['lon'][1] + 0.25, 2.0), 
                    labels=[True,True,False,True])
m.scatter(lons,lats,c=values,latlon=True,edgecolors='none')
# interpolating on the regular grid
nx = ny = 500
mapx = np.linspace(r['lon'][0],r['lon'][1],nx)
mapy = np.linspace(r['lat'][0],r['lat'][1],ny)
mapgridx,mapgridy = np.meshgrid(mapx,mapy)
mapdata = interpolate.griddata(list(zip(lons,lats)),values,
                   (mapgridx,mapgridy),method='nearest')
# plotting the interpolated data
plt.figure(figsize=(5, 5))
m.drawparallels(np.arange(r['lat'][0], r['lat'][1] + 0.25, 2.0),
                    labels=[True,False,True,False])
m.drawmeridians(np.arange(r['lon'][0], r['lon'][1] + 0.25, 2.0), 
                    labels=[True,True,False,True])
m.imshow(mapdata)


I am seeing this with SciPy 0.17.0
",,,,false,
https://stackoverflow.com/questions/48675322,true,"The issue involves unexpected results from the scipy.interpolate.griddata function, which suggests an API exhibiting unexpected behavior under specific runtime conditions.",,,,,,,How to make all elements in many columns of a sparse csc matrix 0,"my_csr_matrix


&lt;338232x1783504 sparse matrix of type ''
    with 15740456 stored elements in Compressed Sparse Column format&gt;

my_csr_matrix[:,736225:1783504] = 0

Traceback (most recent call last):
  File ""C:\Users\abhatia\AppData\Local\Continuum\anaconda3\lib\site-packages\IPython\core\interactiveshell.py"", line 2862, in run_code
    exec(code_obj, self.user_global_ns, self.user_ns)
  File ""&lt;ipython-input-135-b0e125d5d27e&gt;"", line 1, in &lt;module&gt;
    my_csr_matrix[:,736225:1783504] = 0
  File ""C:\Users\abhatia\AppData\Local\Continuum\anaconda3\lib\site-packages\scipy\sparse\compressed.py"", line 695, in __setitem__
    i, j = self._swap((i.ravel(), j.ravel()))
MemoryError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:\Users\abhatia\AppData\Local\Continuum\anaconda3\lib\site-packages\IPython\core\interactiveshell.py"", line 2802, in run_ast_nodes
    if self.run_code(code, result):
  File ""C:\Users\abhatia\AppData\Local\Continuum\anaconda3\lib\site-packages\IPython\core\interactiveshell.py"", line 2879, in run_code
    self.showtraceback(running_compiled_code=True)
TypeError: showtraceback() got an unexpected keyword argument 'running_compiled_code'

",,,,false,
https://stackoverflow.com/questions/25033995,false,The issue does not involve an API exhibiting unexpected failures or unpredictable behaviors under specific runtime conditions.,,,,,,,How do I calculate expected values of a Poisson distributed random variable in Python using Scipy?,,,,,false,
https://stackoverflow.com/questions/10518729,false,The issue does not involve an API exhibiting unexpected failures or unpredictable behaviors under specific runtime conditions.,,,,,,,add2virtualenv (virtualenv wrapper) does not work with scipy,"I want to create a virtualenv without global python packages, but with the scipy distribution that is shared; installing scipy takes quite a while and I don't want to go through the motions too often.

So I run add2virtualenv /Library/Python/2.7/site-packages/scipy and after running add2virtualenv it shows the directory is added. (I doublechecked, it is the right directory).
Then I issue workon myfile to be sure the working directories are reloaded. 
However, when I try to load scipy, it is an ImportError: No module named scipy. This is unexpected.

Has anyone used a global scipy in a non-global-sitepackages virtualenv?
","So, to summarize, the actual problem here is that the directory including the packages to be imported must be used, instead of the specific package. That is, instead of

add2virtualenv /Library/Python/2.7/site-packages/scipy


It should be

add2virtualenv /Library/Python/2.7/site-packages


Beware: this solution has the drawback that you do not only include scipy, but any other packages in /Library/Python/2.7/site-packages.



An alternate, space-efficent solution could be symlinking scipy directory inside the virtual env's site-package. This can be done, in your virtual env, through:

cdsitepackages
ln -s /Library/Python/2.7/site-packages/scipy scipy




All credits go to @rubik (see the comments)

Check out this answer to find your site-packages path in case it is different than the one used here.
",,,false,
https://stackoverflow.com/questions/40690045,false,The issue does not involve an API exhibiting unexpected failures or unpredictable behaviors under specific runtime conditions.,,,,,,,conda update failed in windows 10,"In Anaconda prompt as an administrator (Window 10 - 64 bit) after this command : conda update --all I see these errors as below:
I want to update and install packages but after enter every command these errors appear.

conda update --all
Fetching package metadata ..........An unexpected error has occurred.
Please consider posting the following information to the
conda GitHub issue tracker at:

    https://github.com/conda/conda/issues



Current conda install:

               platform : win-64
          conda version : 4.2.9
       conda is private : False
      conda-env version : 4.2.9
    conda-build version : 2.0.2
         python version : 3.5.2.final.0
       requests version : 2.11.1
       root environment : D:\Anaconda3  (writable)
    default environment : D:\Anaconda3
       envs directories : D:\Anaconda3\envs
          package cache : D:\Anaconda3\pkgs
           channel URLs : http://www.lfd.uci.edu/~gohlke/pythonlibs/win-64/
                          http://www.lfd.uci.edu/~gohlke/pythonlibs/noarch/
                          http://github.com/scipy/scipy.git/win-64/
                          http://github.com/scipy/scipy.git/noarch/
                          https://repo.continuum.io/pkgs/free/win-64/
                          https://repo.continuum.io/pkgs/free/noarch/
                          https://repo.continuum.io/pkgs/pro/win-64/
                          https://repo.continuum.io/pkgs/pro/noarch/
                          https://repo.continuum.io/pkgs/msys2/win-64/
                          https://repo.continuum.io/pkgs/msys2/noarch/
            config file : C:\Users\Behnaz Eslami\.condarc
           offline mode : False



`$ D:\Anaconda3\Scripts\conda-script.py update --all`




    Traceback (most recent call last):
      File ""D:\Anaconda3\lib\site-packages\conda\fetch.py"", line 131, in fetch_repodata
        cache = json.loads(json_str)
      File ""D:\Anaconda3\lib\json\__init__.py"", line 319, in loads
        return _default_decoder.decode(s)
      File ""D:\Anaconda3\lib\json\decoder.py"", line 339, in decode
        obj, end = self.raw_decode(s, idx=_w(s, 0).end())
      File ""D:\Anaconda3\lib\json\decoder.py"", line 357, in raw_decode
        raise JSONDecodeError(""Expecting value"", s, err.value) from None
    json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)

    During handling of the above exception, another exception occurred:

    Traceback (most recent call last):
      File ""D:\Anaconda3\lib\site-packages\conda\exceptions.py"", line 473, in conda_exception_handler
        return_value = func(*args, **kwargs)
      File ""D:\Anaconda3\lib\site-packages\conda\cli\main.py"", line 144, in _main
        exit_code = args.func(args, p)
      File ""D:\Anaconda3\lib\site-packages\conda\cli\main_update.py"", line 65, in execute
        install(args, parser, 'update')
      File ""D:\Anaconda3\lib\site-packages\conda\cli\install.py"", line 238, in install
        prefix=prefix)
      File ""D:\Anaconda3\lib\site-packages\conda\api.py"", line 24, in get_index
        index = fetch_index(channel_urls, use_cache=use_cache, unknown=unknown)
      File ""D:\Anaconda3\lib\site-packages\conda\fetch.py"", line 293, in fetch_index
        for url in urls]
      File ""D:\Anaconda3\lib\site-packages\conda\fetch.py"", line 293, in &lt;listcomp&gt;
        for url in urls]
      File ""D:\Anaconda3\lib\site-packages\conda\fetch.py"", line 70, in func
        res = f(*args, **kwargs)
      File ""D:\Anaconda3\lib\site-packages\conda\fetch.py"", line 137, in fetch_repodata
        .format(url, filename, e))
    conda.exceptions.CondaRuntimeError: Runtime error: Invalid index file: http://www.lfd.uci.edu/~gohlke/pythonlibs/win-64/repodata.json: Expecting value: line 1 column 1 (char 0)


How can I solve this issue?
","Try to run conda update conda on command prompt (admin)

or Go to your anaconda installation folder, right-click on ""Anaconda3"", go to Security, in ""Group or user names"" click on Users, then check all under ""Allow"".

See this Image
","You need to add the following reference to your (environment variables) path:
&lt;AnacondaInstallationDirectory&gt;\condabin

It's important to add it before any other existing Anaconda references there, so that conda.exe can be executed with the right privileges.
Also you make sure that you have these as well:
&lt;AnacondaInstallationDirectory&gt;\Library\bin
&lt;AnacondaInstallationDirectory&gt;\Scripts
&lt;AnacondaInstallationDirectory&gt;

","instead of running running in a normal terminal window or as administrator, open an anaconda terminal.

In the windows 10 search bar, type cmd (don't press enter)
In the results, you should see one titled ""Anaconda prompt (anaconda 3)""
type your command in this anaconda terminal e.g. conda update conda

",false,
https://stackoverflow.com/questions/61973962,false,The issue does not involve an API exhibiting unexpected failures or unpredictable behaviors under specific runtime conditions.,,,,,,,pypy3 dyld can not be found in homebrew,"I got the following error when I use homebrew. Does anybody know how to fix the problem?

dyld: Library not loaded: /usr/local/opt/libffi/lib/libffi.6.dylib
  Referenced from: /usr/local/Cellar/pypy/7.1.1_1/libexec/lib/libpypy-c.dylib
  Reason: image not found


Here is the output of brew doctor.

$ brew doctor
Please note that these warnings are just used to help the Homebrew maintainers
with debugging if you file an issue. If everything you use Homebrew for is
working fine: please don't worry or file an issue; just ignore this. Thanks!

Warning: Some installed formulae were deleted!
You should find replacements for the following formulae:
  inchi
  python@2
  libmpc@0.8
  pygtk
  isl@0.11
  f2c
  indigo
  mpfr@2
  gmp@4
  phantomjs
  phantomjs
  rdkit
  casperjs
  mumps
  ecj
  pygobject
  isl@0.12
  gcc@4.8
  gcc@4.8
Warning: Calling a Formula#patches definition is deprecated! Use 'patch do' block calls instead.
Please report this issue to the docmunch/pdftk tap (not Homebrew/brew or Homebrew/core), or even better, submit a PR to fix it:
  /usr/local/Homebrew/Library/Taps/docmunch/homebrew-pdftk/pdftk.rb:15


Warning: You have the following deprecated, official taps tapped:
  Homebrew/homebrew-dupes
  Homebrew/homebrew-fuse
  Homebrew/homebrew-head-only
  Homebrew/homebrew-versions
  Homebrew/homebrew-x11
Untap them with `brew untap`.

Warning: ""config"" scripts exist outside your system or Homebrew directories.
`./configure` scripts often look for *-config scripts to determine if
software packages are installed, and which additional flags to use when
compiling and linking.

Having additional scripts in your path can confuse software installed via
Homebrew if the config script overrides a system or Homebrew-provided
script of the same name. We found the following ""config"" scripts:
  /Library/Frameworks/Python.framework/Versions/3.7/bin/python3.7-config
  /Library/Frameworks/Python.framework/Versions/3.7/bin/python3.7m-config
  /Library/Frameworks/Python.framework/Versions/3.7/bin/python3-config
  /Library/Frameworks/Python.framework/Versions/2.7/bin/python2-config
  /Library/Frameworks/Python.framework/Versions/2.7/bin/python2.7-config
  /Library/Frameworks/Python.framework/Versions/2.7/bin/python-config

Warning: Putting non-prefixed coreutils in your path can cause gmp builds to fail.

Warning: Putting non-prefixed findutils in your path can cause python builds to fail.

Warning: Unbrewed dylibs were found in /usr/local/lib.
If you didn't put them there on purpose they could cause problems when
building Homebrew formulae, and may need to be deleted.

Unexpected dylibs:
  /usr/local/lib/libgcc_ext.10.4.dylib
  /usr/local/lib/libgcc_ext.10.5.dylib
  /usr/local/lib/libgcc_s.10.4.dylib
  /usr/local/lib/libgcc_s.10.5.dylib
  /usr/local/lib/libgcc_s_x86_64.1.dylib
  /usr/local/lib/libgfortran.2.0.0.dylib
  /usr/local/lib/libgfortran.3.dylib
  /usr/local/lib/libgomp.1.dylib
  /usr/local/lib/libpocketsphinx.3.dylib
  /usr/local/lib/libpostal.1.dylib
  /usr/local/lib/libquadmath.0.dylib
  /usr/local/lib/libsphinxad.3.dylib
  /usr/local/lib/libsphinxbase.3.dylib
  /usr/local/lib/libtcl8.6.dylib
  /usr/local/lib/libtk8.6.dylib
  /usr/local/lib/libwkhtmltox.0.12.1.dylib
  /usr/local/lib/libwkhtmltox.0.12.2.dylib
  /usr/local/lib/libwkhtmltox.0.12.4.dylib

Warning: Unbrewed header files were found in /usr/local/include.
If you didn't put them there on purpose they could cause problems when
building Homebrew formulae, and may need to be deleted.

Unexpected header files:
  /usr/local/include/fakemysql.h
  /usr/local/include/fakepq.h
  /usr/local/include/fakesql.h
  /usr/local/include/itcl.h
  /usr/local/include/itcl2TclOO.h
  /usr/local/include/itclDecls.h
  /usr/local/include/itclInt.h
  /usr/local/include/itclIntDecls.h
  /usr/local/include/itclMigrate2TclCore.h
  /usr/local/include/itclTclIntStubsFcn.h
  /usr/local/include/libpostal/libpostal.h
  /usr/local/include/mysqlStubs.h
  /usr/local/include/node/js_native_api.h
  /usr/local/include/node/js_native_api_types.h
  /usr/local/include/node/libplatform/libplatform-export.h
  /usr/local/include/node/libplatform/libplatform.h
...  /usr/local/include/tclPlatDecls.h
  /usr/local/include/tclThread.h
  /usr/local/include/tclTomMath.h
  /usr/local/include/tclTomMathDecls.h
  /usr/local/include/tdbc.h
  /usr/local/include/tdbcDecls.h
  /usr/local/include/tdbcInt.h
  /usr/local/include/tk.h
  /usr/local/include/tkDecls.h
  /usr/local/include/tkPlatDecls.h
  /usr/local/include/wkhtmltox/image.h
  /usr/local/include/wkhtmltox/pdf.h

Warning: Unbrewed .la files were found in /usr/local/lib.
If you didn't put them there on purpose they could cause problems when
building Homebrew formulae, and may need to be deleted.

Unexpected .la files:
  /usr/local/lib/libgfortran.la
  /usr/local/lib/libgomp.la
  /usr/local/lib/libpocketsphinx.la
  /usr/local/lib/libpostal.la
  /usr/local/lib/libquadmath.la
  /usr/local/lib/libsphinxad.la
  /usr/local/lib/libsphinxbase.la

Warning: Unbrewed .pc files were found in /usr/local/lib/pkgconfig.
If you didn't put them there on purpose they could cause problems when
building Homebrew formulae, and may need to be deleted.

Unexpected .pc files:
  /usr/local/lib/pkgconfig/libpostal.pc
  /usr/local/lib/pkgconfig/pocketsphinx.pc
  /usr/local/lib/pkgconfig/sphinxbase.pc
  /usr/local/lib/pkgconfig/tcl.pc
  /usr/local/lib/pkgconfig/tk.pc

Warning: Unbrewed static libraries were found in /usr/local/lib.
If you didn't put them there on purpose they could cause problems when
building Homebrew formulae, and may need to be deleted.

Unexpected static libraries:
  /usr/local/lib/libgfortran.a
  /usr/local/lib/libgomp.a
  /usr/local/lib/libpocketsphinx.a
  /usr/local/lib/libpostal.a
  /usr/local/lib/libquadmath.a
  /usr/local/lib/libsphinxad.a
  /usr/local/lib/libsphinxbase.a
  /usr/local/lib/libtclstub8.6.a
  /usr/local/lib/libtkstub8.6.a

Warning: You have unlinked kegs in your Cellar.
Leaving kegs unlinked can lead to build-trouble and cause brews that depend on
those kegs to fail to run properly once built. Run `brew link` on these:
  inchi
  mpich
  ssed
  python
  libmpc@0.8
  nss
  isl@0.11
  numpy
  mpfr@2
  gmp@4
  poppler
  nspr
  scipy
  gcc@4.8

Warning: Some installed formulae are not readable:
  indigo: Unsupported special dependency :python

  rdkit: Unsupported special dependency :python3

Warning: Your Xcode (10.3) is outdated.
Please update to Xcode 11.3.1 (or delete it).
Xcode can be updated from the App Store.


Warning: Broken symlinks were found. Remove them with `brew cleanup`:
  /usr/local/share/man/man5/package-lock.json.5
  /usr/local/share/man/man7/removing-npm.7

Warning: Some installed formulae are missing dependencies.
You should `brew install` the missing dependencies:
  brew install imlib2 libcaca opusfile

Run `brew missing` for more details.

","This is a duplicate of issue 3229 on the PyPy issue tracker. It seems our buildbot is using a brew installed version of libffi. Until we solve this, you should be able to do brew install pypy3. Help from someone who can make the PyPy build look more like the CPython one (which apparently does not have this problem) are welcome.
",,,false,
https://stackoverflow.com/questions/25161815,false,The issue does not involve an API exhibiting unexpected failures or unpredictable behaviors under specific runtime conditions.,,,,,,,scipy curve_fit returns error for keyword absolute_sigma,"I am using ubuntu 14.04 on python 2.7 with numpy version 1.8.1 and scipy version 0.13.3. When I do a curve_fit with the keyword absolute_sigma=True, I get the message:


  TypeError: leastsq() got an unexpected keyword argument 'absolute_sigma'


I updated numpy and scipy to the versions mentioned above as the keyword absolute_sigma seems to be a new feature. However, no success!

any ideas how to solve this issue? Do I need scipy 0.14... ?

Thanks a lot
","The absolute_sigma parameter was added in version 0.14. Compare the call signatures:


version 0.14
version 0.13

",,,false,
https://stackoverflow.com/questions/3812896,false,The issue does not involve an API exhibiting unexpected failures or unpredictable behaviors under specific runtime conditions.,,,,,,,Can some explain this strange behavior of the hypergeometric distribution in scipy?,"I am running Python 2.6.5 on Mac OS X 10.6.4 (this is not the native version, I installed it myself) with Scipy 0.8.0. If I do the following:

&gt;&gt;&gt; from scipy.stats import hypergeom
&gt;&gt;&gt; hypergeom.sf(5,10,2,5)


I get an IndexError. Then I do:

&gt;&gt;&gt; hypergeom.sf(2,10,2,2)
-4.44....


I suspect the negative value is due to bad floating point precision. Then I do the first one again:

&gt;&gt;&gt; hypergeom.sf(5,10,2,5)
0.0


Now it works! Can someone explain this? Are you seeing this behavior too?
","The problem seems to arise based if the first call to the survival function is in the range that should obviously be zero (see my comment to the previous answer).  E.g., for calls to hypergeom.sf(x,M,n,N) it fails if the first call to a hypergeometric function to the function is a situation where x &gt; n, where the survival function will always be zero.  

You could trivially fix this temporarily by:

def new_hypergeom_sf(k, *args, **kwds):
    from scipy.stats import hypergeom
    (M, n, N) = args[0:3]
    try:
        return hypergeom.sf(k, *args, **kwds)
    except Exception as inst:
        if k &gt;= n and type(inst) == IndexError:
            return 0 ## or conversely 1 - hypergeom.cdf(k, *args, **kwds)
        else:
            raise inst


Now if you have no problem editing the /usr/share/pyshared/scipy/stats/distributions.py (or equivalent file), the fix is likely on line 3966 where right now it reads:

    place(output,cond,self._sf(*goodargs))
    if output.ndim == 0:
        return output[()]
    return output


But if you change it to:

    if output.ndim == 0:
        return output[()]
    place(output,cond,self._sf(*goodargs))
    if output.ndim == 0:
        return output[()]
    return output


It now works without the IndexError.  Basically if the output is zero dimensional because it fails the checks, it tries to call place, fails, and doesn't generate the distribution.  (This doesn't happen if a previous distribution has already been created which is likely why this wasn't caught on earlier tests.)  Note that place (defined in numpy's function_base.py) will change elements of the array (though I'm not sure if it changes the dimensionality) so it may be best to still have it leave the 0 dim check after place too.  I haven't fully tested this to see if this change breaks anything else (and it applies to all discrete random variable distributions), so it maybe its best to do the first fix.

It does break it; e.g., stats.hypergeom.sf(1,10,2,5) returns as zero (instead of 2/9).  

This fix seems to work much better, in the same section:

class rv_discrete(rv_generic):
...
    def sf(self, k, *args, **kwds):
    ...
        if any(cond):
            place(output,cond,self._sf(*goodargs))
        if output.ndim == 0:
            return output[()]
        return output

","I don't know python, but the function is defined like this:
hypergeom.sf(x,M,n,N,loc=0) 

M is the number of interesting objects, N the total number of objects, and n is how often you ""pick one"" (Sorry, German statistician).

If you had a bowl with 20 balls, 7 of those yellow (an interesting yellow), then N is 20 and M is 7.

Perhaps the function behaves undefined for the (nonsense) case when M&gt;N ?
",,false,
https://stackoverflow.com/questions/65585639,true,The issue involves the behavior of eigenvectors and eigenvalues calculated from square matrices using standard libraries in Python. The assumptions made about the results of the calculations are not entirely accurate.,"numpy, scipy","numpy.linalg.eig, scipy.linalg.eig, numpy.linalg.eigh, scipy.linalg.eigh","The issue arises when calculating eigenvectors and eigenvalues from real symmetric matrices using the mentioned libraries. The calculations produce complex numbers in the output, which contradicts the assumption that all results should be real.","The APIs are designed to calculate eigenvectors and eigenvalues from square matrices, including real symmetric matrices.",The issue occurs when calculating eigenvectors and eigenvalues from real symmetric matrices with the mentioned libraries.,This issue might be challenging to detect for users who assume that the APIs will always produce real results for real symmetric matrices.,Eigenvalues in Python: A Bug?,"Here are two assumptions about eigenvectors and eigenvalues of square matrices. I believe that both are true:

If a matrix is symmetric and contains only real values, then it is a Hermitian matrix, and then all eigenvalues should be real numbers and all components of all eigenvectors should also be real numbers. No complex numbers should appear in the results when you calculate eigenvectors and eigenvalues from Hermitian matrices.

The eigenvector of a given eigenvalue, calculated from a given matrix should always point into a direction that is determined only by the matrix and the eigenvalue. The algorithm used to calculate it has no influence on the result, as long as the algorithm is implemented correctly.


But both assumptions do not hold when you use standard libraries in Python to calculate eigenvectors and eigenvalues. Do those methods contain bugs?
There are four different methods to calculate eigenvalues and eigenvectors from Hermitian matrices:

numpy.linalg.eig
scipy.linalg.eig
numpy.linalg.eigh
scipy.linalg.eigh

#1 and #2 can be used for any square matrix (including Hermitian matrices).
#3 and #4 are made for Hermitian matrices only. As far as I did understand their purpose is just that they run faster, but the results should be the same (as long as the input is really Hermitian).
But the four methods deliver three different results for the very same input. Here is the program that I used to test all four methods:
#!/usr/bin/env python3

import numpy as np
import scipy.linalg as la

A = [
    [19, -1, -1, -1, -1, -1, -1, -1],
    [-1, 19, -1, -1, -1, -1, -1, -1],
    [-1, -1, 19, -1, -1, -1, -1, -1],
    [-1, -1, -1, 19, -1, -1, -1, -1],
    [-1, -1, -1, -1, 19, -1, -1, -1],
    [-1, -1, -1, -1, -1, 19, -1, -1],
    [-1, -1, -1, -1, -1, -1, 19, -1],
    [-1, -1, -1, -1, -1, -1, -1, 19]
]

A = np.array(A, dtype=np.float64)

delta = 1e-12
A[5,7] += delta
A[7,5] += delta

if np.array_equal(A, A.T):
    print('input is symmetric')
else:
    print('input is NOT symmetric')

methods = {
    'np.linalg.eig'  : np.linalg.eig,
    'la.eig'         : la.eig,
    'np.linalg.eigh' : np.linalg.eigh,
    'la.eigh'        : la.eigh
}

for name, method in methods.items():

    print('============================================================')
    print(name)
    print()

    eigenValues, eigenVectors = method(A)
    
    for i in range(len(eigenValues)):
        print('{0:6.3f}{1:+6.3f}i '.format(eigenValues[i].real, eigenValues[i].imag), end=' |  ')
        line = eigenVectors[i]
        for item in line:
            print('{0:6.3f}{1:+6.3f}i '.format(item.real, item.imag), end='')
        print()

    print('---------------------')

    for i in range(len(eigenValues)):
        if eigenValues[i].imag == 0:
            print('real    ', end=' |  ')
        else:
            print('COMPLEX ', end=' |  ')
        line = eigenVectors[i]
        for item in line:
            if item.imag == 0:
                print('real    ', end='')
            else:
                print('COMPLEX ', end='')
        print()

    print()

And here is the output it produces:
input is symmetric
============================================================
np.linalg.eig

12.000+0.000i  |  -0.354+0.000i  0.913+0.000i  0.204+0.000i -0.013+0.016i -0.013-0.016i  0.160+0.000i -0.000+0.000i  0.130+0.000i 
20.000+0.000i  |  -0.354+0.000i -0.183+0.000i  0.208+0.000i  0.379-0.171i  0.379+0.171i -0.607+0.000i  0.000+0.000i -0.138+0.000i 
20.000+0.000i  |  -0.354+0.000i -0.182+0.000i  0.203+0.000i -0.468-0.048i -0.468+0.048i  0.153+0.000i  0.001+0.000i -0.271+0.000i 
20.000+0.000i  |  -0.354+0.000i -0.182+0.000i  0.203+0.000i  0.657+0.000i  0.657-0.000i  0.672+0.000i -0.001+0.000i  0.617+0.000i 
20.000-0.000i  |  -0.354+0.000i -0.182+0.000i  0.203+0.000i -0.276+0.101i -0.276-0.101i -0.361+0.000i  0.001+0.000i -0.644+0.000i 
20.000+0.000i  |  -0.354+0.000i -0.001+0.000i -0.612+0.000i -0.001+0.000i -0.001-0.000i  0.001+0.000i  0.706+0.000i -0.000+0.000i 
20.000+0.000i  |  -0.354+0.000i -0.182+0.000i  0.203+0.000i -0.276+0.101i -0.276-0.101i -0.018+0.000i -0.000+0.000i  0.306+0.000i 
20.000+0.000i  |  -0.354+0.000i -0.001+0.000i -0.612+0.000i -0.001+0.000i -0.001-0.000i  0.001+0.000i -0.708+0.000i  0.000+0.000i 
---------------------
real     |  real    real    real    COMPLEX COMPLEX real    real    real    
real     |  real    real    real    COMPLEX COMPLEX real    real    real    
real     |  real    real    real    COMPLEX COMPLEX real    real    real    
COMPLEX  |  real    real    real    real    real    real    real    real    
COMPLEX  |  real    real    real    COMPLEX COMPLEX real    real    real    
real     |  real    real    real    COMPLEX COMPLEX real    real    real    
real     |  real    real    real    COMPLEX COMPLEX real    real    real    
real     |  real    real    real    COMPLEX COMPLEX real    real    real    

============================================================
la.eig

12.000+0.000i  |  -0.354+0.000i  0.913+0.000i  0.204+0.000i -0.013+0.016i -0.013-0.016i  0.160+0.000i -0.000+0.000i  0.130+0.000i 
20.000+0.000i  |  -0.354+0.000i -0.183+0.000i  0.208+0.000i  0.379-0.171i  0.379+0.171i -0.607+0.000i  0.000+0.000i -0.138+0.000i 
20.000+0.000i  |  -0.354+0.000i -0.182+0.000i  0.203+0.000i -0.468-0.048i -0.468+0.048i  0.153+0.000i  0.001+0.000i -0.271+0.000i 
20.000+0.000i  |  -0.354+0.000i -0.182+0.000i  0.203+0.000i  0.657+0.000i  0.657-0.000i  0.672+0.000i -0.001+0.000i  0.617+0.000i 
20.000-0.000i  |  -0.354+0.000i -0.182+0.000i  0.203+0.000i -0.276+0.101i -0.276-0.101i -0.361+0.000i  0.001+0.000i -0.644+0.000i 
20.000+0.000i  |  -0.354+0.000i -0.001+0.000i -0.612+0.000i -0.001+0.000i -0.001-0.000i  0.001+0.000i  0.706+0.000i -0.000+0.000i 
20.000+0.000i  |  -0.354+0.000i -0.182+0.000i  0.203+0.000i -0.276+0.101i -0.276-0.101i -0.018+0.000i -0.000+0.000i  0.306+0.000i 
20.000+0.000i  |  -0.354+0.000i -0.001+0.000i -0.612+0.000i -0.001+0.000i -0.001-0.000i  0.001+0.000i -0.708+0.000i  0.000+0.000i 
---------------------
real     |  real    real    real    COMPLEX COMPLEX real    real    real    
real     |  real    real    real    COMPLEX COMPLEX real    real    real    
real     |  real    real    real    COMPLEX COMPLEX real    real    real    
COMPLEX  |  real    real    real    real    real    real    real    real    
COMPLEX  |  real    real    real    COMPLEX COMPLEX real    real    real    
real     |  real    real    real    COMPLEX COMPLEX real    real    real    
real     |  real    real    real    COMPLEX COMPLEX real    real    real    
real     |  real    real    real    COMPLEX COMPLEX real    real    real    

============================================================
np.linalg.eigh

12.000+0.000i  |  -0.354+0.000i  0.000+0.000i  0.000+0.000i -0.086+0.000i  0.905+0.000i -0.025+0.000i  0.073+0.000i  0.205+0.000i 
20.000+0.000i  |  -0.354+0.000i  0.000+0.000i -0.374+0.000i  0.149+0.000i -0.236+0.000i -0.388+0.000i  0.682+0.000i  0.206+0.000i 
20.000+0.000i  |  -0.354+0.000i  0.001+0.000i  0.551+0.000i  0.136+0.000i -0.180+0.000i  0.616+0.000i  0.317+0.000i  0.201+0.000i 
20.000+0.000i  |  -0.354+0.000i  0.001+0.000i -0.149+0.000i  0.719+0.000i -0.074+0.000i -0.042+0.000i -0.534+0.000i  0.207+0.000i 
20.000+0.000i  |  -0.354+0.000i -0.005+0.000i  0.505+0.000i -0.386+0.000i -0.214+0.000i -0.556+0.000i -0.274+0.000i  0.203+0.000i 
20.000+0.000i  |  -0.354+0.000i -0.707+0.000i -0.004+0.000i  0.002+0.000i  0.001+0.000i  0.002+0.000i -0.000+0.000i -0.612+0.000i 
20.000+0.000i  |  -0.354+0.000i  0.003+0.000i -0.529+0.000i -0.535+0.000i -0.203+0.000i  0.398+0.000i -0.262+0.000i  0.203+0.000i 
20.000+0.000i  |  -0.354+0.000i  0.707+0.000i  0.001+0.000i  0.001+0.000i  0.000+0.000i -0.005+0.000i -0.001+0.000i -0.612+0.000i 
---------------------
real     |  real    real    real    real    real    real    real    real    
real     |  real    real    real    real    real    real    real    real    
real     |  real    real    real    real    real    real    real    real    
real     |  real    real    real    real    real    real    real    real    
real     |  real    real    real    real    real    real    real    real    
real     |  real    real    real    real    real    real    real    real    
real     |  real    real    real    real    real    real    real    real    
real     |  real    real    real    real    real    real    real    real    

============================================================
la.eigh

12.000+0.000i  |  -0.354+0.000i  0.000+0.000i  0.000+0.000i -0.225+0.000i  0.882+0.000i  0.000+0.000i  0.065+0.000i -0.205+0.000i 
20.000+0.000i  |  -0.354+0.000i  0.000+0.000i -0.395+0.000i  0.332+0.000i -0.156+0.000i  0.227+0.000i  0.701+0.000i -0.205+0.000i 
20.000+0.000i  |  -0.354+0.000i  0.001+0.000i  0.612+0.000i  0.011+0.000i -0.204+0.000i -0.597+0.000i  0.250+0.000i -0.200+0.000i 
20.000+0.000i  |  -0.354+0.000i  0.001+0.000i -0.086+0.000i  0.689+0.000i  0.030+0.000i -0.054+0.000i -0.589+0.000i -0.205+0.000i 
20.000+0.000i  |  -0.354+0.000i -0.005+0.000i  0.413+0.000i -0.264+0.000i -0.245+0.000i  0.711+0.000i -0.165+0.000i -0.205+0.000i 
20.000+0.000i  |  -0.354+0.000i -0.707+0.000i -0.004+0.000i -0.000+0.000i  0.001+0.000i -0.002+0.000i -0.001+0.000i  0.612+0.000i 
20.000+0.000i  |  -0.354+0.000i  0.003+0.000i -0.540+0.000i -0.542+0.000i -0.309+0.000i -0.290+0.000i -0.261+0.000i -0.205+0.000i 
20.000+0.000i  |  -0.354+0.000i  0.707+0.000i  0.001+0.000i -0.000+0.000i  0.001+0.000i  0.005+0.000i -0.001+0.000i  0.612+0.000i 
---------------------
real     |  real    real    real    real    real    real    real    real    
real     |  real    real    real    real    real    real    real    real    
real     |  real    real    real    real    real    real    real    real    
real     |  real    real    real    real    real    real    real    real    
real     |  real    real    real    real    real    real    real    real    
real     |  real    real    real    real    real    real    real    real    
real     |  real    real    real    real    real    real    real    real    
real     |  real    real    real    real    real    real    real    real 

As you can see, numpy.linalg.eig and scipy.linalg.eig produce complex numbers in their output, but they shouldn't. This could be accepted as some kind of rounding error, if the magnitude of the imaginary part would by tiny compared to the magnitude of the real part. But this is not the case. One of the numbers that are produced is -0.013+0.016i. Here the imaginary part has an even higher magnitude than the real part.
Even worse: The four methods produce three different results.
All four methods calculate only once an eigenvalue of 12 and 7 times an eigenvalue of 20. And all eigenvectors always have the length 1. This means, all four methods should produce the very same eigenvector for eigenvalue 12. But only numpy.linalg.eig and scipy.linalg.eig produce the same output.
Here are the components of the eigenvector for eigenvalue 12. Have a closer look to the lines marked with an arrow (&lt;==). Here you find three different values, but the values should be exactly equal. And if you have a second look, you will see, that only in the 1st line all three values are equal. In all other lines you will find 2 or 3 different values.
numpy.linalg.eig  |                     |
scipy.linalg.eig  |  numpy.linalg.eigh  |  scipy.linalg.eigh
------------------+---------------------+-------------------
   -0.354+0.000i  |      -0.354+0.000i  |      -0.354+0.000i
    0.913+0.000i  |       0.000+0.000i  |       0.000+0.000i
    0.204+0.000i  |       0.000+0.000i  |       0.000+0.000i
   -0.013+0.016i  |      -0.086+0.000i  |      -0.225+0.000i   &lt;===
   -0.013-0.016i  |       0.905+0.000i  |       0.882+0.000i   &lt;===
    0.160+0.000i  |      -0.025+0.000i  |       0.000+0.000i   &lt;===
   -0.000+0.000i  |       0.073+0.000i  |       0.065+0.000i   &lt;===
    0.130+0.000i  |       0.205+0.000i  |      -0.205+0.000i

Here are my questions:

How is this possible?
Are these bugs?
Is one of the results correct?
If there is a method that delivers correct results: Which is it?


p.s: Here are relevant version informations:

I did run this code on an iMac (macOS Catalina Version 10.15.7)
The python version is 3.8.5
The version of numpy is 1.19.5
The version of scipy is 1.6.0


This is the output of numpy.show_config()
(as requested in a comment):
blas_mkl_info:
  NOT AVAILABLE
blis_info:
  NOT AVAILABLE
openblas_info:
    libraries = ['openblas', 'openblas']
    library_dirs = ['/usr/local/lib']
    language = c
    define_macros = [('HAVE_CBLAS', None)]
blas_opt_info:
    libraries = ['openblas', 'openblas']
    library_dirs = ['/usr/local/lib']
    language = c
    define_macros = [('HAVE_CBLAS', None)]
lapack_mkl_info:
  NOT AVAILABLE
openblas_lapack_info:
    libraries = ['openblas', 'openblas']
    library_dirs = ['/usr/local/lib']
    language = c
    define_macros = [('HAVE_CBLAS', None)]
lapack_opt_info:
    libraries = ['openblas', 'openblas']
    library_dirs = ['/usr/local/lib']
    language = c
    define_macros = [('HAVE_CBLAS', None)]
None


ADDENDUM (added 1 day after the question was asked)
Reaction to comments:

complex eigenvectors of real symmetric matrices
@Rethipher: Thank you! I did read and understand the question you linked to (Can a real symmetric matrix have complex eigenvectors?), and I also did read the answers, but I didn’t understand them. Did they say “yes” or “no”? (rhetoric question, no need to answer, see next line)
@Mark Dickinson &amp; @bnaecker: Thank you for making clear, that my assumption was wrong.

real symmetric matrices vs. Hermitian matrices
@bnaecker: The set of real numbers is a subset of the set of complex numbers. Those complex numbers which are equal to their own complex conjugate are called real. So, the set of real symmetric matrices is a subset of Hermitian matrices. This is important, because numpy.linalg.eigh and scipy.linalg.eigh are designed to handle Hermitian matrices. And because every real symmetric matrix is a Hermitian matrix, those modules also can be used for my purposes.

mixing up rows and columns
@Mark Dickinson &amp; @bnaecker: Thank you, I think you are right. Also the documentations says so, I should have read it more carefully. But even if you compare columns instead of rows you will still find that the 4 methods produce 3 different results. But if the result contains a 7-dimensional subspace that can be described with 7 real basis vectors only, I still find it strange, that an algorithm produces a complex basis.

“a bug would be surprising”
@bnaecker: This is true, but surprising bugs do exist. (Like Heartbleed and some others.) So, this is not really an argument.

“I get reals” - “your sample matrix doesn't contain floats”
@Stef &amp; @JohanC: Sorry, you didn’t read my program carefully enough. I added a value of 1e-12 to A[5,7] and A[7,5] to simulate tiny rounding errors that appear inevitably in my real app before it comes to the calculation of eigenvalues and eigenvectors. (What I’ve posted here is just a tiny test program, just big enough to demonstrate the issue.)
And you are right, Stef: Without adding this tiny noise, I also get real results. But only a tiny change of one millionth of one millionth makes such a big difference, and I can't understand why.


Reaction to DavidB2013’s answer :
I tried the tool you suggested, and I got different results. I think you also forgot to add that little noise of 1e-12 to A[5,7] and A[7,5]. However, all results are still real. I did get these eigenvalues:
12.000000000000249
20
20.00000000000075
19.999999999999
20
20
20
20

and these eigenvectors:
0.3535533905932847   0.9128505045937204      0.20252576206455747  0.002673672081814904   -0.09302397289286794   -0.09302397289286794   -0.09302397289286794   -0.09302397289286794     
0.3535533905932848  -0.18259457246238117     0.20444330131542393 -0.00009386949436945406 -0.20415317121194954   -0.20415317121194954   -0.20415317121194954   -0.20415317121194954     
0.3535533905932848  -0.18259457246238117     0.20444330131542393 -0.00009386949436945406 -0.20415317121194954   -0.20415317121194954   -0.20415317121194954    0.9080920678356449     
0.3535533905932848  -0.18259457246238117     0.20444330131542393 -0.00009386949436945406 -0.20415317121194954    0.9080920678356449    -0.20415317121194954   -0.20415317121194954     
0.3535533905932848  -0.18259457246238117     0.20444330131542393 -0.00009386949436945406  0.9080920678356449    -0.20415317121194954   -0.20415317121194954   -0.20415317121194954     
0.35355339059324065 -0.00011103276380548543 -0.6116010247648269   0.7060012169461334      0.0005790869815273477  0.0005790869815273477  0.0005790869815273477  0.0005790869815273477     
0.3535533905932848  -0.18259457246238117     0.20444330131542393 -0.00009386949436945406 -0.20415317121194954   -0.20415317121194954    0.9080920678356449    -0.20415317121194954     
0.35355339059324054  0.0002333904819895115  -0.6131412438770024  -0.7082055415560993      0.0009655029234935232  0.0009655029234935232  0.0009655029234935232  0.0009655029234935232     

Only the vector for eigenvalue 12 has the same values as your calculation: (There is a difference of approx. 1.1e-14 in 6 dimensions and 3.3e-14 in the two other dimensions, but I count this as rounding error.) All other vectors are significantly different (the smallest differences are of the size of 0.02). It puzzles me, that a tiny rounding error of 1e-12 in just 2 elements of the input matrix can produce so big differences.

I calculated the eigenvalues with another method (with the help of https://www.wolframalpha.com), and when I didn’t add the tiny delta values, which should simulate rounding errors, I only get two different eigenvalues which are 12 and 20.
The characteristic polynomial of the given matrix is:
(20 - λ)^7 * (12 - λ)

So, it has one root at λ=12 and 7 roots at λ=20 and these 8 roots are the 8 eigenvalues. All of them real numbers.
When I add the tiny delta values, I get this characteristic polynomial:
(20 - λ)^5 * (19999999999999/1000000000000 - λ) * (1000000000000 λ^2 - 32000000000001 λ + 240000000000014)/1000000000000

It has these roots:
λ=12.00000000000024999999999998 (rounded)
λ=19.999999999999 (exact value)
λ=20 (exact value)  
λ=20 (exact value)  
λ=20 (exact value)  
λ=20 (exact value)  
λ=20 (exact value)  
λ=20.00000000000075000000000002 (rounded)

And again all 8 eigenvalues are real numbers.
Then I calculated the eigenvectors. Without adding 1e-12 I get this results:
Vector for eigenvalue 12:
v = (1,1,1,1,1,1,1,1)

The length of this vector is sqrt(8), and if you multiply the vector with 1/sqrt(8), you get exactly the result from the other calculations (0.35355339 in each dimension).
But the seven eigenvectors for eigenvalue 20 are very different. They are:
(-1,1,0,0,0,0,0,0)
(-1,0,1,0,0,0,0,0)
(-1,0,0,1,0,0,0,0)
(-1,0,0,0,1,0,0,0)
(-1,0,0,0,0,1,0,0)
(-1,0,0,0,0,0,1,0)
(-1,0,0,0,0,0,0,1)

Even if you bring them to the length 1, they are different from all other results and it is very easy to see that they are correct. The other results are also correct, but I would prefer these simple results.
I also calculated the eigenvalues for the version with the tiny noise. All 8 vectors are so close to the noise-less results, that even Wolfram Alpha rounded them to exactly the same values as before. And this is exactly the behavior that I would expect from an algorithm that calculates eigenvalues and eigenvectors:

Tiny variations in the input should - when ever it is possible - return tiny variations in the results.

","As far as I know, assumption 1 is correct, but assumption 2 is not.
A Real Symmetric matrix produces eigenvalues and eigenvectors that are real only.
However, for a given eigenvalue, the associated eigenvector isn't necessarily unique.
Furthermore, round-off error shouldn't be so significant for a matrix that actually isn't that big, or contain numbers that aren't very small.
For comparison, I ran your test matrix through a JavaScript version of RG.F (Real General, from the EISPACK Library):  Eigenvalues and Eigenvectors Calculator
Here is the output:
Eigenvalues:
   20
   12
   20
   20
   20
   20
   20
   20

Eigenvectors:
 0.9354143466934854     0.35355339059327395     -0.021596710639534     -0.021596710639534     -0.021596710639534     -0.021596710639534     -0.021596710639533997     -0.021596710639533997
-0.1336306209562122     0.3535533905932738     -0.15117697447673797     -0.15117697447673797     -0.15117697447673797     -0.15117697447673797     -0.15117697447673797     -0.15117697447673797
-0.1336306209562122     0.3535533905932738     0.9286585574999623     -0.15117697447673797     -0.15117697447673797     -0.15117697447673797     -0.15117697447673797     -0.15117697447673797
-0.1336306209562122     0.3535533905932738     -0.15117697447673797     0.9286585574999623     -0.15117697447673797     -0.15117697447673797     -0.15117697447673797     -0.15117697447673797
-0.1336306209562122     0.3535533905932738     -0.15117697447673797     -0.15117697447673797     0.9286585574999623     -0.15117697447673797     -0.15117697447673797     -0.15117697447673797
-0.1336306209562122     0.3535533905932738     -0.15117697447673797     -0.15117697447673797     -0.15117697447673797     0.9286585574999623     -0.15117697447673797     -0.15117697447673797
-0.1336306209562122     0.3535533905932738     -0.15117697447673797     -0.15117697447673797     -0.15117697447673797     -0.15117697447673797     0.9286585574999622     -0.15117697447673797
-0.1336306209562122     0.3535533905932738     -0.15117697447673797     -0.15117697447673797     -0.15117697447673797     -0.15117697447673797     -0.15117697447673797     0.9286585574999622

No imaginary components.
To confirm, or deny, the validity of results, you could always write a small program that plugs the results back into the original equation. Simple matrix and vector multiplication. Then you'd know for sure whether or not the outputs are correct. Or, if they are wrong, how far away from correct answers they are.
",,,false,
https://stackoverflow.com/questions/22591174/pandas-multiple-conditions-while-indexing-data-frame-unexpected-behavior,false,"The behavior observed in pandas while filtering rows based on multiple conditions is due to a misunderstanding of logical operators and conditions, rather than a problem with the API itself.",,,,,,,,,,,,false,
https://stackoverflow.com/questions/25752444,false,"The issue is related to compatibility and installation problems with numpy, scipy, and scikit-learn libraries, rather than an API exhibiting unexpected failures or behaviors.",,,,,,,"Scipy error: numpy.dtype size changed, may indicate binary incompatibility (and associated strange behavior)","I am installing numpy/scipy/scikit-learn on OS X 10.9.4, and am getting errors about ""numpy.dtype size changed, may indicate binary incompatibility"".

Here's what I did to construct the repo:

mkvirtualenv thm
workon thm
pip install numpy scipy pandas ipython # and some other stuff
cd /path/to/our/repo
# run tests


Here's a traceback excerpt of a relevant warning (turned into an error because we use warnings.simplefilter('error') at the beginning of our tests):

======================================================================
ERROR: Failure: RuntimeWarning (numpy.dtype size changed, may indicate binary in
compatibility)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/Users/ben/.virtualenvs/thm/lib/python2.7/site-packages/nose/loader.py"",
 line 414, in loadTestsFromName
    addr.filename, addr.module)
  File ""/Users/ben/.virtualenvs/thm/lib/python2.7/site-packages/nose/importer.py
"", line 47, in importFromPath
    return self.importFromDir(dir_path, fqname)
  File ""/Users/ben/.virtualenvs/thm/lib/python2.7/site-packages/nose/importer.py
"", line 94, in importFromDir
    mod = load_module(part_fqname, fh, filename, desc)
  File ""/Users/ben/code/thm/alpha/prosper/base/stats/test_auc.py"", line 3, in &lt;m
odule&gt;
    import sklearn.metrics
  File ""/Users/ben/.virtualenvs/thm/lib/python2.7/site-packages/sklearn/metrics/
__init__.py"", line 6, in &lt;module&gt;
    from .metrics import (accuracy_score,
  File ""/Users/ben/.virtualenvs/thm/lib/python2.7/site-packages/sklearn/metrics/metrics.py"", line 27, in &lt;module&gt;
    from scipy.spatial.distance import hamming as sp_hamming
  File ""/Users/ben/.virtualenvs/thm/lib/python2.7/site-packages/scipy/spatial/__init__.py"", line 90, in &lt;module&gt;
    from .ckdtree import *
  File ""__init__.pxd"", line 155, in init scipy.spatial.ckdtree (scipy/spatial/ckdtree.c:20570)
RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility


I'm told that this warning is caused by scipy being compiled against a different version of numpy than the one installed. However, I installed them all with pip in what I thought was a pretty standard way, so this shouldn't be a problem, I would think.

Weirdly, although running our entire test suite as a whole (via python -m unittest discover) gives these errors, running the individual tests (via python -m unittest &lt;module&gt;) works fine.

According to the tests, here's some relevant version info:

numpy version 1.9.0 (rev 07601a64cdfeb1c0247bde1294ad6380413cab66)
scipy version 0.14.0 (built against numpy 1.9.0)
sklearn version 0.15.2
pandas version 0.14.1


Happy to provide more info on request!
","How did you build sklearn 0.14.1? Did you build it against the same version of numpy as you did for scipy?

Recent versions of scikit-learn, scipy and numpy have prebuilt-packages. In particular scikit-learn 0.15.2 should be binary compatible with numpy 1.7+. I think the same is true with scipy 0.14.0 but you said you built it yourself from source, which is not what pip should do by default (it should just install the prebuilt whl package).

Edit: have you tried to do:

pip install -U scipy scikit-learn pandas


to make sure that you are using the latest stable versions of the whl for those packages?

Edit: The comment below has the actual answer that works and is presumably why this answer was accepted.  Namely:

pip uninstall -y scipy scikit-learn
pip install --no-binary scipy scikit-learn

",,,false,
https://stackoverflow.com/questions/17130795,false,"The issue is related to differences in the periodogram implementation between Octave/Matlab and scipy, rather than an API exhibiting unexpected failures or behaviors.",,,,,,,Periodogram in Octave/Matlab vs Scipy,"I am porting some matlab code to python using scipy and got stuck with the following line:

Matlab/Octave code

[Pxx, f] = periodogram(x, [], 512, 5)


Python code

f, Pxx = signal.periodogram(x, 5, nfft=512)


The problem is that I get different output on the same data. More specifically, Pxx vectors are different. I tried different windows for signal.periodogram, yet no luck (and it seems that default scypy's boxcar window is the same as default matlab's rectangular window) Another strange behavior is that in python, first element of Pxx is always 0, no matter what data input is.

Am i missing something? Any advice would be greatly appreciated!  



Simple Matlab/Octave code with actual data: http://pastebin.com/czNeyUjs
Simple Python+scipy code with actual data: http://pastebin.com/zPLGBTpn
","After researching octave's and scipy's periodogram source code I found that they use different algorithm to calculate power spectral density estimate. Octave (and MATLAB) use FFT, whereas scipy's periodogram use the Welch method. 

As @georgesl has mentioned, the output looks quite alike, but still, it differs. And for porting reason it was critical. In the end, I simply wrote a small function to calculate PSD estimate using FFT, and now output is the same. According to timeit testing, it works ~50% faster (1.9006s vs 2.9176s on a loop with 10.000 iterations). I think it's due to the FFT being faster than Welch in scipy's implementation, of just being faster. 

Thanks to everyone who showed interest.
","I faced the same problem but then I came across the documentation of scipy's periodogram  

As you would see there that detrend='constant' is the default argument. This means that python automatically subtracts the mean of the input data from each point. (Read here). While Matlab/Octave do no such thing. I believe that is the reason why the outputs are different. Try specifying detrend=False, while calling scipy's periodogram you should get the same output as Matlab.
","After reading the Matlab and Scipy documentation, another contribution to the different values could be that they use different default window function. Matlab uses a Hamming window, and Scipy uses a Hanning. The two window functions and similar but not identical.
",false,
https://stackoverflow.com/questions/20037444,false,"The issue is related to the performance difference in resampling operations in scipy.signal.resample, rather than an API exhibiting unexpected failures or behaviors.",,,,,,,scipy.signal.resample behaves strangely,"I am currently working on some signal processing (using scipy), but I encountered a strange problem and can't figure out what's wrong. Namely, I am reading some audio data from a .wav file, but have to resample before further processing. The signal has more than 500,000 samples.

Now, scipy.signal.resample takes more than 10 minutes on just one of the channels. OK, I thought, this might be normal because there are a lot of samples. However, then I decided to experiment with two other ""signals"" (i.e. a randomly generated array of numbers and an array of zeros) with 1,000,000 samples and resample these ones. Strangely, resampling in this case takes only a few milliseconds, so the size is obviously not a problem.

My final experiment was extracting the zeros from my original signal (there are about 50,000 samples that are zero-valued) and resampling them. I was totally surprised to see that resampling only 50,000 zeros takes about a minute. Previously, I resampled an array of zeros that had 1,000,000 samples in a few milliseconds and now I have to wait about a minute for an array of 50,000 samples. Something has to be wrong, but I can't figure out what.

I really don't see any reason for this behavior; especially the zeros (1,000,000 and just a few milliseconds vs 50,000 and a minute) surprise me a lot.

Here's a sample code, so that you know what I'm talking about:

import scipy.io.wavfile as wavfile
import numpy
import scipy.signal as signal

sample_rate, signal_data = wavfile.read('file.wav')

test_channel = numpy.array(signal_data[:,0], dtype=float)
channel_zeros = numpy.array(signal_data[numpy.where(signal_data[:,0]==0)[0],0], dtype=float)
test_signal = numpy.random.rand((1000000))
test_signal_2 = numpy.zeros((1000000))

number_of_samples = 500

#both of these are executed in less than a second
resampled_random = signal.resample(test_signal, number_of_samples)
resampled_zeros = signal.resample(test_signal_2, number_of_samples)

#this takes minutes
resamples_original_signal = signal.resample(test_channel, number_of_samples)

#this takes about a minute
resampled_original_zeros = signal.resample(channel_zeros, number_of_samples)


Do you have any idea what might be wrong with this? Thanks in advance.
","The numpy implementation of FFT (based on FFTPACK) is fastest when the length of the data is a power of 2 (e.g. 2, 4, 8, 16, 32) and slowest when it is a prime. To speed up processing of the signal, you can zero-pad the data to a power of 2 length.

In Python you can use the following code to find the next largest power of 2 for a given number:

y = np.floor(np.log2(n))
nextpow2 = np.power(2, y+1)


You can use this with numpy.pad to pad your data array to this size:

sample_rate, signal_data = wavfile.read('file.wav')
n = signal_data.shape[0]

y = np.floor(np.log2(n))
nextpow2  = np.power(2, y+1)

signal_data  = np.pad(signal_data , ((0, nextpow2-n), (0,0)), mode='constant')


For more background on scipy/numpy and FFT in general in see this question.
",,,false,
https://stackoverflow.com/questions/37592643,false,"The issue is related to the behavior of scipy.interpolate.interp1d and the return type of the interpolation result, rather than an API exhibiting unexpected failures or behaviors.",,,,,,,Scipy interpolate returns a &#39;dimensionless&#39; array,"I understand that interp1d expects an array of values to interpolate, but the behavior when passing it a float is strange enough to ask what is going on and what exactly is being returned

import numpy as np
from scipy.interpolate import interp1d

x = np.array([1,2,3,4])
y = np.array([5,7,9,15])
f = interp1d(x,y, kind='cubic')
a = f(2.5)

print(repr(a))
print(""type is {}"".format(type(a)))
print(""shape is {}"".format(a.shape))
print(""ndim is {}"".format(a.ndim))
print(a)


Output:

array(7.749999999999992)
type is &lt;class 'numpy.ndarray'&gt;
shape is ()
ndim is 0
7.749999999999992


EDIT: To clarify, I would not expect numpy to even have a dimensionless, shapeless array much less a scipy function return one. 

print(""Numpy version is {}"".format(np.__version__))
print(""Scipy version is {}"".format(scipy.__version__))

Numpy version is 1.10.4
Scipy version is 0.17.0

","The interp1d returns a value that matches the input in shape - after wrapping in np.array() if needed:

In [324]: f([1,2,3])
Out[324]: array([ 5.,  7.,  9.])

In [325]: f([2.5])
Out[325]: array([ 7.75])

In [326]: f(2.5)
Out[326]: array(7.75)

In [327]: f(np.array(2.5))
Out[327]: array(7.75)


Many numpy operations do return scalars instead of 0d arrays.

In [330]: np.arange(3).sum()
Out[330]: 3


though actually it returns a numpy object

In [341]: type(np.arange(3).sum())
Out[341]: numpy.int32


which does have a shape () and ndim 0.

Whereas interp1d returns an array. 

In [344]: type(f(2.5))
Out[344]: numpy.ndarray


You can extract the value with [()] indexing

In [345]: f(2.5)[()]
Out[345]: 7.75

In [346]: type(f(2.5)[()])
Out[346]: numpy.float64


This may just be an oversight in the scipy code.  How often do people want to interpolate at just one point?  Isn't interpolating over a regular grid of points more common?

==================

The documentation for f.__call__ is quite explicit about returning an array.

Evaluate the interpolant

Parameters
----------
x : array_like
    Points to evaluate the interpolant at.

Returns
-------
y : array_like
    Interpolated values. Shape is determined by replacing
    the interpolation axis in the original array with the shape of x.


===============

The other side to the question is why does numpy even have a 0d array.  The linked answer probably is sufficient.  But often the question is asked by people who are used to MATLAB.  In MATLAB nearly everything is 2d.  There aren't any (true) scalars.  Now MATLAB has structures and cells, and matrices with more than 2 dimensions.  But I recall a time (in the 1990s) when it didn't have those.  Everything, literal, was a 2d matrix.

The np.matrix approximates that MATLAB case, fixing its arrays at 2d. But it does have a _collapse method that can return a 'scalar'.
",,,false,
https://stackoverflow.com/questions/27004245,false,"The issue is related to the behavior of numpy arrays and their typecasting behavior, rather than an API exhibiting unexpected failures or behaviors.",,,,,,,Numpy array bug or feature (catsing to int behind the scenes)?,"Today I have noticed strange behavior of Numpy/Scipy arrays. It looks like adding array cell with integer inside to float can have two different results, depending on the variable to which the result is assigned. Instead of long explanations, below I present the code:

import scipy as sp
array_int = sp.array([[0], [0]])
float_operand = 0.1
print array_int[0, 0] + float_operand #gives 0.1


But

import scipy as sp
array_int = sp.array([[0], [0]])
float_operand = 0.1
array_int[0, 0] = array_int[0, 0] + float_operand 
print array_int[0, 0] #gives 0


I could understand if this kind of behavior was inherited from Python, but:

In contrary to behavior of ""bare"" Python (2.7):

integer_for_sure = int(0) 
integer_for_sure = integer_for_sure + 0.1
print integer_for_sure #gives 0.1 as expected


Is this kind of feature somwhere documented? Has anybody encounterd it before?
","Henry Keiter has explained it well enough. I would only add one technical detail.

In contrast to the regular assignment which simply rewires integer_for_sure to refer to the float object that results from integer_for_sure + 0.1, thus changing the type of the variable, assignment to array elements such as

array_int[0, 0] = array_int[0, 0] + float_operand


is actually syntactic sugar for the more verbose

array_int.__setitem__((0,0), array_int.__getitem__((0,0)) + float_operand)


(this applies to old-style classes; it looks a bit different for new-style classes but the idea stays the same)

The __setitem__ method for each array type performs a typecast of its value argument to the type of the array. The actual C code that implements the assignment is kind of ugly and involves a custom preprocessor.

On the other side

print array_int[0, 0] + float_operand


is

print array_int.__getitem__((0,0)) + float_operand


i.e. it fetches the integer value from array_int, sums it with float_operand and the resulting float object is passed to print. There is no intermediate assignment to array_int[0, 0] and therefore no typecast.
","This is not a behavior ""inherited from Python"" -- as you can see, adding a float to an int in pure Python produces a float result. Rather, you can think of this behavior as ""inherited from C."" Unlike Python lists, numpy arrays have strong element types. The array constructor includes a telltale optional keyword argument that alludes to this:

dtype : data-type, optional
The desired data-type for the array. If not given, then the type will be determined as the minimum type required to hold the objects in the sequence. This argument can only be used to ‘upcast’ the array.

The emphasis is mine. When you create an array with np.array([[0], [0]]), you get a two-dimentional integer array, because an integer is the smallest datatype that can contain 0. Once the integer array is created, it may only contain integers. If you attempt to insert a float, it will be cast to an integer as you noticed in order to be placed in the array.

If you want to store floats in the array eventually, you need to initialize your array as a float array ('upcast' it). This can be accomplished by using the dtype argument mentioned above, or simply by putting a float value into the initial array (e.g. 0.0 instead of the integer 0).
import scipy as sp
array_int = sp.array([[0], [0]])
array_float = sp.array([[0.0], [0]])  # Note the extra .0 to make one element a float

array_int[0, 0] = array_int[0, 0] + 0.1
print array_int[0, 0] # 0

array_float[0, 0] = array_float[0, 0] + 0.1
print array_float[0, 0] # 0.1

",,false,
https://stackoverflow.com/questions/65513251,false,"The issue is related to the behavior of scipy.sparse.coo_matrix.max and the in-place modification of the data attribute, rather than an API exhibiting unexpected failures or behaviors.",,,,,,,Scipy coo_matrix.max() alters data attribute,"I am building a recommendation system using an open source library, LightFM. This library requires certain pieces of data to be in a sparse matrix format, specifically the scipy coo_matrix. It is here that I am encountering strange behavior. It seems like a bug, but it's more likely that I am doing something wrong.
Basically, I let LightFM.Dataset build me a sparse matrix, like so:
interactions, weights = dataset.build_interactions(data=_get_interactions_data())

The method, build_interactions, returns ""Two COO matrices: the interactions matrix and the corresponding weights matrix"" -- LightFM Official Doc.
When I inspect the contents of this sparse matrix (in practice, I use a debugger), like so:
for i in interactions.data:
    print(i, end=', ')

1, 1, 1, 1, 1, ....
It prints a long list of 1s, which indicates that the sparse matrix's nonzero elements are only 1s.
However, when I first check the max of the sparse matrix, it indicates that the maximum values in the sparse matrix is not a 1, its a 3. Furthermore, printing the matrix after that check will print a long list of 1s, 2s, and 3s. This is the code for that:
print(interactions.max())
for i in interactions.data:
    print(i, end=', ')

3 
1, 1, 3, 2, 1, 2, ...
Any idea what is going on here? Python is 3.6.8. Scipy is 1.5.4. CentOS7.
Thank you.
","A 'raw' coo_matrix can have duplicate elements (repeats of the same row and col values), but when converted to csr format for calculations those duplicates are summed. It must be doing the same, but in-place, in order to find that max.
In [9]: from scipy import sparse
In [10]: M = sparse.coo_matrix(([1,1,1,1,1,1],([0,0,0,0,0,0],[0,0,1,0,1,2])))
In [11]: M.data
Out[11]: array([1, 1, 1, 1, 1, 1])
In [12]: M.max()
Out[12]: 3
In [13]: M.data
Out[13]: array([3, 2, 1])
In [14]: M
Out[14]: 
&lt;1x3 sparse matrix of type '&lt;class 'numpy.int64'&gt;'
    with 3 stored elements in COOrdinate format&gt;

Tracing through the max code I find it uses sum_duplicates
In [33]: M = sparse.coo_matrix(([1,1,1,1,1,1],([0,0,0,0,0,0],[0,0,1,0,1,2])))
In [34]: M.data
Out[34]: array([1, 1, 1, 1, 1, 1])
In [35]: M.sum_duplicates?
Signature: M.sum_duplicates()
Docstring:
Eliminate duplicate matrix entries by adding them together

This is an *in place* operation
File:      /usr/local/lib/python3.8/dist-packages/scipy/sparse/coo.py
Type:      method
In [36]: M.sum_duplicates()
In [37]: M.data
Out[37]: array([3, 2, 1])

",,,false,
https://stackoverflow.com/questions/58164513,true,"The issue involves the behavior of the bisplrep and bisplev functions from the scipy.interpolate module. These functions are used for spline interpolation, and the unexpected behavior is observed when plotting the interpolated data. The issue is likely related to the input data or the way the interpolation is performed.",SciPy,"scipy.interpolate.bisplrep, scipy.interpolate.bisplev","When using the bisplrep and bisplev functions to perform spline interpolation, the resulting plot appears rotated about 90 degrees. This behavior is observed even when the x and y values are exchanged. The issue is not related to the specific input data or the interpolation method used.",The bisplrep and bisplev functions are applied to a set of data points to perform spline interpolation.,The issue is triggered when plotting the interpolated data using the bisplev function.,This issue might be challenging to detect during development and testing because the unexpected behavior only becomes apparent when visualizing the interpolated data.,Problems with bisplrep and bisplev from scipy.interpolate,"I receive some quite strange behavior with bisplrep and bisplev from the scipy.interpolate module. I try to reproduce the example from the scipy homepage (https://docs.scipy.org/doc/scipy/reference/tutorial/interpolate.html). I have some function f(x,y) which calculates me some z-values which I then interpolate using bisplrep. If I recalculate the data using bisplev and plot the data the resulting values appear rotated about 90°. Even if I exchange the x and y values the plot is still rotated. Can somebody tell me if I am doing something completely wrong here? The following code should be sufficient to reproduce the error. I am using the most recent version of scipy and the error occurs in Jupyter, Spyder and in IDLE. 

import matplotlib.pyplot as plt
import numpy as np
from scipy import interpolate

def f(x, y):
    return x**2 + y**2

x, y = np.linspace(0, 5, 15), np.linspace(-2*np.pi, 2*np.pi, 15)
xx, yy = np.meshgrid(x, y)
zz = f(xx, yy)

tck = interpolate.bisplrep(xx, yy, zz)

plt.pcolor(xx, yy, zz)

x_new, y_new = np.linspace(0, 5, 100), np.linspace(-2*np.pi, 2*np.pi, 100)

z_new = interpolate.bisplev(x_new, y_new, tck)

plt.figure()    
plt.pcolor(x_new, y_new, z_new)

plt.figure()    
plt.pcolor(y_new, x_new, z_new)

plt.show()

","I used another definition for the grid. Now, it should work:

import matplotlib.pyplot as plt
import numpy as np
from scipy import interpolate

def f(x, y):
    return x**2 + y**2

#x, y = np.linspace(0, 5, 15), np.linspace(-2*np.pi, 2*np.pi, 15)
xx, yy = np.mgrid[0:5:15j, -2*np.pi:2*np.pi:15j]
zz = f(xx, yy)

tck = interpolate.bisplrep(xx, yy, zz)

plt.pcolor(xx, yy, zz)

#x_new, y_new = np.linspace(0, 5, 100), np.linspace(-2*np.pi, 2*np.pi, 100)
xx_new, yy_new = np.mgrid[0:5:100j, -2*np.pi:2*np.pi:100j]

zz_new = interpolate.bisplev(xx_new[:,0], yy_new[0,:], tck)

plt.figure()    
plt.pcolor(xx_new, yy_new, zz_new)

plt.figure()    
plt.pcolor(yy_new, xx_new, zz_new)

plt.show()

",,,false,
https://stackoverflow.com/questions/31685156,true,The issue involves the behavior of the numpy.linalg.norm function when applied to a masked array. The unexpected behavior is observed when normalizing a vector containing masked values. The issue is related to the implementation of the argsort method in the masked_array class.,NumPy,numpy.linalg.norm,"When applying numpy.linalg.norm to a masked array, the resulting ranks are not as expected. The issue is caused by the argsort method in the masked_array class, which does not handle masked values properly.",The numpy.linalg.norm function is applied to a regular array or vector.,The issue is triggered when applying numpy.linalg.norm to a masked array.,"This issue might be challenging to detect during development and testing because it involves the interaction between the numpy.linalg.norm function and the masked_array class, which might not be immediately obvious.",Normalizing vector produces nan in Numpy,"I'm getting some strange behavior from scipy/numpy that I suspect is a bug but someone may know better?  I've got a pair of long arrays which I'm breaking into frames which are of length 2-4 for debugging purposes.  I want to normalize each pair of frames and take the dot product.  The code that does it (with some debugging output) is:

   tf = numpy.copy(t_frame) / norm(t_frame)
   pf = numpy.copy(p_frame) / norm(p_frame)
   print ""OPF:""
   print p_frame
   print ""PF: ""
   print pf
   print ""TF norm is: "" + str(norm(tf))
   print ""PF norm is: "" + str(norm(pf))
   print numpy.dot(tf, pf)
   return numpy.dot(tf, pf)


This does what I'd expect for a while (specifically giving a norm of 1 for tf and pf) but then I start to see lines like this:


  OPF:
  
  [ -91 -119 -137 -132]
  
  PF: 
  
  [ nan  nan  nan  nan]


What??  This can be normalized fine in a new Python window:

&gt;&gt;&gt; p = [ -91, -119, -137, -132] 
&gt;&gt;&gt; p / norm(p)
array([-0.37580532, -0.49143773, -0.56577285, -0.54512421])


For what it's worth, I've tried numpy.linalg.norm, scipy.linalg.norm, and defining a function to return the square root of the dot product.

Any ideas?

UPDATE:
Thanks for the suggestions!  I tried switching the dtype to float128 and am sadly getting similar behavior.  I'm actually inclined to believe that it's a bug in Python rather than numpy at this point:


If it were a straightforward overflow issue, it seems like I'd get it consistently with a given list.  But the norm computes fine if I do it in a new python session.
I tried rolling my own:

def norm(v):
   return (  sum(numpy.array(v)*numpy.array(v)))**(0.5)


This only uses numpy  to represent the arrays.  I still get the same issue, but later in the data set (and no runtime warnings).  It's doing about 37000 of these computations.
I'm actually computing the norm on two frames, a t_frame and a p_frame.  The computation of one chokes if and only if the computation for the other one does.


Put together, I think there's some weird buffer overflow somewhere in the bowels of Python (2.7.9)???  I ultimately need these computations to be fast as well; so I'm thinking of just switching over to Cython for that computation.

Update 2:
I tried really rolling my own:

def norm(v):
  sum = float(0)
  for i in range(len(v)):
    sum += v[i]**2
  return sum**(0.5)


and the problem disappears.  So I would guess that it is a bug in numpy (1.9.0 on Gentoo Linux).
","It looks like this is a bug in numpy.  I can reproduce the problem if the data type of the array is np.int16:

In [1]: np.__version__
Out[1]: '1.9.2'

In [2]: x = np.array([ -91, -119, -137, -132], dtype=np.int16)

In [3]: x
Out[3]: array([ -91, -119, -137, -132], dtype=int16)

In [4]: np.linalg.norm(x)
/Users/warren/anaconda/lib/python2.7/site-packages/numpy/linalg/linalg.py:2061: RuntimeWarning: invalid value encountered in sqrt
  return sqrt(sqnorm)
Out[4]: nan


The problem also occurs in the master branch of the development version of numpy.  I created an issue here: https://github.com/numpy/numpy/issues/6128

If p_frame is, in fact, a 16 bit integer array, a simple work-around is something like:

x = np.asarray(p_frame, dtype=np.float64)
pf = x / norm(x)

","Following one of Warren's links, I get this warning:

In [1016]: np.linalg.norm(100000*np.ones(2).astype('int16'))
/usr/local/lib/python2.7/site-packages/numpy/linalg/linalg.py:2051: RuntimeWarning: invalid value encountered in sqrt
  return sqrt(add.reduce((x.conj() * x).real, axis=None))


For this x2, the inner expression is negative - the result of overflow in a small dtype.

In [1040]: x2=100000*np.ones(2).astype('int16')
In [1041]: np.add.reduce((x2.conj()*x2).real,axis=None)
Out[1041]: -1474836480


similarly with an x1:

In [1042]: x1
Out[1042]: array([ -9100, -11900, -13700, -13200], dtype=int16)
In [1043]: np.add.reduce((x1.conj()*x1).real,axis=None)
Out[1043]: -66128


If the sum of the 'dot' becomes too large for the dtype, it can be negative, producing a nan when passed through sqrt.

(I'm using 1.8.2 and 1.9.0 under linux).
",,false,
https://stackoverflow.com/questions/77227994,true,The issue involves the behavior of the scipy.optimize.milp function when used in parallel with multiprocessing. The unexpected behavior is observed when running the optimization process in multiple processes sequentially. The issue is likely related to the interaction between the multiprocessing module and the scipy.optimize.milp function.,SciPy,scipy.optimize.milp,"When running the scipy.optimize.milp function in multiple processes using the multiprocessing module, the process hangs indefinitely after the first run. The issue is not related to the specific optimization problem or the data being used.",The scipy.optimize.milp function is applied to an optimization problem in a single process.,The issue is triggered when running the scipy.optimize.milp function in multiple processes using the multiprocessing module.,"This issue might be challenging to detect during development and testing because it involves the interaction between the scipy.optimize.milp function and the multiprocessing module, which might not be immediately obvious.",Parallel scipy milp optimizing with multiprocessing,"I want to solve a linear programming problem using the scipy. Everything works in one process. If you try to do this in several processes, then everything works too.
However, if you first do optimization using multiprocessing, then without it, and then with it, then everything hangs. Any idea why?
if you run the code below, it will run for an infinite time:
import numpy as np
from multiprocessing import Pool

from scipy.optimize import LinearConstraint, milp

def task(identifier):
    c = -np.array([1., 2])
    A = np.array([[-1., 1], [-3, 2], [2, 3]])
    b_u = np.array([1., 12, 12])
    b_l = np.full_like(b_u, -np.inf)

    constraints = LinearConstraint(A, b_l, b_u)
    integrality = np.ones_like(c)

    res = milp(c=c, constraints=constraints, integrality=integrality)
    return res.x


with Pool() as pool:
    for result in pool.map(task, range(3)):
        print(f'Before: Got result: {result}')

print(task(4)) # if you comment out - everything works, otherwise it hangs

with Pool() as pool:
    for result in pool.map(task, range(3)):
        print(f'After: Got result: {result}')

Python version and Libraries:
Python 3.10.11, Numpy 1.24.2, SciPy 1.11.3
Who knows why such strange behavior and how it can be fixed?
","I was able to run your code from the command line with a script containing:
import numpy as np
from multiprocessing import Pool

from scipy.optimize import LinearConstraint, milp


def task(identifier):
    c = -np.array([1.0, 2])
    A = np.array([[-1.0, 1], [-3, 2], [2, 3]])
    b_u = np.array([1.0, 12, 12])
    b_l = np.full_like(b_u, -np.inf)

    constraints = LinearConstraint(A, b_l, b_u)
    integrality = np.ones_like(c)

    res = milp(c=c, constraints=constraints, integrality=integrality)
    return res.x


if __name__ == ""__main__"":
    with Pool() as pool:
        for result in pool.map(task, range(3)):
            print(f""Before: Got result: {result}"")

    print(task(4))  # if you comment out - everything works, otherwise it hangs

    with Pool() as pool:
        for result in pool.map(task, range(3)):
            print(f""After: Got result: {result}"")

Result:
Before: Got result: [3. 2.]
Before: Got result: [3. 2.]
Before: Got result: [3. 2.]
[3. 2.]
After: Got result: [3. 2.]
After: Got result: [3. 2.]
After: Got result: [3. 2.]

If you are trying to run your code within Jupyter/IPython, there are known complications with using multiprocessing in these environments. Take a look at this question for some possible solutions.
",,,false,
https://stackoverflow.com/questions/51089817,true,The issue involves the behavior of the rankdata function in scipy.stats.mstats when applied to a masked array. The unexpected behavior is observed when ranking a masked array with missing values. The issue is related to the implementation of the argsort method in the masked_array class.,SciPy,scipy.stats.mstats.rankdata,"When applying the rankdata function to a masked array, the resulting ranks are not as expected. The issue is caused by the argsort method in the masked_array class, which does not handle masked values properly.",The rankdata function is applied to a regular array or vector.,The issue is triggered when applying the rankdata function to a masked array.,"This issue might be challenging to detect during development and testing because it involves the interaction between the rankdata function and the masked_array class, which might not be immediately obvious.",scipy rankdata with masked array,"I notice the following strange behavior with rankdata with maksed_array. Here is the code:

import numpy as np
import scipy.stats as stats

m = [True, False]
print(stats.mstats.rankdata(np.ma.masked_array([1.0, 100], mask=m)))
# result [0. 1.]

print(stats.mstats.rankdata(np.ma.masked_array([1.0, np.nan], mask=m)))
# result [1. 0.]

print(stats.mstats.rankdata([1.0, np.nan]))
# result [1. 2.]


According the scipy doc, masked values will be assigned 0 (use_missing=False). So why it outputs [1 0] in the 2nd one? Bug?
","After tracing I find it is related to the argsort method of masked_array. When mstats.rankdata calls argsort, it does not specify fill_value, and endwith input parameters, which defaults to np.nan and True respectively. Based on the following code from numpy, the fill_value is np.nan.

if fill_value is None:
    if endwith:
        # nan &gt; inf
        if np.issubdtype(self.dtype, np.floating):
            fill_value = np.nan


So in the case of masked_array of [1, 100], it is argsorting [nan, 100], which is [1, 0]. In the case of masked_array of [1, np.nan], it is argsoring [nan, nan], which can be [0,1]. Then in the rankdata function, it assume first n (n=1) from argsort is valid, which is not correct here.

n = data.count()
rk = np.empty(data.size, dtype=float)
idx = data.argsort()
rk[idx[:n]] = np.arange(1,n+1)

",,,false,
https://stackoverflow.com/questions/23897118,true,The issue involves the behavior of the pylab.plot function from matplotlib.pyplot after calling the scipy.stats.multivariate_normal function. The unexpected behavior is observed when plotting the samples generated by multivariate_normal. The issue is likely related to the interaction between the multivariate_normal function and the plot function.,"SciPy, Matplotlib","scipy.stats.multivariate_normal, matplotlib.pyplot.plot","After generating samples using the multivariate_normal function, plotting the samples using the plot function results in a ""cannot convert float NaN to integer"" error. The issue is not related to the specific input data or the plotting method used.","The multivariate_normal function is used to generate samples from a multivariate normal distribution, and the plot function is used to visualize the samples.",The issue is triggered when plotting the samples generated by the multivariate_normal function using the plot function.,"This issue might be challenging to detect during development and testing because it involves the interaction between the multivariate_normal function and the plot function, which might not be immediately obvious.",pylab.plot &quot;cannot convert float NaN to integer&quot; after calling scipy.stats.multivariate_normal,"While testing a regression algorithm I found this strange behavior: for some covariance matrices, the multivariate_normal function gives correct samples but then an exception is raised (only) the first time pylab.plot() is called:

ValueError: cannot convert float NaN to integer

The following code reproduces the error:
import numpy as np
from scipy.stats import multivariate_normal as mnorm
from matplotlib import pyplot as plt

B = np.array([ 0, 0, 0])

# works fine
v1 = np.array([[1, 0, 0],
              [0, 1, 0],
              [0, 0, 1]])


# OK. non positive semidefinite, well raised exception
v2 = np.array([[ 0.2 , -0.2, -0.3],
              [-0.2,  0.4, -0.9],
              [-0.3, -0.9,  0.7]])

# KO. exception (?)
v3 = np.array([[ 0.2 , -0.02, -0.026],
              [-0.02,  0.014, -0.009],
              [-0.026, -0.009,  0.017]])



w = mnorm(mean=B, cov=v3).rvs()
print w

plt.plot(w)
plt.show()

And if plt.plot(w) is called a second time, then it works. Any ideas?
Versions:

python 2.7.5 Anaconda 1.9.1 (64-bit)
scipy 0.14.0
matplotlib 1.3.1
numpy 1.8.1

","Well, it works fine here, and says :

[-0.72849048  0.15439657  0.00146853]


and shows :



I use python 2.7.6

other packages are same as yours.

Hope it helped. Good luck !
",,,false,
https://stackoverflow.com/questions/17097236,false,"The issue is related to replacing values with None in a Pandas DataFrame. However, it does not involve any unexpected behavior or unpredictable API failures. The issue can be resolved by using appropriate methods provided by Pandas.",,,,,,,Replace invalid values with None in Pandas DataFrame,"Is there any method to replace values with None in Pandas in Python?

You can use df.replace('pre', 'post') and can replace a value with another, but this can't be done if you want to replace with None value, which if you try, you get a strange result.

So here's an example:

df = DataFrame(['-',3,2,5,1,-5,-1,'-',9])
df.replace('-', 0)


which returns a successful result.

But,

df.replace('-', None)


which returns a following result:

0
0   - // this isn't replaced
1   3
2   2
3   5
4   1
5  -5
6  -1
7  -1 // this is changed to `-1`...
8   9


Why does such a strange result be returned?

Since I want to pour this data frame into MySQL database, I can't put NaN values into any element in my data frame and instead want to put None. Surely, you can first change '-' to NaN and then convert NaN to None, but I want to know why the dataframe acts in such a terrible way.


  Tested on pandas 0.12.0 dev on Python 2.7 and OS X 10.8. Python is a
  pre-installed version on OS X and I installed pandas by using SciPy
  Superpack script, for your information.

","With Pandas version ≥1.0.0, I would use DataFrame.replace or Series.replace:
df.replace(old_val, pd.NA, inplace=True)

This is better for two reasons:

It uses pd.NA instead of None or np.nan.
It optionally works in-place which could be more memory efficient depending upon the internal implementation.

","Alternatively you can also use mask:
df.mask(df=='-', None)

","df.replace('-', np.nan).astype(""object"")


This will ensure that you can use isnull() later on your dataframe
",false,
https://stackoverflow.com/questions/25328818,false,"The issue is related to the behavior of the scipy.fftpack.dct and scipy.fftpack.idct functions when applied to a 2D matrix. However, it does not involve any unexpected behavior or unpredictable API failures. The issue can be resolved by normalizing the output of idct2(dct2(a)) and converting the data type to uint8.",,,,,,,python 2.7: cannot pip on windows &quot;bash: pip: command not found&quot;,"I am trying to install the SciPy stack located at https://scipy.org/stackspec.html [I am only allowed 2 links; trying to use them wisely].  I realize that there are much easier ways to do this, but I think there is a lot to be learned by doing it manually.  I am relatively new to a lot of this stuff, so I apologize if I sound ignorant at any point.
I am running  Windows 7 Enterprise - 64 bit.  Here is what I have done so far:

Installed python-2.7.8.msi (32-bit) from https://www.python.org/download/releases/2.7.8/

Installed numpy-1.8.1-win32-superpack-python2.7 from
http://sourceforge.net/projects/numpy/files/
Test: import numpy as np ---&gt; no errors

Installed scipy library,
scipy-0.14.0-win32-superpack-python2.7.exe from
(SCIPY DOT ORG LINK REMOVED)
Test: import scipy as sp ---&gt; no errors

Installed matplotlib: matplotlib-1.3.1.win32-py2.7.exe from
(MATPLOTLIB DOT ORG LINK REMOVED)

Installed PIP by running script here:
https://raw.githubusercontent.com/pypa/pip/master/contrib/get-pip.py
I just copied-pasted script to a new file in IDLE,
saved as C:\Python27\Scripts\pip_install.py and clicked Run&gt;module. No errors reported.


Does the path on which I saved
pip_install.py matter?




HERE IS WHERE I FAIL
Attempted to install matlibplot dependency dateutil: Opened a
Cygwin Shell, and typed
        cd C:\Python27          ! is it necessary to cd to python directtory?
        pip install python-dateutil

This results in the error:
    bash: pip: command not found

I get the same error attempting from cmd.
Any help  is appreciated; the closest I found was bash: pip: command not found.  But the OSX nature of it is just enough to confise me further.

UPDATE:
I added the pip-path per Paul H's suggestion below.  It made the error go away, but strangely, nothing I pip actually installs. For example, in Cygwin, I type:
cbennett2&gt; pip install python-dateutil
cbennett2&gt;                            

You can see that there is no output or feedback from the shell (which I think there should be).  Then when I go to a new python shell:
&gt;&gt;&gt; from dateutil.parser import parse
Traceback (most recent call last):
  File ""&lt;pyshell#12&gt;"", line 1, in &lt;module&gt;
    from dateutil.parser import parse
ImportError: No module named dateutil.parser
&gt;&gt;&gt;&gt;

This happens with all of the modules that I thought I had pip'd ... pandas, tornado, etc.
","The problem is that your Python version and the library you want to use are not same versionally (Python). Even if you install Python's latest version, your PATH might not change properly and automatically. Thus, you should change it manually.After matching their version, it will work.

Ex: When I tried to install Django3, I got same error. I noticed that my PATH still seems C:\python27\Scripts though I already install Python3.8, so that I manually edited my PATH C:\python38\Scripts and reinstalled pip install Django and everything worked well. 
",,,false,
https://stackoverflow.com/questions/34890585,false,"The issue is related to the performance of sparse matrix-dense vector multiplication in SciPy. However, it does not involve any unexpected behavior or unpredictable API failures. The issue can be resolved by using appropriate methods provided by SciPy for efficient sparse matrix operations.",,,,,,,In scipy why doesn&#39;t idct(dct(a)) equal to a?,"I am trying to implement JPEG compression using python. When I tried to apply the DCT, quantization, IDCT process for a tiff image, I found something strange for scipy.fftpack.dct/idct.

Since there is only 1D dct/idct within scipy package, I was doing this for a 2D dct

import numpy as np
from scipy.fftpack import dct, idct

def dct2(block):
    return dct(dct(block.T).T)

def idct2(block):
    return idct(idct(block.T).T)


I tested the 2D dct/idct using a simple 3x3 matrix. I was expecting to get a True matrix with this test case.

a = np.random.randint(0,255,9).reshape(3,3)
print a == idct2(dct2(a))


However it turned out that after idct2(dct2(a)) the result was scaled by a constant factor compared with the original a matrix.

I would like to ask if there is a way to implement a set of 2D dct/idct such that after a idct(dct(a)) operation I can get the same output as the input.
","You need to normalize idct2(dct2(a)) and convert the data type to uint8. One option is to use cv2 for this normalization:
import numpy as np
from scipy.fftpack import dct, idct

def dct2(block):
    return dct(dct(block.T).T)

def idct2(block):
    return idct(idct(block.T).T)
a = np.random.randint(0,255,9).reshape(3,3)

# convert to uint8
norm_image = cv2.normalize(idct2(dct2(a)), None, 
                           alpha = a.min(), beta = a.max(), 
                           norm_type = cv2.NORM_MINMAX, 
                           dtype = cv2.CV_32F).astype(np.uint8)
print(a == norm_image)
# [[ True  True  True]
#  [ True  True  True]
#  [ True  True  True]]

",,,false,
https://stackoverflow.com/questions/16746974,false,"The issue is related to the behavior of the scipy.fftpack.dct and scipy.fftpack.idct functions when applied to a 2D matrix. However, it does not involve any unexpected behavior or unpredictable API failures. The issue can be resolved by normalizing the output of idct2(dct2(a)) and converting the data type to uint8.",,,,,,,Scipy Sparse Matrix - Dense Vector Multiplication Performance - Blocks vs Large Matrix,,,,,false,
https://stackoverflow.com/questions/64324685,false,The issue does not involve an API exhibiting unexpected failures or unpredictable behaviors under specific runtime conditions. It is primarily a question about the correct implementation of PCA and a misunderstanding of rotation and axis swap in relation to the PCA results.,,,,,,,Why my PCA is not invariant to rotation and axis swap?,"I have a voxel (np.array) with size 3x3x3, filled with some values, this setup is essential for me. I want to have rotation-invariant representation of it. For this case, I decided to try PCA representation which is believed to be invariant to orthogonal transformations. another
For simplicity, I took some axes swap, but in case I'm mistaken there can be np.rot90.
I have interpereted my 3d voxels as a set of weighted 3d cube point vectors which I incorrectly called ""basis"", total 27 (so that is some set of 3d point in space, represented by the vectors, obtained from cube points, scaled by voxel values).
import numpy as np

voxel1 = np.random.normal(size=(3,3,3))
voxel2 =  np.transpose(voxel1, (1,0,2)) #np.rot90(voxel1) #


basis = []
for i in range(3):
    for j in range(3):
        for k in range(3):
            basis.append([i+1, j+1, k+1]) # avoid 0
basis = np.array(basis)


voxel1 = voxel1.reshape((27,1))
voxel2 = voxel2.reshape((27,1))

voxel1 = voxel1*basis # weighted basis vectors
voxel2 = voxel2*basis

print(voxel1.shape)
(27, 3)

Then I did PCA to those 27 3-dimensional vectors:
def pca(x):
    center = np.mean(x, 0)
    x = x - center

    cov = np.cov(x.T) / x.shape[0]

    e_values, e_vectors = np.linalg.eig(cov)

    order = np.argsort(e_values)

    v = e_vectors[:, order].transpose()

    return x.dot(v)

vp1 = pca(voxel1)
vp2 = pca(voxel2)

But the results in vp1 and vp2 are different. Perhaps, I have a mistake (though I beleive this is the right formula), and the proper code must be
x.dot(v.T)
But in this case the results are very strange. The upper and bottom blocks of the transofrmed data are the same up to the sign:
&gt;&gt;&gt; np.abs(np.abs(vp1)-np.abs(vp2)) &gt; 0.01
array([[False, False, False],
       [False, False, False],
       [False, False, False],
       [ True,  True,  True],
       [ True,  True,  True],
       [ True,  True,  True],
       [ True,  True,  True],
       [ True,  True,  True],
       [ True, False,  True],
       [ True,  True,  True],
       [ True,  True,  True],
       [ True,  True,  True],
       [False, False, False],
       [False, False, False],
       [False, False, False],
       [ True,  True,  True],
       [ True,  True,  True],
       [ True,  True,  True],
       [ True,  True,  True],
       [ True,  True,  True],
       [ True, False,  True],
       [ True,  True,  True],
       [ True,  True,  True],
       [ True,  True,  True],
       [False, False, False],
       [False, False, False],
       [False, False, False]])

What I'm doing wrong?
What I want to do is to find some invariant representation of my weighted voxel, something like positioning according to the axes of inertia or principal axes. I would really appreciate if someone helps me.
UPD: Found the question similar to mine, but code is unavailable
EDIT2: Found the code InertiaRotate and managed to monkey-do the following:
import numpy as np

# https://github.com/smparker/orient-molecule/blob/master/orient.py

voxel1 = np.random.normal(size=(3,3,3))
voxel2 =  np.transpose(voxel1, (1,0,2))

voxel1 = voxel1.reshape((27,))
voxel2 = voxel2.reshape((27,))


basis = []
for i in range(3):
    for j in range(3):
        for k in range(3):
            basis.append([i+1, j+1, k+1]) # avoid 0
basis = np.array(basis)
basis = basis - np.mean(basis, axis=0)



def rotate_func(data, mass):

    #mass = [ masses[n.lower()] for n in geom.names ]

    inertial_tensor = -np.einsum(""ax,a,ay-&gt;xy"", data, mass, data)
    # negate sign to reverse the sorting of the tensor
    eig, axes = np.linalg.eigh(-inertial_tensor)
    axes = axes.T

    # adjust sign of axes so third moment moment is positive new in X, and Y axes
    testcoords = np.dot(data, axes.T) # a little wasteful, but fine for now
    thirdmoment = np.einsum(""ax,a-&gt;x"", testcoords**3, mass)

    for i in range(2):
        if thirdmoment[i] &lt; 1.0e-6:
            axes[i,:] *= -1.0

    # rotation matrix must have determinant of 1
    if np.linalg.det(axes) &lt; 0.0:
        axes[2,:] *= -1.0

    return axes

axes1 = rotate_func(basis, voxel1)
v1 = np.dot(basis, axes1.T)
axes2 = rotate_func(basis, voxel2)
v2 = np.dot(basis, axes2.T)


print(v1)
print(v2)

It seems to use basis (coordinates) and mass separately. The results are quite similar to my problem above: some parts of the transformed data match up to the sign, I believe those are some cube sides
print(np.abs(np.abs(v1)-np.abs(v2)) &gt; 0.01)
[[False False False]
 [False False False]
 [False False False]
 [ True  True  True]
 [ True  True  True]
 [ True  True  True]
 [ True  True  True]
 [False False False]
 [ True  True  True]
 [ True  True  True]
 [ True  True  True]
 [ True  True  True]
 [False False False]
 [False False False]
 [False False False]
 [ True  True  True]
 [ True  True  True]
 [ True  True  True]
 [ True  True  True]
 [False False False]
 [ True  True  True]
 [ True  True  True]
 [ True  True  True]
 [ True  True  True]
 [False False False]
 [False False False]
 [False False False]]


Looking for some explanation. This code is designed for molecules, and must work...
UPD: Tried to choose 3 vectors as a new basis from those 24 - the one with biggest norm, the one with the smallest and their cross product. Combined them into the matrix V, then used the formula V^(-1)*X to transform coordinates, and got the same problem - the resulting sets of vectors are not equal for rotated voxels.

UPD2: I agree with meTchaikovsky that my idea of multiplying voxel vectors by weights and thus creating some non-cubic point cloud was incorrect. Probably, we indeed need to take the solution for rotated ""basis""(yes, this is not a basis, but rather a way to determine point cloud) which will work later when ""basis"" is the same, but the weights are rotated according to the 3D rotation.
Based on the answer and the reference provided by meTchaikovsky, and finding other answers we together with my friend came to conclusion that rotate_func from molecular package mentioned above tries to invent some convention for computing the signs of the components. Their solution tries to use 3rd moment for the first 2 axes and determinant for the last axis (?). We tried a bit another approach and succeeded to have half of the representations matching:
# -*- coding: utf-8 -*-
""""""
Created on Fri Oct 16 11:40:30 2020

@author: Dima
""""""


import numpy as np
from numpy.random import randn
from numpy import linalg as la
from scipy.spatial.transform import Rotation as R
np.random.seed(10)

rotate_mat = lambda theta: np.array([[np.cos(theta),-np.sin(theta),0.],[np.sin(theta),np.cos(theta),0.],[0.,0.,1.]])

def pca(feat, x):
    # pca with attemt to create convention on sign changes
    
    x_c =x- np.mean(x,axis=0)
    x_f= feat*x
    x_f-= np.mean(x_f, axis=0)
    cov = np.cov(x_f.T)
    e_values, e_vectors = np.linalg.eig(cov)

    order = np.argsort(e_values)[::-1]
    #print(order)
    v = e_vectors[:,order]
    v= v/np.sign(v[0,:])
    if(la.det(v)&lt;0):
        v= -v
    return x_c @ v

def standardize(x):
    # take vector with biggest norm, with smallest and thir cross product as basis
    x -= np.mean(x,axis=0)
    nrms= la.norm(x, axis=1)
    imin= argmin(nrms)
    imax= argmax(nrms)
    vec1= x[imin, :]
    vec2= x[imax, :]
    vec3= np.cross(vec1, vec2)
    Smat= np.stack([vec1, vec2, vec3], axis=0)
    if(la.det(Smat)&lt;0):
        Smat= -Smat
    return(la.inv(Smat)@x.T)

    

angles = np.linspace(0.0,90.0,91)
voxel1 = np.random.normal(size=(3,3,3))    
res = []
for angle in angles:

    
    voxel2 = voxel1.copy()
    voxel1 = voxel1.reshape(27,1)
    voxel2 = voxel2.reshape(27,1)
    
    basis1 = np.array([[i+1,j+1,k+1] for k in range(3) for j in range(3) for i in range(3)]).astype(np.double)
    basis1 = basis1+1e-4*randn(27,3) # perturbation
    basis2 = basis1 @rotate_mat(np.deg2rad(angle))
    #voxel1 = voxel1*basis1
    #voxel2 = voxel2*basis2

    #print(angle,(np.abs(pca(voxel1) - pca(voxel2) )))
    #gg= np.abs(standardize(basis1) - standardize(basis2) )
    gg= np.abs(pca(voxel1, basis1) - pca(voxel1, basis2) )
    ss= np.sum(np.ravel(gg))
    bl= np.all(gg&lt;1e-4) 
           
    print(angle,ss,  bl)
    #res.append(np.all(np.abs(pca(voxel1) - pca(voxel2) &lt; 1e-6)))
    del basis1, basis2

The results are good up to 58 degree angle (yet we're still experimenting with rotation of x, y axes). After that we have constant difference which indicates some uncounted sign reverse. This is better than the less consistent result of rotate_func:
0.0 0.0 True
1.0 1.1103280567106161e-13 True
2.0 5.150139890290964e-14 True
3.0 8.977126225544196e-14 True
4.0 5.57341699240722e-14 True
5.0 4.205149954378956e-14 True
6.0 3.7435437643664957e-14 True
7.0 1.2943967187158123e-13 True
8.0 5.400185371573149e-14 True
9.0 8.006410204958181e-14 True
10.0 7.777189536904011e-14 True
11.0 5.992073021576436e-14 True
12.0 6.3716122222085e-14 True
13.0 1.0120048110065158e-13 True
14.0 1.4193029076233626e-13 True
15.0 5.32774440341853e-14 True
16.0 4.056702432878251e-14 True
17.0 6.52062429116855e-14 True
18.0 1.3237663595853556e-13 True
19.0 8.950259695710006e-14 True
20.0 1.3795067925438317e-13 True
21.0 7.498727794307339e-14 True
22.0 8.570866862371226e-14 True
23.0 8.961510590826412e-14 True
24.0 1.1839169916779899e-13 True
25.0 1.422193407555868e-13 True
26.0 6.578778015788652e-14 True
27.0 1.0042963537887101e-13 True
28.0 8.438153062569065e-14 True
29.0 1.1299103064863272e-13 True
30.0 8.192453876745831e-14 True
31.0 1.2618492405483406e-13 True
32.0 4.9237819394886296e-14 True
33.0 1.0971028569666842e-13 True
34.0 1.332138304559801e-13 True
35.0 5.280024600049296e-14 True

From the code above, you can see that we tried to use another basis: vector with the biggest norm, vector with the smallest and their cross product. Here we should have only two variants (direction of the cross product) which could be later fixed, but I couldn't manage this alternative solution to work.
I hope that someone can help me finish this and obtain rotation-invariant representation for voxels.

EDIT 3. Thank you very much meTchaikovsky, but the situation is still unclear. My problem initially lies in processing 3d voxels which are (3,3,3) numpy arrays. We reached the conclusion that for finding invariant representation, we just need to fix 3d voxel as weights used for calculating cov matrix, and apply rotations on the centered ""basis"" (some vectors used for describing point cloud).
Therefore, when we achieved invariance to ""basis"" rotations, the problem should have been solved: now, when we fix ""basis"" and use rotated voxel, the result must be invariant. Surprisingly, this is not so. Here I check 24 rotations of the cube with basis2=basis1 (except small perturbation):
import scipy.ndimage

def pca(feat, x):

    # pca with attemt to create convention on sign changes
    x_c = x - np.mean(x,axis=0)
    x_f = feat * x
    x_f -= np.mean(x_f,axis=0)
    cov = np.cov(x_f.T)
    e_values, e_vectors = np.linalg.eig(cov)
    order = np.argsort(e_values)[::-1]
    v = e_vectors[:,order]

    # here is the solution, we switch the sign of the projections
    # so that the projection with the largest absolute value along a principal axis is positive
    proj = x_c @ v
    asign = np.sign(proj)
    max_ind = np.argmax(np.abs(proj),axis=0)[None,:]
    sign = np.take_along_axis(asign,max_ind,axis=0)
    proj = proj * sign

    return proj

def rotate_3d(image1, alpha, beta, gamma):
    # z
    # The rotation angle in degrees.
    image2 = scipy.ndimage.rotate(image1, alpha, mode='nearest', axes=(0, 1), reshape=False)

    # rotate along y-axis
    image3 = scipy.ndimage.rotate(image2, beta, mode='nearest', axes=(0, 2), reshape=False)

    # rotate along x-axis
    image4 = scipy.ndimage.rotate(image3, gamma, mode='nearest', axes=(1, 2), reshape=False)
    return image4



voxel10 = np.random.normal(size=(3,3,3))

angles = [[x,y,z] for x in [-90,0,90] for y in [-90,0,90] for z in [-90,0,90]]
res = []
for angle in angles:

    voxel2 = rotate_3d(voxel10, angle[0], angle[1], angle[2])
    voxel1 = voxel10.reshape(27,1)
    voxel2 = voxel2.reshape(27,1)

    basis1 = np.array([[i+1,j+1,k+1] for k in range(3) for j in range(3) for i in range(3)]).astype(np.double)

    basis1 += 1e-4*np.random.normal(size=(27, 1)) # perturbation
    basis2 = basis1
    original_diff = np.sum(np.abs(basis1-basis2))
    gg= np.abs(pca(voxel1, basis1) - pca(voxel2, basis2))
    ss= np.sum(np.ravel(gg))
    bl= np.all(gg&lt;1e-4)
    print('difference before pca %.3f,' % original_diff, 'difference after pca %.3f' % ss,bl)
    res.append(bl)

    del basis1, basis2

print('correct for %.1f percent of time' % (100*(np.sum(res) / len(res))))


difference before pca 0.000, difference after pca 45.738 False
difference before pca 0.000, difference after pca 12.157 False
difference before pca 0.000, difference after pca 26.257 False
difference before pca 0.000, difference after pca 37.128 False
difference before pca 0.000, difference after pca 52.131 False
difference before pca 0.000, difference after pca 45.436 False
difference before pca 0.000, difference after pca 42.226 False
difference before pca 0.000, difference after pca 18.959 False
difference before pca 0.000, difference after pca 38.888 False
difference before pca 0.000, difference after pca 12.157 False
difference before pca 0.000, difference after pca 26.257 False
difference before pca 0.000, difference after pca 50.613 False
difference before pca 0.000, difference after pca 52.132 False
difference before pca 0.000, difference after pca 0.000 True
difference before pca 0.000, difference after pca 52.299 False

Here basis1=basis2 (hence basis difference before pca=0), and you can see 0 for (0,0,0) rotation. But rotated voxels give different result. In case scipy does something wrong, I've checked the approach with numpy.rot90 with the same result:
rot90 = np.rot90

def rotations24(polycube):
    # imagine shape is pointing in axis 0 (up)

    # 4 rotations about axis 0
    yield from rotations4(polycube, 0)

    # rotate 180 about axis 1, now shape is pointing down in axis 0
    # 4 rotations about axis 0
    yield from rotations4(rot90(polycube, 2, axis=1), 0)

    # rotate 90 or 270 about axis 1, now shape is pointing in axis 2
    # 8 rotations about axis 2
    yield from rotations4(rot90(polycube, axis=1), 2)
    yield from rotations4(rot90(polycube, -1, axis=1), 2)

    # rotate about axis 2, now shape is pointing in axis 1
    # 8 rotations about axis 1
    yield from rotations4(rot90(polycube, axis=2), 1)
    yield from rotations4(rot90(polycube, -1, axis=2), 1)

def rotations4(polycube, axis):
    """"""List the four rotations of the given cube about the given axis.""""""
    for i in range(4):
        yield rot90(polycube, i, axis)



def rot90(m, k=1, axis=2):
    """"""Rotate an array k*90 degrees in the counter-clockwise direction around the given axis""""""
    m = np.swapaxes(m, 2, axis)
    m = np.rot90(m, k)
    m = np.swapaxes(m, 2, axis)
    return m


voxel10 = np.random.normal(size=(3,3,3))

gen = rotations24(voxel10)

res = []
for voxel2 in gen:

    #voxel2 = rotate_3d(voxel10, angle[0], angle[1], angle[2])
    voxel1 = voxel10.reshape(27,1)
    voxel2 = voxel2.reshape(27,1)

    basis1 = np.array([[i+1,j+1,k+1] for k in range(3) for j in range(3) for i in range(3)]).astype(np.double)

    basis1 += 1e-4*np.random.normal(size=(27, 1)) # perturbation
    basis2 = basis1

    original_diff = np.sum(np.abs(basis1-basis2))
    gg= np.abs(pca(voxel1, basis1) - pca(voxel2, basis2))
    ss= np.sum(np.ravel(gg))
    bl= np.all(gg&lt;1e-4)
    print('difference before pca %.3f,' % original_diff, 'difference after pca %.3f' % ss,bl)
    res.append(bl)

    del basis1, basis2

print('correct for %.1f percent of time' % (100*(np.sum(res) / len(res))))

I tried to investigate this case, and the only perhaps irrelevant thing I found the following:
voxel1 = np.ones((3,3,3))
voxel1[0,0,0] = 0 # if I change 0 to 0.5 it stops working at all

# mirrored around diagonal
voxel2 = np.ones((3,3,3))
voxel2[2,2,2] = 0

for angle in range(1):

    voxel1 = voxel1.reshape(27,1)
    voxel2 = voxel2.reshape(27,1) 

    basis1 = np.array([[i+1,j+1,k+1] for k in range(3) for j in range(3) for i in range(3)]).astype(np.double)

    basis1 = basis1 + 1e-4 * randn(27,3) # perturbation
    basis2 = basis1

# If perturbation is used we have 

# difference before pca 0.000, difference after pca 0.000 True
# correct for 100.0 percent of time

# eigenvalues for both voxels
# [1.03417495 0.69231107 0.69235402]
# [0.99995368 0.69231107 0.69235402]


# If no perturbation applied for basis, difference is present

# difference before pca 0.000, difference after pca 55.218 False
# correct for 0.0 percent of time

# eignevalues for both voxels (always have 1.):
# [0.69230769 1.03418803 0.69230769]
# [1.         0.69230769 0.69230769]




Currently don't know how to proceed from there.

EDIT4:
I'm currently thinking that there is some problem with voxel rotations transformed into basis coefficients via voxel.reshape()
Simple experiment with creating array of indices
indices = np.arange(27)
indices3d = indices.reshape((3,3,3))
voxel10 = np.random.normal(size=(3,3,3))
assert voxel10[0,1,2] == voxel10.ravel()[indices3d[0,1,2]]

And then using it for rotations
gen = rotations24(indices3d)

res = []
for ind2 in gen:

    basis1 = np.array([[i+1,j+1,k+1] for k in range(3) for j in range(3) for i in range(3)]).astype(np.double)
    voxel1 = voxel10.copy().reshape(27,1) #np.array([voxel10[i,j,k] for k in range(3) for j in range(3) for i in range(3)])[...,np.newaxis]

    voxel2 = voxel1[ind2.reshape(27,)]

    basis1 += 1e-4*np.random.normal(size=(27, 1)) # perturbation
    basis2 = basis1[ind2.reshape(27,)]

    original_diff = np.sum(np.abs(basis1-basis2))
    gg= np.abs(pca(voxel1, basis1) - pca(voxel2, basis2))
    ss= np.sum(np.ravel(gg))
    bl= np.all(gg&lt;1e-4)
    print('difference before pca %.3f,' % original_diff, 'difference after pca %.3f' % ss,bl)
    res.append(bl)

    del basis1, basis2

print('correct for %.1f percent of time' % (100*(np.sum(res) / len(res))))

Shows that those rotations are not correct, because on my opinion rotated voxel and basis should match:
difference before pca 0.000, difference after pca 0.000 True
difference before pca 48.006, difference after pca 87.459 False
difference before pca 72.004, difference after pca 70.644 False
difference before pca 48.003, difference after pca 71.930 False
difference before pca 72.004, difference after pca 79.409 False
difference before pca 84.005, difference after pca 36.177 False


EDIT 5: Okaaay, so here we go at least for 24 rotations. At first, we had a slight change of logic lurked into our pca function. Here we center x_c (basis) and forget about it, further centering x_f (features*basis) and transforming it with pca. This does not work perhaps because our basis is not centered and multiplication by features further increased the bias. If we center x_c first, and multiply it by features, everything will be Ok. Also, previously we had proj = x_c @ v with v computed from x_f which was totally wrong in this case, as x_f and x_c were centered around different centers.
def pca(feat, x):
    
    # pca with attemt to create convention on sign changes
    x_c = x - np.mean(x,axis=0)
    x_f = feat * x
    x_f -= np.mean(x_f,axis=0)
    cov = np.cov(x_f.T)
    e_values, e_vectors = np.linalg.eig(cov)
    order = np.argsort(e_values)[::-1]
    v = e_vectors[:,order]
    
    # here is the solution, we switch the sign of the projections
    # so that the projection with the largest absolute value along a principal axis is positive
    proj = x_f @ v
    
    
    return proj

Secondly, as we already found, we need to sort vectors obtained by pca, for example by the first column:
    basis2 = basis1

    original_diff = np.sum(np.abs(basis1-basis2))

    a = pca(voxel1, basis1)
    t1 = a[a[:,0].argsort()]

    a = pca(voxel2, basis2)
    t2 = a[a[:,0].argsort()]

    gg= np.abs(t1-t2)

And the last thing we also discovered already, is that simple reshape is wrong for voxel, it must correspond to rotation:
voxel2 = voxel1[ind2.reshape(27,)] #np.take(voxel10, ind2).reshape(27,1).
One more important comment to understand the solution. When we perform PCA on the 3d vectors (point cloud, defined by our basis) with weights assigned (analogously to the inertia of the rigid body), the actual assignment of the weights to the points is sort of external information, which becomes hard-defined for the algorithm. When we rotated basis by applying rotation matrices, we did not change the order of the vectors in the array, hence the order of the mass assignments wasn't changed too. When we start to rotate voxel, we change the order of the masses, so in general PCA algorithm will not work without the same transformation applied to the basis. So, only if we have some array of 3d vectors, transformed by some rotation AND the list of masses re-arranged accordingly, we can detect the rotation of the rigid body using PCA. Otherwise, if we detach masses from points, that would be another body in general.
So how does it work for us then? It works because our points are fully symmetric around the center after centering basis. In this case reassignment of the masses does not change ""the body"" because vector norms are the same. In this case we can use the same (numerically) basis2=basis1 for testing 24 rotations and rotated voxel2 (rotated point cloud cubes match, just masses migrate). This correspond to the rotation of the point cloud with mass points around the center of the cube. PCA will transform vectors with the same lengths and different masses in the same way according to the body's ""inertia"" then (after we reached convention on the signs of the components). The only thing left is to sort the pca transformed vectors in the end, because they have different position in the array (because our body was rotated, mass points changed their positions). This makes us lose some information related to the order of the vectors but it looks inevitable.
Here is the code which checks the solution for 24 rotations. If should theoretically work in the general case as well, giving some closer values for more complicated objects rotated inside a bigger voxel:
import numpy as np
from numpy.random import randn

#np.random.seed(20)

def pca(feat, x):
    # pca with attemt to create convention on sign changes
    x_c = x - np.mean(x,axis=0)
    x_f = feat * x_c
    cov = np.cov(x_f.T)
    e_values, e_vectors = np.linalg.eig(cov)
    order = np.argsort(e_values)[::-1]
    v = e_vectors[:,order]
    # here is the solution, we switch the sign of the projections
    # so that the projection with the largest absolute value along a principal axis is positive
    proj = x_f @ v
    asign = np.sign(proj)
    max_ind = np.argmax(np.abs(proj),axis=0)[None,:]
    sign = np.take_along_axis(asign,max_ind,axis=0)
    proj = proj * sign
    return proj


# must be correct https://stackoverflow.com/questions/15230179/how-to-get-the-linear-index-for-a-numpy-array-sub2ind
indices = np.arange(27)
indices3d = indices.reshape((3,3,3))
voxel10 = np.random.normal(size=(3,3,3))
assert voxel10[0,1,2] == voxel10.ravel()[indices3d[0,1,2]]

rot90 = np.rot90

def rotations24(polycube):
    # imagine shape is pointing in axis 0 (up)

    # 4 rotations about axis 0
    yield from rotations4(polycube, 0)

    # rotate 180 about axis 1, now shape is pointing down in axis 0
    # 4 rotations about axis 0
    yield from rotations4(rot90(polycube, 2, axis=1), 0)

    # rotate 90 or 270 about axis 1, now shape is pointing in axis 2
    # 8 rotations about axis 2
    yield from rotations4(rot90(polycube, axis=1), 2)
    yield from rotations4(rot90(polycube, -1, axis=1), 2)

    # rotate about axis 2, now shape is pointing in axis 1
    # 8 rotations about axis 1
    yield from rotations4(rot90(polycube, axis=2), 1)
    yield from rotations4(rot90(polycube, -1, axis=2), 1)

def rotations4(polycube, axis):
    """"""List the four rotations of the given cube about the given axis.""""""
    for i in range(4):
        yield rot90(polycube, i, axis)



def rot90(m, k=1, axis=2):
    """"""Rotate an array k*90 degrees in the counter-clockwise direction around the given axis""""""
    m = np.swapaxes(m, 2, axis)
    m = np.rot90(m, k)
    m = np.swapaxes(m, 2, axis)
    return m


gen = rotations24(indices3d)

res = []

for ind2 in gen:

    basis1 = np.array([[i+1,j+1,k+1] for k in range(3) for j in range(3) for i in range(3)]).astype(np.double)
    voxel1 = voxel10.copy().reshape(27,1)

    voxel2 = voxel1[ind2.reshape(27,)] #np.take(voxel10, ind2).reshape(27,1)

    basis1 += 1e-6*np.random.normal(size=(27, 1)) # perturbation
    basis2 = basis1

    original_diff = np.sum(np.abs(basis1-basis2))
    a = pca(voxel1, basis1)
    t1 = a[a[:,0].argsort()]
    a = pca(voxel2, basis2)
    t2 = a[a[:,0].argsort()]
    gg= np.abs(t1-t2)
    ss= np.sum(np.ravel(gg))
    bl= np.all(gg&lt;1e-4)
    print('difference before pca %.3f,' % original_diff, 'difference after pca %.3f' % ss,bl)
    res.append(bl)

    del basis1, basis2

print('correct for %.1f percent of time' % (100*(np.sum(res) / len(res))))

difference before pca 0.000, difference after pca 0.000 True
difference before pca 0.000, difference after pca 0.000 True
difference before pca 0.000, difference after pca 0.000 True
difference before pca 0.000, difference after pca 0.000 True


PS. I want to propose better ordering theme to take into account zero values in the voxel which might confuse previous approach when entire first column of PCA vectors is zero, etc. I propose to sort by vector norms, multiplied by the sign of the sum of elements. Here is tensorflow 2 code:

def infer_shape(x):
    x = tf.convert_to_tensor(x)

    # If unknown rank, return dynamic shape
    if x.shape.dims is None:
        return tf.shape(x)

    static_shape = x.shape.as_list()
    dynamic_shape = tf.shape(x)

    ret = []
    for i in range(len(static_shape)):
        dim = static_shape[i]
        if dim is None:
            dim = dynamic_shape[i]
        ret.append(dim)

    return ret

def merge_last_two_dims(tensor):
    shape = infer_shape(tensor)
    shape[-2] *= shape[-1]
    #shape.pop(1)
    shape = shape[:-1]
    return tf.reshape(tensor, shape)


def pca(inpt_voxel):
        patches = tf.extract_volume_patches(inpt_voxel, ksizes=[1,3,3,3,1], strides=[1, 1,1,1, 1], padding=""VALID"")
        features0 = patches[...,tf.newaxis]*basis
        # centered basises
        basis1_ = tf.ones(shape=tf.shape(patches[...,tf.newaxis]), dtype=tf.float32)*basis
        basis1 = basis1_ - tf.math.divide_no_nan(tf.reduce_sum(features0, axis=-2), tf.reduce_sum(patches, axis=-1)[...,None])[:,:,:,:,None,:]
        features = patches[...,tf.newaxis]*basis1
        features_centered_basis = features - tf.reduce_mean(features, axis=-2)[:,:,:,:,None,:]
        x = features_centered_basis
        m = tf.cast(x.get_shape()[-2], tf.float32)
        cov = tf.matmul(x,x,transpose_a=True)/(m - 1)
        e,v = tf.linalg.eigh(cov,name=""eigh"")
        proj = tf.matmul(x,v,transpose_b=False)
        asign = tf.sign(proj)
        max_ind = tf.argmax(tf.abs(proj),axis=-2)[:,:,:,:,None,:]
        sign = tf.gather(asign,indices=max_ind, batch_dims=4, axis=-2)
        sign = tf.linalg.diag_part(sign)
        proj = proj * sign
        # But we can have 1st coordinate zero. In this case,
        # other coordinates become ambiguous
        #s = tf.argsort(proj[...,0], axis=-1)
        # sort by l2 vector norms, multiplied by signs of sums
        sum_signs = tf.sign(tf.reduce_sum(proj, axis=-1))
        norms = tf.norm(proj, axis=-1)
        s = tf.argsort(sum_signs*norms, axis=-1)
        proj = tf.gather(proj, s, batch_dims=4, axis=-2)
        return merge_last_two_dims(proj)

","Firstly, your pca function is not correct, it should be
def pca(x):
    
    x -= np.mean(x,axis=0)
    cov = np.cov(x.T)
    e_values, e_vectors = np.linalg.eig(cov)

    order = np.argsort(e_values)[::-1]
    v = e_vectors[:,order]
    
    return x @ v

You shouldn't transpose the e_vectors[:,order] because we want each column of the v array is an eigenvector, therefore, x @ v will be projections of x on those eigenvectors.
Secondly, I think you misunderstand the meaning of rotation. It is not voxel1 that should be rotated, but the basis1. If you rotate (by taking transposition) voxel1, what you really do is to rearrange the indices of grid points, while the coordinates of the points basis1 are not changed.
In order to rotate the points (around the z axis for example), you can first define a function to calculate the rotation matrix given an angle
rotate_mat = lambda theta: np.array([[np.cos(theta),-np.sin(theta),0.],[np.sin(theta),np.cos(theta),0.],[0.,0.,1.]])

with the rotation matrix generated by this function, you can rotate the array basis1 to create another array basis2
basis2 = basis1 @ rotate_mat(np.deg2rad(angle))

Now it comes to the title of your question ""Why my PCA is not invariant to rotation and axis swap?"", from this post, the PCA result is not unique, you can actually run a test to see this
import numpy as np

np.random.seed(10)

rotate_mat = lambda theta: np.array([[np.cos(theta),-np.sin(theta),0.],[np.sin(theta),np.cos(theta),0.],[0.,0.,1.]])

def pca(x):
    
    x -= np.mean(x,axis=0)
    cov = np.cov(x.T)
    e_values, e_vectors = np.linalg.eig(cov)

    order = np.argsort(e_values)[::-1]
    v = e_vectors[:,order]
    return x @ v


angles = np.linspace(0,90,91)
    
res = []
for angle in angles:

    voxel1 = np.random.normal(size=(3,3,3))
    voxel2 = voxel1.copy()
    voxel1 = voxel1.reshape(27,1)
    voxel2 = voxel2.reshape(27,1)
    
    basis1 = np.array([[i+1,j+1,k+1] for k in range(3) for j in range(3) for i in range(3)])
    # basis2 = np.hstack((-basis1[:,1][:,None],basis1[:,0][:,None],-basis1[:,2][:,None]))
    basis2 = basis1 @ rotate_mat(np.deg2rad(angle))
    voxel1 = voxel1*basis1
    voxel2 = voxel2*basis2

    print(angle,np.all(np.abs(pca(voxel1) - pca(voxel2) &lt; 1e-6)))
    res.append(np.all(np.abs(pca(voxel1) - pca(voxel2) &lt; 1e-6)))
    
print()
print(np.sum(res) / len(angles))

After you run this script, you will see that in only 21% of times the two PCA results are the same.

UPDATE
I think instead of focusing on the eigenvectors of the principal components, you can instead focus on the projections. For two clouds of points, even though they are essentially the same, the eigenvectors can be drastically different. Therefore, hardcoding in order to somehow let the two sets of eigenvectors to be the same is a very difficult task.
However, based on this post, for the same cloud of points, two sets of eigenvectors can be different only up to a minus sign. Therefore, the projections upon the two sets of eigenvectors are also different only up to a minus sign. This actually offers us an elegant solution, for the projections along an eigenvector (principal axis), all we need to do is to switch the sign of the projections so that the projection with the largest absolute value along that principal axis is positive.
import numpy as np
from numpy.random import randn

#np.random.seed(20)

rotmat_z = lambda theta: np.array([[np.cos(theta),-np.sin(theta),0.],[np.sin(theta),np.cos(theta),0.],[0.,0.,1.]])
rotmat_y = lambda theta: np.array([[np.cos(theta),0.,np.sin(theta)],[0.,1.,0.],[-np.sin(theta),0.,np.cos(theta)]])
rotmat_x = lambda theta: np.array([[1.,0.,0.],[0.,np.cos(theta),-np.sin(theta)],[0.,np.sin(theta),np.cos(theta)]])
# based on https://en.wikipedia.org/wiki/Rotation_matrix
rot_mat = lambda alpha,beta,gamma: rotmat_z(alpha) @ rotmat_y(beta) @ rotmat_x(gamma)

deg2rad = lambda alpha,beta,gamma: [np.deg2rad(alpha),np.deg2rad(beta),np.deg2rad(gamma)]

def pca(feat, x):
    
    # pca with attemt to create convention on sign changes
    x_c = x - np.mean(x,axis=0)
    x_f = feat * x
    x_f -= np.mean(x_f,axis=0)
    cov = np.cov(x_f.T)
    e_values, e_vectors = np.linalg.eig(cov)
    order = np.argsort(e_values)[::-1]
    v = e_vectors[:,order]
    
    # here is the solution, we switch the sign of the projections
    # so that the projection with the largest absolute value along a principal axis is positive
    proj = x_f @ v
    asign = np.sign(proj)
    max_ind = np.argmax(np.abs(proj),axis=0)[None,:]
    sign = np.take_along_axis(asign,max_ind,axis=0)
    proj = proj * sign
    
    return proj

ref_angles = np.linspace(0.0,90.0,10)
angles = [[alpha,beta,gamma] for alpha in ref_angles for beta in ref_angles for gamma in ref_angles]


voxel1 = np.random.normal(size=(3,3,3))
res = []
for angle in angles:

    voxel2 = voxel1.copy()
    voxel1 = voxel1.reshape(27,1)
    voxel2 = voxel2.reshape(27,1)

    basis1 = np.array([[i+1,j+1,k+1] for k in range(3) for j in range(3) for i in range(3)]).astype(np.double)
    basis1 = basis1 + 1e-4 * randn(27,3) # perturbation
    basis2 = basis1 @ rot_mat(*deg2rad(*angle))
   
    original_diff = np.sum(np.abs(basis1-basis2))
    gg= np.abs(pca(voxel1, basis1) - pca(voxel1, basis2))
    ss= np.sum(np.ravel(gg))
    bl= np.all(gg&lt;1e-4)
    print('difference before pca %.3f,' % original_diff, 'difference after pca %.3f' % ss,bl)
    res.append(bl)

    del basis1, basis2

print('correct for %.1f percent of time' % (100*(np.sum(res) / len(res))))

As you can see by running this script, the projections on the principal axis are the same, this means we have resolved the issue of PCA results being not unique.

Reply to EDIT 3
As for the new issue you raised, I think you missed an important point, it is the projections of the cloud of points onto the principal axes that are invariant, not anything else. Therefore, if you rotate voxel1 and obtain voxel2, they are the same in the sense that their own respective projections onto the principal axes of the cloud of points are the same, it actually does not make too much sense to compare pca(voxel1,basis1) with pca(voxel2,basis1).
Furthermore, the method rotate of scipy.ndimage actually changes information, as you can see by running this script
image1 = np.linspace(1,100,100).reshape(10,10)
image2 = scipy.ndimage.rotate(image1, 45, mode='nearest', axes=(0, 1), reshape=False)
image3 = scipy.ndimage.rotate(image2, -45, mode='nearest', axes=(0, 1), reshape=False)

fig,ax = plt.subplots(nrows=1,ncols=3,figsize=(12,4))
ax[0].imshow(image1)
ax[1].imshow(image2)
ax[2].imshow(image3)

The output image is
As you can see the matrix after rotation is not the same as the original one, some information of the original matrix is changed.


Reply to EDIT 4
Actually, we are almost there, the two pca results are different because we are comparing pca components for different points.
indices = np.arange(27)
indices3d = indices.reshape((3,3,3))
# apply rotations to the indices, it is not weights yet
gen = rotations24(indices3d)

# construct the weights
voxel10 = np.random.normal(size=(3,3,3))

res = []
count = 0
for ind2 in gen:
    count += 1
    # ind2 is the array of indices after rotation
    # reindex the weights with the indices after rotation 
    voxel1 = voxel10.copy().reshape(27,1) 
    voxel2 = voxel1[ind2.reshape(27,)]

    # basis1 is the array of coordinates where the points are
    basis1 = np.array([[i+1,j+1,k+1] for k in range(3) for j in range(3) for i in range(3)]).astype(np.double)
    basis1 += 1e-4*np.random.normal(size=(27, 1))
    # reindex the coordinates with the indices after rotation
    basis2 = basis1[ind2.reshape(27,)]

    # add a slight modification to pca, return the axes also 
    pca1,v1 = pca(voxel1,basis1)
    pca2,v2 = pca(voxel2,basis2)
    # sort the principal components before comparing them 
    pca1 = np.sort(pca1,axis=0)
    pca2 = np.sort(pca2,axis=0)
    
    gg= np.abs(pca1 - pca2)
    ss= np.sum(np.ravel(gg))
    bl= np.all(gg&lt;1e-4)
    print('difference after pca %.3f' % ss,bl)
    res.append(bl)
    
    del basis1, basis2

print('correct for %.1f percent of time' % (100*(np.sum(res) / len(res))))

Running this script, you will find, for each rotation, the two sets of principal axes are different only up to a minus sign. The two sets of pca results are different because the indices of the cloud of points before and after rotation are different (since you apply rotation to the indices). If you sort the pca results before comparing them, you will find the two pca results are exactly the same.

Summary
The answer to this question can be divided into two parts. In the first part, the rotation is applied to the basis (the coordinates of points), while the indices and the corresponding weights are unchanged. In the second part, the rotation is applied to the indices, then the weights and the basis are rearranged with the new indices. For both of the two parts, the solution pca function is the same
def pca(feat, x):

    # pca with attemt to create convention on sign changes
    x_c = x - np.mean(x,axis=0)
    x_f = feat * x
    x_f -= np.mean(x_f,axis=0)
    cov = np.cov(x_f.T)
    e_values, e_vectors = np.linalg.eig(cov)
    order = np.argsort(e_values)[::-1]
    v = e_vectors[:,order]

    # here is the solution, we switch the sign of the projections
    # so that the projection with the largest absolute value along a principal axis is positive
    proj = x_f @ v
    asign = np.sign(proj)
    max_ind = np.argmax(np.abs(proj),axis=0)[None,:]
    sign = np.take_along_axis(asign,max_ind,axis=0)
    proj = proj * sign

    return proj

The idea of this function is, instead of matching the principal axes, we can match the principal components since it is the principal components that are rotationally invariant after all.
Based on this function pca, the first part of this answer is easy to understand, since the indices of the points are unchanged while we only rotate the basis. In order to understand the second part of this answer (Reply to EDIT 5), we must first understand the function rotations24. This function rotates the indices rather than the coordinates of the points, therefore, if we stay at the same position observing the points, we will feel that the positions of the points are changed.

With this in mind, it is not hard to understand Reply to EDIT 5.
Actually, the function pca in this answer can be applied to more general cases, for example (we rotate the indices)
num_of_points_per_dim = 10
num_of_points = num_of_points_per_dim ** 3

indices = np.arange(num_of_points)
indices3d = indices.reshape((num_of_points_per_dim,num_of_points_per_dim,num_of_points_per_dim))
voxel10 = 100*np.random.normal(size=(num_of_points_per_dim,num_of_points_per_dim,num_of_points_per_dim))

gen = rotations24(indices3d)

res = []
for ind2 in gen:

    voxel1 = voxel10.copy().reshape(num_of_points,1)
    voxel2 = voxel1[ind2.reshape(num_of_points,)]

    basis1 = 100*np.random.rand(num_of_points,3)
    basis2 = basis1[ind2.reshape(num_of_points,)]

    pc1 = np.sort(pca(voxel1, basis1),axis=0)
    pc2 = np.sort(pca(voxel2, basis2),axis=0)
    
    gg= np.abs(pc1-pc2)
    ss= np.sum(np.ravel(gg))
    bl= np.all(gg&lt;1e-4)
    print('difference after pca %.3f' % ss,bl)
    res.append(bl)

    del basis1, basis2

print('correct for %.1f percent of time' % (100*(np.sum(res) / len(res))))

",,,false,
https://stackoverflow.com/questions/23699301,false,The issue does not meet the criteria for deeper analysis as it does not involve an API exhibiting unexpected failures or unpredictable behaviors under specific runtime conditions.,,,,,,,Scipy strange results using curve fitting,,,,,false,
https://stackoverflow.com/questions/76439150,false,The issue does not meet the criteria for deeper analysis as it does not involve an API exhibiting unexpected failures or unpredictable behaviors under specific runtime conditions.,,,,,,,Scipy banded eigensolver much slower than standard eigensolver,"I'm observing a strange behaviour concerning the scipy.linalg.eig_banded eigensolver.
I am generating banded matrices of size N=p*f that have a specific structure. The matrices are symmetric tri-block-diagonal with p blocks of size fxf on the main diagonal and p-1 identity matrices of size f*f on the off diagonals.
Example with p=3 and f=3:
 [2 2 2 1 0 0 0 0 0]
 [2 2 2 0 1 0 0 0 0]
 [2 2 2 0 0 1 0 0 0]
 [1 0 0 3 3 3 1 0 0]
 [0 1 0 3 3 3 0 1 0]
 [0 0 1 3 3 3 0 0 1]
 [0 0 0 1 0 0 4 4 4]
 [0 0 0 0 1 0 4 4 4]
 [0 0 0 0 0 1 4 4 4]

Usually these matrices are of size p = 100, f=30, N=p*f=3000 but can easily grow much larger.
Given the structure of these matrices I was hoping that the banded eigensolver in scipy was going to be much faster than the dense eigensolver, however it seems like this is not the case.
I am benchmarking the solvers with the following code:
# Set dimension of problem
f = 50
p = 80
a = 1

print(f""p={p}, f={f}, size={f*p, f*p}"")

print(f""Matrix containing random numbers in {(-a, a)}"")
A = generate_matrix(p, f, -a, a)

# Benchmark standard eigensolver
start = time()
D, Q = linalg.eigh(A)
end = time()

# Test correctness
D = np.diag(D)
print(f""Time for dense solver {end - start}"")
print(f""||AQ - QD|| = {np.linalg.norm(A@Q - Q@D)}"")


# Convert A to banded format
A_banded = banded_format(A, upper = f)

# Benchmark banded eigensolver
start = time()
D, Q = linalg.eig_banded(A_banded)
end = time()

# Test correctness
D = np.diag(D)
print(f""Time for banded solver {end - start}"")
print(f""||AQ - QD|| = {np.linalg.norm(A@Q - Q@D)}"")


The results I get indicate that the banded eigensolver is much slower than the dense one:
p=80, f=50, size=(4000, 4000)
Matrix containing random numbers in (-1, 1)

Time for dense solver 13.475645780563354
||AQ - QD|| = 3.1334336527852233e-12

Time for banded solver 24.427151203155518
||AQ - QD|| = 1.589349711533356e-11

I have already tried storing the matrix in lower diagonal format and passing the overwrite_a_band=True option, but the performance remains the same.
Numpy configuration:
blas_mkl_info:
  NOT AVAILABLE
blis_info:
  NOT AVAILABLE
openblas_info:
    libraries = ['openblas', 'openblas']
    library_dirs = ['/cluster/apps/gcc-8.2.0/openblas-0.2.20-5gatj7a35vypgjekzf3ibbtz54tlbk3m/lib']
    language = c
    define_macros = [('HAVE_CBLAS', None)]
    runtime_library_dirs = ['/cluster/apps/gcc-8.2.0/openblas-0.2.20-5gatj7a35vypgjekzf3ibbtz54tlbk3m/lib']
blas_opt_info:
    libraries = ['openblas', 'openblas']
    library_dirs = ['/cluster/apps/gcc-8.2.0/openblas-0.2.20-5gatj7a35vypgjekzf3ibbtz54tlbk3m/lib']
    language = c
    define_macros = [('HAVE_CBLAS', None)]
    runtime_library_dirs = ['/cluster/apps/gcc-8.2.0/openblas-0.2.20-5gatj7a35vypgjekzf3ibbtz54tlbk3m/lib']
lapack_mkl_info:
  NOT AVAILABLE
openblas_lapack_info:
    libraries = ['openblas', 'openblas']
    library_dirs = ['/cluster/apps/gcc-8.2.0/openblas-0.2.20-5gatj7a35vypgjekzf3ibbtz54tlbk3m/lib']
    language = c
    define_macros = [('HAVE_CBLAS', None)]
    runtime_library_dirs = ['/cluster/apps/gcc-8.2.0/openblas-0.2.20-5gatj7a35vypgjekzf3ibbtz54tlbk3m/lib']
lapack_opt_info:
    libraries = ['openblas', 'openblas']
    library_dirs = ['/cluster/apps/gcc-8.2.0/openblas-0.2.20-5gatj7a35vypgjekzf3ibbtz54tlbk3m/lib']
    language = c
    define_macros = [('HAVE_CBLAS', None)]
    runtime_library_dirs = ['/cluster/apps/gcc-8.2.0/openblas-0.2.20-5gatj7a35vypgjekzf3ibbtz54tlbk3m/lib']

Scipy configuration:
lapack_mkl_info:
  NOT AVAILABLE
openblas_lapack_info:
    libraries = ['openblas', 'openblas']
    library_dirs = ['/cluster/apps/gcc-8.2.0/openblas-0.2.20-5gatj7a35vypgjekzf3ibbtz54tlbk3m/lib']
    language = c
    define_macros = [('HAVE_CBLAS', None)]
    runtime_library_dirs = ['/cluster/apps/gcc-8.2.0/openblas-0.2.20-5gatj7a35vypgjekzf3ibbtz54tlbk3m/lib']
lapack_opt_info:
    libraries = ['openblas', 'openblas']
    library_dirs = ['/cluster/apps/gcc-8.2.0/openblas-0.2.20-5gatj7a35vypgjekzf3ibbtz54tlbk3m/lib']
    language = c
    define_macros = [('HAVE_CBLAS', None)]
    runtime_library_dirs = ['/cluster/apps/gcc-8.2.0/openblas-0.2.20-5gatj7a35vypgjekzf3ibbtz54tlbk3m/lib']
blas_mkl_info:
  NOT AVAILABLE
blis_info:
  NOT AVAILABLE
openblas_info:
    libraries = ['openblas', 'openblas']
    library_dirs = ['/cluster/apps/gcc-8.2.0/openblas-0.2.20-5gatj7a35vypgjekzf3ibbtz54tlbk3m/lib']
    language = c
    define_macros = [('HAVE_CBLAS', None)]
    runtime_library_dirs = ['/cluster/apps/gcc-8.2.0/openblas-0.2.20-5gatj7a35vypgjekzf3ibbtz54tlbk3m/lib']
blas_opt_info:
    libraries = ['openblas', 'openblas']
    library_dirs = ['/cluster/apps/gcc-8.2.0/openblas-0.2.20-5gatj7a35vypgjekzf3ibbtz54tlbk3m/lib']
    language = c
    define_macros = [('HAVE_CBLAS', None)]
    runtime_library_dirs = ['/cluster/apps/gcc-8.2.0/openblas-0.2.20-5gatj7a35vypgjekzf3ibbtz54tlbk3m/lib']

I also tried running the same benchmark on a different cluster using MKL as a backend instead of OpenBLAS and I observed very similar results. Also setting the number of threads with OMP_NUM_THREADS and/or MKL_NUM_THREADS has a very small effect on performance.
Does anyone have any ideas on why this is happening?
Thanks
","I did some digging into the source code of SciPy and the Intel MKL documentation and I have figured out why this is happening.
The scipy eig_banded solver delegates the problem to the LAPACK dsbevd routine which computes all eigenvalues and eigenvectors of a matrix in banded format using a variation of the Cuppen divide and conquer algorithm. This offers an advantage in terms of memory usage because of the banded storage format, but the actual algorithm scales in O(n^3) flops after tridiagonalization with respect to matrix size.
On the other hand, the scipy dense eigensolver delegates the problem to the dsyev routine which for real symmetric matrices calls the dsyevr routine which computes the eigenvalues and eigenvectors using the MRRR algorithm in O(n^2) flops after tridiagonalization.
I am still unsure why there is no MRRR implementation for banded matrix format in MKL.
",,,false,
https://stackoverflow.com/questions/59406477,false,The issue does not meet the criteria for deeper analysis as it does not involve an API exhibiting unexpected failures or unpredictable behaviors under specific runtime conditions.,,,,,,,Correct normalization of discrete power spectral density in python for a real problem,"I am struggling with the correct normalization of the power spectral density (and its inverse).

I am given a real problem, let's say the readings of an accelerometer in the form of the power spectral density (psd) in units of Amplitude^2/Hz. I would like to translate this back into a randomized time series. However, first I want to understand the ""forward"" direction, time series to PSD.

According to [1], the PSD of a time series x(t) can be calculated by:

PSD(w) = 1/T * abs(F(w))^2 = df * abs(F(w))^2


in which T is the sampling time of x(t) and F(w) is the Fourier transform of x(t) and df=1/T is the frequency resolution in the Fourier space. However, the results I am getting are not equal to what I am getting using the scipy Welch method, see code below.

This first block of code is taken from the scipy.welch documentary:

from scipy import signal
import matplotlib.pyplot as plt

fs = 10e3
N = 1e5
amp = 2*np.sqrt(2)
freq = 1234.0
noise_power = 0.001 * fs / 2
time = np.arange(N) / fs
x = amp*np.sin(2*np.pi*freq*time)
x += np.random.normal(scale=np.sqrt(noise_power), size=time.shape)

f, Pxx_den = signal.welch(x, fs, nperseg=1024)
plt.semilogy(f, Pxx_den)
plt.ylim(\[0.5e-3, 1\])
plt.xlabel('frequency \[Hz\]')
plt.ylabel('PSD \[V**2/Hz\]')
plt.show()


First thing I noticed is that the plotted psd changes with the variable fs which seems strange to me. (Maybe I need to adjust the nperseg argument then accordingly? Why is nperseg not set to fs automatically then?)

My code would be the following: (Note that I defined my own fft_full function which already takes care of the correct fourier transform normalization, which I verified by checking Parsevals theorem).

import scipy.fftpack as fftpack

def fft_full(xt,yt):
    dt = xt[1] - xt[0]
    x_fft=fftpack.fftfreq(xt.size,dt)
    y_fft=fftpack.fft(yt)*dt
    return (x_fft,y_fft)

xf,yf=fft_full(time,x)
df=xf[1] - xf[0]
psd=np.abs(yf)**2 *df
plt.figure()
plt.semilogy(xf, psd)
#plt.ylim([0.5e-3, 1])
plt.xlim(0,)
plt.xlabel('frequency [Hz]')
plt.ylabel('PSD [V**2/Hz]')
plt.show()


Unfortunately, I am not yet allowed to post images but the two plots do not look the same!

I would greatly appreciate if someone could explain to me where I went wrong and settle this once and for all :)


  [1]: Eq. 2.82. Random Vibrations in Spacecraft Structures Design
  Theory and Applications, Authors: Wijker, J. Jaap, 2009

","The scipy library uses the Welch's method to estimate a PSD. This method is more complex than just taking the squared modulus of the discrete Fourier transform. In short terms, it proceeds as follows:

Let x be the input discrete signal that contains N samples.

Split x into M overlapping segments, such that each segment sm contains nperseg samples and that each two consecutive segments overlap in noverlap samples, so that nperseg = K * (nperseg - noverlap), where K is an integer (usually K = 2). Note also that:
N = nperseg + (M - 1) * (nperseg - noverlap) = (M + K - 1) * nperseg / K

From each segment sm, subtract its mean (this removes the DC component):
tm = sm - sum(sm) / nperseg

Multiply the elements of the obtained zero-mean segments tm by the elements of a suitable (nonsymmetric) window function, h (such as the Hann window):
um = tm * h

Calculate the Fast Fourier Transform of all vectors um. Before performing these transformations, we usually first append so many zeros to each vector um that its new dimension becomes a power of 2 (the nfft argument of the function welch is used for this purpose). Let us suppose that len(um) = 2p. In most cases, our input vectors are real-valued, so it is best to apply FFT for real data. Its results are then complex-valued vectors vm = rfft(um), such that len(vm) = 2p - 1 + 1.

Calculate the squared modulus of all transformed vectors:
am = abs(vm) ** 2,
or more efficiently:
am = vm.real ** 2 + vm.imag ** 2

Normalize the vectors am as follows:
bm = am / sum(h * h)
bm[1:-1] *= 2 (this takes into account the negative frequencies),
where h is a real vector of the dimension nperseg that contains the window coefficients. In case of the Hann window, we can prove that
sum(h * h) = 3 / 8 * len(h) = 3 / 8 * nperseg

Estimate the PSD as the mean of all vectors bm:
psd = sum(bm) / M
The result is a vector of the dimension len(psd) = 2p - 1 + 1. If we wish that the sum of all psd coefficients matches the mean squared amplitude of the windowed input data (rather than the sum of squared amplitudes), then the vector psd must also be divided by nperseg. However, the scipy routine omits this step. In any case, we usually present psd on the decibel scale, so that the final result is:
psd_dB = 10 * log10(psd).


For a more detailed description, please read the original Welch's paper. See also Wikipedia's page and chapter 13.4 of Numerical Recipes in C
",,,false,
https://stackoverflow.com/questions/44710838,false,The issue does not meet the criteria for deeper analysis as it does not involve an API exhibiting unexpected failures or unpredictable behaviors under specific runtime conditions.,,,,,,,Calling BLAS / LAPACK directly using the SciPy interface and Cython,,,,,false,
https://stackoverflow.com/questions/39196057,false,The issue does not meet the criteria for deeper analysis as it does not involve an API exhibiting unexpected failures or unpredictable behaviors under specific runtime conditions.,,,,,,,Libraries in &quot;virtualenv&quot; much bigger than system libraries,,,,,false,
https://stackoverflow.com/questions/36706163,false,The issue does not meet the criteria for deeper analysis as it does not involve an API exhibiting unexpected failures or unpredictable behaviors under specific runtime conditions.,,,,,,,python multiprocessing module: strange behaviour and processor load when using Pool,,,,,false,
https://stackoverflow.com/questions/71146140,false,The issue is not related to an API exhibiting unexpected failures or unpredictable behaviors under specific runtime conditions. It is about using a trained RNN model without having PyTorch installed.,,,,,,,Using RNN Trained Model without pytorch installed,"I have trained an RNN model with pytorch. I need to use the model for prediction in an environment where I'm unable to install pytorch because of some strange dependency issue with glibc. However, I can install numpy and scipy and other libraries. So, I want to use the trained model, with the network definition, without pytorch.
I have the weights of the model as I save the model with its state dict and weights in the standard way, but I can also save it using just json/pickle files or similar.
I also have the network definition, which depends on pytorch in a number of ways. This is my RNN network definition.
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
import random

torch.manual_seed(1)
random.seed(1)
device = torch.device('cpu')

class RNN(nn.Module):
  def __init__(self, input_size, hidden_size, output_size,num_layers, matching_in_out=False, batch_size=1):
    super(RNN, self).__init__()
    self.input_size = input_size
    self.hidden_size = hidden_size
    self.output_size = output_size
    self.num_layers = num_layers
    self.batch_size = batch_size
    self.matching_in_out = matching_in_out #length of input vector matches the length of output vector 
    self.lstm = nn.LSTM(input_size, hidden_size,num_layers)
    self.hidden2out = nn.Linear(hidden_size, output_size)
    self.hidden = self.init_hidden()
  def forward(self, feature_list):
    feature_list=torch.tensor(feature_list)
    
    if self.matching_in_out:
      lstm_out, _ = self.lstm( feature_list.view(len( feature_list), 1, -1))
      output_space = self.hidden2out(lstm_out.view(len( feature_list), -1))
      output_scores = torch.sigmoid(output_space) #we'll need to check if we need this sigmoid
      return output_scores #output_scores
    else:
      for i in range(len(feature_list)):
        cur_ft_tensor=feature_list[i]#.view([1,1,self.input_size])
        cur_ft_tensor=cur_ft_tensor.view([1,1,self.input_size])
        lstm_out, self.hidden = self.lstm(cur_ft_tensor, self.hidden)
        outs=self.hidden2out(lstm_out)
      return outs
  def init_hidden(self):
    #return torch.rand(self.num_layers, self.batch_size, self.hidden_size)
    return (torch.rand(self.num_layers, self.batch_size, self.hidden_size).to(device),
            torch.rand(self.num_layers, self.batch_size, self.hidden_size).to(device))

I am aware of this question, but I'm willing to go as low level as possible. I can work with numpy array instead of tensors, and reshape instead of view, and I don't need a device setting.
Based on the class definition above, what I can see here is that I only need the following components from torch to get an output from the forward function:

nn.LSTM
nn.Linear
torch.sigmoid

I think I can easily implement the sigmoid function using numpy. However, can I have some implementation for the nn.LSTM and nn.Linear using something not involving pytorch? Also, how will I use the weights from the state dict into the new class?
So, the question is, how can I ""translate"" this RNN definition into a class that doesn't need pytorch, and how to use the state dict weights for it?
Alternatively, is there a ""light"" version of pytorch, that I can use just to run the model and yield a result?
EDIT
I think it might be useful to include the numpy/scipy equivalent for both nn.LSTM and nn.linear. It would help us compare the numpy output to torch output for the same code, and give us some modular code/functions to use. Specifically, a numpy equivalent for the following would be great:
rnn = nn.LSTM(10, 20, 2)
input = torch.randn(5, 3, 10)
h0 = torch.randn(2, 3, 20)
c0 = torch.randn(2, 3, 20)
output, (hn, cn) = rnn(input, (h0, c0))

and also for linear:
m = nn.Linear(20, 30)
input = torch.randn(128, 20)
output = m(input)

","You should try to export the model using torch.onnx. The page gives you an example that you can start with.
An alternative is to use TorchScript, but that requires torch libraries.
Both of these can be run without python. You can load torchscript in a C++ application https://pytorch.org/tutorials/advanced/cpp_export.html
ONNX is much more portable and you can use in languages such as C#, Java, or Javascript
https://onnxruntime.ai/ (even on the browser)
A running example
Just modifying a little your example to go over the errors I found
Notice that via tracing any if/elif/else, for, while will be unrolled
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
import random

torch.manual_seed(1)
random.seed(1)
device = torch.device('cpu')

class RNN(nn.Module):
  def __init__(self, input_size, hidden_size, output_size,num_layers, matching_in_out=False, batch_size=1):
    super(RNN, self).__init__()
    self.input_size = input_size
    self.hidden_size = hidden_size
    self.output_size = output_size
    self.num_layers = num_layers
    self.batch_size = batch_size
    self.matching_in_out = matching_in_out #length of input vector matches the length of output vector 
    self.lstm = nn.LSTM(input_size, hidden_size,num_layers)
    self.hidden2out = nn.Linear(hidden_size, output_size)
  def forward(self, x, h0, c0):
    lstm_out, (hidden_a, hidden_b) = self.lstm(x, (h0, c0))
    outs=self.hidden2out(lstm_out)
    return outs, (hidden_a, hidden_b)
  def init_hidden(self):
    #return torch.rand(self.num_layers, self.batch_size, self.hidden_size)
    return (torch.rand(self.num_layers, self.batch_size, self.hidden_size).to(device).detach(),
            torch.rand(self.num_layers, self.batch_size, self.hidden_size).to(device).detach())

# convert the arguments passed during onnx.export call
class MWrapper(nn.Module):
    def __init__(self, model):
        super(MWrapper, self).__init__()
        self.model = model;
    def forward(self, kwargs):
        return self.model(**kwargs)

Run an example
rnn = RNN(10, 10, 10, 3)
X = torch.randn(3,1,10)
h0,c0  = rnn.init_hidden()
print(rnn(X, h0, c0)[0])

Use the same input to trace the model and export an onnx file

torch.onnx.export(MWrapper(rnn), {'x':X,'h0':h0,'c0':c0}, 'rnn.onnx', 
                  dynamic_axes={'x':{1:'N'},
                               'c0':{1: 'N'},
                               'h0':{1: 'N'}
                               },
                  input_names=['x', 'h0', 'c0'],
                  output_names=['y', 'hn', 'cn']
                 )

Notice that you can use symbolic values for the dimensions of some axes of some inputs. Unspecified dimensions will be fixed with the values from the traced inputs. By default LSTM uses dimension 1 as batch.
Next we load the ONNX model and pass the same inputs
import onnxruntime
ort_model = onnxruntime.InferenceSession('rnn.onnx')
print(ort_model.run(['y'], {'x':X.numpy(), 'c0':c0.numpy(), 'h0':h0.numpy()}))

","Basically implementing it in numpy and copying weights from your pytorch model can do the trick.  For your usecase you will only need to do a forward pass so we just need to implement that only
#Set Parameters for a small LSTM network
input_size  = 2 # size of one 'event', or sample, in our batch of data
hidden_dim  = 3 # 3 cells in the LSTM layer
output_size = 1 # desired model output

num_layers=3
torch_lstm = RNN( input_size, 
                 hidden_dim ,
                 output_size,
                 num_layers,
                 matching_in_out=True
                 )

state = torch_lstm.state_dict() # state will capture the weights of your model

Now for LSTM in numpy these functions will be used:
got the below code from this link: https://towardsdatascience.com/the-lstm-reference-card-6163ca98ae87
### NOT MY CODE
import numpy as np 
from scipy.special import expit as sigmoid

def forget_gate(x, h, Weights_hf, Bias_hf, Weights_xf, Bias_xf, prev_cell_state):
    forget_hidden  = np.dot(Weights_hf, h) + Bias_hf
    forget_eventx  = np.dot(Weights_xf, x) + Bias_xf
    return np.multiply( sigmoid(forget_hidden + forget_eventx), prev_cell_state )

def input_gate(x, h, Weights_hi, Bias_hi, Weights_xi, Bias_xi, Weights_hl, Bias_hl, Weights_xl, Bias_xl):
    ignore_hidden  = np.dot(Weights_hi, h) + Bias_hi
    ignore_eventx  = np.dot(Weights_xi, x) + Bias_xi
    learn_hidden   = np.dot(Weights_hl, h) + Bias_hl
    learn_eventx   = np.dot(Weights_xl, x) + Bias_xl
    return np.multiply( sigmoid(ignore_eventx + ignore_hidden), np.tanh(learn_eventx + learn_hidden) )


def cell_state(forget_gate_output, input_gate_output):
    return forget_gate_output + input_gate_output

  
def output_gate(x, h, Weights_ho, Bias_ho, Weights_xo, Bias_xo, cell_state):
    out_hidden = np.dot(Weights_ho, h) + Bias_ho
    out_eventx = np.dot(Weights_xo, x) + Bias_xo
    return np.multiply( sigmoid(out_eventx + out_hidden), np.tanh(cell_state) )


We would need the sigmoid function as well so
def sigmoid(x):
    return 1/(1 + np.exp(-x))

Because pytorch stores weights in stacked manner so we need to break it up for that we would need the below function
def get_slices(hidden_dim):
    slices=[]
    breaker=(hidden_dim*4)
    slices=[[i,i+3] for i in range(0, breaker, breaker//4)]
    return slices

Now we have the functions ready for lstm, now we create an lstm class to copy the weights from pytorch class and get the output from it.
class numpy_lstm:
    def __init__( self, layer_num=0, hidden_dim=1, matching_in_out=False):
        self.matching_in_out=matching_in_out
        self.layer_num=layer_num
        self.hidden_dim=hidden_dim
        
    def init_weights_from_pytorch(self, state):
        slices=get_slices(self.hidden_dim)
        print (slices)

        #Event (x) Weights and Biases for all gates
        
        lstm_weight_ih='lstm.weight_ih_l'+str(self.layer_num)
        self.Weights_xi = state[lstm_weight_ih][slices[0][0]:slices[0][1]].numpy()  # shape  [h, x]
        self.Weights_xf = state[lstm_weight_ih][slices[1][0]:slices[1][1]].numpy()  # shape  [h, x]
        self.Weights_xl = state[lstm_weight_ih][slices[2][0]:slices[2][1]].numpy()  # shape  [h, x]
        self.Weights_xo = state[lstm_weight_ih][slices[3][0]:slices[3][1]].numpy() # shape  [h, x]

        
        lstm_bias_ih='lstm.bias_ih_l'+str(self.layer_num)
        self.Bias_xi = state[lstm_bias_ih][slices[0][0]:slices[0][1]].numpy()  #shape is [h, 1]
        self.Bias_xf = state[lstm_bias_ih][slices[1][0]:slices[1][1]].numpy()  #shape is [h, 1]
        self.Bias_xl = state[lstm_bias_ih][slices[2][0]:slices[2][1]].numpy()  #shape is [h, 1]
        self.Bias_xo = state[lstm_bias_ih][slices[3][0]:slices[3][1]].numpy() #shape is [h, 1]
        
        
        lstm_weight_hh='lstm.weight_hh_l'+str(self.layer_num)

        #Hidden state (h) Weights and Biases for all gates
        self.Weights_hi = state[lstm_weight_hh][slices[0][0]:slices[0][1]].numpy()  #shape is [h, h]
        self.Weights_hf = state[lstm_weight_hh][slices[1][0]:slices[1][1]].numpy()  #shape is [h, h]
        self.Weights_hl = state[lstm_weight_hh][slices[2][0]:slices[2][1]].numpy()  #shape is [h, h]
        self.Weights_ho = state[lstm_weight_hh][slices[3][0]:slices[3][1]].numpy() #shape is [h, h]
        
        
        lstm_bias_hh='lstm.bias_hh_l'+str(self.layer_num)

        self.Bias_hi = state[lstm_bias_hh][slices[0][0]:slices[0][1]].numpy()  #shape is [h, 1]
        self.Bias_hf = state[lstm_bias_hh][slices[1][0]:slices[1][1]].numpy()  #shape is [h, 1]
        self.Bias_hl = state[lstm_bias_hh][slices[2][0]:slices[2][1]].numpy()  #shape is [h, 1]
        self.Bias_ho = state[lstm_bias_hh][slices[3][0]:slices[3][1]].numpy() #shape is [h, 1]
    def forward_lstm_pass(self,input_data):
        h = np.zeros(self.hidden_dim)
        c = np.zeros(self.hidden_dim)
        
        output_list=[]
        for eventx in input_data:
            f = forget_gate(eventx, h, self.Weights_hf, self.Bias_hf, self.Weights_xf, self.Bias_xf, c)
            i =  input_gate(eventx, h, self.Weights_hi, self.Bias_hi, self.Weights_xi, self.Bias_xi, 
                        self.Weights_hl, self.Bias_hl, self.Weights_xl, self.Bias_xl)
            c = cell_state(f,i)
            h = output_gate(eventx, h, self.Weights_ho, self.Bias_ho, self.Weights_xo, self.Bias_xo, c)
            if self.matching_in_out: # doesnt make sense but it was as it was in main code :(
                output_list.append(h)
        if self.matching_in_out:
            return output_list
        else:
            return h


Similarly for fully connected layer,
    
    
class fully_connected_layer:
    def __init__(self,state, dict_name='fc', ):
        self.fc_Weight = state[dict_name+'.weight'][0].numpy()
        self.fc_Bias = state[dict_name+'.bias'][0].numpy() #shape is [,output_size]
        
    def forward(self,lstm_output, is_sigmoid=True):
        res=np.dot(self.fc_Weight, lstm_output)+self.fc_Bias
        print (res)
        if is_sigmoid:
            return sigmoid(res)
        else:
            return res
        

Now we would need one class to call all of them together and generalise them with respect to multiple layers
You can modify the below class if you need more Fully connected layers or want to set false condition for sigmoid etc.
        
class RNN_model_Numpy:
    def __init__(self, state, input_size, hidden_dim, output_size, num_layers, matching_in_out=True):
        self.lstm_layers=[]
        for i in range(0, num_layers):
            lstm_layer_obj=numpy_lstm(layer_num=i, hidden_dim=hidden_dim, matching_in_out=True)
            lstm_layer_obj.init_weights_from_pytorch(state) 
            self.lstm_layers.append(lstm_layer_obj)
        
        self.hidden2out=fully_connected_layer(state, dict_name='hidden2out')
        
    def forward(self, feature_list):
        for x in self.lstm_layers:
            lstm_output=x.forward_lstm_pass(feature_list)
            feature_list=lstm_output
            
        return self.hidden2out.forward(feature_list, is_sigmoid=False)

Sanity check on a numpy variable:
data = np.array(
           [[1,1],
            [2,2],
            [3,3]])



check=RNN_model_Numpy(state, input_size, hidden_dim, output_size, num_layers)
check.forward(data)

EXPLANATION:
Since we just need forward pass, we would need certain functions that are required in LSTM, for that we have the forget gate, input gate, cell gate and output gate. They are just some operations that are done on the input that you give.
For get_slices function, this is used to break down the weight matrix that we get from pytorch state dictionary (state dictionary) is the dictionary which contains the weights of all the layers that we have in our network.
For LSTM particularly have it in this order  ignore, forget, learn, output. So for that we would need to break it up for different LSTM cells.
For numpy_lstm class, we have init_weights_from_pytorch function which must be called, what it will do is that it will extract the weights from state dictionary which we got earlier from pytorch model object and then populate the numpy array weights with the pytorch weights. You can first train your model and then save the state dictionary through pickle and then use it.
The fully connected layer class just implements the hidden2out neural network.
Finally our rnn_model_numpy class is there to ensure that if you have multiple layers then it is able to send the output of one layer of lstm to other layer of lstm.
Lastly there is a small sanity check on data variable.
IMPORTANT NOTE: PLEASE NOTE THAT YOU MIGHT GET DIMENSION ERROR AS PYTORCH WAY OF HANDLING INPUT IS COMPLETELY DIFFERENT SO PLEASE ENSURE THAT YOU INPUT NUMPY IS OF SIMILAR SHAPE AS DATA VARIABLE.
Important references:
https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html
https://christinakouridi.blog/2019/06/19/backpropagation-lstm/
",,false,
https://stackoverflow.com/questions/54671752,false,The issue is not related to an API exhibiting unexpected failures or unpredictable behaviors under specific runtime conditions. It is about the type of the response variable in the glmnet package for Python.,,,,,,,"Why does glmnet.py throw a TypeError at cvglmnet when I provide floats, if glmnet does require floats?","I am trying to run a glm for logistic regression using Glmnet package for python following Glmnet Vignette and I am having problems with the type of my response variable 'y'.
I want to perform both the glmnet function and the cvglmnet function. The vignette says that ""For binomial logistic regression, the response variable y should be either a factor with two levels, or a two-column matrix of counts or proportions."".
I got my y values from one column of a pandas dataframe. Similarly my input matrix x is from several columns of a pandas dataframe. So my y.values is a numpy.ndarray of shape (290,) made of 1's and 2's, so an array of integers.

To check my types, if I do:

isinstance(y.values,scipy.ndarray)
True
isinstance(y.values,np.ndarray)
True
isinstance(y.values,int)
False
type(y.values[1])
numpy.int64
isinstance(y.values, float)
False


If I run

fit = glmnet(x = fold1_sp.copy(), y = y.values, family = 'binomial')


I get an error 

ValueError: y input must be a scipy float64 ndarray


I found in https://github.com/bbalasub1/glmnet_python/issues/15 that they tried to address this issue mentioning that any search for scipy float 64 ndarrays returns numpy float ndarray results because they are basically the same object type, so I convert to numpy float: 

fit = glmnet(x = fold1_sp.copy(), y = np.float64(y.values), family = 'binomial')


and it runs just fine.
But this way my y array is made of floats 1.'s and 2.'s

If I now try using the cvglmnet function, it complains it needs an integer

cvfit = cvglmnet(x = fold1_sp.copy(), y = np.float64(y.values), family = 'binomial', ptype = 'class')


throwing the following error:

TypeError: 'numpy.float64' object cannot be interpreted as an integer


which is strange at least to me since it asked for a float before, and if I use my y as integers using the original y.values, then it complains again it needs scipy float64.

So how can I overcome this TypeError problem?.

Thanks ever so much
","I was having a similar problem but with X data, and I solve it by putting
cvglmnet(x=X.astype(np.float64), y=y,family = ""gaussian"",alpha=1)

",,,false,
https://stackoverflow.com/questions/24824192,false,The issue does not provide enough information to determine if it involves an API exhibiting unexpected failures or unpredictable behaviors under specific runtime conditions.,,,,,,,Redisplaying plots in Matplotlib in conjunction with PyQt,,,,,false,
https://stackoverflow.com/questions/17753178,false,The issue does not provide enough information to determine if it involves an API exhibiting unexpected failures or unpredictable behaviors under specific runtime conditions.,,,,,,,howto emulate 2-sample t-test in scipy,,,,,false,
https://stackoverflow.com/questions/10283053,false,The issue does not provide enough information to determine if it involves an API exhibiting unexpected failures or unpredictable behaviors under specific runtime conditions.,,,,,,,Downloading a .csv file from the web (with redirects) in python,,,,,false,
https://stackoverflow.com/questions/76787449,true,"The issue involves the lsim command in SciPy, which is expected to behave identically to the Python Control Systems Library. However, the output of the SciPy command seems incorrect compared to the expected behavior.",SciPy,lsim,"The output of the lsim command in SciPy does not match the expected behavior, resulting in incorrect results. This discrepancy is observed when comparing the output of lsim with the Python Control Systems Library.",The lsim command is expected to simulate the output of a continuous-time LTI system and should produce results consistent with the Python Control Systems Library.,"The issue occurs when using the lsim command in SciPy, specifically when comparing its output with the Python Control Systems Library.",This issue might be challenging to detect during development and testing because it requires comparing the output of lsim with the expected behavior from the Python Control Systems Library.,SciPy&#39;s lsim not behaving as expected compared to Python Control Systems Library,"I am doing some control engineering in Python, and I have come accross a very strange discrepency between SciPy and the Python Control Systems Library for the implementation of the lsim command. To the best of my knowledge, these commands should behave identically, but it seems they are not: more particularly, the output of the SciPy command seems like nonsense. I have included a MWE below.
""""""
Minimal working example (MWE) replicating the discrepency between lsim for the SciPy module and control module
""""""

# Import modules
import numpy as np
import scipy.signal as sgnl
import matplotlib.pyplot as plt
import control as ctrl
import control.matlab as ctrl_mtl

# Number of samples
N_k = 100    # Total number of outputs

# Continous-time (unstable) open loop system dynamics
A = np.array([[0, 1],
              [-2, 3]])
B = np.array([[0],
              [1]])
n_x = A.shape[0]
n_u = B.shape[1]

# Initial state
x_0 = np.array([[1], [1]], dtype=float)

# Sampling period
h = 0.5

# Controller (full-state feedback)
K = np.array([[0, -4]])

# Closed-loop (stable) continous-time system
sys_cl_scipy = sgnl.StateSpace(A + B @ K, np.zeros((n_x, 1)), np.eye(n_x), np.zeros((n_x, 1)))
sys_cl_ctrl = ctrl.StateSpace(A + B @ K, np.zeros((n_x, 1)), np.eye(n_x), np.zeros((n_x, 1)))

# Simulate dynamics
# NOTE: The only difference is that for SciPy to work I need to add np.ravel(), as otherwise I get the error
# ""ValueError: could not broadcast input array from shape (2,1) into shape (2,)""
_, _, x_scipy = sgnl.lsim(sys_cl_scipy, None, np.linspace(0, h * (N_k - 1), N_k), X0=np.ravel(x_0), interp=False)
_, _, x_ctrl = ctrl_mtl.lsim(sys_cl_ctrl, T=np.linspace(0, h * (N_k - 1), N_k), X0=x_0)
x_scipy = x_scipy.T
x_ctrl = x_ctrl.T

# Plot the state through state-space (verification)
fig_verify, ax = plt.subplots()
ax.plot(np.ravel(x_scipy[0, :]), np.ravel(x_scipy[1, :]), label=""x SciPy"", color=""green"")
ax.plot(np.ravel(x_ctrl[0, :]), np.ravel(x_ctrl[1, :]), label=""x control"", color=""blue"")
plt.legend()
plt.title(""State trajectory"")

# Display all plots
plt.show()

This gives this as result.

The control implementation works as expected, but the SciPy implementation seems completely wrong. Is there something I am missing, or how should I interpret this? Thanks in advance.
I tried to compare both methods on an identical setup, and they produce different results, whilst they both should simulate the output of a continous-time LTI system.
","You appear to be running into a typing problem (as in the numerical type of the variables, not hitting keys with your fingers 😀). Your array A is an array of integers, and lsim uses the data type of A to determine the data type of the output array.  You can fix this by ensuring that the data type of A is floating point:
A = np.array([[0.0, 1.0],
              [-2.0, 3.0]])

P.S. I'm pretty sure anyone running lsim--which simulates a continuous-time system--would expect the calculation to use floating point values, so I'd call this a bug.  I created an issue for this in the SciPy github repo: https://github.com/scipy/scipy/issues/18982
",,,false,
https://stackoverflow.com/questions/55169294,false,The issue does not involve an API exhibiting unexpected failures or unpredictable behaviors. It is a request for a solution or recommendation for converting AsciiMath or MathML sources to SVG and/or PNG formats.,,,,,,,Convert AsciiMath/MathML to SVG/PNG,"I'm looking for a way to convert my AsciiMath (or MathML) sources to SVG and/or PNG. I've found a nodeJS library for SVG conversion but calling that from Python is not very convenient and the output is not entirely satisfying.

Taking the fact I'd like to render mathematical formulas to svg/png it seems logical to look for a solution in math libraries (NumPy, SciPy, Pandas, Matplotlib, Sympy, etc...) but to no avail. All my google results combining all possible permutations of asciimath+mathml+svg+png lead to nothing which is strange.

Please recommend me either search patterns to find a solution or share your experiences/ideas to get this seemingly simple job done in Python.

All help would be highly appreciated!
","I just created ziamath for exactly this purpose. It comes bundled with the STIX math font, so no setup is required beyond the pip install, but it should also work with other math-enabled fonts. Pure-Python, so it does not need a Latex installation or anything else to work. This first version doesn't quite cover the full MathML specification, but the most useful parts are in there.
To bundle it into an app with something like PyInstaller, you'll need to make sure the STIX font gets included, but that should just be one line in the PyInstaller config.
",,,false,
https://stackoverflow.com/questions/54810650,true,"The issue involves the solve_ivp function in SciPy, specifically when using implicit methods such as 'BDF' or 'Radau'. These methods produce an error, while explicit integrators do not.",SciPy,solve_ivp,"When using implicit methods like 'BDF' or 'Radau' in the solve_ivp function, an error occurs. The error message indicates that there are too many indices for an array, suggesting a problem with the implicit integrators.",The solve_ivp function is expected to numerically solve initial value problems for ordinary differential equations (ODEs) using various integration methods.,The issue is triggered when using implicit integration methods like 'BDF' or 'Radau' in the solve_ivp function.,This issue might be challenging to detect during development and testing because it requires using specific implicit integration methods and comparing the results with explicit integrators.,Strange behaviour in scipy.solve_ivp when using an implicit method,"I recently ran into a question about integration and encountered a strange bug. I attempt a very simple problem using solve_ivp:

from scipy.integrate import solve_ivp
import numpy as np

def f(y, t):
    return y

y0 = [1,1,1,1]
method = 'RK23'
s = solve_ivp(f, (0,1), y0, method=method, t_eval=np.linspace(0,1))


And it works fine. When I change to method='BDF' or method='Radau' I get an error:

Traceback (most recent call last):

  File ""&lt;ipython-input-222-f11c4406e92c&gt;"", line 10, in &lt;module&gt;
    s = solve_ivp(f, (0,1), y0, method=method, t_eval=np.linspace(0,1))

  File ""C:\ProgramData\Anaconda3\lib\site-packages\scipy\integrate\_ivp\ivp.py"", line 455, in solve_ivp
    solver = method(fun, t0, y0, tf, vectorized=vectorized, **options)

  File ""C:\ProgramData\Anaconda3\lib\site-packages\scipy\integrate\_ivp\radau.py"", line 299, in __init__
    self.jac, self.J = self._validate_jac(jac, jac_sparsity)

  File ""C:\ProgramData\Anaconda3\lib\site-packages\scipy\integrate\_ivp\radau.py"", line 345, in _validate_jac
    J = jac_wrapped(t0, y0, self.f)

  File ""C:\ProgramData\Anaconda3\lib\site-packages\scipy\integrate\_ivp\radau.py"", line 343, in jac_wrapped
    sparsity)

  File ""C:\ProgramData\Anaconda3\lib\site-packages\scipy\integrate\_ivp\common.py"", line 307, in num_jac
    return _dense_num_jac(fun, t, y, f, h, factor, y_scale)

  File ""C:\ProgramData\Anaconda3\lib\site-packages\scipy\integrate\_ivp\common.py"", line 318, in _dense_num_jac
    diff = f_new - f[:, None]

IndexError: too many indices for array


I also get an error with method = 'LSODA', although different (i.e. all implicit integrators). I do not get an error with any of the explicit integrators.

I tried this in spyder with scipy version 1.0.0 and in google colab (scipy version 1.1.0), with the same results.

Is this a bug or am I missing some argument I need for implicit integrators??
","It appears that the Radau and BDF methods do not handle single-valued RHS functions. Making the function f above output a 1-D list solves your issue. Additionally, as mentioned by Weckesser in the comments, solve_ivp expects the RHS to be f(t, y) and not f(y, t).
Like this
def f(t, y):
    return [y]

",,,false,
https://stackoverflow.com/questions/47931749,false,The issue does not provide enough information or context to determine if it involves an API exhibiting unexpected failures or unpredictable behaviors. The issue title and body are empty.,,,,,,,Why is Scipy&#39;s percentileofscore returning a different result than Excel&#39;s PERCENTRANK.INC?,,,,,false,
https://stackoverflow.com/questions/37487532,true,"The issue involves the FFT function in SciPy, which produces a strange result with alternating signs in the frequency domain. The issue is related to the input signal and can be resolved by applying a circular shift to the time domain signal.",SciPy,FFT,"The FFT function in SciPy produces a frequency domain representation with alternating signs, which is unexpected. This behavior is caused by a circular shift in the time domain signal and can be resolved by applying an inverse shift.",The FFT function in SciPy is expected to compute the discrete Fourier transform of a signal.,"The issue is triggered when applying the FFT function to a specific input signal, such as a Gaussian pulse.",This issue might be challenging to detect during development and testing because it requires analyzing the frequency domain representation and understanding the circular shift in the time domain signal.,Strange Result from FFT using Scipy,"I'm trying to do an FFT of some data (a Gaussian pulse), but I'm finding a strange result. The real and imaginary components of the resultant FFT alternate in sign every index of the array. The absolute values of the arrays, however, are continuous. So, I get something that looks like this:



Does anybody have an idea on what is causing this? Thanks!
","Alternating signs in the frequency domain corresponds to an exp(j*pi*n) complex factor which by the shift theorem corresponds to a time domain circular shift of N/2 samples. Looking at your time domain Gaussian pulse you should notice that the peak indeed appears at N/2 instead of index 0.
Shifting back your time domain Gaussian pulse with ifftshift should give you a pulse centered at 0 whose frequency domain representation does not have this sign alternation.
",,,false,
https://stackoverflow.com/questions/20061141,false,The issue does not provide enough information or context to determine if it involves an API exhibiting unexpected failures or unpredictable behaviors. The issue title and body are empty.,,,,,,,pymc 3.0 Predictive Posterior Distribution,,,,,false,
https://stackoverflow.com/questions/17527869,false,The issue does not provide enough information or context to determine if it involves an API exhibiting unexpected failures or unpredictable behaviors. The issue title and body are empty.,,,,,,,Curve fit fails with exponential but zunzun gets it right,,,,,false,
https://stackoverflow.com/questions/16929179,false,The issue does not provide enough information or context to determine if it involves an API exhibiting unexpected failures or unpredictable behaviors. The issue title and body are empty.,,,,,,,Strange result with python&#39;s (scipy) curve fitting,,,,,false,
https://stackoverflow.com/questions/74073866,true,"The issue involves the minimize function in SciPy, specifically when using a function with multiple lambda functions as arguments. The issue is related to the way arguments are passed to the function and can be resolved by using an additional lambda to unpack the arguments.",SciPy,minimize,"When using the minimize function in SciPy with a function that contains multiple lambda functions as arguments, a 'TypeError' occurs. The error message indicates that a required positional argument is missing. The issue is related to how arguments are passed to the function.",The minimize function in SciPy is expected to minimize a scalar function of one or more variables using various optimization algorithms.,The issue is triggered when using a function with multiple lambda functions as arguments in the minimize function.,This issue might be challenging to detect during development and testing because it requires understanding the way arguments are passed to the function and using an additional lambda to unpack the arguments.,SciPy Minimize doesn&#39;t pass all guesses on?,"I am trying to minimize a function of two variables using SciPy. The function itself is a chain of multiple lambda functions (makes it complicated but unfortunately it is the easiest way to write the expressions I need).
However, when using SciPy's minimize routine, I get the error ""TypeError: () missing 1 required positional argument: 'labour'""
Strangely enough, if I pass the arguments to the function directly, there is no error, so I assume that my chaining was correct.
Here is a minimum reproducible example:
# Preliminaries 0: import packages
import numpy as np
from scipy import optimize

# Preliminaries 1: Set parameters

alpha = 0.4
gamma = 0.4
delta = 0.05
beta = 0.95

# Preliminaries 2: Define functions

production_f = lambda capital, labour : (capital** alpha) * (labour ** (1-alpha))
utility_f_uni = lambda consumption, labour : np.log(consumption) + gamma * np.log(1-labour) if (consumption &gt; 0 and labour &gt; 0 and labour &lt; 1) else -5000
law_of_motion_f = lambda current_capital, next_capital, labour : production_f(current_capital, labour) - next_capital + (1-delta) * current_capital
utility_f_multi = lambda current_capital, next_capital, labour : utility_f_uni(law_of_motion_f(current_capital, next_capital, labour), labour)

optimization_f = lambda current_capital, next_capital, labour, value_f: utility_f_multi(current_capital, next_capital, labour) + beta * value_f(next_capital)

max_capital = lambda capital : production_f(capital, 1) + (1 - delta) * capital

For those knowledgeable of Dynamic Programming, I am trying to derive the value function of a growth model using Value Function iteration, but I didn't get so far yet. The period payoff is given by utility_f_multi. The Value function guess is given by optimization_f, which takes in four arguments including the previous value function guess.
In my example, I generate an interpolation that is closer to the true value function, but for all intents and purposes the constant 0 function also suffices. I then go on to create the optimization problem given our state variable
initial_value = lambda x : 0
current_optimization_f = lambda next_capital, labour: -optimization_f(3, next_capital, labour, initial_value)

Finally, I pass on the problem to the minimize function, from which the error results:
optimized_problem = optimize.minimize(current_optimization_f, [2,0.2])

The message, as mentioned previously, is ""() missing 1 required positional argument: 'labour'""
However, if I just pass on the two arguments to the function by hand, I receive no issue
print(current_optimization_f(2, 0.3))

which returns a value without problems.
Any help on this issue would be appreciated!
","The function that you pass to scipy.minimize must use a singular argument for all the numerical inputs.
Imaging you wrote your function like:
def current_optimization_f(next_capital, labour):
   return optimization_f(3, next_capital, labour, initial_value)

scipy will call:
current_optimization([2, 0.3])

rather than
current_optimization(2, 0.3)

You can use an additional lambda to unpack the arguments:
obj_func = lambda x: current_optimization(*x)

rtn = optimize.minimize(obj_func, [2, 0.3])

And also, unrelated to your question, you should look at CasADI to solve these types of questions
",,,false,
https://stackoverflow.com/questions/70371720,false,The issue does not provide enough information or context to determine if it involves an API exhibiting unexpected failures or unpredictable behaviors. The issue title and body are empty.,,,,,,,scipy curve_fit strange result matplotlib,"inbefore my code:
def func (x,a,b):
    return a*np.exp(b*x)

xFit= np.arange(0.0, 20, 0.01)
dev_Fluenz1= np.array([68.9, 21.81, 9.38, 3.73])
dev_Fluenz2= np.array([137.68 , 42.34, 18.75, 7.47 ])
dev_Fluenz3= np.array([80.34, 23.82 , 10.06, 3.76 ])
dev_Fluenz4= np.array([203.7, 61.67 , 10.06, 10.33 ])
dev_Fluenz5= np.array([135.74, 46.23 , 19.42 , 11.21 ])


dev_Fluenz6= np.array([382.83, 112.95, 50.02, 14.95])
dev_Fluenz7= np.array([382.45 , 117.62 , 50.01, 14.95 ])
dev_Fluenz8= np.array([147.32 , 43.67 , 17.88 , 5.01 ])
dev_Fluenz9= np.array([282.91, 85.64 , 35.63 , 13.77 ])
dev_Fluenz10= np.array([150.82, 51.37 , 20.5 , 11.21 ])


dev_x2= np.array([2,5,10,20])

plt.plot(dev_x2,dev_Fluenz1, 'bo')
popt, pcov = curve_fit(func, dev_x2, dev_Fluenz1)
plt.plot(xFit, func(xFit,*popt),color='b', linestyle='--',label=f'Reales DOE Sa &lt;= 0,3 Fluenz 1 J/cm**2 \u03bcm F(x) = {round(popt[0])} * e^({round(popt[1])}*x)')

results in : enter image description here
changing xFit= np.arange(0.0, 20, 0.01) to xFit= np.arange(0.0, 200, 0.01)
and
`dev_x2= np.array([2,5,10,20])` to `dev_x2= np.array([20,50,100,200])`

results in enter image description here
Why?
Thank you for your help!
","curve_fit takes an initial guess as a starting point of the fit, see https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.curve_fit.html.
If you do not provide it, as you did not, the default is used:

p0array_like, optional
Initial guess for the parameters (length N). If None, then the initial values will all be 1 (if the number of parameters for the function can be determined using introspection, otherwise a ValueError is raised).

For your second test case that blows up, it matters because with the default initial value of the exponent b in func, for the given range of dev_x2`, you get very large values and the optimizer cannot find a good solution
So you need to provide a sensible starting value. For example, note the p0 = [... bit below
xFit= np.arange(0.0, 200, 0.01)
dev_x2= np.array([20,50,100,200])

plt.plot(dev_x2,dev_Fluenz1, 'bo')
popt, pcov = curve_fit(func, dev_x2, dev_Fluenz1, p0 = [100,  -0.01])
plt.plot(xFit, func(xFit,*popt),color='b', linestyle='--',label=f'Reales DOE Sa &lt;= 0,3 Fluenz 1 J/cm**2 \u03bcm F(x) = {round(popt[0])} * e^({round(popt[1])}*x)')


you get a sensible result:

",,,false,
https://stackoverflow.com/questions/69420714,true,"The issue involves the deconvolve function in SciPy, which produces unexpected results when deconvolving a signal from an impulse response function. The issue is related to the nature of the deconvolution process and can be resolved by using the division of FFTs instead.",SciPy,deconvolve,The deconvolve function in SciPy produces unexpected results when deconvolving a signal from an impulse response function. The output may be affected by noise and the characteristics of the input signals. Using the division of FFTs can provide a more reliable deconvolution result.,"The deconvolve function in SciPy is expected to perform deconvolution, which is the process of reversing the convolution operation between two signals.",The issue is triggered when using the deconvolve function to deconvolve a signal from an impulse response function.,This issue might be challenging to detect during development and testing because it requires understanding the deconvolution process and the impact of noise on the output.,Python deconvolution giving unexpected result,"Below I have plotted the signal (Lifetime decay) I am trying to deconvolve from an impulse response function, i.e. the divider (IRF). So I should just get the decay a bit sharper.
Here is an example of a topic I look at that gives what I need:
Understanding scipy deconvolve

Please not for my code, I am using only the peak of the divider (IRF), not the entire array sequence as shown on the image.
I am using the following code to do that:
IRF = IRF * (max(decay)/max(IRF))
# replace 0s to avoid error message 
IRF = np.where(IRF == 0, 0.1, IRF)    
decay = np.where(decay == 0, 0.1, decay)  
# take only the quotient part of the result 
deconv = scipy.signal.deconvolve(decay, IRF)[0]
# ""padding"" the deconvolved signal so it has the same size as the original signal 
s = int((len(decay)-(len(deconv)))/2)  ## difference on each side 
deconv_res = np.zeros(len(decay))   
end = int(len(decay)-s-1)  # final index
deconv_res[s:end] = deconv
deconv = deconv_res
# convolved normalized to decay height for plotting 
deconv_n = deconv * (max(decay)/max(deconv))   

The IRF is an array of float64, the signal is an array of uint16.
I admit I'm not so familiar with the maths of deconvolution, so I am trying blindly different things, like trying different divider functions, but nothing is producing anywhere near as expected.
The last result I got looks like this (see plot of the original signal and what the signal it tried to deconvolve..)

Could anyone give me some idea if it's something in scipy.deconvolve I don't understand, what the reason could be for this strange behaviour, or even some high-level reading material that might help me out?
Or if you think this problem is more physics-y than coding-related, a better place to ask such a question?
","The problem is that deconvolve is a sort of polynomial division, it decomposes the output signal in $conv(h, x) + r$, if your signal is noisy it may give strange results. Also if the first sample in the inpulse response is small it tends to produce the exponentially growing output.
What I would do for this problem is the division of FFTs.
N = 2**(ceil(log2(len(IRF) + len(decay)))
filtered = ifft(fft(decay, N) / fft(IRF, N))

",,,false,
https://stackoverflow.com/questions/69077658,false,The issue does not meet the criteria for deeper analysis as it does not involve an API exhibiting unexpected failures or unpredictable behaviors under specific runtime conditions.,,,,,,,Doubts on what scipy.optimize.minimize is really doing,"I am trying to minimize a cost function and I got very strange results from scipy.optimize.minimize (with methods and 'SLSQP', 'L-BFGS-B').
I print the value of the cost function after each evaluation. First it performs the small perturbations before going into the supposedly right direction (ok). But then occurs something strange: it seems to change the initial cost function by something like value of the cost function at first evaluation - value of the cost function in the current evaluation and converges towards the value of the first evaluation of the cost function.
To illustrate that I created a toy function of 2 parameters (0.25 + 1000 * x1 ** 2 + 100 * x2 ** 2 + 0.1 * random()). x1 and x2 are restricted to the interval [0, 1] (bounds). X0 is set to (0.5, 0.5). Here is what i get:
cost function: 275.3414617153509 x1: 0.5 x2: 0.5
cost function: 275.34428666473536 x1: 0.5000000149011612 x2: 0.5
cost function: 275.3542128554434 x1: 0.5 x2: 0.5000000149011612
cost function: 0.2665482586461191 x1: 0.0 x2: 0.0
cost function: 68.9989043756609 x1: 0.24986835289808013 x2: 0.24986835289808013
cost function: 154.87646326641064 x1: 0.374835397734792 x2: 0.374835397734792
cost function: 210.70119869030185 x1: 0.4373600232007103 x2: 0.4373600232007103
cost function: 241.8621094503892 x1: 0.4686490613793924 x2: 0.4686490613793924
cost function: 258.36597245010955 x1: 0.4843084999840323 x2: 0.4843084999840323
cost function: 266.6807722679986 x1: 0.4921461216177911 x2: 0.4921461216177911
cost function: 270.96794190195914 x1: 0.49606891372760337 x2: 0.49606891372760337
cost function: 273.0999396362265 x1: 0.49803236262951744 x2: 0.49803236262951744
cost function: 274.23903284113646 x1: 0.4990151079476797 x2: 0.4990151079476797
cost function: 274.7564047455383 x1: 0.4995070260788122 x2: 0.4995070260788122

 fun: 274.7564047455383
 jac: array([189579.1440506 , 855714.52631378])
 message: 'Optimization terminated successfully'
nfev: 14
 nit: 1
njev: 1
status: 0
success: True
x: array([0.49950703, 0.49950703])

So I do not understand:

why the final result is 2.74.756... and not 0.2666
why it starts to converge towards X0

What makes me think that the cost function is ""modified"" (i.e., what it tries to minimize is not the cost function but initial cost function evaluation - current cost function evaluation) is that, sometimes, due the random() part of the toy function, the first guessed evaluation is a higher value than the perturbation evaluations and it also converges towards X0.
I am using Python 3.9.6 and scipy 1.6.1
Edit:
Here is the full code:
def toto(X):
   val  = 0.25 + 1000 * X[0] ** 2 + 100 * X[1] ** 2 + 0.1 * random();
   print(""cost function:"", val, 'x1:', X[0], 'x2:', X[1])
   return val

optimization = minimize(toto, [0.5, 0.5], method=”SLSQP”, bounds= [[0.0, 1.0], [0.0, 1.0]])
print(optimization)

Mathieu
","Trying your code, I get basically the same results.
I can't say I have a full solution to your problem, but I can point out a few issues. One is that scipy.optimize.minimize defaults to using a very small step to compute numerical gradients (e.g. for L-BFGS-B, the default step size eps  equals 1e-8). To see why this is a problem, consider if you computed a numerical derivative from the optimal solution (0,0). The deterministic part of the derivative will be roughly 0, but what will be the stochastic part. It should the difference of the two random values divided by 1e-8. The most likely value for the difference will be 0.05 (based on the difference having a triangular distribution), so your derivative will roughly on the order of 1e6. So while the function doesn't differ much with this random noise, it has a substantial effect on the numerical derivative.
But if the gradients are so large, why is it saying it converged? Both the methods you listed also have an ftol convergence criteria, which causes convergence when the relative change in the function value between steps is below the threshold. SLSQP doesn't provide any description in it's convergence message, but L-BFGS-B at least gives a brief description of why it converged. For the cases where it was far away from (0,0), convergence was related to this ftol criteria. I don't think there is anything in the code specifically drawing it back towards your initial point; rather it just seems like a random step away from this doesn't lead to much of a change in the function value. If I ran the code repeatedly, it would converge to a lot of different solutions and not always come back to near this initial point.
You can't entirely fix this with just a numerical gradient based optimizer, but you can at least improve your result by changing the value of eps. I found that if I changed eps to 1e-4, it tended to converge to (0,0) or near it. Increasing eps doesn't entirely fix, as the gradient can still be significantly altered by random portion.
Other options are discussed in this prior post and include methods for denoising your function before evaluating gradients or evaluating the function across the range, fitting it with splines, and then optimizing the fitting function.
If your interest is in diagnosing the problems with this specific code, someone who knows more about the technical details of the scipy implementation can probably help. However, if your interest is generally about finding minima of noisy functions, I think this example makes clear that a numerical gradient based optimizer won't be sufficient for this purpose.
",,,false,
https://stackoverflow.com/questions/65825332,false,The issue does not meet the criteria for deeper analysis as it is a question about the definition of scipy's coordinate system and does not involve an API exhibiting unexpected failures or unpredictable behaviors under specific runtime conditions.,,,,,,,Scipy Coordinate system,"i cannot find the definition of scipy's coordinate system.
i have tried several values (assuming a right hand system) but got a strange result.
for example:
from scipy.spatial.transform import Rotation as R
R.from_euler('zyx', angles=np.array([90,0,0]), degrees=True).as_matrix()
[ [ 0., -1.,  0.], [ 1.,  0.,  0.],  [ 0.,  0.,  1.]]

meaninig the counterclockwise rotation about the z axis (true for a right hand system) is inverse (meaning a left coordinate system)...
where can i find the definition??
Thanks!!!
","The full documentation for Scipy's Rotation module can be found here. For your problem in particular, I am not sure there actually is a problem. Looking at Wikipedia, a 90-degree rotation is indeed counter-clockwise so that a vector originally aligned with the x-axis becomes aligned with the y-axis. This, I believe, is in agreement with the result of the code below.
from scipy.spatial.transform import Rotation as R

point = (5, 0, -2)
print(R.from_euler('z', angles=90, degrees=True).as_matrix() @ point)
# [0, 5, -2]

","In short, I think giving positive angle means negative rotation about the axis, since it makes sense with the result.
Normally, positive direction of rotation about z-axis is rotating from x-axis to y-axis; negative direction is from y to x.
The Documentation shows that using from_euler to initial a counter-clockwise rotation of 90 degrees about the z-axis is
R.from_euler('z', 90, degrees=True)

I guess ""the counter-clockwise rotation about z-axis"" from doc means ""negative direction about z-axis"" instead of ""positive direction about z-axis"".
",,false,
https://stackoverflow.com/questions/61091824,false,The issue does not meet the criteria for deeper analysis as it is a question about understanding the documentation of scipy's normal distribution and does not involve an API exhibiting unexpected failures or unpredictable behaviors under specific runtime conditions.,,,,,,,scipy normal distribution with scale greater and less than 1,"I'm using the normal distribution from numpy and having a hard time understanding its documentation. Let's say I have a normal distribution with mean of 5 and standard deviation of 0.5:

import numpy as np
from matplotlib import pyplot as plt
from scipy.stats import norm

mean = 5
std = 0.25

x = np.linspace(mean - 3*std, mean + 3*std, 1000)
y = norm(loc=mean, scale=std).pdf(x)
plt.plot(x,y)




The resulting chart is the familiar bell curve but with its peak at around 1.6. How can the probability of any value exceed 1? If I multiply it by scale  then the probabilities are correct.

No such problem when std (and scale) are greater than 1 however:

mean = 5
std = 10

x = np.linspace(mean - 3*std, mean + 3*std, 1000)
y = norm(loc=mean, scale=std).pdf(x)
plt.plot(x,y)




The documentation on norm says loc is the mean and scale is the standard deviation. Why does it behave so strangely with scale greater and less than 1?

Python 3.8.2. Scipy 1.4.1
",,,,false,
https://stackoverflow.com/questions/60787710,false,The issue does not meet the criteria for deeper analysis as it is a question about the TNC method in scipy optimization and does not involve an API exhibiting unexpected failures or unpredictable behaviors under specific runtime conditions.,,,,,,,"Scipy Optimization TNC method, what does scale do ? solution to &quot;unable to progress&quot;?","I am running multiple bounded optimizations with known gradient (100 to 300 Variables). Sometimes, TNC is returning ""unable to progress"".

For my objective function L-BFGS-B is much slower and outputs poor results compared to TNC. (Maybe because TNC is better when number of variables are large)
Using Basinhopping with L-BFGS-B and niter_success to 10, I am getting results close to TNC with 20x slower speed. When TNC returns ""unable to progress"", ""L-BFGS-B"" returns better results. So my current solution is to run Basinhopping when TNC fails with status 6 - ""unable to progress"".

It seems that ""unable to progress"" is returned when TNC is unable to reduce the objective function for x number of iterations. I played a little with the scale factor and inconsistently I got better results.

To my knowledge, Scale in an optimization problem let's the optimizer know which variable is more effective. I have this information and I believe this will reduce number of ""unable to progress"" I am getting. According to the docs,
https://docs.scipy.org/doc/scipy/reference/optimize.minimize-tnc.html
""The default scale array are up-low for interval bounded variables and 1+|x] fo the others"". So it's upper bound - lower bound for variables which are bounded, unable to understand how unbounded is treated. what is 1+|x]  ?

Also, I manually calculated up - low and set unbounded to 1, this is returning different results every time I run the optimization with the same input. (Strange ?)

I also tried to look into the code, how TNC is handling scale, the spicy wrapper sends an empty array or the input array to C code https://github.com/scipy/scipy/blob/master/scipy/optimize/tnc/moduleTNC.c. In the C code I am unable to find where the scale array is created or how it's used. Also could not find when ""unable to progress"" is triggered. Can someone point me where I should look into ?
",,,,false,
https://stackoverflow.com/questions/53000952,false,The issue does not meet the criteria for deeper analysis as it is a question about an ImportError and does not involve an API exhibiting unexpected failures or unpredictable behaviors under specific runtime conditions.,,,,,,,ImportError: cannot import name &#39;LinearNDInterpolator&#39;,,,,,false,
https://stackoverflow.com/questions/52027309,false,The issue does not meet the criteria for deeper analysis as it is a question about calculating p-values and does not involve an API exhibiting unexpected failures or unpredictable behaviors under specific runtime conditions.,,,,,,,calculating p value from pearson r different to scipy,,,,,false,
https://stackoverflow.com/questions/45356848,false,The issue does not meet the criteria for deeper analysis as it is a question about scipy curve_fit and does not involve an API exhibiting unexpected failures or unpredictable behaviors under specific runtime conditions.,,,,,,,scipy curve_fit not producing smooth graph when fitting fourier function,,,,,false,
https://stackoverflow.com/questions/44474327,false,The issue does not meet the criteria for deeper analysis as it does not involve an API exhibiting unexpected failures or unpredictable behaviors under specific runtime conditions.,,,,,,,Real 1D DFT in fftw,,,,,false,
https://stackoverflow.com/questions/40008017,false,The issue does not meet the criteria for deeper analysis as it is a question about scipy curve_fit and does not involve an API exhibiting unexpected failures or unpredictable behaviors under specific runtime conditions.,,,,,,,scipy curve_fit strange result,,,,,false,
https://stackoverflow.com/questions/77686694,false,The issue does not meet the criteria for deeper analysis as it is a question about SVD decomposition for linear equation solving and does not involve an API exhibiting unexpected failures or unpredictable behaviors under specific runtime conditions.,,,,,,,SVD Decomposition for linear equation solving,"looking for Algo here &amp; trying to implement this code, I'm getting different l2-norms for resulting vectors of params for linear equation. Where am I mistaken in my attempt to adopt the code?
import numpy as np
from scipy import linalg

np.random.seed(123)
v = np.random.rand(4)
A = v[:,None] * v[None,:]
b = np.random.randn(4)

x = linalg.inv(A.T.dot(A)).dot(A.T).dot(b) #Usually not recommended because of Numerical Instability of the Normal Equations  https://johnwlambert.github.io/least-squares/
l2_0= linalg.norm(A.dot(x) - b)
print(""manually: "", l2_0)

x = linalg.lstsq(A, b)[0]
l2_1= linalg.norm(A.dot(x) - b)
print(""scipy.linalg.lstsq: "", l2_1)

# 2-norm of two calculations compared
print(np.allclose(l2_0, l2_1, rtol=1.3e-1))

def direct_ls_svd(x,y):
  # append a columns of 1s (these are the biases)
  x = np.column_stack([np.ones(x.shape[0]), x])

  # calculate the economy SVD for the data matrix x
  U,S,Vt = linalg.svd(x, full_matrices=False)

  # solve Ax = b for the best possible approximate solution in terms of least squares
  x_hat = Vt.T @ linalg.inv(np.diag(S)) @ U.T @ y
  #print(x_hat)

  # perform train and test inference
  #y_pred = x @ x_hat
    
  return y-x @ x_hat     #x_hat

x= direct_ls_svd(A, b)
l2_svd= linalg.norm(A.dot(x) - b)
print(""svd: "", l2_svd)

# LU
x= linalg.solve(A.T@A, A.T@b)
l2_solve= linalg.norm(A.dot(x) - b)
print(""scipy.linalg.solve: "", l2_solve)

# manually:  2.9751344995811313
# scipy.linalg.lstsq:  2.9286130558050654
# True
# svd:  6.830550019041984
# scipy.linalg.solve:  2.928613055805065

if my error is in SVD-decomposition Algorithm imlementation for solving Least-Squares problem or perhaps in Numpy relative Scipy rounding or precision differences ? How to correct svd-algo for Least-Squares to become compareable with scipy's? And will this algorithm be faster &amp; less memory-consuming than Iterative Least-Squares methods?
P.S.
SVD applications or here, SVD for PCA &amp; PLS-SVD is my final goal -- will be the algo the same as for Least-Squares approximation ? I'm confused with such a question in general (even with code examples). Can somebody add some clarity for newbie like me, please?
P.P.S.
applying such implementation - I'm getting even worse result: svd:  10.031259300735462 for l2-norm
P.P.P.S.
also I lack some understanding of svd in singular spectral decomposition if exists ref, as 1st for unsupervised dim.reduction &amp; 2nd for non-parametric TS analisys, for practice
P.P.P.P.S. ! PCA is used preliminary estimating if Multicollinearity exists, otherwise strange results can get (biased etc.)... (if no collinearity =&gt; no sensitivity to error of estimation, aka shown in small condition number of OLS analysis, - vc.vs huge cond.num.==collinearity for multivariable/multidimensional regression)
","The most important part here is to filter out the singular vallues that are 0. For your example data S is [9.22354602e-01 3.92705914e-17 1.10667017e-17 5.55744006e-18], notice that you have one singular value close of ~9.22 and the other three are tiny (&lt; 1e-16).
If you attempt to reconstruct the solution using the small errors some elements of Vt and U, that should be about the same magnitude will get divided by these small values and will add up to a significant error to the result.
What can be done in this case is, you assume that any singular value that are small enough are an exact zero. In the following modified version of your function I am assuming all the singular values that are less than rcond times the maximum singular value should be zero. Then I compute a mask m and drop the corresponding rows and columns of the U and Vt matrices.
def direct_ls_svd(A,b,rcond=1e-7):
  # calculate the economy SVD for the data matrix x
  U,S,Vt = linalg.svd(A, full_matrices=False)
  # Mask to remove the zeroes
  m = (abs(S) / np.max(abs(S))) &gt; rcond
  U, S, Vt = U[:,m], S[m], Vt[m, :]
  assert np.allclose( U @ np.diag(S) @ Vt, A)
  # solve Ax = b for the best possible approximate solution
  # in terms of least squares
  x_hat = Vt.T @ ((U.T @ b) / S)
    
  return x_hat

An alternative solution is to set S[m]=0 then you could avoid an extra copy in the worst case, but you lose the potential savings from the reduction of the number of multiplications in the very low rank cases.
",,,false,
https://stackoverflow.com/questions/76610841,false,The issue does not meet the criteria for deeper analysis as it is a question about the behavior of scipy.stats.kstest for the Poisson distribution and does not involve an API exhibiting unexpected failures or unpredictable behaviors under specific runtime conditions.,,,,,,,Why does the p-value of the scipy.stats.ketest drop when the number of samples increases?,"Now days I am working on a time-series event, and want to show it is NOT randomly generated events. For this purpose, I am trying to use the kstest in scipy. By the way, I have a question about the cipy.stats.kstest for the poisson distribution as follows,
--- Code 1---
from scipy.stats import poisson, kstest

noPts = 100 # &lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;---------------
lambdaPoisson = 10

my_data = poisson.rvs(size = noPts, mu = lambdaPoisson)

ks_statistic, p_value = kstest(my_data, 'poisson', args=(lambdaPoisson,0))
print(ks_statistic, p_value)

0.18677614630310613 0.0015821590670650476

--- code 2 ---
from scipy.stats import poisson, kstest

noPts = 1000 #&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;--------------
lambdaPoisson = 10

my_data = poisson.rvs(size = noPts, mu = lambdaPoisson)

ks_statistic, p_value = kstest(my_data, 'poisson', args=(lambdaPoisson,0))
print(ks_statistic, p_value)

0.13477614630310608 2.6511802749311937e-16

I expected that the larger number of data points would result in a more precise statistical match between the two distributions (poisson.rvs vs. kstest(poisson). Yes, the ks_statistics is getting smaller. BUT the p-value becomes very small which rejects the null hypothesis, that is very strange because the two distributions are the same 'Poisson' with the same parameter. What am I doing wrong?
Can someone understand and explain to me the discrepancy here?
","The documentation for kstest says it is only valid for continuous distributions. Poissons are discrete.
",,,false,
https://stackoverflow.com/questions/75586614,true,The issue involves the behavior of the integrate function in SciPy when calculating the integral of a variant of the Hill Equation. The discrepancy arises when comparing the results obtained using the integrate function with the results obtained by approximating the integral using Simpson's rule. This indicates that there might be unexpected behavior or limitations in the integrate function under certain conditions.,SciPy,integrate,The integrate function in SciPy returns a different result when calculating the integral of a variant of the Hill Equation compared to the result obtained by approximating the integral using Simpson's rule. The discrepancy suggests that the integrate function may have limitations or unexpected behavior in certain scenarios.,The integrate function works as expected under normal conditions when calculating integrals.,The issue is triggered when the integrate function is used to calculate the integral of a specific variant of the Hill Equation.,"This issue might be challenging to detect during development and testing because it involves comparing the results of the integrate function with an approximation method, and the discrepancy may not be immediately apparent.",Different results when computing integral analytically/with scipy VS approximating integral,"I'm trying to calculate the integral of a variant of the Hill Equation shown here. 
When I try implementing this with the integrate function in SciPy, I get the following:
from scipy import integrate

Top = 0.9015038230670139
Bottom = 0.5972679490151096
ic50 = 17561.998143066336
Coef = -1.245569789770613

def Hill_formula(X):
    return Bottom + (Top - Bottom)/(1 + ((10**np.log10(ic50))/10**X)**Coef)

integrate.quad(Hill_formula, 0.001, 40.0)

with the result coming out to (25.18116866489653, 1.3873362345430754e-08)
However, looking at the graph of this equation with 100 evenly spaced points from 0.001 to 40.0, it clearly looks nothing close to the given answer of ~25: (EDIT: It actually does look right on hindsight because I misread the y-axis of the plot. Still would appreciate help on why this discrepancy exists though) 
Checking with an approximation method for the area under the curve gives a similar result:
from scipy.integrate import simpson
points = np.linspace(0.001, 40.0, num=100)
curve_results = []
for val in points:
    curve_results.append(Hill_formula(val))

np.abs(simpson(points, curve_results))

resulting in 1.2913519756923537. Is there a reason why this discrepancy exists?
I tried calculating the analytical solution of the integral and using that resulting equation to directly obtain the area under the curve between the bounds, but ran into that same strange ~25 answer.
","From the documentation of scipy.simpson:
scipy.integrate.simpson(y, x=None, dx=1.0, axis=-1, even='avg')

Integrate y(x) using samples along the given axis and the composite
Simpson's rule. If x is None, spacing of dx is assumed.

If there are an even number of samples, N, then there are an odd
number of intervals (N-1), but Simpson's rule requires an even number
of intervals. The parameter 'even' controls how this is handled.

Parameters
----------
y : array_like
    Array to be integrated.
x : array_like, optional
    If given, the points at which `y` is sampled.
...

That means that you've probably meant to write simpson(curve_results, points) instead of np.abs(simpson(points, curve_results)). That results in 25.18116860801739, which has the same first 9 digits as the one you've obtained and the the analytical result evaluating to 25.181168664896531....
",,,false,
https://stackoverflow.com/questions/72985838,true,The issue involves the usage of the gumbel_r distribution in scipy.stats to fit various distributions onto data. The discrepancy arises when comparing the plotted distribution with the histogram of the data. This indicates that there might be unexpected behavior or incorrect usage of the gumbel_r distribution.,SciPy,scipy.stats.gumbel_r,"When using the gumbel_r distribution in scipy.stats to fit distributions onto data, the plotted distribution does not match the histogram of the data. This suggests that there might be unexpected behavior or incorrect usage of the gumbel_r distribution.",The gumbel_r distribution works as expected under normal conditions when fitting distributions onto data.,The issue is triggered when the gumbel_r distribution is used to fit distributions onto specific data.,This issue might be challenging to detect during development and testing because it requires comparing the plotted distribution with the histogram of the data and understanding the correct usage of the gumbel_r distribution.,Scipy Gumbel Fit does not Fit - What is the correct use?,"I'm trying to fit various distributions onto my data and test (chi-squared?) which fits best. I started out by using the gumbel_r distribution of scipy, as this is the one often used in literature.
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
import scipy.stats as ss

data = pd.read_csv(""data.csv"")
data

sns.histplot(data[""score""], kde=True, stat='probability')
plt.show()

x = np.linspace(0,1,101)
hist, bins = np.histogram(data[""score""], bins=x, density=True)
loc, scale = ss.gumbel_r.fit(hist)
dist = ss.gumbel_r(loc=loc,scale=scale)
plt.plot(x, dist.pdf(x))
plt.show()

Inspecting the plots yields strange results. For example my data has a peak at ~0.09 of around ~0.025. However, the plotted gumbel looks completely off.
My questions are now:

Why are the plots not looking similar? I'm also suspecting stat='probability' could be the culprit here?
What do I need to do, such that the second plot will look somewhat similar to the first one?
Optimally I would get another hist for the same bins of the fitted distribution and input into scipy.stats.chisquare to quantify how good the fit of the distribution is and see which fits best. Is that correct?

","Don't give hist to gumbel_r.fit().  It expects the original data.  Change the line that calls fit() to
loc, scale = ss.gumbel_r.fit(data['score'].to_numpy())

Also, to get the Seaborn plot on the same scale as the plot of the PDF, change stat='probability' to stat='density' in the histplot() call.
",,,false,
https://stackoverflow.com/questions/66856730,false,"The issue does not involve an API exhibiting unexpected failures or unpredictable behaviors under specific runtime conditions. Instead, it seems to be a discrepancy between the LAPACK's zgesvd results and scipy.linalg's SVD results for different sizes of matrices.",,,,,,,LAPACK&#39;s zgesvd results different than scipy.linalg&#39;s SVD,"So I'm trying to compute SVD of an NxN matrix. Strangely, for all the cases of 2x2 matrices, the SVD from both lapack and scipy match but they differ when I go for a 3x3 or 4x4 matrices.
// LAPACK (C) Case: 2x2

#include &lt;stdlib.h&gt;
#include &lt;stdio.h&gt;
#include &lt;Accelerate/Accelerate.h&gt;

/* Complex datatype */
struct _dcomplex { double re, im; };
typedef struct _dcomplex dcomplex;

/* ZGESVD prototype */
extern void zgesvd( char* jobu, char* jobvt, int* m, int* n, dcomplex* a,
                   int* lda, double* s, dcomplex* u, int* ldu, dcomplex* vt, int* ldvt,
                   dcomplex* work, int* lwork, double* rwork, int* info );
/* Auxiliary routines prototypes */
extern void print_matrix( char* desc, int m, int n, dcomplex* a, int lda );
extern void print_rmatrix( char* desc, int m, int n, double* a, int lda );

/* Parameters */
#define M 2
#define N 2
#define LDA M
#define LDU M
#define LDVT N

/* Main program */
int main() {
    /* Locals */
    int m = M, n = N, lda = LDA, ldu = LDU, ldvt = LDVT, info, lwork;
    dcomplex wkopt;
    dcomplex* work;
    /* Local arrays */
    /* rwork dimension should be at least max( 1, 5*min(m,n) ) */
    double s[M], rwork[5*M];
    dcomplex u[LDU*M], vt[LDVT*N];
//    dcomplex a[LDA*N] = {
//        {0, 0}, {0, 0}, {1,  0},
//        {-0.36599657,  -0.27449743}, {-0.27449743,  0.36599657}, {0.76249285, 0},
//        {-0.36599657, 0.27449743}, {-0.27449743, -0.36599657}, {0.76249285, 0},
//    };
    dcomplex a[LDA*N] = {
        {0.70710678, 0}, {0, -0.70710678},
        {0.70710678,  0}, {0,  0.70710678},
    };
    /* Executable statements */
    printf( "" ZGESVD Example Program Results\n"" );
    /* Query and allocate the optimal workspace */
    lwork = -1;
    zgesvd( ""All"", ""All"", &amp;m, &amp;n, a, &amp;lda, s, u, &amp;ldu, vt, &amp;ldvt, &amp;wkopt, &amp;lwork,
           rwork, &amp;info );
    lwork = (int)wkopt.re;
    work = (dcomplex*)malloc( lwork*sizeof(dcomplex) );
    /* Compute SVD */
    zgesvd( ""All"", ""All"", &amp;m, &amp;n, a, &amp;lda, s, u, &amp;ldu, vt, &amp;ldvt, work, &amp;lwork,
           rwork, &amp;info );
    /* Check for convergence */
    if( info &gt; 0 ) {
        printf( ""The algorithm computing SVD failed to converge.\n"" );
        exit( 1 );
    }
    /* Print singular values */
    print_rmatrix( ""Singular values"", 1, m, s, 1 );
    /* Print left singular vectors */
    print_matrix( ""Left singular vectors (stored columnwise)"", m, m, u, ldu );
    /* Print right singular vectors */
    print_matrix( ""Right singular vectors (stored rowwise)"", m, n, vt, ldvt );
    /* Free workspace */
    free( (void*)work );
    exit( 0 );
} /* End of ZGESVD Example */

/* Auxiliary routine: printing a matrix */
void print_matrix( char* desc, int m, int n, dcomplex* a, int lda ) {
    int i, j;
    printf( ""\n %s\n"", desc );
    for( i = 0; i &lt; m; i++ ) {
        for( j = 0; j &lt; n; j++ )
        printf( "" (%6.2f,%6.2f)"", a[i+j*lda].re, a[i+j*lda].im );
        printf( ""\n"" );
    }
}

/* Auxiliary routine: printing a real matrix */
void print_rmatrix( char* desc, int m, int n, double* a, int lda ) {
    int i, j;
    printf( ""\n %s\n"", desc );
    for( i = 0; i &lt; m; i++ ) {
        for( j = 0; j &lt; n; j++ ) printf( "" %6.2f"", a[i+j*lda] );
        printf( ""\n"" );
    }
}

Yields
&lt;!-- language: lang-bash --&gt;
ZGESVD Example Program Results

 Singular values
   1.00   1.00

 Left singular vectors (stored columnwise)
 ( -0.71,  0.00) ( -0.71,  0.00)
 ( -0.00,  0.71) (  0.00, -0.71)

 Right singular vectors (stored rowwise)
 ( -1.00, -0.00) ( -0.00, -0.00)
 ( -0.00, -0.00) ( -1.00, -0.00)

and scipy.linalg.SVD yields
&lt;!-- language: lang-bash --&gt;
Singular values: [1. 1.]
Left singular vectors: [[-0.70710678+0.j         -0.70710678+0.j        ]
                        [ 0.        +0.70710678j  0.        -0.70710678j]]
Right singular vectors: [[-1.+0.j -0.+0.j]
                         [-0.+0.j -1.+0.j]]

So far so good. Now when I try to input a 3x3 or NxN matrix, the results are like
// LAPACK C Case: 3x3

#include &lt;stdlib.h&gt;
#include &lt;stdio.h&gt;
#include &lt;Accelerate/Accelerate.h&gt;

/* Complex datatype */
struct _dcomplex { double re, im; };
typedef struct _dcomplex dcomplex;

/* ZGESVD prototype */
extern void zgesvd( char* jobu, char* jobvt, int* m, int* n, dcomplex* a,
                   int* lda, double* s, dcomplex* u, int* ldu, dcomplex* vt, int* ldvt,
                   dcomplex* work, int* lwork, double* rwork, int* info );
/* Auxiliary routines prototypes */
extern void print_matrix( char* desc, int m, int n, dcomplex* a, int lda );
extern void print_rmatrix( char* desc, int m, int n, double* a, int lda );

/* Parameters */
#define M 3
#define N 3
#define LDA M
#define LDU M
#define LDVT N

/* Main program */
int main() {
    /* Locals */
    int m = M, n = N, lda = LDA, ldu = LDU, ldvt = LDVT, info, lwork;
    dcomplex wkopt;
    dcomplex* work;
    /* Local arrays */
    /* rwork dimension should be at least max( 1, 5*min(m,n) ) */
    double s[M], rwork[5*M];
    dcomplex u[LDU*M], vt[LDVT*N];
    dcomplex a[LDA*N] = {
        {0, 0}, {0, 0}, {1,  0},
        {-0.36599657,  -0.27449743}, {-0.27449743,  0.36599657}, {0.76249285, 0},
        {-0.36599657, 0.27449743}, {-0.27449743, -0.36599657}, {0.76249285, 0},
    };
//    dcomplex a[LDA*N] = {
//        {0.70710678, 0}, {0, -0.70710678},
//        {0.70710678,  0}, {0,  0.70710678},
//    };
    /* Executable statements */
    printf( "" ZGESVD Example Program Results\n"" );
    /* Query and allocate the optimal workspace */
    lwork = -1;
    zgesvd( ""All"", ""All"", &amp;m, &amp;n, a, &amp;lda, s, u, &amp;ldu, vt, &amp;ldvt, &amp;wkopt, &amp;lwork,
           rwork, &amp;info );
    lwork = (int)wkopt.re;
    work = (dcomplex*)malloc( lwork*sizeof(dcomplex) );
    /* Compute SVD */
    zgesvd( ""All"", ""All"", &amp;m, &amp;n, a, &amp;lda, s, u, &amp;ldu, vt, &amp;ldvt, work, &amp;lwork,
           rwork, &amp;info );
    /* Check for convergence */
    if( info &gt; 0 ) {
        printf( ""The algorithm computing SVD failed to converge.\n"" );
        exit( 1 );
    }
    /* Print singular values */
    print_rmatrix( ""Singular values"", 1, m, s, 1 );
    /* Print left singular vectors */
    print_matrix( ""Left singular vectors (stored columnwise)"", m, m, u, ldu );
    /* Print right singular vectors */
    print_matrix( ""Right singular vectors (stored rowwise)"", m, n, vt, ldvt );
    /* Free workspace */
    free( (void*)work );
    exit( 0 );
} /* End of ZGESVD Example */

/* Auxiliary routine: printing a matrix */
void print_matrix( char* desc, int m, int n, dcomplex* a, int lda ) {
    int i, j;
    printf( ""\n %s\n"", desc );
    for( i = 0; i &lt; m; i++ ) {
        for( j = 0; j &lt; n; j++ )
        printf( "" (%6.2f,%6.2f)"", a[i+j*lda].re, a[i+j*lda].im );
        printf( ""\n"" );
    }
}

/* Auxiliary routine: printing a real matrix */
void print_rmatrix( char* desc, int m, int n, double* a, int lda ) {
    int i, j;
    printf( ""\n %s\n"", desc );
    for( i = 0; i &lt; m; i++ ) {
        for( j = 0; j &lt; n; j++ ) printf( "" %6.2f"", a[i+j*lda] );
        printf( ""\n"" );
    }
}


Yeilds
&lt;!-- language: lang-bash --&gt;
 ZGESVD Example Program Results

 Singular values
   1.55   0.65   0.42

 Left singular vectors (stored columnwise)
 (  0.26,  0.00) (  0.49, -0.34) ( -0.75,  0.00)
 (  0.20, -0.00) ( -0.66,  0.46) ( -0.57,  0.00)
 ( -0.94, -0.00) (  0.00,  0.00) ( -0.33,  0.00)

 Right singular vectors (stored rowwise)
 ( -0.61, -0.00) ( -0.56, -0.00) ( -0.56, -0.00)
 (  0.00,  0.00) (  0.41, -0.58) ( -0.41,  0.58)
 ( -0.79, -0.00) (  0.43, -0.00) (  0.43, -0.00)

&lt;!-- language: lang-bash --&gt;
# Python
Singular values: [1.55161905 0.64699664 0.41698163]
Left singular vectors: [[ 0.26480555-9.68622857e-18j  0.57973136-1.54633603e-01j
  -0.75490266+2.76133169e-17j]
 [ 0.19860416+1.15286199e-17j -0.77297515+2.06178138e-01j
  -0.56617699-3.28655711e-17j]
 [-0.94362832+0.00000000e+00j  0.        +0.00000000e+00j
  -0.33100694+0.00000000e+00j]]
Right singular vectors: [[-0.60815722+0.j         -0.5613131 +0.j         -0.5613131 +0.j        ]
 [ 0.        +0.j          0.18223745-0.68321996j -0.18223745+0.68321996j]
 [-0.7938166 +0.j          0.43003209+0.j          0.43003209+0.j        ]]

Now for a fact I know that the results computed by Scipy are perfect as the purpose that I'm using SVD for is point perfect and gives perfect results and my goal is to generate results like scipy. Now I know that Scipy also uses LAPACK's Drivers but why the difference then? Where am I messing it up.
","The conditions you should be testing is not that the output matrices are the same.
The condition u @ np.diag(s) @ vh = A with u, s, vh = np.linalg.svd(A), is where you should focus.
Also pay atention to the order of the singular values, if you have two set of singular values s1 and s2 that are the same when sorted, you can construct a matrix P such that P @ np.diag(s1) @ P.T = s2 if you are lucky you can then use u1 @ P.inv() = u2 and P.T.inv() @ vh1 = vh2 since (u1 @ P.inv()) @ P @ np.diag(s1) @ P.T @ (P.T.inv() @ vh1) = u1 @ np.diag(1) @ vh1.
",,,false,
https://stackoverflow.com/questions/65678500,false,"The issue does not involve an API exhibiting unexpected failures or unpredictable behaviors under specific runtime conditions. Instead, it relates to the usage of the print() command and the consistency of the shape of a numpy array during integration using scipy.",,,,,,,"How to use print() command and make the shape of a numpy array consistent, during integration using scipy?","I tried examining how spicy.integrate.ode works. The code below is simple code to do this.
def func(t, z, p):
    x = z[0]
    y = z[1]
    print('x :', x)
    print('x.shape :', x.shape)
    print('y :', y)
    print('y.shape :', y.shape)
    return [x*0, y*0]

t_ini = 0
t_fin = 1
x_ini = np.array([[2, 2]])
y_ini = np.array([[2, 2]])

solver = ode(func)
solver.set_integrator('dopri5')
solver.set_initial_value([x_ini, y_ini], t_ini)
solver.set_f_params([0])
solver.integrate(t_fin)
x_fin, y_fin = solver.y
print('x_fin :', x_fin)
print('y_fin :', y_fin)

However,
print('x :', x)
print('x.shape :', x.shape)
print('y :', y)
print('y.shape :', y.shape)
return [x*0, y*0]

didn't work. The result of the code was
x_fin : [[2. 2.]]
y_fin : [[2. 2.]]

.
Interestingly, when I changed x_ini and y_ini into
x_ini = np.array([[2]])
y_ini = np.array([[2]])

, the print() command worked and the result of the code was the repetition of
x : 2.0
x.shape : ()
y : 2.0
y.shape : ()

with the two lines after the repetition which are
x_fin : [[2.]]
y_fin : [[2.]]

.
It was strange that even if I put x_ini and y_ini having (1, 1) shape, both print(x.shape) and print(y.shape) showed ().
So the questions are:

Why the print() didn't worked for x_ini = y_ini = np.array([[2, 2]]) and what I should to to make them work?
Why the shape of the numpy arrays which are x and y became () instead of (1, 1).
How to make the shape of the numpy arrays which are x and y be (1, 1) during the integration using scipy. What should I do if the shape of both x_ini and y_ini is (2, 2) and I want to make the shape consistent during the integration using scipy.

Is there any guys who know about these?
","I get a warning when using your initial value array:
In [9]: x_ini = np.array([[2, 2]])
   ...: y_ini = np.array([[2, 2]])
In [10]: solver.set_initial_value([x_ini, y_ini], 0)
Out[10]: &lt;scipy.integrate._ode.ode at 0x7f1ab8953d60&gt;
In [11]: solver.integrate(.1)
/usr/local/lib/python3.8/dist-packages/scipy/integrate/_ode.py:1181: UserWarning: dopri5: input is not consistent
  warnings.warn('{:s}: {:s}'.format(self.__class__.__name__,
Out[11]: 
array([[[2., 2.]],

       [[2., 2.]]])

The output is the same as the input
In [12]: np.array([x_ini, y_ini])
Out[12]: 
array([[[2, 2]],

       [[2, 2]]])

With
x_ini = np.array([[2]])
y_ini = np.array([[2]])

The initial value is a (2,1,1) array
In [18]: np.array([x_ini, y_ini])
Out[18]: 
array([[[2]],

       [[2]]])

That does run, but the values passed to your function are 0d arrays
x : 2.0
x.shape : ()
y : 2.0
y.shape : ()

===
Let's simplify the func:
In [20]: def func(t, z, p):
    ...:     print(type(z), z.shape, z)
    ...:     return z*0
    ...: 
In [21]: solver = ode(func)
    ...: solver.set_integrator('dopri5')
Out[21]: &lt;scipy.integrate._ode.ode at 0x7f1ab6debe50&gt;
In [22]: solver.set_f_params([0])
Out[22]: &lt;scipy.integrate._ode.ode at 0x7f1ab6debe50&gt;
In [23]: solver.set_initial_value([1,2], 0)
Out[23]: &lt;scipy.integrate._ode.ode at 0x7f1ab6debe50&gt;
In [24]: solver.integrate(.1)
&lt;class 'numpy.ndarray'&gt; (2,) [1. 2.]
...

If I change the initial value to a (2,1,1), func gets the same inputs:
In [27]: solver.set_initial_value([[[1]],[[2]]], 0)
Out[27]: &lt;scipy.integrate._ode.ode at 0x7f1ab6debe50&gt;
In [28]: solver.integrate(.1)
&lt;class 'numpy.ndarray'&gt; (2,) [1. 2.]

Change the input to a 3 element array:
In [31]: solver.set_initial_value([1,2,3], 0)
Out[31]: &lt;scipy.integrate._ode.ode at 0x7f1ab6debe50&gt;
In [32]: solver.integrate(.1)
&lt;class 'numpy.ndarray'&gt; (3,) [1. 2. 3.]

From the docs:
f : callable ``f(t, y, *f_args)``
    Right-hand side of the differential equation. t is a scalar,
    ``y.shape == (n,)``.
    ``f_args`` is set by calling ``set_f_params(*args)``.
    `f` should return a scalar, array or list (not a tuple).

f returns dy/dt.  The y will be a 1d array, and it's supposed to return a like size array.  Note the y.shape requirement.
The y that ode passes to the function is derived from the initial value array.  A (2,1,1) input is flattened to (2,).  A (2,1,2) produces the warning.
",,,false,
https://stackoverflow.com/questions/64392302,false,"The issue does not involve an API exhibiting unexpected failures or unpredictable behaviors under specific runtime conditions. Instead, it seems to be related to issues with scipy and numpy when running the code in a Docker container on Windows 10.",,,,,,,"Scipy, Numpy, Django, Docker Issue","Having issues with scipy and numpy.
This code, part of a Django app, runs perfectly when run on my windows 10 system:
try:
    # sparse_load is a scipy.sparse.csr_matrix
    sparse_load = scipy.sparse.load_npz(cache)
    logger.info('Got sparse_load')
    concept_alias_tfidfs = sparse_load.astype(numpy.float32)
except:
    logger.exception('Something went wrong!' ) 
# code continues here ...

It also runs perfectly when running inside a docker container deployed on a Linux server.
The issue is that the type conversion (sparse_load.astype(numpy.float32)) crashes my app when running in a docker container deployed on Docker Desktop running on Windows 10.  The strange thing is that logger.exception is never executed!  I've tried other type conversions with the same result and also tried removing the astype altogether which resulted in another crash further down in the code (again w/o hitting the exception handler placed around that piece of code.
Thoughts?
","Even though OOMKIlled is false (Killed due to Out of memory), I increased the memory from 2 to 8 GB on docker desktop and voila my app worked!
",,,false,
https://stackoverflow.com/questions/62479689,true,The issue involves the behavior of the Butterworth filter in SciPy.,SciPy,signal.butter,"The issue arises when plotting the frequency response of the Butterworth filter using plt.plot(). Only the real component of the complex-valued response is shown, leading to the strange results observed. The imaginary part of the response is discarded when casting complex values to real.",The Butterworth filter is designed to have an increasingly rectangular frequency response as the order increases.,The issue is triggered when plotting the complex-valued frequency response using plt.plot() without considering the magnitude of the response.,This issue might be challenging to detect for users who are not familiar with the complex-valued nature of the frequency response and the need to plot the magnitude of the response to observe the expected behavior.,Butterworth filters look very strange as increasing order,"I'm trying to design a simple Butterworth bandpass filter in SciPy, and I'm running into some strange results. 

import scipy.signal as signal
import numpy as np
import matplotlib.pyplot as plt

def butter_bandpass(lowcut, highcut, fs, freqs,order=3, label=None):
    nyq = 0.5 * fs
    low = lowcut / nyq
    high = highcut / nyq
    sos = signal.butter(order, [low, high], btype='band', output='sos')

    w, h = signal.sosfreqz(sos,worN=freqs,whole=True,fs=fs)

    return w,h

freqs = 650

for i in np.arange(1,10):
    w,h = butter_bandpass(0.01, 0.1, fs=1/0.68, freqs=freqs, order=i)
    plt.plot(h)


This is giving strange results, as can be seen from the image below (Butterworth filters from order 1-10). I thought the filter was supposed to become increasingly rectangular as the order increased? 



Does anyone know how to design a simple Butterworth filter in SciPy?
","The frequency response of the Butterworth filter is not real-valued. When plotting the complex-valued response using plt.plot(), only the real component is shown. You should see a warning:

ComplexWarning: Casting complex values to real discards the imaginary part


To examine the filter's gain, plot the magnitude of the frequency response:

plt.plot(np.abs(h))


You will see an increasingly square response, as expected:


",,,false,
https://stackoverflow.com/questions/54369255,false,Insufficient information provided in the issue.,,,,,,,Signal processing - Why a signal is not completely filtered out at my cutoff frequency?,,,,,false,
https://stackoverflow.com/questions/53187760,false,Insufficient information provided in the issue.,,,,,,,scipy.linalg.sparse.eigsh returns negative and non-consistent eigenvalues for positive semi-definite matrix,,,,,false,
https://stackoverflow.com/questions/40540229,false,Insufficient information provided in the issue.,,,,,,,Python Scipy Optimize Error &quot;ValueError: Lengths must match to compare&quot;,,,,,false,
https://stackoverflow.com/questions/17543432,false,Insufficient information provided in the issue.,,,,,,,"Matlab calling python, returning vector, str2num not working on returned vector",,,,,false,
https://stackoverflow.com/questions/39350557,true,The issue meets the criteria for deeper analysis as it involves unexpected behavior in the scipy.integrate.ode API under specific runtime conditions.,scipy,scipy.integrate.ode,"The multi-threaded execution of scipy.integrate.ode with a jacobian parameter exhibits significant variability in execution times, ranging from 10ms to 5000ms per call. This behavior is unexpected and not observed in single-threaded execution.",The scipy.integrate.ode API is expected to solve a system of ordinary differential equations (ODEs) accurately and efficiently.,The issue is triggered when scipy.integrate.ode is executed in a multi-threaded environment with the jacobian parameter specified.,This issue might be challenging to detect during development and testing because the variability in execution times depends on the specific runtime conditions and may not be consistently reproducible.,Running scipy.integrate.ode in multiprocessing Pool results in huge performance hit,"I'm using python's scipy.integrate to simulate a 29-dimensional linear system of differential equations. Since I need to solve several problem instances, I thought I could speed it up by doing computations in parallel using multiprocessing.Pool. Since there is no shared data or synchronization necessary between threads (the problem is embarrassingly parallel), I thought this should obviously work. After I wrote the code to do this, however, I got very strange performance measurements:


Single-threaded, without jacobian: 20-30 ms per call
Single-threaded, with jacobian: 10-20 ms per call
Multi-threaded, without jacobian: 20-30 ms per call
Multi-threaded, with jacobian: 10-5000 ms per call


What's shocking is that what I thought should be the fastest setup, was actually the slowest, and the variability was two orders of magnitude. It's a deterministic computation; computers aren't supposed to work this way. What could possibly be causing this?

Effect seems system-dependent

I tried the same code on another computer and I didn't see this effect. 

Both machines were using Ubuntu 64 bit, Python 2.7.6, scipy version 0.18.0, and numpy version 1.8.2. I didn't see the variability with an Intel(R) Core(TM) i5-5300U CPU @ 2.30GHz processor. I did see the issue with an Intel(R) Core(TM) i7-2670QM CPU @ 2.20GHz.

Theories

One thought was that there might be a shared cache among processors, and by running it in parallel I can't fit two instances of the jacobian matrix in the cache, so they constantly battle each other for the cache slowing each other down compared with if they are run serially or without the jacobian. But it's not a million variable system. The jacobian is a 29x29 matrix, which takes up 6728 bytes. The level 1 cache on the processor is 4 x 32 KB, much larger. Are there any other shared resources between processors that might be to blame? How can we test this?

Another thing I noticed is that each python process seems to take several hundred percent of the CPU as it's running. This seems to mean that the code is already parallelized at some point (perhaps in the low-level library). This could mean that further parallelization wouldn't help, but I wouldn't expect such a dramatic slowdown.

Code

It would be good to try out the on more machines to see if (1) other people can experience the slowdown at all and (2) what are the common features of systems where the slowdown occurs. The code does 10 trials of two parallel computations using a multiprocessing pool of size two, printing out the time per scipy.ode.integrate call for each of the 10 trials. 

'odeint with multiprocessing variable execution time demonsrtation'

from numpy import dot as npdot
from numpy import add as npadd
from numpy import matrix as npmatrix
from scipy.integrate import ode
from multiprocessing import Pool
import time

def main():
    ""main function""

    pool = Pool(2) # try Pool(1)
    params = [0] * 2

    for trial in xrange(10):
        res = pool.map(run_one, params)
        print ""{}. times: {}ms, {}ms"".format(trial, int(1000 * res[0]), int(1000 * res[1]))

def run_one(_):
    ""perform one simulation""

    final_time = 2.0
    init_state = [0.1 if d &lt; 7 else 0.0 for d in xrange(29)]
    (a_matrix, b_vector) = get_dynamics()

    derivative = lambda dummy_t, state: npadd(npdot(a_matrix, state), b_vector)
    jacobian = lambda dummy_t, dummy_state: a_matrix
    #jacobian = None # try without the jacobian

    #print ""jacobian bytes:"", jacobian(0, 0).nbytes

    solver = ode(derivative, jacobian)
    solver.set_integrator('vode')
    solver.set_initial_value(init_state, 0)

    start = time.time()
    solver.integrate(final_time)
    dif = time.time() - start

    return dif

def get_dynamics():
    ""return a tuple (A, b), which are the system dynamics x' = Ax + b""

    return \
    (
        npmatrix([
        [0, 0, 0, 0.99857378006, 0.053384274244, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ],
        [0, 0, 1, -0.003182219341, 0.059524655342, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ],
        [0, 0, -11.570495605469, -2.544637680054, -0.063602626324, 0.106780529022, -0.09491866827, 0.007107574493, -5.20817921341, -23.125876742495, -4.246931301528, -0.710743697134, -1.486697327603, -0.044548215175, 0.03436637817, 0.022990248611, 0.580153205353, 1.047552018229, 11.265023544535, 2.622275290571, 0.382949404795, 0.453076470454, 0.022651889536, 0.012533628369, 0.108399390974, -0.160139432044, -6.115359574845, -0.038972389136, 0, ],
        [0, 0, 0.439356565475, -1.998182296753, 0, 0.016651883721, 0.018462046981, -0.001187470742, -10.778778281386, 0.343052863546, -0.034949331535, -3.466737362551, 0.013415853489, -0.006501746896, -0.007248032248, -0.004835912875, -0.152495086764, 2.03915052839, -0.169614300211, -0.279125393264, -0.003678218266, -0.001679708185, 0.050812027754, 0.043273505033, -0.062305315646, 0.979162836629, 0.040401368402, 0.010697028656, 0, ],
        [0, 0, -2.040895462036, -0.458999156952, -0.73502779007, 0.019255757332, -0.00459562242, 0.002120360732, -1.06432932386, -3.659159530947, -0.493546966858, -0.059561101143, -1.953512259413, -0.010939065041, -0.000271004496, 0.050563886711, 1.58833954495, 0.219923768171, 1.821923233098, 2.69319056633, 0.068619628466, 0.086310028398, 0.002415425662, 0.000727041422, 0.640963888079, -0.023016712545, -1.069845542887, -0.596675149197, 0, ],
        [-32.103607177734, 0, -0.503355026245, 2.297859191895, 0, -0.021215811372, -0.02116791904, 0.01581159234, 12.45916782984, -0.353636907076, 0.064136531117, 4.035326800046, -0.272152744884, 0.000999589868, 0.002529691904, 0.111632959213, 2.736421830861, -2.354540136198, 0.175216915979, 0.86308171287, 0.004401276193, 0.004373406589, -0.059795009475, -0.051005479746, 0.609531777761, -1.1157829788, -0.026305051933, -0.033738880627, 0, ],
        [0.102161169052, 32.057830810547, -2.347217559814, -0.503611564636, 0.83494758606, 0.02122657001, -0.037879735231, 0.00035400386, -0.761479736492, -5.12933410588, -1.131382179292, -0.148788337148, 1.380741054924, -0.012931029503, 0.007645723855, 0.073796656681, 1.361745395486, 0.150700793731, 2.452437244444, -1.44883919298, 0.076516270282, 0.087122640348, 0.004623192159, 0.002635233443, -0.079401941141, -0.031023369979, -1.225533436977, 0.657926151362, 0, ],
        [-1.910972595215, 1.713829040527, -0.004005432129, -0.057411193848, 0, 0.013989634812, -0.000906753354, -0.290513515472, -2.060635522957, -0.774845915178, -0.471751979387, -1.213891560083, 5.030515136324, 0.126407660877, 0.113188603433, -2.078420624662, -50.18523312358, 0.340665548784, 0.375863242926, -10.641168797333, -0.003634153255, -0.047962774317, 0.030509705209, 0.027584169642, -10.542357589006, -0.126840767097, -0.391839285172, 0.420788121692, 0, ],
        [0.126296110212, -0.002898250629, -0.319316070797, 0.785201711657, 0.001772374259, 0.00000584372, 0.000005233812, -0.000097899495, -0.072611454126, 0.001666291957, 0.195701043078, 0.517339177294, 0.05236528267, -0.000003359731, -0.000003009077, 0.000056285381, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ],
        [-0.018114066432, 0.077615035084, 0.710897211118, 2.454275059389, -0.012792968774, 0.000040510624, 0.000036282541, -0.000678672106, 0.010414324729, -0.044623231468, 0.564308412696, -1.507321670112, 0.066879720068, -0.000023290783, -0.00002085993, 0.000390189123, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ],
        [-0.019957254425, 0.007108972111, 122.639137999354, 1.791704310155, 0.138329792976, 0.000000726169, 0.000000650379, -0.000012165459, -8.481152717711, -37.713895394132, -93.658221074435, -4.801972165378, -2.567389718833, 0.034138340146, -0.038880106034, 0.044603217363, 0.946016722396, 1.708172458034, 18.369114490772, 4.275967542224, 0.624449778826, 0.738801257357, 0.036936909247, 0.020437742859, 0.176759579388, -0.261128576436, -9.971904607075, -0.063549647738, 0, ],
        [0.007852964982, 0.003925745426, 0.287856349997, 58.053471054491, 0.030698062827, -0.000006837601, -0.000006123962, 0.000114549925, -17.580742026275, 0.55713614874, 0.205946900184, -43.230778067404, 0.004227082975, 0.006053854501, 0.006646690253, -0.009138926083, -0.248663457912, 3.325105302428, -0.276578605231, -0.455150962257, -0.005997822569, -0.002738986905, 0.082855748293, 0.070563187482, -0.101597078067, 1.596654829885, 0.065879787896, 0.017442923517, 0, ],
        [0.011497315687, -0.012583019909, 13.848373855148, 22.28881517216, 0.042287331657, 0.000197558695, 0.000176939544, -0.003309689199, -1.742140233901, -5.959510415282, -11.333020298294, -14.216479234895, -3.944800806497, 0.001304578929, -0.005139259078, 0.08647432259, 2.589998222025, 0.358614863147, 2.970887395829, 4.39160430183, 0.111893402319, 0.140739944934, 0.003938671797, 0.001185537435, 1.045176603318, -0.037531801533, -1.744525005833, -0.972957942438, 0, ],
        [-16.939142002537, 0.618053512295, 107.92089190414, 204.524147386814, 0.204407545189, 0.004742101706, 0.004247169746, -0.079444150933, -2.048456967261, -0.931989524708, -66.540858220883, -116.470289129818, -0.561301215495, -0.022312225275, -0.019484747345, 0.243518778973, 4.462098610572, -3.839389874682, 0.285714413078, 1.40736916669, 0.007176864388, 0.007131419303, -0.097503691021, -0.083171197416, 0.993922379938, -1.819432085819, -0.042893874898, -0.055015718216, 0, ],
        [-0.542809857455, 7.081822285872, -135.012404429101, 460.929268260027, 0.036498617908, 0.006937238413, 0.006213200589, -0.116219147061, -0.827454697348, 19.622217613195, 78.553728334274, -283.23862765888, 3.065444785639, -0.003847616297, -0.028984525722, 0.187507140282, 2.220506417769, 0.245737625222, 3.99902408961, -2.362524402134, 0.124769923797, 0.142065016461, 0.007538727793, 0.004297097528, -0.129475392736, -0.050587718062, -1.998394759416, 1.072835822585, 0, ],
        [-1.286456393795, 0.142279456389, -1.265748910581, 65.74306027738, -1.320702989799, -0.061855995532, -0.055400100872, 1.036269854556, -4.531489334771, 0.368539277612, 0.002487097952, -42.326462719738, 8.96223401238, 0.255676968878, 0.215513465742, -4.275436802385, -81.833676543035, 0.555500345288, 0.612894852362, -17.351836610113, -0.005925968725, -0.078209662789, 0.049750119549, 0.044979645917, -17.190711833803, -0.206830688253, -0.638945907467, 0.686150823668, 0, ],
        [0, 0, 0, 0, 0, -0.009702263896, -0.008689641059, 0.162541456323, 0, 0, 0, 0, 0, 0, 0, 0, -0.012, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ],
        [-8.153162937544, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -0.005, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ],
        [0, -3.261265175018, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -0.005, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ],
        [0, 0, 0, 0.17441246156, -3.261265175018, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -0.01, 0, 0, 0, 0, 0, 0, 0, 0, 0, ],
        [0, 0, -3.261265175018, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -8.5, -18, 0, 0, 0, 0, 0, 0, 0, ],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, ],
        [0, 0, 0, -8.153162937544, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -8.5, -18, 0, 0, 0, 0, 0, ],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, ],
        [0, 0, 0, 0, 0, 0, 0, 0, 0.699960862226, 0.262038222227, 0.159589891262, 0.41155156501, -1.701619176699, -0.0427567124, -0.038285155304, 0.703045934017, 16.975651534025, -0.115788018654, -0.127109026104, 3.599544290134, 0.001229743857, 0.016223661959, -0.01033400498, -0.00934235613, -6.433934989563, 0.042639567847, 0.132540852847, -0.142338323726, 0, ],
        [0, 0, 0, 0, 0, 0, 0, 0, -37.001496211974, 0.783588795613, -0.183854784348, -11.869599790688, -0.106084318011, -0.026306590251, -0.027118088888, 0.036744952758, 0.76460150301, 7.002366574508, -0.390318898363, -0.642631203146, -0.005701671024, 0.003522251111, 0.173867535377, 0.147911422248, 0.056092715216, -6.641979472328, 0.039602243105, 0.026181724138, 0, ],
        [0, 0, 0, 0, 0, 0, 0, 0, 1.991401999957, 13.760045912368, 2.53041689113, 0.082528789604, 0.728264862053, 0.023902766734, -0.022896554363, 0.015327568208, 0.370476566397, -0.412566245022, -6.70094564846, -1.327038338854, -0.227019235965, -0.267482033427, -0.008650986307, -0.003394359441, 0.098792645471, 0.197714179668, -6.369398456151, -0.011976840769, 0, ],
        [0, 0, 0, 0, 0, 0, 0, 0, 1.965859332057, -3.743127938662, -1.962645156793, 0.018929412474, 11.145046656101, -0.03600197464, -0.001222148117, 0.602488409354, 11.639787952728, -0.407672972316, 1.507740702165, -12.799953897143, 0.005393102236, -0.014208764492, -0.000915158115, -0.000640326416, -0.03653528842, 0.012458973237, -0.083125038259, -5.472831842357, 0, ],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ],
        ])
    , 
        npmatrix([1.0 if d == 28 else 0.0 for d in xrange(29)])
    )



if __name__ == ""__main__"":
    main()


Example Output

Here's an example of the output that demonstrates the problem (each run is slightly different). Notice the large variability in execution times (over two orders of magnitude!). Again, this all goes away if I either use a pool of size 1 (or run the code without a pool), or if I don't use an explicit jacobian in the call to integrate.


  
  times: 5847ms, 5760ms
  times: 4177ms, 3991ms
  times: 229ms, 36ms
  times: 1317ms, 1544ms
  times: 87ms, 100ms
  times: 113ms, 102ms
  times: 4747ms, 5077ms
  times: 597ms, 48ms
  times: 9ms, 49ms
  times: 135ms, 109ms
  

","Based on the variability of the execution times you posted for the machine showing the problem, I wonder what else that computer is doing at the time you are running your test.  Here are times I saw when I ran your code on an AWS r3.large server (2 cores, 15 GB of RAM) that normally runs interactive R sessions but is currently mostly idle:


  
  times: 11ms, 11ms
  times: 9ms, 9ms
  times: 9ms, 9ms
  times: 9ms, 9ms
  times: 10ms, 10ms
  times: 10ms, 10ms
  times: 10ms, 10ms
  times: 11ms, 10ms
  times: 11ms, 10ms
  times: 9ms, 9ms
  


Is it possible your machine is swapping and you do not know it?  vmstat 5 will give you a lot of information about swap in and outs, but not about cache evictions.

Intel makes some very nice tools for monitoring--two at a time--thousands of different types of operations and errors going on in a processor--including L2 cache evictions--but they are a bit of a firehose: there is information generated every microsecond--or more frequently--and you have to decide what you are going to monitor and how often you want an interrupt to deliver the numbers into your software.  Likely it will take many runs just to narrow down the stats you want to track and you still have to filter out the noise generated by the operating system and everything else running at the time.  It is a time consuming process, but if you follow it to the end and run many different tests you will come to understand what is going on.  

But is this--shared cache resources in a processor--really your question?  It seems more like you just want to figure out why you have variable run times on one machine and, second, why multi-threaded is slower on both machines than single threaded.  Do I have it right?  If not I will edit my answer and we can talk about processor cache, cache snooping and cache coherency.

So, as to the variability on the i7-2670QM CPU machine, I would start with htop, vmstat 5 and iostat 5 to see if the machine is doing something you didn't realize.  That much variability says the executable is getting stalled because the processor is busy doing something else: going off to the network and not finding a share it expects, unable to connect to a DNS server, getting kerbios failures: it could be a lot of things including hardware failures from a hard disk that is being continually reset.  Oh, and move your program to /dev/shm and cd there before you start it.  That won't help you if there are Python libraries in a bad place on a disk, but at least you won't have issues with your local directory.  Report back what you find and we can make further suggestions.

Your second question as I see it, which is perhaps where you started, is why is your program slower when run multi-threaded than single-threaded.  This is a big subject that will come a lot more in focus if we can see how you multi-threaded it.  But even before we do you have to realize that there are several things that can cause a multi-threaded program to run slower than a single-threaded program, and it can have as much to do with the support infrastructure around your program--libraries and operating system calls you make--as your program.  Just because you do not need mutexes does not mean the libraries and operating system do not need them when they are being called from a multi-threaded application.  Locking a mutex is an expensive operation, especially as different threads are rotated between different cores.  

On top of that, since the vode is not re-entrant, if you called it from multiple threads it is possible that it is having trouble finding convergence and having to recalculate the same values many times before it ""gets lucky"" and has enough processor time to complete an iteration before it is swapped out and intermediate results are overwritten.  Give us the code you are using for your multi-threaded runs and I will add to this answer.
","This is intended as a formatted comment regarding the mathematical background raised in a comment by @Dietrich. As it doesn't address the programming question, I intend to delete this answer in a little while until the bounty blows over.

As @Dietrich noted, you can solve your ODE exactly, since if

x' = A*x,


then the exact solution is

x(t) = exp(A*t)*x0


Already I'd say that an exact solution is always superior than a numerical approximation, but this can indeed be faster than a numerical integration. As you noted in a comment, you're worried about efficiency. So don't compute the matrix exponential for each t: compute the eigensystem of A only once:

A*v_i = L_i*v_i


then

x(t) = sum_i c_i*v_i*exp(L_i*t),


and the coefficients c_i can be determined from the linear equations

x0 = sum_i c_i*v_i.


Now, having an inhomogeneous term doesn't change much, as long as your matrix is not singular:

x' = A*x + b
(x - A^(-1)*b)' = A*(x - A^(-1)*b)


so we can solve the homogeneous equation for y = x - A^(-1)*b and in a final step recover x = y + A^(-1)*b.

This all works nicely while the matrix is regular, but in your specific case it's singular. But it turns out that this is due to your final dimension:

&gt;&gt;&gt; np.linalg.det(A)
0.0
&gt;&gt;&gt; np.linalg.det(A[:-1,:-1])
1920987.0461154305


And also note that the final row of A is all zeros (this is the reason for the singularity of A). So the last dimension of x is constant (or changes linearly due to b).

I suggest eliminating this variable, rewriting your equation for the rest of the variables, and solving the non-singular inhomogeneous linear system of ODEs using the above procedure, exactly. It should be faster and precise.



The following will be a bit speculative, see also the caveat at the end.

In case of user-input A and b, things might get trickier. Finding a zero row/column in your matrix would be easy, but A can be singular even though none of its rows/columns are fully zero. I'm not an expert in the subject, but I think your best bet is using something akin to principal component analysis: transforming your system of equations according to the eigensystem of A. My following thoughts will still assume that A is diagonalizable, but mostly because I'm unfamiliar with singular value decomposition. In realistic cases I'd expect your matrices to be diagonalizable, even if singular.

So I'll assume that the matrix A can be decomposed as

A = V * D * V^(-1),


where D is a diagonal matrix containing the eigenvalues of A, and columns of V are the eigenvectors of A corresponding to each respective eigenvalue. The very same decomposition can be obtained in numpy using

DD,V = np.linalg.eig(A)
D = np.asmatrix(np.diag(DD))


I usually prefer using ndarrays instead of matrices, but this way V*D*np.linalg.inv(V) would really correspond to the matrix product of the three matrices, rather than calling np.dot twice.

Now, rewrite your equation again:

x' = A*x + b
x' = V*D*V^(-1)*x + b
V^(-1)*x' = D*V^(-1)*x + V^(-1)*b


By defining the auxiliary variables

X = V^(-1)*x
B = V^(-1)*b


we obtain

X' = D*X + B


i.e. the usual inhomogeneous form, but now D is a diagonal matrix containing the eigenvalues of A in the diagonal.

Since A is singular, some of the eigenvalues are zero. Look for zero elements in D (well, you can do that already with DD from eig()), and you'll know that they behave trivially during time-evolution. The remaining variables behave well, although at this point we see that the equations for X are decoupled due to D being diagonal, so you could integrate each independently and analytically. For this you need to first go from your initial condition x0 to X0 = np.linalg.inv(V)*x0, then after solving the equations, back to x = V*X.

Caveat: as I said, I'm not an expert in this subject. I can easily imagine that the inversions involved in the diagonalization can be a numerical issue in practical applications. So I'd first test if the matrix is singular, and only carry on with this procedure if it is (or nearly is). It's possible that the above carries a lot of error, in which case numerical integration might be better (I really can't tell).
","on my compiled linux kernel :


times: 8ms, 7ms
times: 5ms, 4ms
times: 4ms, 4ms
times: 8ms, 8ms
times: 4ms, 4ms
times: 5ms, 4ms
times: 4ms, 8ms
times: 8ms, 8ms
times: 8ms, 8ms
times: 4ms, 5ms


Intel(R) Core(TM) i5-4300U CPU @ 1.90GHz

be sure your processor runs at fixed speed, noswap.
/tmp is mounted in RAM.
",false,
https://stackoverflow.com/questions/22591174/pandas-multiple-conditions-while-indexing-data-frame-unexpected-behavior,false,The issue does not meet the criteria for deeper analysis as it stems from a misunderstanding of logical operators and conditions in pandas rather than a problem with the API itself.,,,,,,,,,,,,false,
https://stackoverflow.com/questions/28056404,false,The issue does not meet the criteria for deeper analysis as it does not involve an API exhibiting unexpected failures or unpredictable behaviors under specific runtime conditions.,,,,,,,MATLAB and SciPy give different results for &#39;buttord&#39; function,"I'm trying to design an analog Butterworth filter using the buttord function (actually, I'm porting a program where this function is called from MATLAB to Python). 

My parameters are:

Passband frequency (Fp) = 10 Hz, giving Wp = 2*pi*10 Hz

Stopband frequency (Fs) = 100 Hz, giving Ws = 2*pi*100 Hz

The passband and stopband losses/attenuations (Rp, Rs) are 3 and 80 dB respectively.

In MATLAB I use this code:

Wp = 2 * pi * 10
Ws = 2 * pi * 100
Rp = 3
Rs = 80
[N, Wn] = buttord(Wp, Ws, Rp, Rs, 's')


that gives me N = 5, Wn = 99.581776302.

In SciPy I tried to do the same:

from numpy import pi
from scipy import signal
Wp = 2 * pi * 10
Ws = 2 * pi * 100
Rp = 3
Rs = 80
(N, Wn) = signal.buttord(Wp, Ws, Rp, Rs, analog=True)


and I get N = 5 and Wn = 62.861698649592753. Wn is different than the value that MATLAB gives, and is strangely close to Wp. What is wrong here?

Digging into SciPy's sources and issues, I found this pull request which might explain things: turns out MATLAB and SciPy have different design goals (MATLAB tries to optimize for matching the stopband frequency and SciPy tries to optimize for matching the passband frequency).

I'm using MATLAB R2013a, Python 3.4.2 and SciPy 0.15.0 if that matters.
","(I also posted the following on the scipy mailing list.)

When you design a Butterworth filter with buttord, there aren't enough
degrees of freedom to meet all the design constraints exactly.  So there
is a choice of which end of the transition region hits the constraints
and which end is ""over-designed"".  A change made in scipy 0.14.0 switched that choice from the stop-band edge to the pass-band edge.

A picture will make it clear.  The script below generates the following plot.  (I changed Rp from 3 to 1.5.  -3 dB coincides with the gain at Wn, that's why your Wn was the same as Wp.)  The filters generated using either the old or new convention both satisfy the design constraints.  With the new convention, the response just bumps against the constraint at the end of the pass-band.



import numpy as np
from scipy.signal import buttord, butter, freqs
import matplotlib.pyplot as plt


# Design results for:
Wp = 2*np.pi*10
Ws = 2*np.pi*100
Rp = 1.5      # instead of 3
Rs = 80

n_old = 5
wn_old = 99.581776302787929

n_new, wn_new = buttord(Wp, Ws, Rp, Rs, analog=True)

b_old, a_old = butter(n_old, wn_old, analog=True)
w_old, h_old = freqs(b_old, a_old)

b_new, a_new = butter(n_new, wn_new, analog=True)
w_new, h_new = freqs(b_new, a_new)


db_old = 20*np.log10(np.abs(h_old))
db_new = 20*np.log10(np.abs(h_new))

plt.semilogx(w_old, db_old, 'b--', label='old')
plt.axvline(wn_old, color='b', alpha=0.25)
plt.semilogx(w_new, db_new, 'g', label='new')
plt.axvline(wn_new, color='g', alpha=0.25)

plt.axhline(-3, color='k', ls=':', alpha=0.5, label='-3 dB')

plt.xlim(40, 1000)
plt.ylim(-100, 5)

xbounds = plt.xlim()
ybounds = plt.ylim()
rect = plt.Rectangle((Wp, ybounds[0]), Ws - Wp, ybounds[1] - ybounds[0],
                     facecolor=""#000000"", edgecolor='none', alpha=0.1, hatch='//')
plt.gca().add_patch(rect)
rect = plt.Rectangle((xbounds[0], -Rp), Wp - xbounds[0], 2*Rp,
                     facecolor=""#FF0000"", edgecolor='none', alpha=0.25)
plt.gca().add_patch(rect)
rect = plt.Rectangle((Ws, ybounds[0]), xbounds[1] - Ws, -Rs - ybounds[0],
                     facecolor=""#FF0000"", edgecolor='none', alpha=0.25)
plt.gca().add_patch(rect)

plt.annotate(""Pass"", (0.5*(xbounds[0] + Wp), Rp+0.5), ha='center')
plt.annotate(""Stop"", (0.5*(Ws + xbounds[1]), -Rs+0.5), ha='center')
plt.annotate(""Don't Care"", (0.1*(8*Wp + 2*Ws), -Rs+10), ha='center')

plt.legend(loc='best')
plt.xlabel('Frequency [rad/s]')
plt.ylabel('Gain [dB]')
plt.show()

",,,false,
https://stackoverflow.com/questions/20600527,false,The issue does not meet the criteria for deeper analysis as it stems from a misunderstanding of logical operators and conditions in pandas rather than a problem with the API itself.,,,,,,,Python deadlock related to packaging and mayavi?,"I have a curious problem that I hope someone can shed some light on.

I have a complex piece of code, that started as a directory full of scripts, that I decided to rework into a package. This code change appears to have been the trigger for some strange deadlocks to appear.

Below is an attempt at canonical reproduction of the problem; which fails, in the sense that this code runs as expected. Actually reproducing the issue may require a lot of code; but I cannot for the life of my imagine what is different for the offending code snippet in-context.

import numpy as np
from scipy.sparse import csr_matrix
from threading import Thread

def dummy():
    print 'this is printed'
    I = np.eye(3)
    print 'all is still fine'
    csr_matrix(I)
    print 'this is never printed; csr_matrix appears to be a trigger for deadlock'
    print np.ones(4)
    print 'same problem; somehow, printing ndarrays is no longer cool either'

thr = Thread(target=dummy)
thr.start()


Perhaps this terse comment in the docs is related? I am not sure I fully appreciate what is being said here

http://docs.python.org/2/library/threading#importing-in-threaded-code

Firstly, other than in the main module, an import should not have the side effect of spawning a new thread and then waiting for that thread in any way. Failing to abide by this restriction can lead to a deadlock if the spawned thread directly or indirectly attempts to import a module.

Some context: I am using python 2.7, numpy 1.8, where I try to spawn this new thread from within a mayavi/traitsui thread (which I dont see why it should be relevant, and which worked fine before the package structure, but ok). Also, there is a boatload of numpy/scipy code in my spawned thread that executes perfectly fine; its just printing ndarrays and creating sparse matrices which so far have proven to be triggers for deadlock.

I am suspecting some funky interaction with mayavi, since closing the mayavi window causes all deadlocked threads to start running again. Perhaps these specific statement trigger the python thread to yield back to the mayavi thread, but they somehow fail to gain focus again?

Any hints that lead to further narrowing down on this mystery are much appreciated!
","From your comments, it looks like you start up the UI event loop at the top level of one of your subsidiary modules. This is not a good idea because it causes exactly the same problems that the documentation alludes to. import foo should never start a UI event loop. The problem is that the main thread grabs the import lock to handle the import of the module. This module starts up the UI event loop before completing the import. This is essentially the same situation as the waiting for the other threads to finish; you are waiting for the UI loop to finish. If your UI starts up other threads, the code that is running in the other threads will not be able to import anything (both csr_matrix() and ndarray.__repr__() import other modules) because the main thread is still holding onto the import lock.
",,,false,
https://stackoverflow.com/questions/6968219,false,"The issue does not meet the criteria for deeper analysis as it pertains to setting up Python, NumPy, and SciPy on OS X Lion rather than involving an API exhibiting unexpected failures or unpredictable behaviors under specific runtime conditions.",,,,,,,scipy on os x lion,"I'm am trying to get my python/numpy/scipy working environment running on OS X Lion.

I've already managed to compile numpy and scipy from their latest sourcecode versions with the usual

python setup.py build
python setup.py install


I do run the Python 2.7.2 version downloaded from python.org:

tobi-mbp:~ tobi$ python
Python 2.7.2 (v2.7.2:8527427914a2, Jun 11 2011, 15:22:34) 
[GCC 4.2.1 (Apple Inc. build 5666) (dot 3)] on darwin
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
&gt;&gt;&gt;


If i than import numpy it shows:

&gt;&gt;&gt; import numpy
&gt;&gt;&gt; numpy.__version__
'1.5.1'
&gt;&gt;&gt; numpy.__file__
'/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/numpy/__init__.pyc'
&gt;&gt;&gt; 


the system instalation of numpy. Whereas

&gt;&gt;&gt; import scipy
&gt;&gt;&gt; scipy.__version__
'0.10.0.dev'
&gt;&gt;&gt; scipy.__file__
'/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/scipy/__init__.pyc'
&gt;&gt;&gt;


gives the installed version of scipy. That means the scipy.test() fails and actually gets stuck.

I did try pip and easy_install as well but the fail to compile scipy and with numpy the same strange bahavior occours. Same for the precompiled binarys provided on sourcefourge.

As this seems to be an issue with PYTHONPATH:

&gt;&gt;&gt; import sys, os, pprint
&gt;&gt;&gt; pprint.pprint(sys.path)
['',
 '/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/distribute-0.6.19-py2.7.egg',
 '/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/pip-1.0.2-py2.7.egg',
 '/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python',
 '/Library/Python/2.7/site-packages/nose-1.1.2-py2.7.egg',
 '/Library/Python/2.7/site-packages/pip-1.0.2-py2.7.egg',
 '/Library/Frameworks/Python.framework/Versions/2.7/lib/python27.zip',
 '/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7',
 '/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/plat-darwin',
 '/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/plat-mac',
 '/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/plat-mac/lib-scriptpackages',
 '/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/lib-tk',
 '/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/lib-old',
 '/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/lib-dynload',
 '/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages',
 '/Library/Python/2.7/site-packages']
&gt;&gt;&gt; pprint.pprint([p for p in os.environ.items() if p[0].startswith('PYTHON')])
[]
&gt;&gt;&gt; 


And a list of the contents of site-packages

tobi-mbp:site-packages tobi$ ls -l
total 752
-rw-rw-r--   1 root  admin     119 12 Jun 00:25 README
drwxr-xr-x  10 root  admin     340  6 Aug 19:47 distribute-0.6.19-py2.7.egg
-rw-r--r--   1 tobi  admin     237  6 Aug 19:48 easy-install.pth
drwxrwxr-x  48 tobi  admin    1632  6 Aug 18:19 numpy
drwxrwxr-x   6 tobi  admin     204 20 Jul 20:44 numpy-1.6.1-py2.7.egg-info
-rw-r--r--   1 root  admin    1670  6 Aug 18:19 numpy-2.0.0.dev_26aa3cf-py2.7.egg-info
drwxr-xr-x   4 root  admin     136  6 Aug 19:48 pip-1.0.2-py2.7.egg
drwxr-xr-x   8 tobi  admin     272  6 Aug 19:48 readline-6.2.0-py2.7.egg-info
-rwxr-xr-x   1 tobi  admin  357048  6 Aug 19:48 readline.so
drwxrwxr-x  42 tobi  admin    1428  6 Aug 18:21 scipy
-rw-r--r--   1 root  admin    1768  6 Aug 18:21 scipy-0.10.0.dev-py2.7.egg-info
drwxrwxr-x   6 tobi  admin     204 27 Feb 14:00 scipy-0.9.0-py2.7.egg-info
-rw-r--r--   1 tobi  admin     144  6 Aug 19:47 setuptools-0.6c11-py2.7.egg-info
-rw-r--r--   1 tobi  admin      30  6 Aug 19:47 setuptools.pth

tobi-mbp:site-packages tobi$ more easy-install.pth
import sys; sys.__plen = len(sys.path)
./distribute-0.6.19-py2.7.egg
./pip-1.0.2-py2.7.egg
import sys; new=sys.path[sys.__plen:]; del sys.path[sys.__plen:]; p=getattr(sys,'__egginsert',0); sys.path[p:p]=new; sys.__egginsert = p+len(new)

tobi-mbp:site-packages tobi$ more setuptools.pth 
./distribute-0.6.19-py2.7.egg


I do see the System Extras folder in the path, now the question is why and how I can get rid of it.

thanks for your help

cheers Tobi
","There appears to be a path issue somewhere. Perhaps you are setting PYTHONPATH?  When you are running the python.org 2.7.2, the Apple-supplied Python 2.7.1's Extras directory should not be on sys.path.  To help figure out what's going on, launch the Python 2.7.2 and examine the following:

&gt;&gt;&gt; import sys, os, pprint
&gt;&gt;&gt; pprint.pprint(sys.path)
&gt;&gt;&gt; pprint.pprint([p for p in os.environ.items() if p[0].startswith('PYTHON')])


Also the contents of the site-packages directory:

$ cd /Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/
$ ls -l
$ more easy-install.pth


and the contents of any other .pth files there.  

If it is still not clear what is going on, edit your answer to show the results of the above.

UPDATE:

Thanks for providing the requested additional info.  It confirms that sys.path does include the Extras directory from the Apple-supplied system Python 2.7.  There is no obvious reason why that should be happening.  Without more information, I can only speculate.  As unlikely as it may seem, my best guess at this point is that you accidentally copied some files from the one Python to the other.  The Apple-supplied Python has a patch in site.py to add the Extras directory:

$ cd /System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7
$ ls -l site.py
-rw-r--r--  1 root  wheel  19929 Jun 16 17:03 site.py
$ grep Extras site.py
            sitepackages.append(os.path.join(prefix, ""Extras"", ""lib"", ""python""))


You should not see that in the python.org Python:

$ cd /Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7
$ ls -l site.py
-rw-rw-r--  1 root  admin  20288 Jun 11 15:25 site.py
$ grep Extras site.py
$


You could also do a more complete recursive search of all of the directories on sys.path for pth files but that seems unlikely to find anything.

You also appear to have installed two versions each of numpy and scipy in your site-packages, one the latest released version and the other cutting-edge version.   I don't know whether you took steps to clear out the numpy and scipy directories between installs or whether that can cause problems but it might.

At this point, without more information, I would recommend that you completely delete the 2.7.2 installation and start over:

$ sudo rm -rf /Library/Frameworks/Python.framework/Versions/2.7


You might consider using another approach like the binary numpy/scipy installers or install using MacPorts.  A better source of advice might be found on one of the project's mailing lists.  Good luck!
",,,false,
https://stackoverflow.com/questions/41049834,false,The issue does not meet the criteria for deeper analysis as it involves a problem with module availability in a virtual environment rather than an API exhibiting unexpected failures or unpredictable behaviors under specific runtime conditions.,,,,,,,Global Modules are not available in VirtualEnv,"I'm using WinPython to work with libraries such as Scipy. I can not install those inside my virtual environment because blas/lapack is not available. The solution was to use the modules (theano/scipy/numpy) which are already installed on my system inside my virtual environment. 
Here is what I tried:

open console (cmd)

python
&gt;&gt;&gt; import numpy


this works, now creating the virtual environment

set VIRTUALENV_PYTHON=C:\WinPython-64bit-3.5.1.1\python-3.5.1.amd64\python.exe
set VIRTUALENV_EXTRA_SEARCH_DIR=""C:\WinPython-64bit-3.5.1.1\python-3.5.1.amd64\libs C:\WinPython-64bit-3.5.1.1\python-3.5.1.amd64\Lib C:\WinPython-64bit-3.5.1.1\python-3.5.1.amd64\DLLs C:\WinPython-64bit-3.5.1.1\python-3.5.1.amd64\ C:\WinPython-64bit-3.5.1.1\python-3.5.1.amd64\Scripts""
virtualenv --system-site-packages -v --always-copy venv


or simply:

virtualenv --system-site-packages venv


but no matter what I try I can't import numpy when entering the venv.

venv\Scripts\activate
python
&gt;&gt;&gt; import numpy
ImportError: No module named 'numpy'


Strange. So I compared the sys.path inside the venv:

&gt;&gt;&gt; sys.path
['', 'C:\\WinPython-64bit-3.5.1.1\\python-3.5.1.amd64', 'C:\\WinPython-64bit-3.5.1.1\\python-3.5.1.amd64\\Scripts', 'C:\\WinPython-64bit-3.5.1.1\\python-3.5.1.amd64\\DLLs', 'C:\\WinPython-64bit-3.5.1.1\\python-3.5.1.amd64\\Lib', 'C:\\WinPython-64bit-3.5.1.1\\python-3.5.1.amd64\\libs', 'C:\\Users\\user\\venv\\Scripts\\python35.zip', 'C:\\Users\\user\\venv\\DLLs', 'C:\\Users\\user\\venv\\lib', 'C:\\Users\\user\\venv\\Scripts', 'C:\\Users\\user\\venv', 'C:\\Users\\user\\venv\\lib\\site-pack
ages']


with the original one (outside):

&gt;&gt;&gt; sys.path
['', 'C:\\WinPython-64bit-3.5.1.1\\python-3.5.1.amd64', 'C:\\WinPython-64bit-3.5.1.1\\python-3.5.1.amd64\\Scripts', 'C:\\WinPython-64bit-3.5.1.1\\python-3.5.1.amd64\\DLLs', 'C:\\WinPython-64bit-3.5.1.1\\python-3.5.1.amd64\\Lib', 'C:\\WinPython-64bit-3.5.1.1\\python-3.5.1.amd64\\libs', 'C:\\WinPython-64bit-3.5.1.1\\python-3.5.1.amd64\\python35.zip', 'C:\\WinPython-64bit-3.5.1.1\\python-3.5.1.amd64\\lib\\site-packages', 'C:\\WinPython-64bit-3.5.1.1\\python-3.5.1.amd64\\lib\\site-packages\\win32', 'C:\\WinPython-64bit-3.5.1.1\\python-3.5.1.amd64\\lib\\site-packages\\win32\\lib', 'C:\\WinPython-64bit-3.5.1.1\\python-3.5.1.amd64\\lib\\site-packages\\Pythonwin']


as you can see, not all the entries got copied over. Why is that and how can I fix it?

And yes I read virtualenv --system-site-packages not using system site packages and deleted my PYTHONPATH variable but the issue remains.
","Figured it out eventually. I did not restart after removing my PYTHONPATH variable. So if you have the same issue:


Delete your PYTHONPATH env. variable. 
Reboot!

",,,false,
https://stackoverflow.com/questions/18705141,true,"The issue involves the griddata function from scipy.interpolate, which exhibits unexpected behavior when called in a specific Python file. Further analysis is required.",,,,,,,TypeError when calling scipy griddata,"I'm having an issue with scipy.interpolate's griddata function. I developed a quick interpolator using griddata in a sandbox, then once I had the interpolator how I liked it, a copied the function into a larger model I had developed. Here is the function:

def windGrid(*sensors):
    """"""Creates grids of wind speed components""""""
    xb = [0, 0, num_x, num_x]
    yb = [0, num_y, num_y, 0]

    xs = [s.lng for s in sensors]
    ys = [s.lat for s in sensors]
    us = [s.u for s in sensors]
    vs = [s.v for s in sensors]

    ub, vb = boundaryWeighting(*sensors)

    x = xb+xs
    y = yb+ys
    u = ub+us
    v = vb+vs

    x_grid, y_grid = np.mgrid[0:num_x, 0:num_y]

    zx = griddata((x, y), u, (x_grid, y_grid))
    zy = griddata((x, y), v, (x_grid, y_grid))

   return zx, zy


The boundaryWeighting() function simply returns wind speed components of the corners of the grid so that interpolations can be made within. Now here is the strange part... When I call griddata in the sandbox (i.e. without other independent functions around it), it works fine. However, when called in the python file to which it was copied, it returns this error:

TypeError: griddata() takes at least 5 arguments (3 given)


According to the Scipy documentation, griddata takes 5 arguments, but the last two are optional. I tried inserting the optional arguments (i.e. method and fill_val), but then I got this error:

TypeError: griddata() got an unexpected keyword argument 'method'


So it seems that the python interpreter is referencing a different version/function of griddata in the different python files.

Here is why this doesn't make sense:
1. Both files are in the same directory, and are using the same interpreter
2. Both files reference the same Scipy version, 0.12.0

I've checked all my variable/function assignments and there are no overlaps. Any clues as to why this would be happening?

Thanks
","Do you have something like from matplotlib.pylab import * in one of your files?  You might be getting a namespace collision with the matplotlib version of griddata.
",,,false,
