search_keywords,issue_link,issue_title,issue_body,issue_score,issue_views,answer_1,answer_2,answer_3
Seaborn unexpected behavior,https://stackoverflow.com/questions/59150064,Seaborn line plot: don&#39;t aggregate and plot all observations,"My raw data look like:

&gt;df
   Jan   Feb    ...  Dec
0   2     4           4
1   5     3           3


where each row is monthly production of a solar panel. I wanted to plot each row, so I can visually check if some panel were having unexpected behavior. So I convert to long and plot it using seaborn:

df = pd.melt(df)
ax = sns.lineplot(x = variable, y = value, data  = df)


However this just give mean for each month.
",2,1494,"IIUC, you can just do:
df.T.plot()

and output:

If you insist on seaborn, you need to pass hue, something like
sns.lineplot(data=df.stack().reset_index(name='val'),
             x='level_1',
             y='val',
             hue='level_0')

where level_0 and level_1 come from .reset_index of unnamed sources index.
You need to do more for correct ordering of x-axis.
",,
Seaborn unexpected behavior,https://stackoverflow.com/questions/47488856,seaborn in Python : barplot appears upside down,"I am learning seaborn and I am getting some unexpected behavior.

This reproducible example uses the surveys.csv dataset that can be found in this link: http://www.datacarpentry.org/python-ecology-lesson/setup/

My code is the following:

import pandas as pd
import numpy as np
import matplotlib as mpl
import matplotlib.pyplot as plt
import seaborn as sns
sns.set(style=""whitegrid"", color_codes=True)

surveys_df = pd.read_csv(""surveys.csv"")
avg_weight = surveys_df.groupby(""plot_id"")[""weight""].mean().to_frame()
avg_weight
    weight
plot_id 
1   51.822911
2   52.251688
3   32.654386
4   47.928189
5   40.947802
6   36.738893
7   20.663009
8   47.758001
9   51.432358
10  18.541219
11  43.451757
12  49.496169
13  40.445660
14  46.277199
15  27.042578
16  24.585417
17  47.889593
18  40.005922
19  21.105166
20  48.665303
21  24.627794
22  54.146379
23  19.634146
24  43.679167

sns.barplot(x = avg_weight.index.values, y = ""weight"", 
            data = avg_weight, palette = sns.palplot(sns.diverging_palette(150, 275, s=80, l=55, n=9)))
plt.xlabel('Animal id')
plt.ylabel('Average Weight')
plt.title('Average Weight by Animal')




The barplot appears upside down.

Why this happens and how can I correct it?

Your advice will be appreciated.

PS: Somehow this problem relates to the value passed to the palette argument, as it was resolved when I chose palette = sns.color_palette(""coolwarm"", 7). Still, I can not understand why.
",1,1079,"By invoking sns.palplot, you are making another plot, which causes the figure properties to be set incorrectly. Remove that, and you should be good:

sns.barplot(x = avg_weight.index.values, y = ""weight"", 
            data = avg_weight, palette = sns.diverging_palette(150, 275, s=80, l=55, n=9))
plt.xlabel('Animal id')
plt.ylabel('Average Weight')
plt.title('Average Weight by Animal')



",,
Seaborn unexpected behavior,https://stackoverflow.com/questions/73374386,"why is ray Tune with pytorch HPO error &#39;trials did not complete, incomplete trials&#39;?","Could someone explain why this code (that I took from here):
## Standard libraries
import os
import json
import math
import numpy as np 
import time

## Imports for plotting
import matplotlib.pyplot as plt
#%matplotlib inline 
#from IPython.display import set_matplotlib_formats
#set_matplotlib_formats('svg', 'pdf') # For export
from matplotlib.colors import to_rgb
import matplotlib
matplotlib.rcParams['lines.linewidth'] = 2.0
import seaborn as sns
sns.reset_orig()
sns.set()
import torch_geometric
import torch_geometric.nn as geom_nn
import torch_geometric.data as geom_data
from pytorch_lightning.callbacks import LearningRateMonitor, ModelCheckpoint
## Progress bar
from tqdm.notebook import tqdm

## PyTorch
import torch
import torchmetrics
from torchmetrics.functional import precision_recall
import torch.nn as nn
import torch.nn.functional as F
import torch.utils.data as data
import torch.optim as optim
# Torchvision
import torchvision
from torchvision.datasets import CIFAR10
from torchvision import transforms
# PyTorch Lightning
import pytorch_lightning as pl
from ray import tune

def __init__(self, config):
  super(LightningMNISTClassifier, self).__init__()
  self.layer_1_size = config[""layer_1_size""]
  self.layer_2_size = config[""layer_2_size""]
  self.lr = config[""lr""]
  self.batch_size = config[""batch_size""]

from ray.tune.integration.pytorch_lightning import TuneReportCallback
callback = TuneReportCallback(
    {
        ""loss"": ""val_loss"",
        ""mean_accuracy"": ""val_accuracy""
    },
    on=""validation_end"")


def train_tune(config, epochs=10, gpus=0):
  model = LightningMNISTClassifier(config)
  trainer = pl.Trainer(
    max_epochs=epochs,
    gpus=gpus,
    progress_bar_refresh_rate=0,
    callbacks=[callback])
  trainer.fit(model)

config = {
  ""layer_1_size"": tune.choice([32, 64, 128]),
  ""layer_2_size"": tune.choice([64, 128, 256]),
  ""lr"": tune.loguniform(1e-4, 1e-1),
  ""batch_size"": tune.choice([32, 64, 128])
}


def train_tune(config, epochs=10, gpus=0):
  model = LightningMNISTClassifier(config)
  trainer = pl.Trainer(
    max_epochs=epochs,
    gpus=gpus,
    progress_bar_refresh_rate=0,
    callbacks=[callback])
  trainer.fit(model)


from functools import partial
tune.run(
  partial(train_tune, epochs=10, gpus=0),
  config=config,
  num_samples=10)

generates this error:
Traceback (most recent call last):
  File ""example_hpo_working.py"", line 89, in &lt;module&gt;
    num_samples=10)
  File ""/root/miniconda3/lib/python3.7/site-packages/ray/tune/tune.py"", line 741, in run
    raise TuneError(""Trials did not complete"", incomplete_trials)
ray.tune.error.TuneError: ('Trials did not complete', [train_tune_6f362_00000, train_tune_6f362_00001, train_tune_6f362_00002, train_tune_6f362_00003, train_tune_6f362_00004, train_tune_6f362_00005, train_tune_6f362_00006, train_tune_6f362_00007, train_tune_6f362_00008, train_tune_6f362_00009])

I can see a similar question was asked here but not answered (the ultimate aim is to use ray hyperparameter optimisation with a pytorch network).
This is the full trace from the code:
2022-08-16 15:44:08,204 WARNING function_runner.py:604 -- Function checkpointing is disabled. This may result in unexpected behavior when using checkpointing features or certain schedulers. To enable, set the train function arguments to be `func(config, checkpoint_dir=None)`.
2022-08-16 15:44:08,411 ERROR syncer.py:147 -- Log sync requires rsync to be installed.
== Status ==
Memory usage on this node: 16.8/86.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 1.0/64 CPUs, 0/0 GPUs, 0.0/62.79 GiB heap, 0.0/9.31 GiB objects
Result logdir: /root/ray_results/train_tune_2022-08-16_15-44-08
Number of trials: 10/10 (9 PENDING, 1 RUNNING)
+------------------------+----------+------------------+--------------+----------------+----------------+-------------+
| Trial name             | status   | loc              |   batch_size |   layer_1_size |   layer_2_size |          lr |
|------------------------+----------+------------------+--------------+----------------+----------------+-------------|
| train_tune_43fd5_00000 | RUNNING  | 172.17.0.2:41684 |           64 |             64 |            256 | 0.00233834  |
| train_tune_43fd5_00001 | PENDING  |                  |           64 |             64 |            256 | 0.00155955  |
| train_tune_43fd5_00002 | PENDING  |                  |          128 |            128 |             64 | 0.00399358  |
| train_tune_43fd5_00003 | PENDING  |                  |          128 |            128 |             64 | 0.000184477 |

...deleted a few similar lines here

..and then there's:
    

(func pid=41684) 2022-08-16 15:44:10,774        ERROR function_runner.py:286 -- Runner Thread raised error.
(func pid=41684) Traceback (most recent call last):
(func pid=41684)   File ""/root/miniconda3/lib/python3.7/site-packages/ray/tune/function_runner.py"", line 277, in run
(func pid=41684)     self._entrypoint()
(func pid=41684)   File ""/root/miniconda3/lib/python3.7/site-packages/ray/tune/function_runner.py"", line 352, in entrypoint
(func pid=41684)     self._status_reporter.get_checkpoint(),
(func pid=41684)   File ""/root/miniconda3/lib/python3.7/site-packages/ray/util/tracing/tracing_helper.py"", line 462, in _resume_span
(func pid=41684)     return method(self, *_args, **_kwargs)
(func pid=41684)   File ""/root/miniconda3/lib/python3.7/site-packages/ray/tune/function_runner.py"", line 645, in _trainable_func
(func pid=41684)     output = fn()
(func pid=41684)   File ""example_hpo_working.py"", line 76, in train_tune
(func pid=41684)     model = LightningMNISTClassifier(config)
(func pid=41684) NameError: name 'LightningMNISTClassifier' is not defined
2022-08-16 15:44:10,977 ERROR trial_runner.py:886 -- Trial train_tune_43fd5_00000: Error processing event.
NoneType: None
Result for train_tune_43fd5_00000:
  date: 2022-08-16_15-44-10
  experiment_id: c8977e85cbf84a9badff15fb2de6f516
  hostname: 0e26c6a24ffa
  node_ip: 172.17.0.2
  pid: 41684
  timestamp: 1660664650
  trial_id: 43fd5_00000
  
(func pid=41722) 2022-08-16 15:44:13,241        ERROR function_runner.py:286 -- Runner Thread raised error.
(func pid=41722) Traceback (most recent call last):
(func pid=41722)   File ""/root/miniconda3/lib/python3.7/site-packages/ray/tune/function_runner.py"", line 277, in run
(func pid=41722)     self._entrypoint()
(func pid=41722)   File ""/root/miniconda3/lib/python3.7/site-packages/ray/tune/function_runner.py"", line 352, in entrypoint
(func pid=41722)     self._status_reporter.get_checkpoint(),
(func pid=41722)   File ""/root/miniconda3/lib/python3.7/site-packages/ray/util/tracing/tracing_helper.py"", line 462, in _resume_span
(func pid=41722)     return method(self, *_args, **_kwargs)
(func pid=41722)   File ""/root/miniconda3/lib/python3.7/site-packages/ray/tune/function_runner.py"", line 645, in _trainable_func
(func pid=41722)     output = fn()
(func pid=41722)   File ""example_hpo_working.py"", line 76, in train_tune
(func pid=41722)     model = LightningMNISTClassifier(config)
(func pid=41722) NameError: name 'LightningMNISTClassifier' is not defined
(func pid=41720) 2022-08-16 15:44:13,253        ERROR function_runner.py:286 -- Runner Thread raised error.
(func pid=41720) Traceback (most recent call last):
(func pid=41720)   File ""/root/miniconda3/lib/python3.7/site-packages/ray/tune/function_runner.py"", line 277, in run
(func pid=41720)     self._entrypoint()
(func pid=41720)   File ""/root/miniconda3/lib/python3.7/site-packages/ray/tune/function_runner.py"", line 352, in entrypoint
(func pid=41720)     self._status_reporter.get_checkpoint(),
(func pid=41720)   File ""/root/miniconda3/lib/python3.7/site-packages/ray/util/tracing/tracing_helper.py"", line 462, in _resume_span
(func pid=41720)     return method(self, *_args, **_kwargs)
(func pid=41720)   File ""/root/miniconda3/lib/python3.7/site-packages/ray/tune/function_runner.py"", line 645, in _trainable_func
(func pid=41720)     output = fn()
(func pid=41720)   File ""example_hpo_working.py"", line 76, in train_tune
(func pid=41720)     model = LightningMNISTClassifier(config)
(func pid=41720) NameError: name 'LightningMNISTClassifier' is not defined
(func pid=41718) 2022-08-16 15:44:13,253        ERROR function_runner.py:286 -- Runner Thread raised error.
(func pid=41718) Traceback (most recent call last):
(func pid=41718)   File ""/root/miniconda3/lib/python3.7/site-packages/ray/tune/function_runner.py"", line 277, in run
(func pid=41718)     self._entrypoint()
(func pid=41718)   File ""/root/miniconda3/lib/python3.7/site-packages/ray/tune/function_runner.py"", line 352, in entrypoint
(func pid=41718)     self._status_reporter.get_checkpoint(),
(func pid=41718)   File ""/root/miniconda3/lib/python3.7/site-packages/ray/util/tracing/tracing_helper.py"", line 462, in _resume_span
(func pid=41718)     return method(self, *_args, **_kwargs)
(func pid=41718)   File ""/root/miniconda3/lib/python3.7/site-packages/ray/tune/function_runner.py"", line 645, in _trainable_func
(func pid=41718)     output = fn()
(func pid=41718)   File ""example_hpo_working.py"", line 76, in train_tune
(func pid=41718)     model = LightningMNISTClassifier(config)
(func pid=41718) NameError: name 'LightningMNISTClassifier' is not defined
(func pid=41734) 2022-08-16 15:44:13,340        ERROR function_runner.py:286 -- Runner Thread raised error.
(func pid=41734) Traceback (most recent call last):
(func pid=41734)   File ""/root/miniconda3/lib/python3.7/site-packages/ray/tune/function_runner.py"", line 277, in run
(func pid=41734)     self._entrypoint()
(func pid=41734)   File ""/root/miniconda3/lib/python3.7/site-packages/ray/tune/function_runner.py"", line 352, in entrypoint
(func pid=41734)     self._status_reporter.get_checkpoint(),
(func pid=41734)   File ""/root/miniconda3/lib/python3.7/site-packages/ray/util/tracing/tracing_helper.py"", line 462, in _resume_span
(func pid=41734)     return method(self, *_args, **_kwargs)
(func pid=41734)   File ""/root/miniconda3/lib/python3.7/site-packages/ray/tune/function_runner.py"", line 645, in _trainable_func
(func pid=41734)     output = fn()
(func pid=41734)   File ""example_hpo_working.py"", line 76, in train_tune
(func pid=41734)     model = LightningMNISTClassifier(config)
(func pid=41734) NameError: name 'LightningMNISTClassifier' is not defined
(func pid=41732) 2022-08-16 15:44:13,325        ERROR function_runner.py:286 -- Runner Thread raised error.
(func pid=41732) Traceback (most recent call last):
(func pid=41732)   File ""/root/miniconda3/lib/python3.7/site-packages/ray/tune/function_runner.py"", line 277, in run
(func pid=41732)     self._entrypoint()
(func pid=41732)   File ""/root/miniconda3/lib/python3.7/site-packages/ray/tune/function_runner.py"", line 352, in entrypoint
(func pid=41732)     self._status_reporter.get_checkpoint(),
(func pid=41732)   File ""/root/miniconda3/lib/python3.7/site-packages/ray/util/tracing/tracing_helper.py"", line 462, in _resume_span
(func pid=41732)     return method(self, *_args, **_kwargs)
(func pid=41732)   File ""/root/miniconda3/lib/python3.7/site-packages/ray/tune/function_runner.py"", line 645, in _trainable_func
(func pid=41732)     output = fn()
(func pid=41732)   File ""example_hpo_working.py"", line 76, in train_tune
(func pid=41732)     model = LightningMNISTClassifier(config)
(func pid=41732) NameError: name 'LightningMNISTClassifier' is not defined
(func pid=41728) 2022-08-16 15:44:13,309        ERROR function_runner.py:286 -- Runner Thread raised error.
(func pid=41728) Traceback (most recent call last):
(func pid=41728)   File ""/root/miniconda3/lib/python3.7/site-packages/ray/tune/function_runner.py"", line 277, in run
(func pid=41728)     self._entrypoint()
(func pid=41728)   File ""/root/miniconda3/lib/python3.7/site-packages/ray/tune/function_runner.py"", line 352, in entrypoint
(func pid=41728)     self._status_reporter.get_checkpoint(),
(func pid=41728)   File ""/root/miniconda3/lib/python3.7/site-packages/ray/util/tracing/tracing_helper.py"", line 462, in _resume_span
(func pid=41728)     return method(self, *_args, **_kwargs)
(func pid=41728)   File ""/root/miniconda3/lib/python3.7/site-packages/ray/tune/function_runner.py"", line 645, in _trainable_func
(func pid=41728)     output = fn()
(func pid=41728)   File ""example_hpo_working.py"", line 76, in train_tune
(func pid=41728)     model = LightningMNISTClassifier(config)
(func pid=41728) NameError: name 'LightningMNISTClassifier' is not defined
(func pid=41730) 2022-08-16 15:44:13,272        ERROR function_runner.py:286 -- Runner Thread raised error.
(func pid=41730) Traceback (most recent call last):
(func pid=41730)   File ""/root/miniconda3/lib/python3.7/site-packages/ray/tune/function_runner.py"", line 277, in run
(func pid=41730)     self._entrypoint()
(func pid=41730)   File ""/root/miniconda3/lib/python3.7/site-packages/ray/tune/function_runner.py"", line 352, in entrypoint
(func pid=41730)     self._status_reporter.get_checkpoint(),
(func pid=41730)   File ""/root/miniconda3/lib/python3.7/site-packages/ray/util/tracing/tracing_helper.py"", line 462, in _resume_span
(func pid=41730)     return method(self, *_args, **_kwargs)
(func pid=41730)   File ""/root/miniconda3/lib/python3.7/site-packages/ray/tune/function_runner.py"", line 645, in _trainable_func
(func pid=41730)     output = fn()
(func pid=41730)   File ""example_hpo_working.py"", line 76, in train_tune
(func pid=41730)     model = LightningMNISTClassifier(config)
(func pid=41730) NameError: name 'LightningMNISTClassifier' is not defined
2022-08-16 15:44:13,444 ERROR trial_runner.py:886 -- Trial train_tune_43fd5_00003: Error processing event.
NoneType: None
Result for train_tune_43fd5_00003:
  date: 2022-08-16_15-44-13
  experiment_id: 02204d81b72943e3bbfcc822d35f02a0
  hostname: 0e26c6a24ffa
  node_ip: 172.17.0.2
  pid: 41722
  timestamp: 1660664653
  trial_id: 43fd5_00003
  
(func pid=41724) 2022-08-16 15:44:13,457        ERROR function_runner.py:286 -- Runner Thread raised error.
(func pid=41724) Traceback (most recent call last):
(func pid=41724)   File ""/root/miniconda3/lib/python3.7/site-packages/ray/tune/function_runner.py"", line 277, in run
(func pid=41724)     self._entrypoint()
(func pid=41724)   File ""/root/miniconda3/lib/python3.7/site-packages/ray/tune/function_runner.py"", line 352, in entrypoint
(func pid=41724)     self._status_reporter.get_checkpoint(),
(func pid=41724)   File ""/root/miniconda3/lib/python3.7/site-packages/ray/util/tracing/tracing_helper.py"", line 462, in _resume_span
(func pid=41724)     return method(self, *_args, **_kwargs)
(func pid=41724)   File ""/root/miniconda3/lib/python3.7/site-packages/ray/tune/function_runner.py"", line 645, in _trainable_func
(func pid=41724)     output = fn()
(func pid=41724)   File ""example_hpo_working.py"", line 76, in train_tune
(func pid=41724)     model = LightningMNISTClassifier(config)
(func pid=41724) NameError: name 'LightningMNISTClassifier' is not defined
== Status ==
Current time: 2022-08-16 15:44:13 (running for 00:00:05.24)
Memory usage on this node: 17.6/86.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 8.0/64 CPUs, 0/0 GPUs, 0.0/62.79 GiB heap, 0.0/9.31 GiB objects
Result logdir: /root/ray_results/train_tune_2022-08-16_15-44-08
Number of trials: 10/10 (2 ERROR, 8 RUNNING)
+------------------------+----------+------------------+--------------+----------------+----------------+-------------+
| Trial name             | status   | loc              |   batch_size |   layer_1_size |   layer_2_size |          lr |
|------------------------+----------+------------------+--------------+----------------+----------------+-------------|
| train_tune_43fd5_00001 | RUNNING  | 172.17.0.2:41718 |           64 |             64 |            256 | 0.00155955  |
| train_tune_43fd5_00002 | RUNNING  | 172.17.0.2:41720 |          128 |            128 |             64 | 0.00399358  |
| train_tune_43fd5_00004 | RUNNING  | 172.17.0.2:41724 |          128 |             64 |            128 | 0.0221855   |
| train_tune_43fd5_00005 | RUNNING  | 172.17.0.2:41726 |           64 |            128 |            128 | 0.00041038  |
| train_tune_43fd5_00006 | RUNNING  | 172.17.0.2:41728 |           64 |             64 |            256 | 0.0105243   |
| train_tune_43fd5_00007 | RUNNING  | 172.17.0.2:41730 |          128 |             32 |            256 | 0.000929454 |
| train_tune_43fd5_00008 | RUNNING  | 172.17.0.2:41732 |           64 |             64 |            128 | 0.00176483  |
| train_tune_43fd5_00009 | RUNNING  | 172.17.0.2:41734 |          128 |             32 |            256 | 0.000113077 |
| train_tune_43fd5_00000 | ERROR    | 172.17.0.2:41684 |           64 |             64 |            256 | 0.00233834  |
| train_tune_43fd5_00003 | ERROR    | 172.17.0.2:41722 |          128 |            128 |             64 | 0.000184477 |
+------------------------+----------+------------------+--------------+----------------+----------------+-------------+
Number of errored trials: 2
+------------------------+--------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Trial name             |   # failures | error file                                                                                                                                                        |
|------------------------+--------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| train_tune_43fd5_00000 |            1 | /root/ray_results/train_tune_2022-08-16_15-44-08/train_tune_43fd5_00000_0_batch_size=64,layer_1_size=64,layer_2_size=256,lr=0.0023_2022-08-16_15-44-08/error.txt  |
| train_tune_43fd5_00003 |            1 | /root/ray_results/train_tune_2022-08-16_15-44-08/train_tune_43fd5_00003_3_batch_size=128,layer_1_size=128,layer_2_size=64,lr=0.0002_2022-08-16_15-44-10/error.txt |
+------------------------+--------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------+

2022-08-16 15:44:13,487 ERROR trial_runner.py:886 -- Trial train_tune_43fd5_00001: Error processing event.
NoneType: None
Result for train_tune_43fd5_00001:
  date: 2022-08-16_15-44-13
  experiment_id: e738348e77c64919931d70c916cbfaf8
  hostname: 0e26c6a24ffa
  node_ip: 172.17.0.2
  pid: 41718
  timestamp: 1660664653
  trial_id: 43fd5_00001
  
2022-08-16 15:44:13,490 ERROR trial_runner.py:886 -- Trial train_tune_43fd5_00007: Error processing event.
NoneType: None
Result for train_tune_43fd5_00007:
  date: 2022-08-16_15-44-13
  experiment_id: f79be7b9e98a43f1a41893071c4e1f6b
  hostname: 0e26c6a24ffa
  node_ip: 172.17.0.2
  pid: 41730
  timestamp: 1660664653
  trial_id: 43fd5_00007
  
2022-08-16 15:44:13,493 ERROR trial_runner.py:886 -- Trial train_tune_43fd5_00002: Error processing event.
NoneType: None
Result for train_tune_43fd5_00002:
  date: 2022-08-16_15-44-13
  experiment_id: 8e7422287e3e44f9b2e7b249a8ae18cd
  hostname: 0e26c6a24ffa
  node_ip: 172.17.0.2
  pid: 41720
  timestamp: 1660664653
  trial_id: 43fd5_00002
  
2022-08-16 15:44:13,512 ERROR trial_runner.py:886 -- Trial train_tune_43fd5_00006: Error processing event.
NoneType: None
Result for train_tune_43fd5_00006:
  date: 2022-08-16_15-44-13
  experiment_id: 2d56b152a6a34e1f9e26dad1aec25d00
  hostname: 0e26c6a24ffa
  node_ip: 172.17.0.2
  pid: 41728
  timestamp: 1660664653
  trial_id: 43fd5_00006
  
2022-08-16 15:44:13,527 ERROR trial_runner.py:886 -- Trial train_tune_43fd5_00008: Error processing event.
NoneType: None
Result for train_tune_43fd5_00008:
  date: 2022-08-16_15-44-13
  experiment_id: b2158026b3b947bfbb9c3da4e6f7b977
  hostname: 0e26c6a24ffa
  node_ip: 172.17.0.2
  pid: 41732
  timestamp: 1660664653
  trial_id: 43fd5_00008
  
2022-08-16 15:44:13,543 ERROR trial_runner.py:886 -- Trial train_tune_43fd5_00009: Error processing event.
NoneType: None
Result for train_tune_43fd5_00009:
  date: 2022-08-16_15-44-13
  experiment_id: 6b5a73f09241440085bd6c09f6f681e9
  hostname: 0e26c6a24ffa
  node_ip: 172.17.0.2
  pid: 41734
  timestamp: 1660664653
  trial_id: 43fd5_00009
  
(func pid=41726) 2022-08-16 15:44:13,484        ERROR function_runner.py:286 -- Runner Thread raised error.
(func pid=41726) Traceback (most recent call last):
(func pid=41726)   File ""/root/miniconda3/lib/python3.7/site-packages/ray/tune/function_runner.py"", line 277, in run
(func pid=41726)     self._entrypoint()
(func pid=41726)   File ""/root/miniconda3/lib/python3.7/site-packages/ray/tune/function_runner.py"", line 352, in entrypoint
(func pid=41726)     self._status_reporter.get_checkpoint(),
(func pid=41726)   File ""/root/miniconda3/lib/python3.7/site-packages/ray/util/tracing/tracing_helper.py"", line 462, in _resume_span
(func pid=41726)     return method(self, *_args, **_kwargs)
(func pid=41726)   File ""/root/miniconda3/lib/python3.7/site-packages/ray/tune/function_runner.py"", line 645, in _trainable_func
(func pid=41726)     output = fn()
(func pid=41726)   File ""example_hpo_working.py"", line 76, in train_tune
(func pid=41726)     model = LightningMNISTClassifier(config)
(func pid=41726) NameError: name 'LightningMNISTClassifier' is not defined
2022-08-16 15:44:13,660 ERROR trial_runner.py:886 -- Trial train_tune_43fd5_00004: Error processing event.
NoneType: None
Result for train_tune_43fd5_00004:
  date: 2022-08-16_15-44-13
  experiment_id: 60f51e072c7942bdb5d9298e0e147555
  hostname: 0e26c6a24ffa
  node_ip: 172.17.0.2
  pid: 41724
  timestamp: 1660664653
  trial_id: 43fd5_00004
  
2022-08-16 15:44:13,687 ERROR trial_runner.py:886 -- Trial train_tune_43fd5_00005: Error processing event.
NoneType: None
Result for train_tune_43fd5_00005:
  date: 2022-08-16_15-44-13
  experiment_id: 79701d1c19ac4c55b5a73746c1872724
  hostname: 0e26c6a24ffa
  node_ip: 172.17.0.2
  pid: 41726
  timestamp: 1660664653
  trial_id: 43fd5_00005
  
== Status ==
Current time: 2022-08-16 15:44:13 (running for 00:00:05.46)
Memory usage on this node: 16.4/86.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 0/64 CPUs, 0/0 GPUs, 0.0/62.79 GiB heap, 0.0/9.31 GiB objects
Result logdir: /root/ray_results/train_tune_2022-08-16_15-44-08
Number of trials: 10/10 (10 ERROR)
+------------------------+----------+------------------+--------------+----------------+----------------+-------------+
| Trial name             | status   | loc              |   batch_size |   layer_1_size |   layer_2_size |          lr |
|------------------------+----------+------------------+--------------+----------------+----------------+-------------|
| train_tune_43fd5_00000 | ERROR    | 172.17.0.2:41684 |           64 |             64 |            256 | 0.00233834  |
| train_tune_43fd5_00001 | ERROR    | 172.17.0.2:41718 |           64 |             64 |            256 | 0.00155955  |
| train_tune_43fd5_00002 | ERROR    | 172.17.0.2:41720 |          128 |            128 |             64 | 0.00399358  |
| train_tune_43fd5_00003 | ERROR    | 172.17.0.2:41722 |          128 |            128 |             64 | 0.000184477 |
| train_tune_43fd5_00004 | ERROR    | 172.17.0.2:41724 |          128 |             64 |            128 | 0.0221855   |
| train_tune_43fd5_00005 | ERROR    | 172.17.0.2:41726 |           64 |            128 |            128 | 0.00041038  |
| train_tune_43fd5_00006 | ERROR    | 172.17.0.2:41728 |           64 |             64 |            256 | 0.0105243   |
| train_tune_43fd5_00007 | ERROR    | 172.17.0.2:41730 |          128 |             32 |            256 | 0.000929454 |
| train_tune_43fd5_00008 | ERROR    | 172.17.0.2:41732 |           64 |             64 |            128 | 0.00176483  |
| train_tune_43fd5_00009 | ERROR    | 172.17.0.2:41734 |          128 |             32 |            256 | 0.000113077 |
+------------------------+----------+------------------+--------------+----------------+----------------+-------------+
Number of errored trials: 10
+------------------------+--------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Trial name             |   # failures | error file                                                                                                                                                        |
|------------------------+--------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| train_tune_43fd5_00000 |            1 | /root/ray_results/train_tune_2022-08-16_15-44-08/train_tune_43fd5_00000_0_batch_size=64,layer_1_size=64,layer_2_size=256,lr=0.0023_2022-08-16_15-44-08/error.txt  |
| train_tune_43fd5_00001 |            1 | /root/ray_results/train_tune_2022-08-16_15-44-08/train_tune_43fd5_00001_1_batch_size=64,layer_1_size=64,layer_2_size=256,lr=0.0016_2022-08-16_15-44-10/error.txt  |
| train_tune_43fd5_00002 |            1 | /root/ray_results/train_tune_2022-08-16_15-44-08/train_tune_43fd5_00002_2_batch_size=128,layer_1_size=128,layer_2_size=64,lr=0.0040_2022-08-16_15-44-10/error.txt |
| train_tune_43fd5_00003 |            1 | /root/ray_results/train_tune_2022-08-16_15-44-08/train_tune_43fd5_00003_3_batch_size=128,layer_1_size=128,layer_2_size=64,lr=0.0002_2022-08-16_15-44-10/error.txt |
| train_tune_43fd5_00004 |            1 | /root/ray_results/train_tune_2022-08-16_15-44-08/train_tune_43fd5_00004_4_batch_size=128,layer_1_size=64,layer_2_size=128,lr=0.0222_2022-08-16_15-44-10/error.txt |
| train_tune_43fd5_00005 |            1 | /root/ray_results/train_tune_2022-08-16_15-44-08/train_tune_43fd5_00005_5_batch_size=64,layer_1_size=128,layer_2_size=128,lr=0.0004_2022-08-16_15-44-10/error.txt |
| train_tune_43fd5_00006 |            1 | /root/ray_results/train_tune_2022-08-16_15-44-08/train_tune_43fd5_00006_6_batch_size=64,layer_1_size=64,layer_2_size=256,lr=0.0105_2022-08-16_15-44-10/error.txt  |
| train_tune_43fd5_00007 |            1 | /root/ray_results/train_tune_2022-08-16_15-44-08/train_tune_43fd5_00007_7_batch_size=128,layer_1_size=32,layer_2_size=256,lr=0.0009_2022-08-16_15-44-10/error.txt |
| train_tune_43fd5_00008 |            1 | /root/ray_results/train_tune_2022-08-16_15-44-08/train_tune_43fd5_00008_8_batch_size=64,layer_1_size=64,layer_2_size=128,lr=0.0018_2022-08-16_15-44-10/error.txt  |
| train_tune_43fd5_00009 |            1 | /root/ray_results/train_tune_2022-08-16_15-44-08/train_tune_43fd5_00009_9_batch_size=128,layer_1_size=32,layer_2_size=256,lr=0.0001_2022-08-16_15-44-10/error.txt |
+------------------------+--------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------+

Traceback (most recent call last):
  File ""example_hpo_working.py"", line 89, in &lt;module&gt;
    num_samples=10)
  File ""/root/miniconda3/lib/python3.7/site-packages/ray/tune/tune.py"", line 741, in run
    raise TuneError(""Trials did not complete"", incomplete_trials)
ray.tune.error.TuneError: ('Trials did not complete', [train_tune_43fd5_00000, train_tune_43fd5_00001, train_tune_43fd5_00002, train_tune_43fd5_00003, train_tune_43fd5_00004, train_tune_43fd5_00005, train_tune_43fd5_00006, train_tune_43fd5_00007, train_tune_43fd5_00008, train_tune_43fd5_00009])

",0,443,"Is there a longer stacktrace where the real error is printed?
Also could you go to the result folder and see the error file?
Usually result folder is under ~/ray_results.
","I believe you have a typo in your code:
def __init__(self, config):
  super(LightningMNISTClassifier, self).__init__()
  self.layer_1_size = config[""layer_1_size""]
  self.layer_2_size = config[""layer_2_size""]
  self.lr = config[""lr""]
  self.batch_size = config[""batch_size""]

You need to define LightningMNISTClassifier properly. Maybe try using this example?
https://docs.ray.io/en/master/tune/examples/includes/mnist_ptl_mini.html
Note that you may need to install the latest version of Ray.
",
Seaborn unexpected output,https://stackoverflow.com/questions/32117579,pandas plots on Seaborn FacetGrid,"I have a problem in a Qt application when I attempt to plot my dataframe as an area plot with a time index using pandas plotting function in combination with Seaborn's FacetGrids. What happens is that a grid layout is correctly created, but the plots do not appear in these grids. Using a Seaborn plotting function works as expected, though.

I tried to figure out what's going on by isolating the drawing routines from the rest of my code, and I've found a rather unexpected behaviour as shown below (using ipython notebook):



%matplotlib inline

import pandas as pd
import seaborn as sns

df = pd.DataFrame({
        ""Home"": [76, 64, 38, 78, 63,    45, 32, 46, 13, 40],
        ""Away"": [55, 67, 70, 56, 59,    69, 72, 24, 45, 21],
        ""Team"": [""T1""] * 5 +            [""T2""] * 5,
        ""Year"": [""1991"", ""1992"", ""1993"", ""1994"", ""1995""] * 2})




Now, what I want to do is to draw two facets, one for each team. Each facet should show the 'Away' and 'Home' columns as two separate time series. In line with the suggestion in another question (Plotting time series using Seaborn FacetGrid), I wrote a function that calls the pandas plotting function for the subset passed to it by map_dataframe():



def plot_area(data, color):
    data[[""Home"", ""Away""]].index = pd.to_datetime(data[""Year""])
    data[[""Home"", ""Away""]].plot(kind=""area"")




However, when using this function, the result is rather unexpected: the FacetGrid is created and initialized correctly, but the two calls to the pandas method do not use this grid as their plotting region, and they appear elsewhere. 



g = sns.FacetGrid(df, col=""Team"")
g.map_dataframe(plot_area)




&lt;seaborn.axisgrid.FacetGrid at 0x1a25110&gt;


Screenshot of output: 



In the post I linked above, @mwaskom notes that methods called in this way 


  must draw a plot on the ""currently active"" matplotlib Axes.


Perhaps that is the problem here? The code as such appears to be correct, because with a different plotting function, everything works as expected, e.g. with a sns.heatmap():



def plot_heatmap(data, color):
    sns.heatmap(data[[""Home"", ""Away""]])    

g = sns.FacetGrid(df, col=""Team"")
g.map_dataframe(plot_heatmap)




&lt;seaborn.axisgrid.FacetGrid at 0x4a6d110&gt;


Screenshot of output: 



So, my question boils down to this: how do I have to change the function plot_area() so that the axes produced by the pandas plotting function appear on the subplots created by Seaborn's FacetGrid?

(pandas version 0.16.0, Seaborn version 0.6.0, ipython 3.2.1, Python 2.7)


",12,7065,"The comment by mwaskom set me on the right track: I have to provide the current axes to the plot function (now this seems so obvious...). For future reference, this is a working solution to my problem: 

def plot_area(data, color):
    data.index = pd.to_datetime(data[""Year""])
    data[[""Home"", ""Away""]].plot(kind=""area"", ax=plt.gca())

g = sns.FacetGrid(df, col=""Team"")
g.map_dataframe(plot_area)

",,
Seaborn unexpected output,https://stackoverflow.com/questions/38966075,How to restrict anaconda from upgrading the module being installed if its a higher level dependency,"I'm trying to use continuum io anaconda packing system to package python-2.7.10 with other dependent modules for our environment. I want to automate the pack distribution to simply be a single installation of python with the modules we require.

The issue I'm having is when I specify the modules under the build parameter in meta.yaml it will upgrade the version of python being installed despite the fact that it is python-2.7.10. This will cause an error in the build process.

Is there a way to pin the version of python being installed so that if there is a dependency it will hard fail, or use an earlier version of the package?

meta.yaml, ive tried not pinning the version of the modules as well.

package:
  name: python
  version: 2.7.10

source:
  fn: Python-2.7.10.tgz
  url: https://www.python.org/ftp/python/2.7.10/Python-2.7.10.tgz
  md5: d7547558fd673bd9d38e2108c6b42521

build:
  no_link: bin/python

requirements:
  build:
    - bzip2 [unix]
    - zlib [unix]
    - sqlite [unix]
    - readline [unix]
    - tk [unix]
    - openssl [unix]
    - system [linux]
    - ipython 5.0.0
    - numpy 1.11.1
    - cython 0.24.1
    - scipy 0.18.0
    - pandas 0.18.1
    - patsy 0.4.1
    - statsmodels 0.6.1
    - matplotlib 1.5.2
    - ggplot 0.9.4
    - scikit-learn 0.17.1
    - distribute 0.6.45
    - backports.ssl-match-hostname 3.5.0.1
    - certifi 14.05.14
    - nose_parameterized 0.5.0
    - pyparsing 2.1.4
    - python-dateutil 2.5.3
    - pytz 2016.6.1
    - pyzmq 15.3.0
    - simplejson 3.3.3
    - six 1.10.0
    - sympy 1.0
    - tornado 4.4.1
    - virtualenv 13.0.1
    - wsgiref 0.1.2
    - python-swiftclient 2.7.0
    #- python-ceilometerclient #issue
    #- python-heatclient #issue
    #- python-keystoneclient 1.6.0
    #- python-novaclient 2.26.0
    #- python-troveclient #issue
    - python-cinderclient 1.1.2
    - python-glanceclient 0.17.2
    - python-neutronclient 2.4.0
    - networkx 1.11
    - pysal 1.11.1
    - pyyaml 3.11
    - shapely 1.5.13
    - beautifulsoup4 4.4.1
    - nltk 3.2.1
    - requests 2.10.0
    - seaborn 0.5.0
    - h5py 2.6.0
    - xlrd 1.0.0
    - markupsafe 0.23
    - crypto 1.1.0
    - jinja2 2.8
    - openpyxl 2.3.2
    - jaro_winkler 1.0.2
    - bokeh 0.12.1
    - numexpr 2.6.1
    - pytables 3.2.3.1
    - pycurl 7.43.0
    - mgrs 1.1.0
    - psutil 4.3.0
    - biopython 1.67
    - enaml 0.9.8
    - mdp 3.5
    - bitarray 0.8.1
    - clusterpy 0.9.9
    - pyside 1.2.1
    - pyqt 4.11.4
    - parsedatetime 1.4
    - pymysql 0.6.7
    - pyodbc 3.0.10
    - tabulate 0.7.2

  run:
    - zlib [unix]
    - sqlite [unix]
    - readline [unix]
    - tk [unix]
    - openssl [unix]
    - system [linux]

test:
  commands:
    - python -V [unix]
    - 2to3 -h [unix]
    - python-config --help [unix]

about:
  home: http://www.python.org/
  summary: general purpose programming language
  license: PSF


The output with the error:

$ conda build .
Removing old build environment
BUILD START: python-2.7.10-0
    (actual version deferred until further download or env creation)
Using Anaconda Cloud api site https://api.anaconda.org

The following packages will be downloaded:

    package                    |            build
    ---------------------------|-----------------
    geos-3.5.0                 |                0        16.9 MB  defaults
    libgcc-4.8.5               |                1         922 KB  r
    pixman-0.32.6              |                0         2.4 MB  defaults
    unixodbc-2.3.4             |                0         688 KB  defaults
    yaml-0.1.6                 |                0         246 KB  defaults
    curl-7.49.0                |                1         543 KB  defaults
    glib-2.43.0                |                2         7.4 MB  r
    hdf5-1.8.17                |                1         1.9 MB  defaults
    atom-0.3.10                |           py27_0         676 KB  defaults
    backports_abc-0.4          |           py27_0           5 KB  defaults
    beautifulsoup4-4.4.1       |           py27_0         116 KB  defaults
    bitarray-0.8.1             |           py27_0          89 KB  defaults
    et_xmlfile-1.0.1           |           py27_0          15 KB  defaults
    future-0.15.2              |           py27_0         616 KB  defaults
    jaro_winkler-1.0.2         |           py27_0          24 KB  auto
    jdcal-1.2                  |           py27_1           9 KB  defaults
    kiwisolver-0.1.3           |           py27_0         571 KB  defaults
    markupsafe-0.23            |           py27_2          31 KB  defaults
    mgrs-1.1.0                 |           py27_0          48 KB  auto
    mpmath-0.19                |           py27_1         873 KB  defaults
    nltk-3.2.1                 |           py27_0         1.7 MB  defaults
    parsedatetime-1.2          |           py27_0          39 KB  auto
    ply-3.8                    |           py27_0          71 KB  defaults
    psutil-4.3.0               |           py27_0         224 KB  defaults
    pycurl-7.43.0              |           py27_0         128 KB  defaults
    pymysql-0.7.6              |           py27_0         116 KB  defaults
    pyodbc-3.0.10              |           py27_0         146 KB  defaults
    pyyaml-3.11                |           py27_4         297 KB  defaults
    pyzmq-15.4.0               |           py27_0         705 KB  defaults
    requests-2.10.0            |           py27_0         611 KB  defaults
    shapely-1.5.16             |           py27_0         494 KB  defaults
    tabulate-0.7.2             |           py27_0          18 KB  auto
    wsgiref-0.1.2              |           py27_0          943 B  auto
    xlrd-1.0.0                 |           py27_0         181 KB  defaults
    biopython-1.67             |      np111py27_0         2.2 MB  defaults
    clusterpy-0.9.9            |           py27_1         101 KB  conda-forge
    cmd2-0.6.7                 |           py27_0          33 KB  auto
    h5py-2.6.0                 |      np111py27_2         2.4 MB  defaults
    jinja2-2.8                 |           py27_1         264 KB  defaults
    jsonschema-2.5.1           |           py27_0          55 KB  defaults
    mdp-3.5                    |           py27_0         477 KB  defaults
    networkx-1.11              |           py27_0         1.1 MB  defaults
    numexpr-2.6.1              |      np111py27_0         347 KB  defaults
    openpyxl-2.3.2             |           py27_0         248 KB  defaults
    rsa-3.4.2                  |           py27_0          50 KB  conda-forge
    singledispatch-3.4.0.3     |           py27_1          17 KB  r
    ssl_match_hostname-3.4.0.2 |           py27_1           6 KB  defaults
    cliff-1.10.1               |           py27_0          36 KB  gus
    crypto-1.1.0               |           py27_0           3 KB  auto
    pysal-1.11.1               |           py27_0        11.2 MB  defaults
    pytables-3.2.3.1           |      np111py27_0         3.4 MB  defaults
    tornado-4.4.1              |           py27_0         552 KB  defaults
    bokeh-0.12.1               |           py27_0         3.2 MB  defaults
    harfbuzz-0.9.35            |                6         1.1 MB  r
    ipython-5.1.0              |           py27_0         936 KB  defaults
    pyopenssl-16.0.0           |           py27_0          66 KB  defaults
    pango-1.36.8               |                3         796 KB  r
    qt-4.8.7                   |                4        32.7 MB  defaults
    python-neutronclient-2.4.0 |           py27_0         222 KB  gus
    shiboken-1.2.1             |           py27_0         883 KB  defaults
    enaml-0.9.8                |           py27_1         944 KB  defaults
    pyside-1.2.1               |           py27_1         5.7 MB  defaults
    seaborn-0.7.1              |           py27_0         272 KB  defaults
    ------------------------------------------------------------
                                           Total:       107.8 MB

The following NEW packages will be INSTALLED:

    atom:                         0.3.10-py27_0       defaults
    babel:                        2.3.3-py27_0        defaults
    backports:                    1.0-py27_0          defaults
    backports.ssl-match-hostname: 3.5.0.1-py27_0      getpantheon
    backports_abc:                0.4-py27_0          defaults
    beautifulsoup4:               4.4.1-py27_0        defaults
    biopython:                    1.67-np111py27_0    defaults
    bitarray:                     0.8.1-py27_0        defaults
    bokeh:                        0.12.1-py27_0       defaults
    brewer2mpl:                   1.4.1-py27_1        conda-forge
    bzip2:                        1.0.6-3             defaults
    cairo:                        1.12.18-6           defaults
    certifi:                      2016.2.28-py27_0    defaults
    cffi:                         1.6.0-py27_0        defaults
    cliff:                        1.10.1-py27_0       gus
    clusterpy:                    0.9.9-py27_1        conda-forge
    cmd2:                         0.6.7-py27_0        auto
    crypto:                       1.1.0-py27_0        auto
    cryptography:                 1.4-py27_0          defaults
    curl:                         7.49.0-1            defaults
    cycler:                       0.10.0-py27_0       defaults
    cython:                       0.24.1-py27_0       defaults
    decorator:                    4.0.10-py27_0       defaults
    distribute:                   0.6.45-py27_1       defaults
    enaml:                        0.9.8-py27_1        defaults
    enum34:                       1.1.6-py27_0        defaults
    et_xmlfile:                   1.0.1-py27_0        defaults
    fontconfig:                   2.11.1-6            defaults
    freetype:                     2.5.5-1             defaults
    functools32:                  3.2.3.2-py27_0      defaults
    future:                       0.15.2-py27_0       defaults
    futures:                      3.0.5-py27_0        defaults
    geos:                         3.5.0-0             defaults
    get_terminal_size:            1.0.0-py27_0        defaults
    ggplot:                       0.11.1-py27_1       conda-forge
    glib:                         2.43.0-2            r
    h5py:                         2.6.0-np111py27_2   defaults
    harfbuzz:                     0.9.35-6            r
    hdf5:                         1.8.17-1            defaults
    idna:                         2.1-py27_0          defaults
    ipaddress:                    1.0.16-py27_0       defaults
    ipython:                      5.1.0-py27_0        defaults
    ipython_genutils:             0.1.0-py27_0        defaults
    iso8601:                      0.1.11-py27_0       defaults
    jaro_winkler:                 1.0.2-py27_0        auto
    jdcal:                        1.2-py27_1          defaults
    jinja2:                       2.8-py27_1          defaults
    jsonpatch:                    1.3-py27_0          auto
    jsonpointer:                  1.2-py27_0          auto
    jsonschema:                   2.5.1-py27_0        defaults
    kiwisolver:                   0.1.3-py27_0        defaults
    libffi:                       3.2.1-0             defaults
    libgcc:                       4.8.5-1             r
    libgfortran:                  3.0.0-1             defaults
    libpng:                       1.6.22-0            defaults
    libsodium:                    1.0.10-0            defaults
    libxml2:                      2.9.2-0             defaults
    markupsafe:                   0.23-py27_2         defaults
    matplotlib:                   1.5.1-np111py27_0   defaults
    mdp:                          3.5-py27_0          defaults
    mgrs:                         1.1.0-py27_0        auto
    mkl:                          11.3.3-0            defaults
    mpmath:                       0.19-py27_1         defaults
    msgpack-python:               0.4.7-py27_0        defaults
    netaddr:                      0.7.18-py27_0       conda-forge
    netifaces:                    0.10.4-py27_0       conda-forge
    networkx:                     1.11-py27_0         defaults
    nltk:                         3.2.1-py27_0        defaults
    nose_parameterized:           0.5.0-py27_0        conda-forge
    numexpr:                      2.6.1-np111py27_0   defaults
    numpy:                        1.11.1-py27_0       defaults
    openpyxl:                     2.3.2-py27_0        defaults
    openssl:                      1.0.2h-1            defaults
    oslo.config:                  1.9.3-py27_0        gus
    oslo.i18n:                    1.5.0-py27_0        gus
    oslo.serialization:           1.4.0-py27_0        gus
    oslo.utils:                   1.4.0-py27_0        gus
    pandas:                       0.18.1-np111py27_0  defaults
    pango:                        1.36.8-3            r
    parsedatetime:                1.2-py27_0          auto
    path.py:                      8.2.1-py27_0        defaults
    pathlib2:                     2.1.0-py27_0        defaults
    patsy:                        0.4.1-py27_0        defaults
    pbr:                          0.11.0-py27_0       defaults
    pexpect:                      4.0.1-py27_0        defaults
    pickleshare:                  0.7.3-py27_0        defaults
    pip:                          8.1.2-py27_0        defaults
    pixman:                       0.32.6-0            defaults
    ply:                          3.8-py27_0          defaults
    prettytable:                  0.7.2-py27_0        conda-forge
    prompt_toolkit:               1.0.3-py27_0        defaults
    psutil:                       4.3.0-py27_0        defaults
    ptyprocess:                   0.5.1-py27_0        defaults
    pyasn1:                       0.1.9-py27_0        defaults
    pycairo:                      1.10.0-py27_0       defaults
    pycparser:                    2.14-py27_1         defaults
    pycurl:                       7.43.0-py27_0       defaults
    pygments:                     2.1.3-py27_0        defaults
    pymysql:                      0.7.6-py27_0        defaults
    pyodbc:                       3.0.10-py27_0       defaults
    pyopenssl:                    16.0.0-py27_0       defaults
    pyparsing:                    2.1.4-py27_0        defaults
    pyqt:                         4.11.4-py27_4       defaults
    pysal:                        1.11.1-py27_0       defaults
    pyside:                       1.2.1-py27_1        defaults
    pytables:                     3.2.3.1-np111py27_0 defaults
    python:                       2.7.12-1            defaults
    python-cinderclient:          1.1.2-py27_0        gus
    python-dateutil:              2.5.3-py27_0        defaults
    python-glanceclient:          0.17.2-py27_0       gus
    python-keystoneclient:        1.3.2-py27_0        gus
    python-neutronclient:         2.4.0-py27_0        gus
    python-swiftclient:           2.7.0-py27_0        chenghlee
    pytz:                         2016.6.1-py27_0     defaults
    pyyaml:                       3.11-py27_4         defaults
    pyzmq:                        15.4.0-py27_0       defaults
    qt:                           4.8.7-4             defaults
    readline:                     6.2-2               defaults
    requests:                     2.10.0-py27_0       defaults
    rsa:                          3.4.2-py27_0        conda-forge
    scikit-learn:                 0.17.1-np111py27_2  defaults
    scipy:                        0.18.0-np111py27_0  defaults
    seaborn:                      0.7.1-py27_0        defaults
    setuptools:                   25.1.6-py27_0       defaults
    shapely:                      1.5.16-py27_0       defaults
    shiboken:                     1.2.1-py27_0        defaults
    simplegeneric:                0.8.1-py27_1        defaults
    simplejson:                   3.8.2-py27_0        defaults
    singledispatch:               3.4.0.3-py27_1      r
    sip:                          4.18-py27_0         defaults
    six:                          1.10.0-py27_0       defaults
    sqlite:                       3.13.0-0            defaults
    ssl_match_hostname:           3.4.0.2-py27_1      defaults
    statsmodels:                  0.6.1-np111py27_1   defaults
    stevedore:                    1.3.0-py27_0        gus
    sympy:                        1.0-py27_0          defaults
    system:                       5.8-2               defaults
    tabulate:                     0.7.2-py27_0        auto
    tk:                           8.5.18-0            defaults
    tornado:                      4.4.1-py27_0        defaults
    traitlets:                    4.2.2-py27_0        defaults
    unixodbc:                     2.3.4-0             defaults
    virtualenv:                   13.0.1-py27_0       defaults
    warlock:                      1.3.0-py27_0        conda-forge
    wcwidth:                      0.1.7-py27_0        defaults
    wheel:                        0.29.0-py27_0       defaults
    wsgiref:                      0.1.2-py27_0        auto
    xlrd:                         1.0.0-py27_0        defaults
    yaml:                         0.1.6-0             defaults
    zeromq:                       4.1.4-0             defaults
    zlib:                         1.2.8-3             defaults

Source cache directory is: /opt/app/anaconda2/conda-bld/src_cache
Found source in cache: Python-2.7.10.tgz
Extracting download
BUILD START: python-2.7.10-0
python is installed as a build dependency. Removing.
An unexpected error has occurred, please consider sending the
following traceback to the conda GitHub issue tracker at:

    https://github.com/conda/conda-build/issues

Include the output of the command 'conda info' in your report.


Traceback (most recent call last):
  File ""/opt/app/anaconda2/bin/conda-build"", line 5, in &lt;module&gt;
    sys.exit(main())
  File ""/opt/app/anaconda2/lib/python2.7/site-packages/conda_build/main_build.py"", line 152, in main
    args_func(args, p)
  File ""/opt/app/anaconda2/lib/python2.7/site-packages/conda_build/main_build.py"", line 415, in args_func
    args.func(args, p)
  File ""/opt/app/anaconda2/lib/python2.7/site-packages/conda_build/main_build.py"", line 358, in execute
    debug=args.debug)
  File ""/opt/app/anaconda2/lib/python2.7/site-packages/conda_build/build.py"", line 561, in build
    assert not plan.nothing_to_do(actions), actions
AssertionError: defaultdict(&lt;type 'list'&gt;, {'op_order': ('RM_FETCHED', 'FETCH', 'RM_EXTRACTED', 'EXTRACT', 'UNLINK', 'LINK', 'SYMLINK_CONDA'), 'PREFIX': '/opt/app/anaconda2/envs/_build_placehold_placehold_placehold_placehold_placehold'})

",3,578,"Unless there's a specific reason you need to compile python yourself, I think what you're actually going after is conda bundle (http://conda.pydata.org/docs/commands/conda-bundle.html).  Unfortunately we've removed it in conda 4.2 which will be coming out soon, intending to move it to conda-build.  Since that hasn't happened yet, and if it ends up actually being useful to people, we can add it back.



You could also try this using conda-build...

Remove the whole source block in your meta.yaml file. Also remove all of the build requirements that are also not run requirements.  Then in your build.sh file

conda install --yes --quiet \
    python=2.7.10 \
    ipython=5.0.0 \
    numpy=1.11.1 \
    cython=0.24.1 \
    scipy=0.18.0 \
    pandas=0.18.1 \
    patsy=0.4.1 \
    statsmodels=0.6.1 \
    matplotlib=1.5.2 \
    ggplot=0.9.4 \
    scikit-learn=0.17.1 \
    distribute=0.6.45 \
    backports.ssl-match-hostname=3.5.0.1 \
    certifi=14.05.14 \
    nose_parameterized=0.5.0 \
    pyparsing=2.1.4 \
    python-dateutil=2.5.3 \
    pytz=2016.6.1 \
    pyzmq=15.3.0 \
    simplejson=3.3.3 \
    six=1.10.0 \
    sympy=1.0 \
    tornado=4.4.1 \
    virtualenv=13.0.1 \
    wsgiref=0.1.2 \
    python-swiftclient=2.7.0 \
    python-cinderclient=1.1.2 \
    python-glanceclient=0.17.2 \
    python-neutronclient=2.4.0 \
    networkx=1.11 \
    pysal=1.11.1 \
    pyyaml=3.11 \
    shapely=1.5.13 \
    beautifulsoup4=4.4.1 \
    nltk=3.2.1 \
    requests=2.10.0 \
    seaborn=0.5.0 \
    h5py=2.6.0 \
    xlrd=1.0.0 \
    markupsafe=0.23 \
    crypto=1.1.0 \
    jinja2=2.8 \
    openpyxl=2.3.2 \
    jaro_winkler=1.0.2 \
    bokeh=0.12.1 \
    numexpr=2.6.1 \
    pytables=3.2.3.1 \
    pycurl=7.43.0 \
    mgrs=1.1.0 \
    psutil=4.3.0 \
    biopython=1.67 \
    enaml=0.9.8 \
    mdp=3.5 \
    bitarray=0.8.1 \
    clusterpy=0.9.9 \
    pyside=1.2.1 \
    pyqt=4.11.4 \
    parsedatetime=1.4 \
    pymysql=0.6.7 \
    pyodbc=3.0.10 \
    tabulate=0.7.2


The big difference: by listing all of those packages as build requirements, you're actually ensuring that they won't be in your final conda package.  Think of build requirements more like a compiler, or something that's necessary when you're building the package, but not when you're actually running it.
",,
Seaborn unexpected output,https://stackoverflow.com/questions/73392060,"Pytorch and ray tune: why the error; raise TuneError(&quot;Trials did not complete&quot;, incomplete_trials)?","I want to embed hyperparameter optimisation with ray into my pytorch script.
I wrote this code (which is a reproducible example):
## Standard libraries
CHECKPOINT_PATH = ""/home/ad1/new_dev_v1""
DATASET_PATH = ""/home/ad1/""
import torch
device = torch.device(""cuda:0"") if torch.cuda.is_available() else torch.device(""cpu"")
from importlib import reload
from itertools import *
import matplotlib
from itertools import groupby
from libs_public.api import get_pantry_token
from matplotlib import pyplot as plt
from matplotlib.colors import to_rgb
from openbabel import pybel
from openbabel.pybel import readstring,descs
from operator import itemgetter
from pathlib import Path
from pytorch_lightning.callbacks import LearningRateMonitor, ModelCheckpoint
from pytorch_lightning.loggers import TensorBoardLogger
from ray import tune
from ray.tune import CLIReporter
from ray.tune.integration.pytorch_lightning import TuneReportCallback, TuneReportCheckpointCallback
from ray.tune.schedulers import ASHAScheduler, PopulationBasedTraining
from sklearn import preprocessing
from sklearn.metrics import f1_score, precision_score, recall_score,roc_auc_score
from socket import TIPC_DEST_DROPPABLE
from torch.nn import Linear
from torch.utils.data import TensorDataset
from torch_geometric.data import Data, Dataset,DataLoader,DenseDataLoader,InMemoryDataset
from torch_geometric.datasets import TUDataset
from torch_geometric.nn import GCNConv
from torch_geometric.nn import global_mean_pool
from torchmetrics.functional import precision_recall
from torchvision import transforms
from torchvision.datasets import CIFAR10
from tqdm.notebook import tqdm
import getpass, argparse
import joblib
import json
import logging
import math
import matplotlib.pyplot as plt
import networkx as nx
import numpy as np 
import openbabel
import os
import pandas as pd
import pytorch_lightning as pl
import random
import re
import requests
import seaborn as sns
import sklearn
import sys
import time
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
import torch.utils.data as data
import torch_geometric
import torch_geometric.data as geom_data
import torch_geometric.nn as geom_nn
import torchmetrics
import torchvision
import warnings
matplotlib.rcParams['lines.linewidth'] = 2.0
pl.seed_everything(42)
print(device)
sns.reset_orig()
sns.set()
sys.path.append('/home/ad1/git/')
torch.backends.cudnn.deterministic = True
warnings.filterwarnings('ignore')



# Setting the seed
pl.seed_everything(42)

# Ensure that all operations are deterministic on GPU (if used) for reproducibility
torch.backends.cudnn.deterministic = True
torch.backends.cudnn.benchmark = False

device = torch.device(""cuda:0"") if torch.cuda.is_available() else torch.device(""cpu"")
print(device)


import torch
from torch_geometric.datasets import TUDataset
from torch.nn import Linear
from torch_geometric.nn import global_mean_pool
from torch_geometric.data import Data, Dataset,DataLoader


from torch.utils.data import TensorDataset
from ray import tune
from ray.tune import CLIReporter
from ray.tune.schedulers import ASHAScheduler, PopulationBasedTraining
from ray.tune.integration.pytorch_lightning import TuneReportCallback, TuneReportCheckpointCallback

dataset = TUDataset(root='/tmp/MUTAG', name='MUTAG', use_node_attr=True)
loader = DataLoader(dataset, batch_size=32, shuffle=True)

train_dataset = dataset
val_dataset = dataset
test_dataset = dataset

graph_train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)
graph_val_loader = DataLoader(val_dataset, batch_size=64) # Additional loader if you want to change to a larger dataset
graph_test_loader = DataLoader(test_dataset, batch_size=64)


#will change this when it makes sense
#config = {
#    ""dropout"": tune.uniform(0.4,0.5)
#    } 

config = {'dropout':0.4}

gnn_layer_by_name = {
    ""GCN"": geom_nn.GCNConv,
    ""GAT"": geom_nn.GATConv,
    ""GraphConv"": geom_nn.GraphConv
}

class GCNLayer(nn.Module):
    def __init__(self, c_in, c_out):
        super().__init__()
        self.projection = nn.Linear(c_in, c_out)
        

    def forward(self, node_feats, adj_matrix):
        """"""
        Inputs:
            node_feats - Tensor with node features of shape [batch_size, num_nodes, c_in]
            adj_matrix - Batch of adjacency matrices of the graph. If there is an edge from i to j, adj_matrix[b,i,j]=1 else 0.
                         Supports directed edges by non-symmetric matrices. Assumes to already have added the identity connections. 
                         Shape: [batch_size, num_nodes, num_nodes]
        """"""
        # Num neighbours = number of incoming edges
        num_neighbours = adj_matrix.sum(dim=-1, keepdims=True)
        node_feats = self.projection(node_feats)
        node_feats = torch.bmm(adj_matrix, node_feats)
        node_feats = node_feats / num_neighbours
        return node_feats

class GNNModel(nn.Module):
    
    def __init__(self, c_in, c_hidden, c_out, num_layers=2, layer_name=""GCN"", dp_rate=config['dropout'], **kwargs):
        """"""
        Inputs:
            c_in - Dimension of input features
            c_hidden - Dimension of hidden features
            c_out - Dimension of the output features. Usually number of classes in classification
            num_layers - Number of ""hidden"" graph layers
            layer_name - String of the graph layer to use
            dp_rate - Dropout rate to apply throughout the network
            kwargs - Additional arguments for the graph layer (e.g. number of heads for GAT)
        """"""
        super().__init__()
        gnn_layer = gnn_layer_by_name[layer_name]
        
        layers = []
        in_channels, out_channels = c_in, c_hidden
        for l_idx in range(num_layers-1):
            layers += [
                gnn_layer(in_channels=in_channels, 
                          out_channels=out_channels,
                          **kwargs),
                nn.ReLU(inplace=True),
                nn.Dropout(config['dropout'])
            ]
            in_channels = c_hidden
        layers += [gnn_layer(in_channels=in_channels, 
                             out_channels=c_out,
                             **kwargs)]
        self.layers = nn.ModuleList(layers)
    
    def forward(self, x, edge_index):
        """"""
        Inputs:
            x - Input features per node
            edge_index - List of vertex index pairs representing the edges in the graph (PyTorch geometric notation)
        """"""
        for l in self.layers:
            # For graph layers, we need to add the ""edge_index"" tensor as additional input
            # All PyTorch Geometric graph layer inherit the class ""MessagePassing"", hence
            # we can simply check the class type.
            if isinstance(l, geom_nn.MessagePassing):
                x = l(x, edge_index)
            else:
                x = l(x)
        return x



class GraphGNNModel(nn.Module):
    
    def __init__(self, c_in, c_hidden, c_out, dp_rate_linear=0.5, **kwargs):
        """"""
        Inputs:
            c_in - Dimension of input features
            c_hidden - Dimension of hidden features
            c_out - Dimension of output features (usually number of classes)
            dp_rate_linear - Dropout rate before the linear layer (usually much higher than inside the GNN)
            kwargs - Additional arguments for the GNNModel object
        """"""
        super().__init__()
        self.GNN = GNNModel(c_in=c_in, 
                            c_hidden=c_hidden, 
                            c_out=c_hidden, # Not our prediction output yet!
                            **kwargs)
        self.head = nn.Sequential(
            nn.Dropout(config['dropout']),
            nn.Linear(c_hidden, c_out)
        )

    def forward(self, x, edge_index, batch_idx):
        """"""
        Inputs:
            x - Input features per node
            edge_index - List of vertex index pairs representing the edges in the graph (PyTorch geometric notation)
            batch_idx - Index of batch element for each node
        """"""
        x = self.GNN(x, edge_index)
        x = geom_nn.global_mean_pool(x, batch_idx) # Average pooling
        x = self.head(x)
        return x


#see https://pytorch-lightning.readthedocs.io/en/stable/common/lightning_module.html
class GraphLevelGNN(pl.LightningModule):
    
    def __init__(self, **model_kwargs):
        super().__init__()
        # Saving hyperparameters
        self.save_hyperparameters()
        
        self.model = GraphGNNModel(**model_kwargs)
        self.loss_module = nn.BCEWithLogitsLoss() #if self.hparams.c_out == 1 else nn.CrossEntropyLoss()

    def forward(self, data, mode=""train""):
        x, edge_index, batch_idx = data.x, data.edge_index, data.batch
        x = self.model(x, edge_index, batch_idx)
        x = x.squeeze(dim=-1)
        
        if self.hparams.c_out == 1:
            preds = (x &gt; 0).float()
            data.y = data.y.float()
        else:
            preds = x.argmax(dim=-1)

        loss = self.loss_module(x, data.y)
        acc = (preds == data.y).sum().float() / preds.shape[0]
        f1 = f1_score(preds,data.y)  ##change f1/precision and recall was just testing
        precision = precision_score(preds,data.y)
        recall = recall_score(preds,data.y)
        #roc_auc = roc_auc_score(preds,data.y)  ##ADD THIS BACK IN
        return loss, acc, f1,precision, recall

    def configure_optimizers(self):
        optimizer = optim.SGD(self.parameters(),lr=0.1) # High lr because of small dataset and small model
        return optimizer

    def training_step(self, batch, batch_idx):
        loss, acc, _,_, _ = self.forward(batch, mode=""train"")
        self.log('train_loss', loss,on_epoch=True,logger=True)
        self.log('train_acc', acc,on_epoch=True,logger=True)
        #self.log('train_precision',precision_and_recall)
        return loss

    def validation_step(self, batch, batch_idx):
        loss, acc, _,_, _ = self.forward(batch, mode=""val"")
        self.log('val_acc', acc,on_epoch=True,logger=True)
        self.log('val_loss', loss,on_epoch=True,logger=True)

    def test_step(self, batch, batch_idx):
        loss, acc, f1,precision, recall = self.forward(batch, mode=""test"")
        self.log('test_acc', acc,on_epoch=True,logger=True)
        self.log('test_f1', f1,on_epoch=True,logger=True)
        self.log('test_precision', precision,on_epoch=True,logger=True)       
        self.log('test_recall', recall,on_epoch=True,logger=True) 
        #self.log('roc_auc', roc_auc,on_epoch=True,logger=True) 


from pytorch_lightning import loggers as pl_loggers
def train_graph_classifier(model_name, **model_kwargs):
    pl.seed_everything(42)
    
    # Create a PyTorch Lightning trainer with the generation callback
    root_dir = os.path.join(CHECKPOINT_PATH, ""GraphLevel"" + model_name)
    os.makedirs(root_dir, exist_ok=True)
    csv_logger = pl_loggers.CSVLogger(save_dir=""logs/"")

    tune_report_callback = TuneReportCheckpointCallback(
    metrics={
    ""val_loss"": ""val_loss"",
    ""val_acc"": ""val_acc"",
    },
    filename=""ray_ckpt"",
    on=""validation_end"",
    )

    trainer = pl.Trainer(default_root_dir=root_dir,
                         callbacks=[ModelCheckpoint(save_weights_only=True, mode=""max"", monitor=""val_acc""),tune_report_callback],
                                 #   TuneReportCallback(
                                #    {
                                #        ""loss"": ""val_loss"",
                                #        ""mean_accuracy"": ""val_accuracy"" 
                                #    },
                                #        on=""test_end"")] # need to change this to validation but error at the minute
                                 #   ,
                         gpus=1 if str(device).startswith(""cuda"") else 0,
                         max_epochs=3,
                         progress_bar_refresh_rate=1,
                         logger=csv_logger,                         
                         )

    trainer.logger._default_hp_metric = None # Optional logging argument that we don't need

    # Check whether pretrained model exists. If yes, load it and skip training
    pretrained_filename = os.path.join(CHECKPOINT_PATH, f""GraphLevel{model_name}.ckpt"")

    if os.path.isfile(pretrained_filename):
        print(""Found pretrained model, loading..."")
        model = GraphLevelGNN.load_from_checkpoint(pretrained_filename)
    else:
        pl.seed_everything(42)
        model = GraphLevelGNN(c_in = dataset.num_node_features, 
                              c_out=1, #if tu_dataset.num_classes==2 else tu_dataset.num_classes, 
                              **model_kwargs)
        trainer.fit(model, graph_train_loader, graph_val_loader)
        model = GraphLevelGNN.load_from_checkpoint(trainer.checkpoint_callback.best_model_path)
        
    # Test best model on validation and test set
    #train_result = trainer.test(model, graph_train_loader, verbose=False)
    #test_result = trainer.test(model, graph_test_loader, verbose=False)
    #result = {""test"": test_result[0]['test_acc'], ""train"": train_result[0]['test_acc']} 
    #return model, result
    return model

# Example of ASHA Scheduler
scheduler_asha = ASHAScheduler(
    max_t=100,
    grace_period=1,
    reduction_factor=2,
)

from ray.tune.integration.pytorch_lightning import TuneReportCallback, TuneReportCheckpointCallback

reporter = CLIReporter(
    parameter_columns=['dropout'],
    metric_columns=[""val_loss"", ""val_acc"", ""training_iteration""]
)


model = train_graph_classifier(model_name=""GraphConv"", 
                                       c_hidden=128, 
                                       layer_name=""GraphConv"", 
                                       num_layers=3, 
                                       dp_rate_linear=0.5,
                                       dp_rate=0.0)


result = tune.run(
    tune.with_parameters(
        model,
        #feature_size=10,
        #target_size=2,
        epochs=50,
        gpus=0
        ),

    resources_per_trial={
        ""cpu"": 1,
        ""gpu"": 0,
    },
    

    local_dir='/home/ad1/ray_ckpt2',  # path for saving checkpoints
    metric=""val_loss"",
    mode=""min"",
    config=config,
    num_samples=16,
    scheduler=scheduler_asha,
    progress_reporter=reporter,
    name=""test"",
)

And the error returned is:
(tune_with_parameters pid=65319) 2022-08-17 16:28:47,053        ERROR function_runner.py:286 -- Runner Thread raised error.
(tune_with_parameters pid=65319) Traceback (most recent call last):
(tune_with_parameters pid=65319)   File ""/root/miniconda3/lib/python3.7/site-packages/ray/tune/function_runner.py"", line 277, in run
(tune_with_parameters pid=65319)     self._entrypoint()
(tune_with_parameters pid=65319)   File ""/root/miniconda3/lib/python3.7/site-packages/ray/tune/function_runner.py"", line 352, in entrypoint
(tune_with_parameters pid=65319)     self._status_reporter.get_checkpoint(),
(tune_with_parameters pid=65319)   File ""/root/miniconda3/lib/python3.7/site-packages/ray/util/tracing/tracing_helper.py"", line 462, in _resume_span
(tune_with_parameters pid=65319)     return method(self, *_args, **_kwargs)
(tune_with_parameters pid=65319)   File ""/root/miniconda3/lib/python3.7/site-packages/ray/tune/function_runner.py"", line 645, in _trainable_func
(tune_with_parameters pid=65319)     output = fn()
(tune_with_parameters pid=65319)   File ""/root/miniconda3/lib/python3.7/site-packages/ray/tune/utils/trainable.py"", line 410, in inner
(tune_with_parameters pid=65319)     trainable(config, **fn_kwargs)
(tune_with_parameters pid=65319)   File ""/root/miniconda3/lib/python3.7/site-packages/torch/nn/modules/module.py"", line 1130, in _call_impl
(tune_with_parameters pid=65319)     return forward_call(*input, **kwargs)
(tune_with_parameters pid=65319) TypeError: forward() got an unexpected keyword argument 'checkpoint_dir'

Traceback (most recent call last):
  File ""test_pytorch.py"", line 390, in &lt;module&gt;
    name=""test"",
  File ""/root/miniconda3/lib/python3.7/site-packages/ray/tune/tune.py"", line 741, in run
    raise TuneError(""Trials did not complete"", incomplete_trials)
ray.tune.error.TuneError: ('Trials did not complete', [tune_with_parameters_a90c2_00000, tune_with_parameters_a90c2_00001, 
...cut for space
tune_with_parameters_a90c2_00014, tune_with_parameters_a90c2_00015])

Could someone show me where I'm going wrong, how to I run HPO with tune in this network and then train the model with the best hyperparameters and then return the model for prediction?
",1,1696,"Ray Tune expects a function trainable in the form of
def train_fn(config):
    # ...

In your case, it is probably best to wrap the train_graph_classifier function, e.g.
def train_fn(config):
    train_graph_classifier(
        model_name=""GraphConv"", 
        layer_name=""GraphConv"",
        **config)


analysis = tune.run(
    train_fn,
    config={
        # provide your hyperparameter search space here
        ""c_hidden"": tune.choice([64, 128]),
        ""dp_rate_linear"": tune.quniform(0.0, 1.0, 0.1),
        # ...
    },
    metric=""val_loss"",
    mode=""min"",
    # ...


print(analysis.best_checkpoint)

If you provide the TuneReportCheckpointCallback to the trainer, the analysis.best_checkpoint should contain the best model that can be then loaded for prediction, e.g.
with analysis.best_checkpoint.as_directory() as tmpdir:
    trainer = GraphLevelGNN.load_from_checkpoint(tmpdir)

","I faced the same error and solved it. You may have solved it since it is an old topic, but I would like to state it to share the information.
It is caused by there being any ERROR exited trial.
According to the error log, this TuneError is raised on tune.run in /ray/tune/tune.py. The tune.run collects trials with its status other than TEMINATED and raises the error if even one trial exited abnormally (not TERMINATED).
In my case, it occurred due to some trials with its status Trial.ERROR caused by CUDA: Out of Memory.
Please try to fix the error described in error log ""error.txt"" in the checkpoint, or set raise_on_failed_trial=True to ignore the TuneError.

""/ray/tune/tune.py"", line 741, in run :

    incomplete_trials = []
        for trial in runner.get_trials():
            if trial.status != Trial.TERMINATED:
              incomplete_trials += [trial]
    
    if incomplete_trials:
        if raise_on_failed_trial and not state[signal.SIGINT]:
            raise TuneError(""Trials did not complete"", incomplete_trials)
        else:
            logger.error(""Trials did not complete: %s"", incomplete_trials)

","running raise_on_failed_trial=False did the trick for me.
best_trial = trainer.hyperparameter_search(
    direction=""maximize"", 
    backend=""ray"", 
    n_trials=10, # number of trials
    raise_on_failed_trial=False, 
)


"
Seaborn unexpected result,https://stackoverflow.com/questions/40748838,Problems with updating anaconda and installing new packages,"I am trying to install seaborn on anaconda on Ubuntu-Linux. 

conda install -c anaconda seaborn=0.7.1


I am getting the following error message:

Fetching package metadata .../home/moritz/Python/anaconda3/lib/python3.5/site-packages/requests/packages/urllib3/connection.py:337: SubjectAltNameWarning: Certificate for conda.anaconda.org has no `subjectAltName`, falling back to check for a `commonName` for now. This feature is being removed by major browsers and deprecated by RFC 2818. (See https://github.com/shazow/urllib3/issues/497 for details.)
  SubjectAltNameWarning
/home/moritz/Python/anaconda3/lib/python3.5/site-packages/requests/packages/urllib3/connection.py:337: SubjectAltNameWarning: Certificate for conda.anaconda.org has no `subjectAltName`, falling back to check for a `commonName` for now. This feature is being removed by major browsers and deprecated by RFC 2818. (See https://github.com/shazow/urllib3/issues/497 for details.)
  SubjectAltNameWarning
..An unexpected error has occurred.
Please consider posting the following information to the
conda GitHub issue tracker at:

    https://github.com/conda/conda/issues

Current conda install:

           platform : linux-64
      conda version : 4.2.12
   conda is private : False
  conda-env version : 4.2.12
conda-build version : 1.19.0
     python version : 3.5.2.final.0
   requests version : 2.12.1
   root environment : /home/moritz/Python/anaconda3  (writable)
default environment : /home/moritz/Python/anaconda3
   envs directories : /home/moritz/Python/anaconda3/envs
      package cache : /home/moritz/Python/anaconda3/pkgs
       channel URLs : https://repo.continuum.io/pkgs/free/linux-64
                      https://repo.continuum.io/pkgs/free/noarch
                      https://repo.continuum.io/pkgs/pro/linux-64
                      https://repo.continuum.io/pkgs/pro/noarch
        config file : None
       offline mode : False


When I run

$ /home/moritz/Python/anaconda3/bin/conda install -c anaconda seaborn=0.7.1


I am getting the following error message:

Traceback (most recent call last):
  File ""/home/moritz/Python/anaconda3/lib/python3.5/site-packages/conda/exceptions.py"", line 479, in conda_exception_handler
    return_value = func(*args, **kwargs)
  File ""/home/moritz/Python/anaconda3/lib/python3.5/site-packages/conda/cli/main.py"", line 145, in _main
    exit_code = args.func(args, p)
  File ""/home/moritz/Python/anaconda3/lib/python3.5/site-packages/conda/cli/main_install.py"", line 80, in execute
    install(args, parser, 'install')
  File ""/home/moritz/Python/anaconda3/lib/python3.5/site-packages/conda/cli/install.py"", line 238, in install
    prefix=prefix)
  File ""/home/moritz/Python/anaconda3/lib/python3.5/site-packages/conda/api.py"", line 24, in get_index
    index = fetch_index(channel_urls, use_cache=use_cache, unknown=unknown)
  File ""/home/moritz/Python/anaconda3/lib/python3.5/site-packages/conda/fetch.py"", line 310, in fetch_index
    repodatas = [(u, f.result()) for u, f in zip(urls, futures)]
  File ""/home/moritz/Python/anaconda3/lib/python3.5/site-packages/conda/fetch.py"", line 310, in &lt;listcomp&gt;
    repodatas = [(u, f.result()) for u, f in zip(urls, futures)]
  File ""/home/moritz/Python/anaconda3/lib/python3.5/concurrent/futures/_base.py"", line 398, in result
    return self.__get_result()
  File ""/home/moritz/Python/anaconda3/lib/python3.5/concurrent/futures/_base.py"", line 357, in __get_result
    raise self._exception
  File ""/home/moritz/Python/anaconda3/lib/python3.5/concurrent/futures/thread.py"", line 55, in run
    result = self.fn(*self.args, **self.kwargs)
  File ""/home/moritz/Python/anaconda3/lib/python3.5/site-packages/conda/fetch.py"", line 74, in func
    res = f(*args, **kwargs)
  File ""/home/moritz/Python/anaconda3/lib/python3.5/site-packages/conda/fetch.py"", line 116, in fetch_repodata
    timeout=(3.05, 60))
  File ""/home/moritz/Python/anaconda3/lib/python3.5/site-packages/requests/sessions.py"", line 501, in get
    return self.request('GET', url, **kwargs)
  File ""/home/moritz/Python/anaconda3/lib/python3.5/site-packages/requests/sessions.py"", line 488, in request
    resp = self.send(prep, **send_kwargs)
  File ""/home/moritz/Python/anaconda3/lib/python3.5/site-packages/requests/sessions.py"", line 609, in send
    r = adapter.send(request, **kwargs)
  File ""/home/moritz/Python/anaconda3/lib/python3.5/site-packages/requests/adapters.py"", line 423, in send
    timeout=timeout
  File ""/home/moritz/Python/anaconda3/lib/python3.5/site-packages/requests/packages/urllib3/connectionpool.py"", line 594, in urlopen
    chunked=chunked)
  File ""/home/moritz/Python/anaconda3/lib/python3.5/site-packages/requests/packages/urllib3/connectionpool.py"", line 350, in _make_request
    self._validate_conn(conn)
  File ""/home/moritz/Python/anaconda3/lib/python3.5/site-packages/requests/packages/urllib3/connectionpool.py"", line 835, in _validate_conn
    conn.connect()
  File ""/home/moritz/Python/anaconda3/lib/python3.5/site-packages/requests/packages/urllib3/connection.py"", line 330, in connect
    cert = self.sock.getpeercert()
  File ""/home/moritz/Python/anaconda3/lib/python3.5/site-packages/requests/packages/urllib3/contrib/pyopenssl.py"", line 324, in getpeercert
    'subjectAltName': get_subj_alt_name(x509)
  File ""/home/moritz/Python/anaconda3/lib/python3.5/site-packages/requests/packages/urllib3/contrib/pyopenssl.py"", line 171, in get_subj_alt_name
    ext = cert.extensions.get_extension_for_class(
AttributeError: 'Extensions' object has no attribute 'get_extension_for_class'`


Any idea what the issue could be here? Many thanks in advance.
",9,4812,"please see here:
https://github.com/conda/conda/issues/3929

The issue was an old version of the cryptography package. It could be fixed by running:

CONDA_SSL_VERIFY=false conda update pyopenssl

","Another way to disable verification of SSL certificates that will work on Windows as well:

add ssl_verify: False to .condarc file.

.condarc file is not included by default, but it is automatically created in the user’s home directory the first time you run the conda config command.

To set ssl_verify the following command can be used:

conda config --set ssl_verify False.
",
Seaborn unexpected result,https://stackoverflow.com/questions/36663613,Python Seaborn Matplotlib setting line style as legend,"I have the following plot build with seaborn using factorplot() method.
Is it possible to use the line style as a legend to replace the legend based on line color on the right?  
graycolors = sns.mpl_palette('Greys_r', 4)
g = sns.factorplot(x=""k"", y=""value"", hue=""class"", palette=graycolors,
                   data=df, linestyles=[""-"", ""--""])

Furthermore I'm trying to get both lines in black color using the color=""black"" parameter in my factorplot method but this results in an exception ""factorplot() got an unexpected keyword argument 'color'"". How can I paint both lines in the same color and separate them by the linestyle only?
",9,6928,"I have been looking for a solution trying to put the linestyle in the legend like matplotlib, but I have not yet found how to do this in seaborn. However, to make the data clear in the legend I have used different markers:
import seaborn as sns
import numpy as np
import pandas as pd

# creating some data
n = 11
x = np.linspace(0,2, n)
y = np.sin(2*np.pi*x)
y2 = np.cos(2*np.pi*x)
data = {'x': np.append(x, x), 'y': np.append(y, y2),
        'class': np.append(np.repeat('sin', n), np.repeat('cos', n))}
df = pd.DataFrame(data)

# plot the data with the markers
# note that I put the legend=False to move it up (otherwise it was blocking the graph)
g=sns.factorplot(x=""x"", y=""y"", hue=""class"", palette=graycolors,
                 data=df, linestyles=[""-"", ""--""], markers=['o','v'], legend=False)
# placing the legend up
g.axes[0][0].legend(loc=1)
# showing graph
plt.show()


","you can try the following:

h = plt.gca().get_lines()
lg = plt.legend(handles=h, labels=['YOUR Labels List'], loc='best')


It worked fine with me.
",
Seaborn unexpected result,https://stackoverflow.com/questions/69916926,Unexpected error when trying to install conda environment from .yaml file,"I am trying to install a conda environment in WSL2 from a .yaml file, the instructions for the installation saying the first step is to run the following command:
conda env create -f devtools/conda-envs/ael-test.yaml

After the WSL2 terminal collects the package metadata and solves the environment, it encounters an error, which I don't really know how to solve. I run the command above from the base environment, from the directory where /devtools/conda-ens/ael-test.yaml is located.
Collecting package metadata (repodata.json): done
Solving environment: done
Preparing transaction: failed

# &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; ERROR REPORT &lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;

    Traceback (most recent call last):
      File ""/home/gheorghe/anaconda3/lib/python3.8/site-packages/conda/exceptions.py"", line 1079, in __call__
        return func(*args, **kwargs)
      File ""/home/gheorghe/anaconda3/lib/python3.8/site-packages/conda_env/cli/main.py"", line 80, in do_call
        exit_code = getattr(module, func_name)(args, parser)
      File ""/home/gheorghe/anaconda3/lib/python3.8/site-packages/conda_env/cli/main_create.py"", line 141, in execute
        result[installer_type] = installer.install(prefix, pkg_specs, args, env)
      File ""/home/gheorghe/anaconda3/lib/python3.8/site-packages/conda_env/installers/conda.py"", line 59, in install
        unlink_link_transaction.execute()
      File ""/home/gheorghe/anaconda3/lib/python3.8/site-packages/conda/core/link.py"", line 245, in execute
        self.verify()
      File ""/home/gheorghe/anaconda3/lib/python3.8/site-packages/conda/common/io.py"", line 88, in decorated
        return f(*args, **kwds)
      File ""/home/gheorghe/anaconda3/lib/python3.8/site-packages/conda/core/link.py"", line 222, in verify
        self.prepare()
      File ""/home/gheorghe/anaconda3/lib/python3.8/site-packages/conda/core/link.py"", line 211, in prepare
        grps = self._prepare(self.transaction_context, stp.target_prefix,
      File ""/home/gheorghe/anaconda3/lib/python3.8/site-packages/conda/core/link.py"", line 288, in _prepare
        packages_info_to_link = tuple(read_package_info(prec, pcrec)
      File ""/home/gheorghe/anaconda3/lib/python3.8/site-packages/conda/core/link.py"", line 288, in &lt;genexpr&gt;
        packages_info_to_link = tuple(read_package_info(prec, pcrec)
      File ""/home/gheorghe/anaconda3/lib/python3.8/site-packages/conda/gateways/disk/read.py"", line 89, in read_package_info
        package_metadata = read_package_metadata(epd)
      File ""/home/gheorghe/anaconda3/lib/python3.8/site-packages/conda/gateways/disk/read.py"", line 144, in read_package_metadata
        data = json.loads(f.read())
      File ""/home/gheorghe/anaconda3/lib/python3.8/json/__init__.py"", line 357, in loads
        return _default_decoder.decode(s)
      File ""/home/gheorghe/anaconda3/lib/python3.8/json/decoder.py"", line 337, in decode
        obj, end = self.raw_decode(s, idx=_w(s, 0).end())
      File ""/home/gheorghe/anaconda3/lib/python3.8/json/decoder.py"", line 355, in raw_decode
        raise JSONDecodeError(""Expecting value"", s, err.value) from None
    json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)

`$ /home/gheorghe/anaconda3/bin/conda-env create -f devtools/conda-envs/ael-test.yaml`

  environment variables:
                 CIO_TEST=&lt;not set&gt;
  CONDA_AUTO_UPDATE_CONDA=false
        CONDA_DEFAULT_ENV=base
                CONDA_EXE=/home/gheorghe/anaconda3/bin/conda
             CONDA_PREFIX=/home/gheorghe/anaconda3
    CONDA_PROMPT_MODIFIER=(base)
         CONDA_PYTHON_EXE=/home/gheorghe/anaconda3/bin/python
               CONDA_ROOT=/home/gheorghe/anaconda3
              CONDA_SHLVL=1
           CURL_CA_BUNDLE=&lt;not set&gt;
                     PATH=/home/gheorghe/anaconda3/bin:/home/gheorghe/.vscode-server/bin/f4af3cb
                          f5a99787542e2a30fe1fd37cd644cc31f/bin:/home/gheorghe/anaconda3/bin:/ho
                          me/gheorghe/anaconda3/condabin:/usr/local/sbin:/usr/local/bin:/usr/sbi
                          n:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/mnt/c/Program Files
                          /WindowsApps/Microsoft.WindowsTerminal_1.11.2921.0_x64__8wekyb3d8bbwe:
                          /mnt/c/Program Files/copasi.org/COPASI 4.29.228/bin:/mnt/c/Windows/sys
                          tem32:/mnt/c/Windows:/mnt/c/Windows/System32/Wbem:/mnt/c/Windows/Syste
                          m32/WindowsPowerShell/v1.0:/mnt/c/Windows/System32/OpenSSH:/mnt/c/Prog
                          ram Files (x86)/Intel/Intel(R) Management Engine
                          Components/DAL:/mnt/c/Program Files/Intel/Intel(R) Management Engine
                          Components/DAL:/mnt/c/Program Files/NVIDIA Corporation/NVIDIA
                          NvDLISR:/mnt/c/Program Files (x86)/NVIDIA
                          Corporation/PhysX/Common:/mnt/c/Program Files/Mullvad VPN/resources:/m
                          nt/c/WINDOWS/system32:/mnt/c/WINDOWS:/mnt/c/WINDOWS/System32/Wbem:/mnt
                          /c/WINDOWS/System32/WindowsPowerShell/v1.0:/mnt/c/WINDOWS/System32/Ope
                          nSSH:/mnt/c/Program Files (x86)/Common Files/Propellerhead
                          Software/ReWire:/mnt/c/Program Files/Common Files/Propellerhead
                          Software/ReWire:/mnt/c/Program Files/NCBI/blast-2.12.0+/bin:/mnt/c/Use
                          rs/gheor/AppData/Local/Programs/Microsoft VS Code/bin:/mnt/c/Users/ghe
                          or/AppData/Local/Microsoft/WindowsApps:/snap/bin
       REQUESTS_CA_BUNDLE=&lt;not set&gt;
            SSL_CERT_FILE=&lt;not set&gt;

     active environment : base
    active env location : /home/gheorghe/anaconda3
            shell level : 1
       user config file : /home/gheorghe/.condarc
 populated config files : /home/gheorghe/.condarc
          conda version : 4.10.3
    conda-build version : 3.21.5
         python version : 3.8.11.final.0
       virtual packages : __linux=5.10.16.3=0
                          __glibc=2.31=0
                          __unix=0=0
                          __archspec=1=x86_64
       base environment : /home/gheorghe/anaconda3  (writable)
      conda av data dir : /home/gheorghe/anaconda3/etc/conda
  conda av metadata url : None
           channel URLs : https://repo.anaconda.com/pkgs/main/linux-64
                          https://repo.anaconda.com/pkgs/main/noarch
                          https://repo.anaconda.com/pkgs/r/linux-64
                          https://repo.anaconda.com/pkgs/r/noarch
          package cache : /home/gheorghe/anaconda3/pkgs
                          /home/gheorghe/.conda/pkgs
       envs directories : /home/gheorghe/anaconda3/envs
                          /home/gheorghe/.conda/envs
               platform : linux-64
             user-agent : conda/4.10.3 requests/2.26.0 CPython/3.8.11 Linux/5.10.16.3-microsoft-standard-WSL2 ubuntu/20.04.3 glibc/2.31
                UID:GID : 1000:1000
             netrc file : None
           offline mode : False


An unexpected error has occurred. Conda has prepared the above report.

If submitted, this report will be used by core maintainers to improve
future releases of conda.

Edit:
This is the .yaml file:
channels:
  - conda-forge
  - pytorch
dependencies:
  - python
  - pip

  - numpy
  - scipy
  - scikit-learn

  - qcelemental
  - openbabel

  - matplotlib
  - seaborn

  - mlflow

  - pytorch
  - torchvision

  - lark-parser
  - cython

  - pip:
    - torchani
    - ""git+https://github.com/RMeli/mdanalysis.git@develop#egg=MDAnalysis&amp;subdirectory=package""

  - black
  - flake8
  - mypy
  - isort

  - pytest
  - pytest-xdist
  - pytest-cov
  - codecov

",1,918,"There is no problem in your environment, I managed to install it, getting
Successfully built MDAnalysis
Installing collected packages: msgpack, tqdm, mrcfile, mmtf-python, gsd, fasteners, biopython, GridDataFormats, torchani, MDAnalysis
Successfully installed GridDataFormats-1.0.1 MDAnalysis-2.6.0.dev0 biopython-1.81 fasteners-0.18 gsd-3.1.1 mmtf-python-1.1.3 mrcfile-1.4.3 msgpack-1.0.5 torchani-2.2.3 tqdm-4.66.1

done
#
# To activate this environment, use
#
#     $ conda activate ael

in the end.
The problem you encountering is probably related to https://github.com/conda/conda/issues/9590
which has a workaround
sudo rm -r ~/.condarc

or editing the ~/.condarc like https://github.com/conda/conda/issues/9590#issuecomment-1003211237
",,
Seaborn unexpected result,https://stackoverflow.com/questions/65472737,TypeError: sample_chain() got an unexpected keyword argument &#39;seed&#39; - Tensorflow 2.0,"Error with Tensorflow 2.0 using MCMC on MacOS 10.13.6
The error on the console:
2020-12-27 22:06:48.253835: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instructions in performance critical operations:  SSE4.1 SSE4.2 AVX
To enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate compiler flags.
2020-12-27 22:06:48.254353: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 4. Tune using inter_op_parallelism_threads for best performance.
objc[69111]: Class zmAppHelper is implemented in both /Library/ScriptingAdditions/zOLPluginInjection.osax/Contents/MacOS/zOLPluginInjection (0x1a48eaf4f0) and /Library/Application Support/Microsoft/ZoomOutlookPlugin/zOutlookPlugin64.bundle/Contents/MacOS/zOutlookPlugin64 (0x1a490e0518). One of the two will be used. Which one is undefined.
objc[69111]: class `ERCalendarEventEditorWindowController' not linked into application
Traceback (most recent call last):
  File ""dc7.py"", line 131, in &lt;module&gt;
    chains, kernel_results = run_chain(initial_state)
  File ""/Users/ram/opt/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py"", line 457, in __call__
    result = self._call(*args, **kwds)
  File ""/Users/ram/opt/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py"", line 503, in _call
    self._initialize(args, kwds, add_initializers_to=initializer_map)
  File ""/Users/ram/opt/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py"", line 408, in _initialize
    *args, **kwds))
  File ""/Users/ram/opt/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py"", line 1848, in _get_concrete_function_internal_garbage_collected
    graph_function, _, _ = self._maybe_define_function(args, kwargs)
  File ""/Users/ram/opt/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py"", line 2150, in _maybe_define_function
    graph_function = self._create_graph_function(args, kwargs)
  File ""/Users/ram/opt/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py"", line 2041, in _create_graph_function
    capture_by_value=self._capture_by_value),
  File ""/Users/ram/opt/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow_core/python/framework/func_graph.py"", line 915, in func_graph_from_py_func
    func_outputs = python_func(*func_args, **func_kwargs)
  File ""/Users/ram/opt/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py"", line 358, in wrapped_fn
    return weak_wrapped_fn().__wrapped__(*args, **kwds)
  File ""/Users/ram/opt/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow_core/python/framework/func_graph.py"", line 905, in wrapper
    raise e.ag_error_metadata.to_exception(e)
TypeError: in converted code:

    dc7.py:117 run_chain  *
        return tfp.mcmc.sample_chain(

    **TypeError: sample_chain() got an unexpected keyword argument 'seed'**

Versions
MacOS 10.13.6 High Sierra

tensorflow                2.0.0           mkl_py37hda344b4_0  
tensorflow-base           2.0.0           mkl_py37h66b1bf0_0  
tensorflow-estimator      2.0.0              pyh2649769_0  
tensorflow-probability    0.8.0                      py_0    conda-forge
jupyter_client            6.1.7                      py_0  
jupyter_core              4.7.0            py37hecd8cb5_0  
jupyterlab_pygments       0.1.2                      py_0  
ipython                   7.19.0           py37h01d92e1_0  
ipython_genutils          0.2.0              pyhd3eb1b0_1  
python                    3.7.9                h26836e1_0  
python-dateutil           2.8.1                      py_0  
python_abi                3.7                     1_cp37m    conda-forge

The source-code:
import os
os.environ['KMP_DUPLICATE_LIB_OK'] = 'True'

from pprint import pprint
import matplotlib.pyplot as plt
import numpy as np
import seaborn as sns

#import tensorflow as tf
#print(tf.__version__)

import tensorflow.compat.v2 as tf
tf.enable_v2_behavior()

import tensorflow_probability as tfp

sns.reset_defaults()
sns.set_context(context = 'talk', font_scale = 0.7)
plt.rcParams['image.cmap'] = 'viridis'

#%matplotlib inline

tfd = tfp.distributions
tfb = tfp.bijectors


#### ============================================

#@title Utils { display-mode: ""form"" }
def print_subclasses_from_module(module, base_class, maxwidth=80):
  import functools, inspect, sys
  subclasses = [name for name, obj in inspect.getmembers(module)
                if inspect.isclass(obj) and issubclass(obj, base_class)]
  def red(acc, x):
    if not acc or len(acc[-1]) + len(x) + 2 &gt; maxwidth:
      acc.append(x)
    else:
      acc[-1] += "", "" + x
    return acc
  print('\n'.join(functools.reduce(red, subclasses, [])))

# Generate some data
def f(x, w):
  # Pad x with 1's so we can add bias via matmul
  x = tf.pad(x, [[1, 0], [0, 0]], constant_values=1)
  linop = tf.linalg.LinearOperatorFullMatrix(w[..., np.newaxis])
  result = linop.matmul(x, adjoint=True)
  return result[..., 0, :]

num_features = 2
num_examples = 50
noise_scale = .5
true_w = np.array([-1., 2., 3.])

xs = np.random.uniform(-1., 1., [num_features, num_examples])
ys = f(xs, true_w) + np.random.normal(0., noise_scale, size=num_examples)

# Visualize the data set
plt.scatter(*xs, c=ys, s=100, linewidths=0)

grid = np.meshgrid(*([np.linspace(-1, 1, 100)] * 2))
xs_grid = np.stack(grid, axis=0)
fs_grid = f(xs_grid.reshape([num_features, -1]), true_w)
fs_grid = np.reshape(fs_grid, [100, 100])
plt.colorbar()
plt.contour(xs_grid[0, ...], xs_grid[1, ...], fs_grid, 20, linewidths=1)
plt.show()

### Sampling the noise scale

# Define the joint_log_prob function, and our unnormalized posterior.
def joint_log_prob(w, sigma, x, y):
  # Our model in maths is
  #   w ~ MVN([0, 0, 0], diag([1, 1, 1]))
  #   y_i ~ Normal(w @ x_i, noise_scale),  i=1..N

  rv_w = tfd.MultivariateNormalDiag(
    loc=np.zeros(num_features + 1),
    scale_diag=np.ones(num_features + 1))
  
  rv_sigma = tfd.LogNormal(np.float64(1.), np.float64(5.))

  rv_y = tfd.Normal(f(x, w), sigma[..., np.newaxis])
  return (rv_w.log_prob(w) +
          rv_sigma.log_prob(sigma) +
          tf.reduce_sum(rv_y.log_prob(y), axis=-1))

# Create our unnormalized target density by currying x and y from the joint.
def unnormalized_posterior(w, sigma):
  return joint_log_prob(w, sigma, xs, ys)


# Create an HMC TransitionKernel
hmc_kernel = tfp.mcmc.HamiltonianMonteCarlo(
  target_log_prob_fn=unnormalized_posterior,
  step_size=np.float64(.1),
  num_leapfrog_steps=4)



# Create a TransformedTransitionKernl
transformed_kernel = tfp.mcmc.TransformedTransitionKernel(
    inner_kernel=hmc_kernel,
    bijector=[tfb.Identity(),    # w
              tfb.Invert(tfb.Softplus())])   # sigma


# Apply a simple step size adaptation during burnin
@tf.function
def run_chain(initial_state, num_results=1000, num_burnin_steps=500):
  adaptive_kernel = tfp.mcmc.SimpleStepSizeAdaptation(
      transformed_kernel,
      num_adaptation_steps=int(.8 * num_burnin_steps),
      target_accept_prob=np.float64(.75))

  return tfp.mcmc.sample_chain(
    num_results=num_results,
    num_burnin_steps=num_burnin_steps,
    current_state=initial_state,
    kernel=adaptive_kernel,
    seed=(0, 1),
    trace_fn=lambda cs, kr: kr)


# Instead of a single set of initial w's, we create a batch of 8.
num_chains = 8
initial_state = [np.zeros([num_chains, num_features + 1]),
                 .54 * np.ones([num_chains], dtype=np.float64)]

chains, kernel_results = run_chain(initial_state)

r_hat = tfp.mcmc.potential_scale_reduction(chains)
print(""Acceptance rate:"", kernel_results.inner_results.inner_results.is_accepted.numpy().mean())
print(""R-hat diagnostic (per w variable):"", r_hat[0].numpy())
print(""R-hat diagnostic (sigma):"", r_hat[1].numpy())

w_chains, sigma_chains = chains

",0,1360,"I was using incompatible versions of tensorflow and tensorflow_probability.
With the following versions, the above Typererror went away:
ipython                   7.19.0                   pypi_0    pypi
ipython-genutils          0.2.0                    pypi_0    pypi
python                    3.7.9                h26836e1_0
python-dateutil           2.8.1                    pypi_0    pypi
tensorboard               2.4.0                    pypi_0    pypi
tensorboard-plugin-wit    1.7.0                    pypi_0    pypi
tensorflow                2.3.0                    pypi_0    pypi
tensorflow-estimator      2.3.0                    pypi_0    pypi
tensorflow-probability    0.11.0                   pypi_0    pypi
",,
Seaborn unexpected result,https://stackoverflow.com/questions/49594789,Column colors in clustermap of Python seaborn give unexpected results,"I am trying to visualize clusters of columns with different colors. However, in my case colors do not really show labels. Not sure what is wrong.

Reproducible example:

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt

data = pd.DataFrame(np.random.normal(0, 1, [100, 100]))
labels = np.repeat([0, 1, 2, 3], [25, 25, 25, 25], axis=0)
data.columns = labels
lut = dict(zip(set(labels), sns.mpl_palette(""autumn"", len(set(labels)))))
col_colors=pd.DataFrame(labels)[0].map(lut)

sns.clustermap(data, col_colors=col_colors,
               col_cluster=False, row_cluster=False)
plt.show()


We can see that col_colors have 4 unique values:

print(len(set(col_colors)))


And we can see that columns represent different labels (bottom of the plot). However, in my case columns are represented in only red color. 

how it looks on my desktop
",0,1682,"for those who are interested. It should be following:

sns.clustermap(data, col_colors=col_colors.values,
           col_cluster=False, row_cluster=False)


Note from a developer:

When the col_colors are a Pandas object, the index information is used to align them with the columns (same for the rows). That means that in your case, only the first three colors are used, which in your example are all red. Pass col_colors.values if you don't want to match on the index.
",,
Seaborn unexpected issue,https://stackoverflow.com/questions/46826093,Permission Error For Conda Update,"Python newbie here. I have encountered a permission problem with anaconda. Everything runs ok, but I do not seem to be able to update conda, create new environments or install new packages.

When I try to update (conda update conda) it I get:


  Fetching package metadata ..... An unexpected error has occurred.
    Please consider posting the following information to the
    conda GitHub issue tracker at:
  
  https://github.com/conda/conda/issues


Current conda version:

platform : osx-64
conda version : 4.3.29
conda is private : False
conda-env version : 4.3.29
conda-build version : not installed
python version : 2.7.11.final.0
requests version : 2.14.2
root environment : /anaconda  (writable)
default environment : /anaconda
envs directories : /anaconda/Users/Tina/.conda/envs
package cache : /anaconda/Users/Tina/.conda/pkgs
channel URLs : https://conda.anaconda.org/anaconda-fusion/osx-64
               https://conda.anaconda.org/anaconda-fusion/noarch
               https://repo.continuum.io/pkgs/main/osx-64
               https://repo.continuum.io/pkgs/main/noarch
               https://repo.continuum.io/pkgs/free/osx-64
               https://repo.continuum.io/pkgs/free/noarch
               https://repo.continuum.io/pkgs/r/osx-64
               https://repo.continuum.io/pkgs/r/noarch
               https://repo.continuum.io/pkgs/pro/osx-64
               https://repo.continuum.io/pkgs/pro/noarch
config file : /Users/Tina/.condarc
netrc file : None
offline mode : False
user-agent : conda/4.3.29 requests/2.14.2 CPython/2.7.11 Darwin/15.5.0 OSX/10.11.5    
UID:GID : 501:20


$ /anaconda/bin/conda update conda

Traceback (most recent call last):
  File ""/anaconda/lib/python2.7/site-packages/conda/exceptions.py"", line 640, in conda_exception_handler
    return_value = func(*args, **kwargs)
  File ""/anaconda/lib/python2.7/site-packages/conda/cli/main.py"", line 140, in _main
    exit_code = args.func(args, p)
  File ""/anaconda/lib/python2.7/site-packages/conda/cli/main_update.py"", line 65, in execute
    install(args, parser, 'update')
  File ""/anaconda/lib/python2.7/site-packages/conda/cli/install.py"", line 231, in install
    unknown=index_args['unknown'], prefix=prefix)
  File ""/anaconda/lib/python2.7/site-packages/conda/core/index.py"", line 101, in get_index
    index = fetch_index(channel_priority_map, use_cache=use_cache)
  File ""/anaconda/lib/python2.7/site-packages/conda/core/index.py"", line 120, in fetch_index
    repodatas = collect_all_repodata(use_cache, tasks)
  File ""/anaconda/lib/python2.7/site-packages/conda/core/repodata.py"", line 75, in collect_all_repodata
    repodatas = _collect_repodatas_serial(use_cache, tasks)
  File ""/anaconda/lib/python2.7/site-packages/conda/core/repodata.py"", line 485, in _collect_repodatas_serial
    for url, schan, pri in tasks]
  File ""/anaconda/lib/python2.7/site-packages/conda/core/repodata.py"", line 115, in func
    res = f(*args, **kwargs)
  File ""/anaconda/lib/python2.7/site-packages/conda/core/repodata.py"", line 467, in fetch_repodata
    touch(cache_path)
  File ""/anaconda/lib/python2.7/site-packages/conda/gateways/disk/update.py"", line 64, in touch
    utime(path, None)
OSError: [Errno 13] Permission denied: '/anaconda/pkgs/cache/9cd9d6b5.json'```


I get the same error when trying to install seaborn or creating an environment. I am reluctant to use sudo because I do not want to break things.

I do not understand what is going on here, so any help would be highly appreciated. 

Thanks so much; 
T
",2,9858,"For humble Windows users that cannot use sudo: You have to open the conda console as Administrator by right clicking on the console icon and then select run as administrator. Then conda update conda should work fine.
","The user that you are using to run conda update conda does not have write permission on /anaconda/pkgs/cache/.

If you don't want to manage anaconda as the superuser, I would recommend that you create a new user group (i.e. anaconda_admin) and run:

sudo groupadd anaconda_admin
sudo chown -R :anaconda_admin /anaconda


Then you will need to ensure that permissions are something like:

sudo chmod -R 775 /anaconda


And finally that your user is in the anaconda_admin group:

sudo adduser &lt;&lt;&lt;your_user&gt;&gt;&gt; anaconda_admin

","You ought to use sudo in order to write certain files into system. It is perfectly fine and will not break you OS, unless you work with sophisticated and rudimentary packages and installers (conda and python libraries are absolutely fine).

sudo conda update conda should do the thing not only with updating conda, but also with other dependencies and packages you wish to install.

In short, the installer tries to write a file into a certain directory (or modify a file in a directory) that it has not got an access to. With sudo you make them do that as you run it with appended priviliges.
"
Seaborn strange behavior,https://stackoverflow.com/questions/33423758,How to create multiple series scatter plot with connected points using seaborn?,"I have a set of data stored in a pandas dataframe. I'm trying to use seaborn's pointplot() to create a multiple-series scatter plot with connected points. Each series has different (x,y) values, and they are stored as floats in my data frame. Each row has a label, differentiating each series. I'm using Python 2.7, seaborn version 0.5.1, and matplotlib version 1.4.3. 

Everything I've managed to find tells me that I can achieve this with the following:

import matplotlib.pyplot as plt
import seaborn as sns

# Suppose my dataframe is called 'df', with columns 'x', 'y', and 'label'.
sns.pointplot(x = 'x', y = 'y', hue = 'label', data = df)


However, this results in some strange behavior:


The colors are correctly identified, but only some of the points are connected 
The numbers on the x-axis overlap and it appears as though each data point is being labeled with it's value rather than scaling it with appropriate, clean values (seems to be treating the x data as a string/label rather than floats).


I attempted to work around this by splitting my data frame into pieces. This is not ideal because I may have about 10+ series to plot simultaneously, and I'd prefer to not split the data manually:

df1 = df[df.test_type.values == ""label 1""]
df2 = df[df.test_type.values == ""label 2""]

ax = sns.pointplot(x = 'x',y='y', color = ""blue"", data = df1)
sns.pointplot(x = 'x', y = 'y', data = df2, color=""red"", ax = ax)


In this case, all points are connected and they are colored appropriately, but again, the x axis is showing very strange behavior. Even though my x-values from each data frame are different, the plot aligns them so that they appear to be the same.

Now, I'm not sure how to post my output/plots cleanly, but some of my problems can be recreated with the following:

#import the necessary modules
import matplotlib.pyplot as plt
import pandas as pd
import seaborn as sns

#Here is some sample data. The 'x2' data is slightly offset from 'x1'
x1 = range(0,100,10)
x2 = range(1,100,10)
x = x1+x2

#The y-values I generate here mimic the general shape of my actual data
y1 = x1[::-1]
y2 = [i+25 for i in x1[::-1]]
y = y1+y2

#Two levels of labels that will be applied to the data
z1 = [""1""]*10
z2 = [""2""]*10
z = z1+z2

#A pandas data frame from the above data
df = pd.DataFrame({'x': x, 'y': y, 'z': z})

#Pointplot using the above data
sns.pointplot(x = 'x', y = 'y', data = df, hue = 'z')


Running this code results in the following:


All x values across all series are spaced evenly. Note that the 'x2' values are the same as 'x1' translated by '1' and they are spaced at intervals of 10 within each series. I did not expect this behavior.
The x axis doesn't have a ""clean"" looking scale. It literally labels each point's corresponding x-value. It labels the points correctly, but does not scale it appropriately. It seems like it's treating the x values as labels, similar to how a bar graph might behave.
The points are colored correctly, but no points are connected.


To summarize my question:

Is there an easier/better/more elegant way to plot multiple-series scatter plots with connected points using data stored in a pandas data frame? Seaborn's pointplot looked ideal, but it is not functioning as I expected and I'm suspecting that it might serve a purpose different from what I need to accomplish. I'm open to other solutions that can achieve this (preferably using python).

Thanks in advance. I will update my question if I can figure out how to upload the output and plots from my code.

I'm 100% new to stackoverflow. I'd love to clarify my question by posting the plots being generated by my code, but I could not figure this out. Any pointers on how to do this would be much appreciated as well so I can update the question.

EDIT: It turns out that seaborn's pointplot uses the x-axis as a categorical axis, which explains the strange behavior I've mentioned above. Is there a way to manually change the x-axis behavior from categorical to numerical? This seems like the easiest approach but I'm not very familiar with fine-tuning plots in python.
",8,17542,"I had a similar problem and I finally solved it using Seaborn's FacetGrid. I used plt.scatter for the points and the plt.plot for lines connecting the points.
g = sns.FacetGrid(df, hue=""z"", size=8)
g.map(plt.scatter, ""x"", ""y"")
g.map(plt.plot, ""x"", ""y"")


Note, this is done in Seaborn version 0.6.0 and version 0.5.1.
","With the help of @mwaskom and this question, I've managed to find a solution to my posted question:

#Assuming df is a pandas data frame with columns 'x', 'y', and 'label'
for key,grp in df.groupby('label'):
    plt.plot(grp.x,grp.y,'o-',label = key)
plt.legend(loc = 'best')

",
Seaborn strange behavior,https://stackoverflow.com/questions/29253027,pandas scatter plot colors with three points and seaborn,"There is a strange behavior when using pandas and seaborn to plot a scatter plot that has only three points: the points don't have the same color. The problem disappears when seaborn is not loaded or when there are more than three points, or when plotting with matplotlib's scatter method directly. See the following example:

from pandas import DataFrame #0.16.0
import matplotlib.pyplot as plt #1.4.3
import seaborn as sns #0.5.1
import numpy as np #1.9.2

df = DataFrame({'x': np.random.uniform(0, 1, 3), 'y': np.random.uniform(0, 1, 3)})
df.plot(kind = 'scatter', x = 'x', y = 'y')
plt.show()




df = DataFrame({'x': np.random.uniform(0, 1, 4), 'y': np.random.uniform(0, 1, 4)})
df.plot(kind = 'scatter', x = 'x', y = 'y')
plt.show()



",5,2111,"I've tracked down the bug. The bug is in pandas technically, not seaborn as I originally thought, though it involves code from pandas, seaborn, and matplotlib...

In pandas.tools.plotting.ScatterPlot._make_plot the following code occurs to choose the colours to be used in the scatter plot

if c is None:
    c_values = self.plt.rcParams['patch.facecolor']
elif c_is_column:
    c_values = self.data[c].values
else:
    c_values = c


In your case c will be equal to None, which is the default value, and so c_values will be given by plt.rcParams['patch.facecolor'].

Now, as part of setting itself up, seaborn modifies plt.rcParams['patch.facecolor'] to (0.5725490196078431, 0.7764705882352941, 1.0) which is an RGB tuple. If seaborn is not used then the value is the matplotlib default which is 'b' (a string indicating the colour ""blue"").

c_values is then used later on to actually plot the graph within ax.scatter

scatter = ax.scatter(data[x].values, data[y].values, c=c_values,
                     label=label, cmap=cmap, **self.kwds)


The issue arises because the keyword argument c can accept multiple different types of argument, it can accept:- 


a string (such as 'b' in the original matplotlib case); 
a sequence of color specifications (say a sequence of RGB values); 
a sequence of values to map onto the current colormap. 


The matplotlib docs specifically state the following, highlighting mine


  c can be a single color format string, or a sequence of color specifications of length N, or a sequence of N numbers to be mapped to colors using the cmap and norm specified via kwargs (see below). Note that c should not be a single numeric RGB or RGBA sequence because that is indistinguishable from an array of values to be colormapped. c can be a 2-D array in which the rows are RGB or RGBA, however.


What basically happens is that matplotlib takes the c_values value (which is a tuple of three numbers) and then maps those colours onto the current colormap (which is set by pandas to be Greys by default). As such, you get three scatter points with different ""greyishness"". When you have more than 3 scatter points, matplotlib assumes that it must be a RGB tuple because the length doesn't match the length of the data arrays (3 != 4) and so uses it as a constant RBG colour.

This has been written up as a bug report on the pandas Github here.
","You might want to try this:

import seaborn.apionly as sns


And see This question for more details.
",
Seaborn strange behavior,https://stackoverflow.com/questions/49185427,Seaborn two plots on one fig: x-values are is off by one,"I'm experience strange behavior when plotting two plots on top of each other in seaborn. The bar plot appears to work fine, but the regplot appears to be off by one. Note the lack of a reg data point for x=1, and compare the x=2 value to the value in the table for x below, it's clearly off by one.



My pandas Dataframe looks like this:

    Threshold per Day   # Alarms    Percent Reduction
0   1                   791         96.72
1   2                   539         93.90
2   3                   439         91.94
3   4                   361         89.82
4   5                   317         88.26
5   6                   263         85.94
6   7                   233         84.41
7   8                   205         82.78
8   9                   196         82.17
9   10                  176         80.66


The code I'm using here is:

%matplotlib inline

import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

fig, ax = plt.subplots()

ax2 = ax.twinx()
sns.barplot(x='Threshold per Day', y=""# Alarms"", data=results_df, ax=ax, color='lightblue')
sns.regplot(x='Threshold per Day', y='Percent Reduction', data=results_df, marker='x', fit_reg=False, ax=ax2)


Any ideas what's going on or how to fix it?
",3,1176,"Caveat: This only addresses a possible fix, I don't know why that is happening in seaborn (but see Edit and comment)

If you're looking just to get a decent plot in the meantime, I would recommend just switching to pure matplotlib, at least just for this plot and any others with similarly strange behaviour. You can get a very similar plot with the following code:

fig, ax = plt.subplots(1,1, sharex=True)
ax2 = ax.twinx()

ax.bar(results_df['Threshold per Day'], results_df['# Alarms'], color='lightblue')
ax2.scatter(results_df['Threshold per Day'], results_df['Percent Reduction'], marker='x')
ax.set_ylabel('# of Alarms')
ax2.set_ylabel('Percent Reduction')
ax.set_xlabel('Threshold Per Day')
plt.xticks(range(1,11))
plt.show()




Edit to take into account ImportanceOfBeingErnest's comment:

You can obtain this plot in seaborn using:

fig, ax = plt.subplots()
ax2 = ax.twinx()

sns.barplot(x=results_df['Threshold per Day'], 
            y=results_df[""# Alarms""], ax=ax, color='lightblue')
sns.regplot(x=np.arange(0,len(results_df)), 
            y=results_df['Percent Reduction'], marker='x', 
            fit_reg=False, ax=ax2)
plt.show()


Turns out that in matplotlib, a barplot's category seems to be interpreted as a numeric when possible, whereas in seaborn, it is interpreted as a string, and the locations start at location 0 by default; as your regplot is evenly spaced on the x axis, you can just force their locations onto a range from 0 to the length of your dataframe as above.
",,
Seaborn strange behavior,https://stackoverflow.com/questions/48197738,matplotlib fill_between leaves gaps,"I'm trying to recreate seaborn's fill-only confidence interval plotting in raw matplotlib.  In doing so, I'm running into strange behavior where the fill_between function leaves gaps between the stuff it's supposed to be filling.

I'm using real-world data on this, but it's well-behaved data: the x values are on the range of about 0-15, and the y values on a range of about 25-85.  I'm using statsmodels to fit the line and generate the confidence intervals with essentially the code from this prior SO, and the fitted values as well as the upper and lower bounds of the confidence intervals are as they should be (the ranges are appropriate, etc.).  So there's nothing wrong with the data.

Here's the relevant part of the code:

def make_plot(x, y):
    fig = plt.figure(figsize=(12, 9))
    ax = fig.add_subplot(1, 1, 1)
    ax.plot(x, y, 'k.', ms=5)
    ax.locator_params(nbins=3)
    ax.spines['top'].set_visible(False)
    ax.spines['right'].set_visible(False)
    regline =  sm.OLS(y,sm.add_constant(x)).fit()
    fitted = regline.fittedvalues
    ax.plot(x, fitted, color=(0.2, 0.2, 0.2, 0.2), linewidth=2)
    ci_low, ci_high = get_ci_values(regline)
    ax.fill_between(x, ci_low, fitted, facecolor=(0.4, 0.4, 0.9, 0.2))
    ax.fill_between(x, ci_high, fitted, facecolor=(0.9, 0.4, 0.4, 0.2))
    return fig


The line fill works fine until it hits around x=10, y=50, and then it starts to leave bizarre gaps where it doesn't come all the way to the regression line.  Here's an example: 



What have I done wrong here?  I've tried a bunch of stuff, including: 


adding lines for the low and high confidence intervals
adding interpolate=True to the fill_between calls
adding where=x&gt;0 to the fill_between calls


but none of that makes any difference.

I also note that seaborn manages to make its beautiful fills using fill_between, using exactly the same strategy, and seaborn's plotting works correctly on the data I'm using...
",1,1197,"One cannot know for sure because the question is missing the essential part, namely the data itself (see Minimal, Complete, and Verifiable example). 

The strong suspicion here would however be that the data is not sorted. 

The (untested) solution would be to sort the data, 

ax.plot(np.sort(x), fitted[np.argsort(x)])
ax.fill_between(np.sort(x), ci_low[np.argsort(x)], fitted[np.argsort(x)])


To understand why values need to be sorted, maybe a picture can tell more than a thousands words.


",,
Seaborn strange output,https://stackoverflow.com/questions/61769201,How to create business ready reports from jupyter notebooks?,"I have took quite some time to get a reasonable answer to my inquiry by myself but ran into a dead end and hope you guys can help me.

Issue:
For the purpose of business reporting, I have created some juypter notebooks which include multiple pandas tables and seaborn / matplotlib plots as code cell output with some occasional markdown cells in between to provide explanations. Now, I want these reports to be in a business-ready format to share them with stakeholders. With business-ready I intend the following requirements:


The report does not include code
Output file format: PDF
The report includes a title page with title, additional information (e.g. date of analysis) and a table of contents
Tables are in a appealing visual format that allows easy reception of information 
The report is well structured


... and I am not able to get all these requirements together. 

So far, I prefer to work with vscode and use the browser based juypter notebook if necessary (which unfortunately lacks some functionalities).

What I have tried:

(1) this was a no brainer, I just --no-input to the nbconvert command in the anaconda shell and whatever I do regarding the next points, it excludes the code

(2) There are two ways I could find so far, which influence all subsequent steps/requirements


Way 1 (""html detour""): I convert the .ipynb to html and print it as PDF (this is a 2-step process, thus I see it as a detour)
Way 2 (""latex conversion""): I convert it to a PDF via nbconvert --to pdf and it uses latex in the background to create a pdf


(3) ...and here start the issues: 
html detour: I can get a toc via the nbextension extension for jupyter notebooks and with it, I can use either the H1 header level as title or include an extra markdown cell and increase the font size with an html command such that it looks appealing. Additional information are added manually in extra code cells. However, the toc only works in the browser version of jupyter, which results in writing the analysis in vscode, going to the browser to add the toc, converting it in the shell, open the html and print it as pdf...  
latex conversion: I can set up a latex template, which is included in the nbconvert command that includes a toc by design. However, it either picks up the filename as title automatically or a title I can set in the metadata of the notebook, which I can only edit from the browser. Further, the date of conversion is added below the title automatically as well, which might note be the date of the analysis in case I have to reconvert it because someone wants a minor change or something. Thus, I cannot turn auto title and date off (at least I couldn't find an option so far) and I have multiple steps as well.

(4) This one makes eventually the difference in the usability of the report 
html detour: The format in the html file itself is the quite appealing format you usually get from tables using display() command on a table in jupyter (which is used anyway if you just call a variable in juyper without print()) or if you build a table in a markdown cell. The table has a bold header and every other row has a grey background. Using pandas .style method, I can format the table in the html file very nicely with red fon color for negative values only or percentage bars as cell background. However, I loose all these formats when I print the PDF. Then its just a bold header, a bold line splitting header and body and the rows. Further, all cell output tables are left aligned in the html (and I refer to the table itself, not its content) and the markdown tables are centered, which looks strange or rather - and this is the issue - unprofessional. The benefit, however, is that these tables are somewhat auto-adjusted to a letter size format in a certain range if the table would be wider than a letter page. 
latex conversion: By design, the tables are not converted. I have to use pandas.set_option(display.large_repr, True) to convert all subsequent pandas table output or add .to_latex()to every single pandas table. This has several downfalls. Using this, all tables are displayed as the code that would be required to build a table in latex and while doing the analysis, this is often harder to interpret... especially if you want to find errors. Adding it only when the analysis is done, creates just unnecessary iterations. Further, I want to use the last report as template for the next and would have to delete the command, do my stuff and add it again. Wider tables taht don't fit the letter size are just cut of regardless of how much wider they are compared to the page size and I would have to check every table (last report were 20+) whether everything is included. ...and headers become longer if they include explanatory information. And finally, the latex table format eventually looks professional, but more scientifically professional and not business professional and can discourage one or another reader in my experience.

(5) So, since everything is made from cells and converted automatically, you get some strange output with headers on the end of one page and text and tables and plots on the next ...or pages with just a plot and so on... 
html detour Its hard to describe the general issues I have. If you have ever printed a website, you have probably got some weird text bulk that looks unstructured with occasional half white pages where they should not be. Thats what you get, when printing the html file of a jupyter. It would help, if I could include a forced pagebreak and you can find several versions of adding pagebreaks in the cell or metadata of cells but they do not work since the html is created with a high level setting prohibiting a pagebreak. Thus, I could only go in the html code and add page breaks manually. Manuel effort I would like to avoid. 
latex conversion:Well, \pagebreakworks.

So, due to the issues above, I currently tend towards the html detour but it does not make it look like an appealing report. I have tried several latex templates but was usually dissatisfied with the output since the .to_latex command makes it tedious and the report eventually looks like a scientific paper and not like a business report. The thing is, while this looks like a high standard, all these requirements are fulfilled by R-mardkown notebooks basically out of the box with slight additions to the yaml command in the top of the file. But I cannot use them for the report I want to create.

So, after this long intro (and I thank everybody for taking the time to read it), my question is  how do I get appealing reports from a jupyter notebook?

Thanks!!!!!
",14,10679,"Honestly, I'm in the same boat as you. It seems quite challenging to generate publication-ready PDF Reports natively from JupyterLab / Jupyter using nbconvert and friends.
Solution (that I'm using): What I can recommend is a different tool that will help you make amazing PDF reports. It's using RStudio's Rmarkdown (completely free) and the new ability to use Python from RStudio. I'm going to be teaching this in my R/Python Teams Course (course waitlist is up).
Report Example

Here's how I'm doing it in my course:
Step 1 - Install Rstudio IDE 1.4+ &amp; R 4.0+
Head over to Rstudio and install their IDE. You'll also need to install R.
Step 2 - Create a Project

Step 3 - Set Python Environment of your Project
Go to Tools &gt; Project Options. Select the Python Interpreter.

Step 4 - Begin Coding Markdown and Python
Use ""Python Code Chunks"".

Step 5 - Knit to PDF
Note that this requires some form of LatTex. You can install easily with this package: tinytex.

Step 6 - Check out your PDF Report
Looks pretty slick.

Try it out and see if it works for you.
","Apart from installation and other pieces there are several aspects which make usage of nbconvert for files conversion quite a tedious task .
Anyone tried out the Jupyter Executable Notebook  or R markdown methods ( they are useful but there is an extra cost of time and efforts which makes it less feasible )
What i found to be very useful is there are many websites serving this purpose it quick, easy and hassle free .
I use this IPYNB TO PDF , there are others as well .
","I'd go like this from terminal (this is to convert to Word, but also PDF is available, just change your last output to .pdf):
jupyter nbconvert --to html notebook.ipynb --TemplateExporter.exclude_input=True &amp;&amp; pandoc notebook.html -s -o results.docx --resource-path=img --toc

"
Seaborn strange output,https://stackoverflow.com/questions/69810007,How to customize the text ticklabels for each subplot of a seaborn catplot,"Let us consider the following example (from Seaborn documentation):
titanic = sns.load_dataset(""titanic"")

fg = sns.catplot(x=""age"", y=""embark_town"",
                hue=""sex"", row=""class"",
                data=titanic[titanic.embark_town.notnull()],
                orient=""h"", height=2, aspect=3, palette=""Set3"",
                kind=""violin"", dodge=True, cut=0, bw=.2)

Output:

I want to change the tick labels on the y axis, for example by prepending a number in parenthesis: (1) Southampton, (2) Cherbourg, (3) Queenstown. I have seen this answer, and I have tried to use a FuncFormatter, but I obtain a strange result. Here is my code:
titanic = sns.load_dataset(""titanic"")

fg = sns.catplot(x=""age"", y=""embark_town"",
                hue=""sex"", row=""class"",
                data=titanic[titanic.embark_town.notnull()],
                orient=""h"", height=2, aspect=3, palette=""Set3"",
                kind=""violin"", dodge=True, cut=0, bw=.2)

from matplotlib.ticker import FuncFormatter
for ax in fg.axes.flat:
    ax.yaxis.set_major_formatter(ticker.FuncFormatter(lambda x, pos: f'({1 + pos}) {x}'))

And here is the output:

It looks like x is the same as pos in the lambda. I was expecting x to be the value of the tick label (i.e. Southampton, Cherbourg, Queenstown). What am I doing wrong?

Software versions:
matplotlib                         3.4.3
seaborn                            0.11.2

",4,2013,"
Similar to the answers for How to rotate xticklabels in a seaborn catplot, but requiring customized text for each tick of each subplot.
Text labels work differently than numeric labels that are in the other example. Numeric labels match the tick position, but that is not the case for text labels.
.get_yticklabels() gets [Text(0, 0, 'Southampton'), Text(0, 1, 'Cherbourg'), Text(0, 2, 'Queenstown')] for each subplot
As shown below, extract the text and position, and the use .set_yticklabels to set the new text label
Tested in python 3.8.12, matplotlib 3.4.3, seaborn 0.11.2

import seaborn as sns

titanic = sns.load_dataset(""titanic"")

fg = sns.catplot(x=""age"", y=""embark_town"",
                hue=""sex"", row=""class"",
                data=titanic[titanic.embark_town.notnull()],
                orient=""h"", height=2, aspect=3, palette=""Set3"",
                kind=""violin"", dodge=True, cut=0, bw=.2)

for ax in fg.axes.flat:  # iterate through each subplot
    labels = ax.get_yticklabels()  # get the position and text for each subplot
    for label in labels:
        _, y = label.get_position()  # extract the y tick position
        txt = label.get_text()  # extract the text
        txt = f'({y + 1}) {txt}'  # update the text string
        label.set_text(txt)  # set the text
    ax.set_yticklabels(labels)  # update the yticklabels


",,
Seaborn strange output,https://stackoverflow.com/questions/48232563,Difficulty plotting sequence of seaborn heatmaps,"Whenever I plot a single seaborn heatmap, I get a beautiful result which looks like this:



But, when I try to plot a sequence of seaborn heatmaps, as follows, I get really strange output:

for i in range(5):

    # create mesh grid:
    res = 0.1 ## set resolution
    X,Y = 10, 10
    xy = np.mgrid[0:int(X):res, 0:int(Y):res].reshape(2,-1).T
    values = np.sum(xy,1)
    sns.heatmap(values.reshape((int(X/res),int(Y/res))),cmap=""YlGnBu"")

    plt.savefig(folder+""_""+str(i)+""_.png"")


The first image in the sequence is perfect but the second looks like this:



The third looks like this:



...and so forth. For reasons I can't understand it appears that seaborn is somehow adding a new colour bar each time. 

This problem is non-existent when I set cbar to False but I actually want a colour bar for each individual heatmap. 
",0,285,"It seems you rather want to create 5 individual figures. In order to create a figure, you may use plt.figure()

import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np

for i in range(5):
    plt.figure()
    # create mesh grid:
    res = 0.1 ## set resolution
    X,Y = 10, 10
    xy = np.mgrid[0:int(X):res, 0:int(Y):res].reshape(2,-1).T
    values = np.sum(xy,1)
    sns.heatmap(values.reshape((int(X/res),int(Y/res))),cmap=""YlGnBu"")

    plt.savefig(""seaborn_""+str(i)+""_.png"")

plt.close(""All"")


Alternatively you may clear the figure with plt.clf().

import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np

for i in range(5):
    plt.clf()
    # create mesh grid:
    res = 0.1 ## set resolution
    X,Y = 10, 10
    xy = np.mgrid[0:int(X):res, 0:int(Y):res].reshape(2,-1).T
    values = np.sum(xy,1)
    sns.heatmap(values.reshape((int(X/res),int(Y/res))),cmap=""YlGnBu"")

    plt.savefig(""seaborn_""+str(i)+""_.png"")

",,
Seaborn strange result,https://stackoverflow.com/questions/53730486,Seaborn Heatmap without lines between cells,"I´m trying to create a heatmap with seaborn with a transparent colormap since an image should be displayed in the background. The heatmap creation works fine so far, however some lines between the cells are still visible even though the linewidth of the heatmap is set to 0.0.

The code for the creation of the heatmap looks like the following:

ax = sns.heatmap(image, cmap=""rocket_r"", linewidths=0.0)
ax.collections[0].set_alpha(0.5)


Where image is 64x64 numpy array. The resulting heatmap looks like this:
Heatmap (sorry not enough repuation for embedding pictures)

The problem are the thin lines between the cells. Strangely they aren´t at every edge.

Anyone knows how to get rid of those lines?

Many Thanks



Update 1 (Complete working example):

image = np.array([[1, 1, 2, 2], [3, 3, 3, 3], [4, 5, 4, 5], [6, 6, 6, 6]])
ax = sns.heatmap(image, cmap=""rocket_r"", linewidths=0.0)
ax.collections[0].set_alpha(0.5)
plt.show()


Results is this heatmap: 



Here you can see that there are thin lines between every column but there isn´t any line between the first and second row.
",4,4289,"The lines are the overlapping of semitransparent patches which cannot be perfectly aligned on the pixel grid.
alpha blending
An option is to not use transparency, but instead create opaque colors with alpha blending.
import matplotlib.pyplot as plt
from matplotlib.colors import ListedColormap
import numpy as np
import seaborn as sns

def get_alpha_blend_cmap(cmap, alpha):
    cls = plt.get_cmap(cmap)(np.linspace(0,1,256))
    cls = (1-alpha) + alpha*cls
    return ListedColormap(cls)

image = np.array([[1, 1, 2, 2], [3, 3, 3, 3], [4, 5, 4, 5], [6, 6, 6, 6]])
ax = sns.heatmap(image, cmap=get_alpha_blend_cmap(""rocket_r"", 0.5), linewidths=0.0)

plt.show()


An obvious advantage of this is that the colorbar has the same colors as the heatmap.
increase dpi
If the above is not an option you may increase the dpi when saving.
import matplotlib.pyplot as plt
import numpy as np
import seaborn as sns

image = np.array([[1, 1, 2, 2], [3, 3, 3, 3], [4, 5, 4, 5], [6, 6, 6, 6]])
ax = sns.heatmap(image, cmap=""rocket_r"", linewidths=0.0, edgecolor=""none"", alpha=0.5)
plt.savefig(""test.png"", dpi=1000)


This does of course not have any effect on the figure shown on screen though.
imshow
Finally, consider not using seaborn here, but instead a matplotlib imshow plot.
import matplotlib.pyplot as plt
import seaborn as sns
plt.style.use(""seaborn-dark"")
plt.rcParams[""axes.facecolor""] = ""white""
import numpy as np

image = np.array([[1, 1, 2, 2], [3, 3, 3, 3], [4, 5, 4, 5], [6, 6, 6, 6]])
im = plt.imshow(image, cmap=""rocket_r"", alpha=0.5)
plt.colorbar(im)
plt.gca().set(xticks=(range(image.shape[1])),yticks=(range(image.shape[0])))
plt.show()


","I have just come across this problem. I need to upload the plot to overleaf, so I do not like the dpi solutions. Here is what I came up with, based on the solution of OP:
import matplotlib.pyplot as plt
import numpy as np
import seaborn as sns

image = np.array([[1, 1, 2, 2], [3, 3, 3, 3], [4, 5, 4, 5], [6, 6, 6, 6]])
ax = sns.heatmap(image, cmap=""rocket_r"", linewidths=0.1)
colors = ax.collections[0].get_facecolors()
ax.collections[0].set_edgecolors(colors)
plt.imshow()

The idea is to create a thin edge around each cell with the cell's facecolor. This removes the lines without changing the original cell colors.
",
Seaborn strange result,https://stackoverflow.com/questions/71161147,How to plot seaborn lineplot and barplot on the same plot with same number of y-axes tickers and both y-axes aligned at 0 in Python,"I am having trouble plotting a lineplot and barplot (with the same inexes) on the same seaborn plot using data from the ‘results’ dataframe which I will show below (with ‘a’ data, ‘b’ data, and ‘percentiles’) in Python.
When I plot them separately, I am able to plot the lineplot fine (using ‘a’ data), however, the x-axis values of the barplot I am trying to plot (using 'b' data) does not show up. Strangely, I can plot a lineplot without any problem using the same data I am trying to plot the barplot with (‘b’ data)
When I try to plot them on the same plot, the x-axis starts from a date that is not even present in the ‘results’ dataframe.
I have even tried exporting the results dataframe as a csv and re-importing it to see if that works, but I run into the same problems.
What I would like to achieve:

Plotting the ‘a’ data as a lineplot, and the ‘b’ data as a bar plot on the same seaborn plot with different y-axes
I would like to have the same number of y-axis tickers and for the 0’s of both the y-axes to be aligned
Finally, I would like the colour of the barplot to be dependent on whether the percentile column indicated ‘low’, ‘mid’, ‘high’ (a different colour for each of these)

# Here is the 'a' and 'b' data that I start with

a = pd.read_csv(r'a.csv',sep="","", parse_dates=['date'], dayfirst=True, index_col=0)

b = pd.read_csv(r'b.csv',sep="","", parse_dates=['date'], dayfirst=True, index_col=0)

'a' DataFrame
'b' DataFrame
# After manipulating the data, here is the 'results' DataFrame I end up with

'results' DataFrame
# Plotting them separately

# Plotting lineplot using 'a' column from 'results' DataFrame

sns.lineplot(data=result.iloc[:, 0], color=""g"")

lineplot
# Plotting barplot using 'b' column from 'results' DataFrame

b_plot = sns.barplot(data=result, x=result.index, y=result.iloc[:, 2], color=""b"")

b_plot.xaxis.set_major_locator(md.YearLocator(base=4)) 
b_plot.xaxis.set_major_formatter(md.DateFormatter('%Y'))
b_plot.margins(x=0)

barplot
# Attempting to plot the lineplot and barplot on the same plot

matplotlib.rc_file_defaults()
ax1 = sns.set_style(style=None, rc=None)
fig, ax1 = plt.subplots(figsize=(12,6))

a_plot = sns.lineplot(data=result.iloc[:, 0], color=""g"", ax=ax1)
ax2 = ax1.twinx()
b_plot = sns.barplot(data=result, x=result.index, y=result.iloc[:, 2], color=""b"", ax=ax2)

b_plot.xaxis.set_major_locator(md.YearLocator(base=4)) 
b_plot.xaxis.set_major_formatter(md.DateFormatter('%Y'))
b_plot.margins(x=0)

lineplot and barplot on same plot
EDIT: I have answered the questions below
",1,1321,"Here are the answers to my questions:
matplotlib.rc_file_defaults()
ax1 = sns.set_style(style=None, rc=None)
fig, ax1 = plt.subplots(figsize=(12,6))
ax2 = ax1.twinx()

# plot the bar plot and make the colours dependent on the values in a seperate column
result_date = result.reset_index()
    
palette = {""low"":""lightgreen"",
           ""mid"":""darkseagreen"", 
           ""high"":""green""}

b_plot = sns.barplot(data = result_date, x=result_date.iloc[:, 0], 

y=result_date.iloc[:, 3], ax=ax1, hue='percentile', palette=palette, dodge = False)

# plot the lineplot
a_plot = sns.pointplot(data=result, x=result.index, y=result.iloc[:, 0], color=""black"", ax=ax2, markers = 'o', scale=0.4)

# set the x tickers to be those of the bar plot
ax1.set_xticks(np.arange(len(result_date)))
ax1.set_xticklabels(result_date.date.apply(lambda x: str(x.year)))
ax1.xaxis.set_major_locator(ticker.AutoLocator())
    
# align axis at 0, and get same number of ticks on both y-axes
max1 = np.nanmax(np.abs(ax1.get_ybound())) 
max2 = np.nanmax(np.abs(ax2.get_ybound()))
nticks = 7 
    
ax1.set_yticks(np.linspace(-max1, max1, nticks))
ax2.set_yticks(np.linspace(-max2, max2, nticks))


","Here's an example of what I suggested in the comments.
import seaborn as sns
import matplotlib.pyplot as plt
import pandas as pd

data = pd.DataFrame({'Day': [1, 2, 3, 4], 'Value': [3, 7, 4, 2], 'Value 2': [1, 7, 4, 5]})

f, ax = plt.subplots()
sns.barplot(data=data, x='Day', y='Value')

Edit: use pointplot here to align the entries.
sns.pointplot(data=data, x='Day', y='Value 2')


",
Seaborn strange result,https://stackoverflow.com/questions/69864891,How to build a population pyramid with pandas dataframe,"How to  plot a population pyramid based on the following starting dataframe?
           Age  Gender  Count
0  50-45 years    male      4
1  50-45 years  female      5
2  55-65 years    male      6
3  55-65 years  female      7
4  65-70 years    male     11
5  65-70 years  female     12

I tried the following, Population Pyramid with Python and Seaborn, but the resulting plot looks strange:
import pnadas as pd
import seaborn as sns

# data
data = {'Age': ['50-45 years', '50-45 years', '55-65 years', '55-65 years', '65-70 years', '65-70 years'],
        'Gender': ['male', 'female', 'male', 'female', 'male', 'female'], 'Count': [4, 5, 6, 7, 11, 12]}

df = pd.DataFrame(data)

# plot
sns.barplot(data=df, x='Count', y='Age',
            hue='Gender', orient='horizontal', 
            dodge=False)

I think the problem is that my age is a string.

",1,1116,"
Unlike in the linked question, 'Count' for both 'Gender' groups is positive, so with dodge=False, the 'Female' bars are drawn on top of the 'Male' bars.
Convert one of the groups to negative values, using .loc and Boolean selection.

# convert male counts to negative
df.loc[df.Gender.eq('male'), 'Count'] = df.Count.mul(-1)

# plot
sns.barplot(data=df, x='Count', y='Age', hue='Gender', orient='horizontal', dodge=False)


",,
Seaborn strange issue,https://stackoverflow.com/questions/62480198,Date format changed using seaborn pointplot,"When I use seaborn to draw a pointplot, the date on the x-axis changes to this strange format 2020-01-06T00:00:00.000000000. The dates from the 'Current Year Week Ending' column are converted to DateTime object before drawing this figure. My other graph (lineplot) uses similar inputs and format, but it doesn't have this issue, the dates are like 2020-01-06.

Does anyone know how to fix this problem?

*The total_us dataframe and sub dataframe I use in my plots are subsets of the same dataset. And I converted the date on that big dataset before creating those two subsets. So the value in 'Current Year Week Ending' column of total_us and sub should have the same DateTime format.

#create a pointplot to capture the variability                             
plt.figure(figsize = (8, 6))                                                  
sns.pointplot(x = 'Current Year Week Ending',                                               
              y = 'ASP Current Year', 
              hue ='Type', 
              data = sub, 
              markers=[""o"", ""x""],
              linestyles=[""-"", ""--""])                                
plt.xticks(rotation=45, horizontalalignment='right', fontweight='light', fontsize='medium')
plt.ticklabel_format(style='plain', axis='y')




#create a lineplot 
plt.figure(figsize=(10,7))
sns.lineplot(x='Current Year Week Ending', 
             y='Total Bulk and Bags Units', 
             hue='Type', 
             data = total_us);
plt.xticks(rotation=45, horizontalalignment='right', fontweight='light', fontsize='medium')
plt.ticklabel_format(style='plain', axis='y')




this is what sub looks like, so the values in 'Current Year Week Ending' column are like ""2020-01-06"". I don't know why it changes when I draw a pointplot.

",5,1371,"I think your date column may just include these trailing zeros, and seaborn deals with it differently depending on how much room there is on the plot. So try the following for your date column:
sub.iloc[:,1] = sub.iloc[:,1].dt.strftime('Y/%m/%d')

If for some reason that doesn't fix the problem (perhaps seaborn point plots and line plots do something different under the hood), then a possible workaround would be to split the text 2020-01-06T00:00:00.000000000 at the letter 'T' and throw out the trailing zeros after. We can use the fact that sns.pointplot returns a matplotlib.Axes object (from the documentation), setting the variable ax equal to your point plot:
ax = sns.pointplot(x = 'Current Year Week Ending',                                               
          y = 'ASP Current Year', 
          hue ='Type', 
          data = sub, 
          markers=[""o"", ""x""],
          linestyles=[""-"", ""--""]) 
ax.set_xticklabels([date_text.get_text().split(""T"")[0] for date_text in ax.get_xticklabels()])

",,
Seaborn strange issue,https://stackoverflow.com/questions/54533396,Issue with axis limits when using seaborn pairplot with kind=&#39;reg&#39;,"I'm having a problem when trying to introduce a regression plot into a pairplot with seaborn.

Without trying to introduce any form of upper or lower plots I have the following:

ff = sns.pairplot(test3,hue='Kp',vars=['L','dtheta','D'],palette=""husl"")


which produces 


However, if I do the following:

ff = sns.pairplot(test3,hue='Kp',vars=['L','dtheta','D'],palette=""husl"")
ff.map_upper(sns.regplot)
ff.map_lower(sns.residplot)


The axis on the regplot behave very strangely


Does anybody know why this might be so? I have also tried with seaborn pairgrid but the same issue occurs!

EDIT: I know how to manually change the axes limits I'm mostly just wondering if there is something going on with seaborn
",2,1448,"A regplot extends the limits of the plot by a certain percentage to let the fitting line sit tight against the axis spines. But if this procedure is performed repeatedly, the second regplot will take the previously determined limits and again extend them, and so forth. This is what you observe in the plots: Starting from orange, each new regplot is some 10% larger then the previous.

An options would be to limit the regression line to the actual point spread. This is done by regplot's truncate keyword argument. 

g.map_upper(sns.regplot, truncate=True)


Note that you wouldn't want to use a pairplot in case you do custom mapping to the upper/lower part of the grid because that would lead to the points appear twice in the grid. Instead use a PairGrid.

df = sns.load_dataset(""iris"", cache=True)

g = sns.PairGrid(df, hue=""species"")

g.map_diag(sns.kdeplot)
g.map_upper(sns.regplot, truncate=True, scatter_kws=dict(alpha=0.2))
g.map_lower(sns.residplot)

plt.show()



",,
Seaborn strange issue,https://stackoverflow.com/questions/53173498,Matplotlib fails with ValueError: cannot convert float NaN to integer,"I encountered a strange issue with the seaborn library.  When generating barplot for data with ranging from very low to very high values, e.g.:

       job      duration type
    0    1  83066.639344    A
    1    2    820.700000    B


it fails with:

ValueError: cannot convert float NaN to integer


This looks like a bug in matplotlib and a duplicate of ""pyplot.savefig fails with ValueError: cannot convert float NaN to integer"". The latter has not been fixed yet. Is there a workaround for it?

Here's the minimal working example to reproduce the issue:

#!/usr/bin/env python3

import matplotlib.pyplot as plt
import pandas as pd
import seaborn as sns

d = {'job': [1, 2]),
     'duration': [83066.639344, 820.700000],
     'type': ['A', 'B']}

df = pd.DataFrame(d)

plot = sns.catplot(x=""duration"", y=""job"", data=df, hue='type',
                   color=""b"", kind=""bar"", height=3, aspect=4)

ax = plot.axes.flat[0]
for p in plt.gca().patches:
    ax.text(p.get_width(),
            p.get_y() + p.get_height() / 2,
            p.get_width())

plot.savefig(""barplot.png"")


Some observations:


The problem does not occur when I do not differentiate between 'type' (no use ofhue='type').


Here's the full stacktrace:

posx and posy should be finite values
posx and posy should be finite values
/Users/dzieciou/virtualenvs/seaborn/lib/python3.7/site-packages/numpy/core/fromnumeric.py:83: RuntimeWarning: invalid value encountered in reduce
  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)
posx and posy should be finite values
posx and posy should be finite values
posx and posy should be finite values
Traceback (most recent call last):
  File ""/Users/dzieciou/projects/example/gocd/reproduce.py"", line 31, in &lt;module&gt;
    plot.savefig(""barplot.png"")
  File ""/Users/dzieciou/virtualenvs/seaborn/lib/python3.7/site-packages/seaborn/axisgrid.py"", line 37, in savefig
    self.fig.savefig(*args, **kwargs)
  File ""/Users/dzieciou/virtualenvs/seaborn/lib/python3.7/site-packages/matplotlib/figure.py"", line 2094, in savefig
    self.canvas.print_figure(fname, **kwargs)
  File ""/Users/dzieciou/virtualenvs/seaborn/lib/python3.7/site-packages/matplotlib/backend_bases.py"", line 2075, in print_figure
    **kwargs)
  File ""/Users/dzieciou/virtualenvs/seaborn/lib/python3.7/site-packages/matplotlib/backends/backend_agg.py"", line 510, in print_png
    FigureCanvasAgg.draw(self)
  File ""/Users/dzieciou/virtualenvs/seaborn/lib/python3.7/site-packages/matplotlib/backends/backend_agg.py"", line 402, in draw
    self.figure.draw(self.renderer)
  File ""/Users/dzieciou/virtualenvs/seaborn/lib/python3.7/site-packages/matplotlib/artist.py"", line 50, in draw_wrapper
    return draw(artist, renderer, *args, **kwargs)
  File ""/Users/dzieciou/virtualenvs/seaborn/lib/python3.7/site-packages/matplotlib/figure.py"", line 1649, in draw
    renderer, self, artists, self.suppressComposite)
  File ""/Users/dzieciou/virtualenvs/seaborn/lib/python3.7/site-packages/matplotlib/image.py"", line 138, in _draw_list_compositing_images
    a.draw(renderer)
  File ""/Users/dzieciou/virtualenvs/seaborn/lib/python3.7/site-packages/matplotlib/artist.py"", line 50, in draw_wrapper
    return draw(artist, renderer, *args, **kwargs)
  File ""/Users/dzieciou/virtualenvs/seaborn/lib/python3.7/site-packages/matplotlib/axes/_base.py"", line 2610, in draw
    mimage._draw_list_compositing_images(renderer, self, artists)
  File ""/Users/dzieciou/virtualenvs/seaborn/lib/python3.7/site-packages/matplotlib/image.py"", line 138, in _draw_list_compositing_images
    a.draw(renderer)
  File ""/Users/dzieciou/virtualenvs/seaborn/lib/python3.7/site-packages/matplotlib/artist.py"", line 50, in draw_wrapper
    return draw(artist, renderer, *args, **kwargs)
  File ""/Users/dzieciou/virtualenvs/seaborn/lib/python3.7/site-packages/matplotlib/axis.py"", line 1185, in draw
    ticks_to_draw = self._update_ticks(renderer)
  File ""/Users/dzieciou/virtualenvs/seaborn/lib/python3.7/site-packages/matplotlib/axis.py"", line 1023, in _update_ticks
    tick_tups = list(self.iter_ticks())  # iter_ticks calls the locator
  File ""/Users/dzieciou/virtualenvs/seaborn/lib/python3.7/site-packages/matplotlib/axis.py"", line 967, in iter_ticks
    majorLocs = self.major.locator()
  File ""/Users/dzieciou/virtualenvs/seaborn/lib/python3.7/site-packages/matplotlib/ticker.py"", line 1985, in __call__
    return self.tick_values(vmin, vmax)
  File ""/Users/dzieciou/virtualenvs/seaborn/lib/python3.7/site-packages/matplotlib/ticker.py"", line 1993, in tick_values
    locs = self._raw_ticks(vmin, vmax)
  File ""/Users/dzieciou/virtualenvs/seaborn/lib/python3.7/site-packages/matplotlib/ticker.py"", line 1932, in _raw_ticks
    nbins = np.clip(self.axis.get_tick_space(),
  File ""/Users/dzieciou/virtualenvs/seaborn/lib/python3.7/site-packages/matplotlib/axis.py"", line 2543, in get_tick_space
    return int(np.floor(length / size))
ValueError: cannot convert float NaN to integer

",2,6128,"Note that this is neither really a bug, nor is it related to the linked bug, which is indeed fixed. 

One could argue that there should be a better error message when plotting text at nan coordinates though.

Before looking at the error, it seems you have another problem in your code, which is that you set the x coordinate of the text to the width of the bar. They are usually unrelated and you might have meant to use p.get_x() instead.

Now two options:

1. Don't position text at invalid coordinates.

    import numpy as np
    import matplotlib.pyplot as plt
    import pandas as pd
    import seaborn as sns

    d = {'job': list(range(1, 3)),
         'duration': [83066.639344, 820.700000],
         'type': ['A', 'B']}

    df = pd.DataFrame(d)

    plot = sns.catplot(x=""duration"", y=""job"", data=df, hue='type',
                       color=""b"", kind=""bar"", height=3, aspect=4)

    ax = plot.axes.flat[0]
    for p in plt.gca().patches:
        height = np.nan_to_num(p.get_height(), 0)
        ax.text(p.get_x(), p.get_y() + height/2., ""My text"")

    plot.savefig(""barplot.png"")
    plt.show()


2. Don't use bbox_inches=""tight"".

If you want to keep your code as it is, you may workaround this by not setting the bbox_inches=""tight"" options in seaborn's savefig.

plot.savefig(""barplot.png"", bbox_inches=None)


Or use matplotlib's savefig option

plot.fig.savefig(""barplot.png"")

",,
Seaborn strange issue,https://stackoverflow.com/questions/71475575,cudf instllation issue on centos7,"I'm new to rapids ai libraries. I've an existing conda environment yaml file where I'm using python 3.8.5, tensorflow 2.7.0, opencv-python-headless 4.5.5.62, numpy 1.22.2, pandas 1.4.1, pandas-profiling 3.1.0, seaborn 0.11.2,  matplotlib 3.5.1, jupyterlab 3.2.9.
I've added below 2 channels to the file:

rapidsai
nvidia

And the below packages:

cudf=22.02
cudatoolkit=11.5

The installation is going on for hours and while trying to find incompatible packages, it seems to be in some sort of loop as I keep seeing below message multiple times in the terminal:
Found conflicts! Looking for incompatible packages.
Is there any known issue/limitations that I should be aware of?
Since we don't get interactive shell on GPU h/w easily, I'm trying the conda environment update on non-GPU machine and once installed, I'll try cudf package on GPU machine.
EDIT1:
This is what I have as working without tensorflow and tensorflow-hub

    name: cudf-env
        channels:
          - default
          - rmg
          - rapidsai
          - nvidia
          - numba
          - conda-forge
          - anaconda
        dependencies:
          - glibc=2.19
          - libgcc-ng=11.2.0
          - python=3.8.5
          - cudf=22.02
          - cudatoolkit=11.2
          - pytest=6.1.2
          - pandas=1.3.5
          - numpy=1.21.5
          - requests=2.25.0
          - scikit-learn=0.24.2
          - dill=0.3.4
          - tqdm=4.62.3
          - ruamel.yaml=0.17.19
          - yappi=1.3.3
          - black=22.1.0
          - pillow=9.0.1
          - jupyterlab=3.2.9
          - matplotlib=3.5.1
          - seaborn=0.11.2
          - plotly=5.6.0
          - pandas-profiling=3.1.0
          - black=22.1.0
        #  - pip
        #  - pip:
        #      - tensorflow==2.7.0
        #      - tensorflow-hub==0.12.0
        #      - opencv-python-headless==4.5.5.62
        #      - opencv-contrib-python-headless==4.5.5.62

Now, if I uncomment the pip section, the anaconda crashes while creating the environment. Since pip may not be supported with cudf, I tried following as well, the conda create env hangs while solving the environment (strangely, it's not resolving from conda-forge channel):

    name: cudf-env
        channels:
          - default
          - rmg
          - rapidsai
          - nvidia
          - numba
          - conda-forge
          - anaconda
        dependencies:
          - glibc=2.19
          - libgcc-ng=11.2.0
          - python=3.8.5
          - cudf=22.02
          - cudatoolkit=11.2
          - pytest=6.1.2
          - pandas=1.3.5
          - numpy=1.21.5
          - requests=2.25.0
          - scikit-learn=0.24.2
          - dill=0.3.4
          - tqdm=4.62.3
          - ruamel.yaml=0.17.19
          - yappi=1.3.3
          - black=22.1.0
          - pillow=9.0.1
          - jupyterlab=3.2.9
          - matplotlib=3.5.1
          - seaborn=0.11.2
          - plotly=5.6.0
          - pandas-profiling=3.1.0
          - black=22.1.0
          - tensorflow
          - tensorflow-hub

My system details are following:
$ cat /etc/os-release
NAME=""CentOS Linux""
VERSION=""7 (Core)""

$ uname -r
3.10.0-1127.10.1.el7.x86_64

EDIT2: I forgot to mention that if I comment out glibc, cudf and cudatoolkit, the tensorflow installation through pip works fine.
",1,467,"Challenges updating conda environments can be tricky to untangle, but in this case the issue (in terms of cuDF) is likely the pinning of pandas to 1.4.1.
cuDF does not yet support pandas=1.4.1. The cuDF nightly packages currently support pandas &gt;=1.0,&lt;1.4.0dev0 (everything from 1.0 up to a dev build of 1.4.0). For the moment, if you switch to
pandas=1.3.5 things will work (assuming the other packages are compatible -- Tensorflow likely will require CUDA Toolkit 11.2, not 11.5, and some others might not be either). You can also let conda solve for the appropriate pandas version.
","I was finally able to complete the installation of cudf along with all the required packages after I reimaged my machine to Ubuntu20.04.
And I dropped below 2 packages as with Ubuntu20.04 I don't need those -
 - glibc=2.19
 - libgcc-ng=11.2.0

",
Seaborn inconsistent behavior,https://stackoverflow.com/questions/58141158,Seaborn renders plots slowly in apache zeppelin notebooks,"I am currently trying to generate visualizations in zeppelin (0.8.1) notebooks using the pyspark interpreter with python 3.7.3.

Generating the following simple plot with seaborn (0.9.0) takes around 5 minutes (with very high CPU usage throughout the duration): 

%pyspark
import seaborn as sns
import numpy as np
import pandas as pd

data = pd.DataFrame(np.random.rand(100,3))

sns.pairplot(data)


This behavior is rather inconsistent as the following (much more data intensive) plot is rendered instantly

%pyspark
import seaborn as sns
import numpy as np
import pandas as pd

df = pd.DataFrame(data = np.random.rand(10000,2))

sns.lineplot(x = 0, y = 1, data = df)


I noticed that using matplotlib (3.1.0) is generally much faster for and almost as snappy as I am used to from jupyter notebook environments.

I have already read about issue ZEPPELIN-1894 but I can render the mentioned scatterplot instantly as well.
",0,722,"Ok, after posting here the solution is to use the %spark.ipyspark interpreter, this might require installing additional packages:

pip install protobuf grpcio

",,
Seaborn inconsistent output,https://stackoverflow.com/questions/54145967,Not able to plot the heatmap of one column with respect to others,"With the help of the question: Correlation heatmap, I have tried the following: 

import pandas
import seaborn as sns 
dataframe = pandas.read_csv(""training.csv"", header=0,index_col=0)
for a in list(['output']):
    for b in list(dataframe.columns.values):
        corr.loc[a, b] = dataframe.corr().loc[a, b]
        print(b)
print(corr)
sns.heatmap(corr['output'])


I got the following error:  

IndexError: Inconsistent shape between the condition and the input (got (8, 1) and (8,))


I do not want to have the all values correlation heatmap with all values. I only want to have the correlation of one column with respect to others.   

Kindly, let me know what I am missing.
",3,7921,"You are trying to build a heatmap from pd.Series - this does not work. pd.Series is a 1D object, while seaborn.heatmap() is commonly used for 2D data structures. 

sns.heatmap(corr[['output']]) - will do the job

df = pd.DataFrame(data=[[1,2,3],[5,4,3],[5,4,12]],index=[0,1,2],columns=['A','B','C'])
df.corr().loc['A',:]


Out[13]:

A    1.0

B    1.0

C    0.5

Name: A, dtype: float64

sns.heatmap(df.corr().loc[['A'],:])



","In the line

sns.heatmap(corr['output'])


corr['output'] is a pd.Series. The documentation states


  
    
      data : rectangular dataset 
      
      2D dataset that can be coerced into an ndarray. If a Pandas DataFrame is provided, the index/column information will be used to label the columns and rows.
    
  


You write


  
    
      I do not want to have the all values correlation heatmap with all values. I only want to have the correlation of one column with respect to others.
    
  


In this case, why a heatmap? Your data is one dimensional. You might want to use a barchart, for example, using pd.DataFrame.corrwith:

dataframe.corrwith(dataframe['some_specific_column']).plot(kind='barh')

",
Seaborn inconsistent output,https://stackoverflow.com/questions/65281536,How to solve the PyGSLIB library installation error?,"I am trying to install the PyGSLIB library and the following error is occurring:
(base) C:\Users\...&gt; conda install -c opengeostat pygslib
Collecting package metadata (current_repodata.json): done
Solving environment: failed with initial frozen solve. Retrying with flexible solve.
Solving environment: failed with repodata from current_repodata.json, will retry with next repodata source.
Collecting package metadata (repodata.json): done
Solving environment: failed with initial frozen solve. Retrying with flexible solve.
Solving environment: /
Found conflicts! Looking for incompatible packages.
This can take several minutes.  Press CTRL-C to abort.
Determining conflicts:   0%|                                                                     | 0/5 [00:00&lt;?, ?it/s]/failed

UnsatisfiableError: The following specifications were found
to be incompatible with the existing python installation in your environment:

Specifications:

  - pygslib -&gt; python[version='&gt;=3.5,&lt;3.6.0a0|&gt;=3.7,&lt;3.8.0a0|&gt;=3.9,&lt;3.10.0a0']

Your python: python=3.8

If python is on the left-most side of the chain, that's the version you've asked for.
When python appears to the right, that indicates that the thing on the left is somehow
not available for the python version you are constrained to. Note that conda will not
change your python version to a different minor version unless you explicitly specify
that.

I installed Anaconda3 5.2.0
(base) C:\WINDOWS\system32&gt;ipython
Python 3.6.5 |Anaconda, Inc.| (default, Mar 29 2018, 13:32:41) [MSC v.1900 64 bit (AMD64)]
Type 'copyright', 'credits' or 'license' for more information
IPython 6.4.0 -- An enhanced Interactive Python. Type '?' for help.

And library conflicts happen again
(base) C:\WINDOWS\system32&gt;conda install -c opengeostat pygslib
Collecting package metadata (repodata.json): done
Solving environment: \
The environment is inconsistent, please check the package plan carefully
The following packages are causing the inconsistency:

  - defaults/win-64::anaconda==5.2.0=py36_3
  - defaults/win-64::astropy==3.0.2=py36h452e1ab_1
  - defaults/win-64::bkcharts==0.2=py36h7e685f7_0
  - defaults/win-64::blaze==0.11.3=py36h8a29ca5_0
  - defaults/win-64::bokeh==0.12.16=py36_0
  - defaults/win-64::bottleneck==1.2.1=py36hd119dfa_0
  - defaults/win-64::dask==0.17.5=py36_0
  - defaults/win-64::datashape==0.5.4=py36h5770b85_0
  - defaults/win-64::h5py==2.7.1=py36h3bdd7fb_2
  - defaults/win-64::imageio==2.3.0=py36_0
  - defaults/win-64::matplotlib==2.2.2=py36h153e9ff_1
  - defaults/win-64::mkl_fft==1.0.1=py36h452e1ab_0
  - defaults/win-64::mkl_random==1.0.1=py36h9258bd6_0
  - defaults/win-64::numba==0.38.0=py36h830ac7b_0
  - defaults/win-64::numexpr==2.6.5=py36hcd2f87e_0
  - defaults/win-64::numpy==1.14.3=py36h9fa60d3_1
  - defaults/win-64::numpy-base==1.14.3=py36h555522e_1
  - defaults/win-64::odo==0.5.1=py36h7560279_0
  - defaults/win-64::pandas==0.23.0=py36h830ac7b_0
  - defaults/win-64::patsy==0.5.0=py36_0
  - defaults/win-64::pytables==3.4.3=py36he6f6034_1
  - defaults/win-64::pytest-arraydiff==0.2=py36_0
  - defaults/win-64::pytest-astropy==0.3.0=py36_0
  - defaults/win-64::pytest-doctestplus==0.1.3=py36_0
  - defaults/win-64::pywavelets==0.5.2=py36hc649158_0
  - defaults/win-64::scikit-image==0.13.1=py36hfa6e2cd_1
  - defaults/win-64::scikit-learn==0.19.1=py36h53aea1b_0
  - defaults/win-64::scipy==1.1.0=py36h672f292_0
  - defaults/win-64::seaborn==0.8.1=py36h9b69545_0
  - defaults/win-64::statsmodels==0.9.0=py36h452e1ab_0
failed with initial frozen solve. Retrying with flexible solve.
Solving environment: \
Found conflicts! Looking for incompatible packages.
This can take several minutes.  Press CTRL-C to abort.
failed

UnsatisfiableError: The following specifications were found to be incompatible with each other:

Output in format: Requested package -&gt; Available versions

I installed Python versions 3.5 and 3.6 and the same error occurs, could someone tell me a way to solve this problem or do you know any other library similar to the one that has the same purpose?
",2,570,"I also notice such behaviour, where it doesn't matter what python version you install.
BAD:
conda install -c opengeostat pygslib

CORRECT:
conda config --add channels conda-forge
conda config --set channel_priority flexible
conda install pygslib -c opengeostat -c conda-forge

In github, correct command is found. Wherease on website, was incorrect.
","It seems that your conda environment has python3.8 installed but this not supported by looking at the requirements.
You can create a new conda environment by changing the python version or try to install it from source
git clone https://github.com/opengeostat/pygslib.git
cd pygslib
python setup.py build
python setup.py install

",
Seaborn inconsistent result,https://stackoverflow.com/questions/57888688,Inconsistent shape between the condition and the input while using seaborn,"I am trying to plot a heatmap with seaborn. Here is the list that I am trying to plot:

b = [5, 4, 4, 4, 13, 4, 4, 1, 9, 4, 3, 9, 1, 4, 4, 1, 7, 1, 5, 3, 7, 1, 9, 4, 3, 9, 5, 4, 2, 1, 4, 1, 9, 4, 3, 9, 4, 8, 1, 7, 1, 9, 4, 8, 1, 7, 1, 4, 8, 1, 7, 1, 4, 1, 7, 1, 4, 10, 4, 3, 4, 7, 1, 8, 5, 10, 8, 9, 4, 1, 3, 9, 4, 1, 9, 4, 3, 7, 7, 1, 1, 3, 4, 9, 5, 5, 4, 1, 1, 9, 4, 9, 4, 7, 1, 9, 4, 10, 9, 4, 4, 4, 8, 10, 3, 9, 5, 4, 4, 1, 3, 9, 4, 10, 5, 4, 1, 1, 8, 1, 7, 5, 1, 8, 8, 5, 3, 1, 8, 8, 8, 1, 3, 4, 2, 1, 2, 9, 4, 10, 1, 5, 3, 9, 5, 4, 4, 4, 1, 1, 7, 1, 8, 2, 1, 8, 5, 9, 5, 10, 9, 5, 4, 1, 10, 7, 1, 8, 5, 2, 1, 3, 4, 7, 1, 2, 1, 7, 1, 4, 4, 8, 5, 3, 7, 1, 2, 1, 10, 9, 4, 1, 2, 1, 3, 9, 4, 10, 9, 1, 9, 5, 4, 3, 9, 4, 1, 8, 5, 9, 4, 1, 1, 3, 9, 4, 9, 5, 4, 1, 1, 9, 4, 3, 4, 10, 1, 9, 4, 3, 4, 10, 7, 1, 7, 1, 9, 4, 3, 4, 4, 1, 1, 9, 5, 4, 3, 5, 4, 1, 8, 5, 7, 1, 3, 9, 4, 10, 9, 4, 9, 1, 8, 5, 3, 9, 4, 1, 3, 9, 5, 3, 9, 4, 1, 3, 4, 4, 4, 8, 8, 3, 9, 5, 4, 3, 5, 4, 10, 4, 7, 1, 5, 7, 1, 1, 3, 9, 4, 10, 4, 4, 1, 9, 4, 1, 1, 5, 4, 4, 3, 5, 4, 3, 5, 4, 3, 4, 4, 1, 1, 2, 1, 4, 3, 5, 4, 3, 4, 4, 4, 4, 4, 1, 1, 8, 8, 9, 5, 8, 4, 7, 1, 2, 4, 3, 9, 4, 10, 1, 1, 3, 4, 7, 1, 4, 1, 1, 8, 5, 3, 9, 5, 4, 10, 8, 3, 4, 4, 1, 1, 3, 9, 4, 2, 5, 5, 4, 4, 1, 1, 2, 1, 7, 3, 4, 9, 1, 4, 10, 9, 4, 9, 6, 4, 11, 5, 4, 10, 4, 4, 1, 9, 5, 4, 3, 9, 4, 3, 9, 5, 12, 4, 4, 4, 1, 1, 3, 9, 5, 4, 1, 3, 5, 4, 4, 4, 10, 1, 4, 4, 10, 4, 1, 5, 3, 5, 4, 4, 7, 1, 8, 4, 1, 2, 1, 9, 4, 3, 7, 1, 9, 5, 4, 4, 10, 9, 5, 4, 4, 3, 5, 10, 5, 4, 4, 1, 9, 4, 7, 1, 5, 3, 1, 4, 3, 4, 5, 3, 1, 5, 4, 5, 3, 4, 10, 8, 5, 3, 9, 4, 3, 4, 3, 7, 9, 1, 9, 4, 4, 3, 9, 4, 4, 4, 8, 9, 4, 3, 9, 5, 4, 4, 2, 5, 4, 1, 8, 3, 9, 4, 4, 10, 7, 1, 1, 9, 4, 3, 4, 9, 4, 1, 2, 1, 10, 1, 9, 4, 2, 1, 4, 1, 8, 5, 4, 3, 9, 4, 1, 9, 4, 3, 9, 3, 9, 4, 1, 4, 4, 1, 7, 7, 1, 2, 1, 3, 4, 2, 1, 4, 10, 1, 7, 1, 3, 7, 1, 11, 1, 3, 9, 4, 1, 9, 4, 7, 1, 1, 4, 2, 1, 9, 4, 3, 4, 1, 8, 1, 9, 4, 3, 4, 1, 8, 4, 1, 7, 7, 1, 7, 1, 4, 3, 9, 5, 4, 7, 1, 8, 9, 5, 4, 7, 1, 3, 9, 4, 3, 4, 7, 1, 1]


This is the code that I am trying to use to plot:

import numpy as np
import seaborn as sns
from matplotlib.colors import ListedColormap
data = np.asarray(b)
sns.heatmap( data,cmap=ListedColormap(['green', 'yellow', 'red']))


After running the above code, this is the error I am getting:


  IndexError: Inconsistent shape between the condition and the input (got (633, 1) and (633,))


I did check some other answers but none of them answered my concerns.

I am not quite sure as to where the problem lies. Here is the result that I get when run data.shape:

(633,)


Any help will be appreciated. Thanks!
",13,21473,"import numpy as np
import seaborn as sns
from matplotlib.colors import ListedColormap
import matplotlib.pyplot as plt
data = np.asarray(b).reshape(633,1)
sns.heatmap(data,cmap=ListedColormap(['green', 'yellow', 'red']))
plt.show()


heatmap requires 2D dataset
https://seaborn.pydata.org/generated/seaborn.heatmap.html
","From the docs:


  data : rectangular dataset 2D dataset that can be coerced into an
  ndarray. If a Pandas DataFrame is provided, the index/column
  information will be used to label the columns and rows.


You have to transform your data into a 2D dataset.

One way to do so is:

sns.heatmap(data[:, np.newaxis], cmap=ListedColormap(['green', 'yellow', 'red']))

",
Seaborn inconsistent result,https://stackoverflow.com/questions/59294900,How to prepare training data for image classification,"I'm new to Machine Learning and have some problems with image classification. Using a simple classifier technique K Nearest Neighbours I'm trying to distinguish Cats from Dogs. 

My code so far:

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt

%matplotlib inline

DATADIR = ""/Users/me/Desktop/ds2/ML_image_classification/kagglecatsanddogs_3367a/PetImages""
CATEGORIES = ['Dog', 'Cat']

IMG_SIZE = 30
data = []
categories = []

for category in CATEGORIES:
    path = os.path.join(DATADIR, category) 
    categ_id = CATEGORIES.index(category)
    for img in os.listdir(path):
        try:
            img_array = cv2.imread(os.path.join(path,img), 0)
            new_array = cv2.resize(img_array, (IMG_SIZE, IMG_SIZE))
            data.append(new_array)
            categories.append(categ_id)
        except Exception as e:
            # print(e)
            pass

print(data[0])


s1 = pd.Series(data)
s2 = pd.Series(categories)
frame = {'Img array': s1, 'category': s2}
df = pd.DataFrame(frame) 


from sklearn.model_selection import train_test_split
from sklearn.neighbors import KNeighborsClassifier

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)

knn = KNeighborsClassifier()
knn.fit(X_train, y_train)


And here I get an error when trying to fit the data:

   ---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
&lt;ipython-input-76-9d98d7b11202&gt; in &lt;module&gt;
      2 from sklearn.neighbors import KNeighborsClassifier
      3 
----&gt; 4 X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)
      5 
      6 print(X_train)

~/opt/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_split.py in train_test_split(*arrays, **options)
   2094         raise TypeError(""Invalid parameters passed: %s"" % str(options))
   2095 
-&gt; 2096     arrays = indexable(*arrays)
   2097 
   2098     n_samples = _num_samples(arrays[0])

~/opt/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py in indexable(*iterables)
    228         else:
    229             result.append(np.array(X))
--&gt; 230     check_consistent_length(*result)
    231     return result
    232 

~/opt/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py in check_consistent_length(*arrays)
    203     if len(uniques) &gt; 1:
    204         raise ValueError(""Found input variables with inconsistent numbers of""
--&gt; 205                          "" samples: %r"" % [int(l) for l in lengths])
    206 
    207 

ValueError: Found input variables with inconsistent numbers of samples: [24946, 22451400]


How to prepare the training the data properly?
Btw. I don't want to use deep learning. This will be the next step for me.

Would appreciate any help here..
",2,1940,"If you don`t use deep learning for image classification,you have to prepare your data that fit to the supervised learning classification.

steps

1) Resize all images to same size.You can loop over each image and resize and save.

2) get the pixel vector of each image and create the dataset.As a example if your cat images are in ""Cat"" folder and Dog images are in ""Dog"" folder,iterate over all images inside the folder and get the pixel values.same time label the data as ""cat""(cat=1) and ""non-cat""(non-cat=0)

import os
import  imageio
import pandas as pd

catimages = os.listdir(""Cat"")
dogimages = os.listdir(""Dog"")
catVec = []
dogVec = []
for img in catimages:
       img = imageio.imread(f""Cat/{img}"")
       ar = img.flatten()
       catVec.append(ar)    
catdf = pd.DataFrame(catVec)    
catdf.insert(loc=0,column =""label"",value=1)

for img in dogimages:
       img = imageio.imread(f""Dog/{img}"")
       ar = img.flatten()
       dogVec.append(ar)    
dogdf = pd.DataFrame(dogVec)    
dogdf.insert(loc=0,column =""label"",value=0)


3) concat catdf and dogdf and shuffle the dataframe

data = pd.concat([catdf,dogdf])      
data = data.sample(frac=1)


now you have dataset with lable for your images.

4) split dataset to train and test and fit to the model.
","For using classical machine learning for image classification, as mentioned earlier, you would need transform the raw images in vectors or numpy arrays and extract features from it.

As suggested, often the preprocessing steps includes:


Rescaling the images and normalizating it
Converting the images to grayscale if colour does not play a vital role in classification
Doing feature extraction, like creating feature vectors by applying varies computer vision filters for edge detection, pixel
density detection etc.
Finally dividing in to train-test split, before feeding  into the model.


I found the following link that might be helpful to you,
https://medium.com/@dataturks/understanding-svms-for-image-classification-cf4f01232700

From the issue that you had posted, I feel you should check the dimensions of X_train, y_train and X_test, y_test. The training data is probably not matching with your training labels.

Do, a quick X_train.shape and y_train.shape to see what are the dimensions coming.
",
Seaborn inconsistent issue,https://stackoverflow.com/questions/46383645,Seaborn and pd.scatter_matrix() plot color issues,"I am making a pd.scatter_matrix() plot from a DataFrame based on the Iris dataset colored by the target variable (plant species). When I run the code below I get a scatter matrix with black, grey and white (!) colored scattering points which hinders visualization. The grid seems inconsistent too, apparently only the plots close to the axis get the respective gridding. I wanted a nice grid and scatter matrix following the sns default color palette (blue, green, red).

Why is seaborn plot style and the use of pd.scatter_matrix() enforcing a different (awful!) color palette then the defaults for the scatter plots and inconsistent grid lines? How can I solve these visualization issues?

I already updated seaborn to a fairly recent version (0.8 of July 2017). Also tried the non-deprecated version the scatter_matrix plot for pandas pd.plotting.scatter_matrix() and had no luck. If I use the 'ggplot' style the color palette is correct for the scatter plots but the grids are still inconsistent.

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
plt.style.use('seaborn')
from sklearn import datasets

iris = datasets.load_iris()
X = iris.data
y = iris.target
df = pd.DataFrame(X, columns = iris.feature_names)

pd.scatter_matrix(df, c=y, figsize = [8,8],
                      s=80, marker = 'D');




Package versions:

pandas version: 0.20.1
matplotlib version: 2.0.2
seaborn version:0.8.0  
",4,13092,"I am not sure if this answers your question but you could use the pairplot. let me know..

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
%matplotlib inline

from sklearn import datasets

iris = datasets.load_iris()
X = iris.data
y = iris.target
df = pd.DataFrame(X, columns = iris.feature_names)

pd.plotting.scatter_matrix(df, c=y, figsize = [8,8],
                      s=80, marker = 'D');
df['y'] = y

sns.pairplot(df,hue='y')


which gives you:



If you want to avoid that the last line of the visualizations then:

import seaborn as sns
sns.set(style=""ticks"", color_codes=True)
iris = sns.load_dataset(""iris"")
%matplotlib inline

iris = sns.load_dataset(""iris"")
sns.pairplot(iris, hue=""species"")



","Default matplotlib setting are not very aesthetic; however, do not underestimate the power of matplotlib.

The simplest solution to your problem might be:

plt.style.use('ggplot') # this is the trick

from sklearn import datasets

iris = datasets.load_iris()
X = iris.data
y = iris.target
df = pd.DataFrame(X, columns = iris.feature_names)

pd.scatter_matrix(df, c=y, figsize = [10,10], s=50);




(full list of styles available can be accessed via plt.style.available)

You may further customize the plot to your needs adjusting matplotlibrc file. An example of what could be done with it could be found here
",
Seaborn inconsistent issue,https://stackoverflow.com/questions/72587474,Not able to set up gcc in aws sagemaker notebook,"I am working in a jupyter notebook in aws sagemaker and want to use prophet for time-series forecast. I am using the conda_python3 kernel. According to the installation instruction from https://facebook.github.io/prophet/docs/installation.html#python I need to set up gcc before installing prophet, but when I try '%conda install gcc', I get the error message that the environment is inconsistent.
Does anyone know how to solve this issue?
Error message:
Collecting package metadata (current_repodata.json): done
Solving environment: /
The environment is inconsistent, please check the package plan carefully
The following packages are causing the inconsistency:

conda-forge/noarch::nbclient==0.5.2=pyhd8ed1ab_0
conda-forge/linux-64::matplotlib==3.3.4=py36h5fab9bb_0
conda-forge/noarch::qdarkstyle==2.8.1=pyhd8ed1ab_2
conda-forge/linux-64::scikit-image==0.16.2=py36hb3f55d8_0
conda-forge/noarch::python-language-server==0.36.2=pyhd8ed1ab_0
conda-forge/linux-64::widgetsnbextension==3.5.1=py36h5fab9bb_4
conda-forge/noarch::flake8==3.8.4=py_0
conda-forge/noarch::ipywidgets==7.6.3=pyhd3deb0d_0
conda-forge/noarch::typing-extensions==3.7.4.3=0
conda-forge/noarch::path.py==12.5.0=0
conda-forge/noarch::dask==2021.2.0=pyhd8ed1ab_0
conda-forge/noarch::nbformat==5.1.2=pyhd8ed1ab_1
conda-forge/linux-64::path==15.1.2=py36h5fab9bb_0
conda-forge/linux-64::nbconvert==6.0.7=py36h5fab9bb_3
conda-forge/linux-64::distributed==2021.2.0=py36h5fab9bb_0
conda-forge/noarch::anaconda-client==1.7.2=py_0
conda-forge/noarch::aioitertools==0.7.1=pyhd8ed1ab_0
conda-forge/linux-64::matplotlib-base==3.3.4=py36hd391965_0
conda-forge/linux-64::pluggy==0.13.1=py36h5fab9bb_4
conda-forge/noarch::black==20.8b1=py_1
conda-forge/linux-64::blaze==0.11.3=py36_0
conda-forge/noarch::pyls-spyder==0.3.2=pyhd8ed1ab_0
conda-forge/noarch::odo==0.5.1=py_1
conda-forge/linux-64::keyring==22.0.1=py36h5fab9bb_0
conda-forge/noarch::anaconda-project==0.9.1=pyhd8ed1ab_0
conda-forge/noarch::importlib_metadata==3.7.0=hd8ed1ab_0
conda-forge/linux-64::jupyter==1.0.0=py36h5fab9bb_6
conda-forge/noarch::jupyterlab_server==2.3.0=pyhd8ed1ab_0
conda-forge/noarch::seaborn-base==0.11.1=pyhd8ed1ab_1
conda-forge/noarch::imageio==2.9.0=py_0
conda-forge/noarch::numpydoc==1.1.0=py_1
conda-forge/linux-64::yarl==1.6.3=py36h8f6f2f9_1
conda-forge/noarch::jsonschema==3.2.0=py_2
conda-forge/noarch::flask==1.1.2=pyh9f0ad1d_0
conda-forge/noarch::seaborn==0.11.1=hd8ed1ab_1
conda-forge/noarch::helpdev==0.7.1=pyhd8ed1ab_0
conda-forge/linux-64::nb_conda==2.2.1=py36h5fab9bb_4
conda-forge/noarch::nbclassic==0.2.6=pyhd8ed1ab_0
conda-forge/noarch::sphinx==3.5.1=pyhd8ed1ab_0
conda-forge/noarch::jupyterlab_launcher==0.13.1=py_2
conda-forge/linux-64::spyder==4.2.0=py36h5fab9bb_0
conda-forge/linux-64::importlib-metadata==3.7.0=py36h5fab9bb_0
conda-forge/linux-64::pytest==6.2.2=py36h5fab9bb_0
conda-forge/noarch::pyls-black==0.4.6=pyh9f0ad1d_0
/ ^C
failed with initial frozen solve. Retrying with flexible solve.

CondaError: KeyboardInterrupt
Note: you may need to restart the kernel to use updated packages.
",1,1070,"Have you confirmed GCC is not installed already?  I ran the below on a SageMaker Notebook Instance with conda_python3 kernel selected.
!gcc --version
gcc (GCC) 4.8.5 20150623 (Red Hat 4.8.5-28)
Copyright (C) 2015 Free Software Foundation, Inc.
This is free software; see the source for copying conditions.  There is NO
warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.

",,
Plotly unexpected behavior,https://stackoverflow.com/questions/44950570,Color legend not consequent in R plotly subplot,"I want to make a plotly bar chart of two synchronized variables. Origin is visits of individuals to a site where two properties are measured in time. In this example individual ""a"" visits the site twice.

Bars should display the measurement value, be ordered in time and colored according to tag (not variable!).

Let's create some data first.

# R version 3.4.0
set.seed(123)
df &lt;- data.frame(id=factor(rep(c(""a"", ""b"", ""c"", ""a""), each=10)),
                 time=as.POSIXct(1:40, origin=""2017-01-01""),
                 var1=abs(rnorm(40)),
                 var2=abs(rnorm(40)))

# &gt; head(df)
#   id                time        var1      var2
# 1  a 2017-01-01 01:00:01 0.005764186 0.1176466
# 2  a 2017-01-01 01:00:02 0.385280401 0.9474746
# 3  a 2017-01-01 01:00:03 0.370660032 0.4905574
# 4  a 2017-01-01 01:00:04 0.644376549 0.2560922
# 5  a 2017-01-01 01:00:05 0.220486562 1.8438620
# 6  a 2017-01-01 01:00:06 0.331781964 0.6519499


Now make two plots of each separate variable, assign to same legend group and hide the legend of the second plot.

library(plotly) # version 4.7.0
p1 &lt;- plot_ly(df, x=~time, y=~var1, color=~id, type=""bar"",
              legendgroup=~id)
p2 &lt;- plot_ly(df, x=~time, y=~var2, color=~id, type=""bar"",
              legendgroup=~id, showlegend=FALSE)


Then add both plots to a single plot with shared X axis.

subplot(p1, p2, shareX=TRUE, nrows=2)


I'm not sure how to add an HTML page in an SO question, but the above code should provide a reproducible example. PNGs added below for reference.



Looks good, right? But for some reason, the legend ""selector"" does not work properly. Deselecting ""a"" only affects plot 1 (unexpected behavior), while deselecting ""c"" affects both plots (expected).



What's going on here? Possible bug, or am I missing something?

I'm aware of ggplotly and could get similar plot by using facets. The problem there is that I cannot output a ggplotly HTML that scales with page width (at least not to my knowledge). I want the resulting image to be an HTML that scales with the browser window, which is what plotly does by default.
",3,1385,"probably It could be a bug. I noticed reordering the data according to the variable get the right result.
It's not the first time btw, I noticed a similar behaviour

df&lt;-df[order(df$id),]
library(plotly) # version 4.7.0
p1 &lt;- plot_ly(df, x=~time, y=~var1, color=~id, type=""bar"",
              legendgroup=~id)
p2 &lt;- plot_ly(df, x=~time, y=~var2, color=~id, type=""bar"",
              legendgroup=~id, showlegend=FALSE)
subplot(p1, p2, shareX=TRUE, nrows=2)

",,
Plotly unexpected behavior,https://stackoverflow.com/questions/59776874,Flask log entries duplicated when using Plotly dashboards,"I'm seeing unexpected behavior when introducing Plotly dashboards into my Flask application. Each Plotly dashboard causes Flask log entries to be duplicated. 

For example, if I attach two Plotly dashboards to my Flask application, log entries (e.g. current_app.logger.info('hi')) will appear three times in my logs. If I remove the Plotly dashboards, the log entry appears once, which is the expected behavior.

I've tried removing existing handlers in my logging config code via app.logger.handlers.clear() and by setting dictConfig's disable_existing_loggers to True, both of which result in nothing being logged. I've also tried using the singleton approach to configuring the logger (again, using dictConfig) and I still see the log entries repeated multiple times.

How can I prevent Plotly dashboards from causing duplicate log entries?

UPDATE:

Here's a simplified version of how Dash apps are being initialized:

def register_dashapp(app):
    from dashboards.dash_files.my_dash import (
        define_layout,
        define_callbacks,
    )
    my_dashapp = dash.Dash(__name__,
                         server=app,
                         url_base_pathname='/my_dash/',
                         assets_folder=""../dashboards/foo/bar/assets"",
                         )
    with app.app_context():
        my_dashapp.title = ""Test""
        define_layout(my_dashapp)
        define_callbacks(my_dashapp)

def create_app():
    app = Flask(__name__, instance_relative_config=False)
    app.config.from_object('config.Config')

    with app.app_context():
        register_extensions(app)
        app.register_blueprint(main)
        register_dash_app(app)
        return app

",3,1004,"I was having a log duplication issue and solved it using the same HasHandlers logic, however I call logs in a slight different way. This requires the python arrow library, which I find superior to datetime in most use cases.
I define the site_logger in my main app.py or _init_.py file.
# define logger
def site_logger(log_name):
    now = arrow.now('US/Eastern').format('YYYY_MM_DD')
    handler = logging.FileHandler('logs/' + log_name + '_' + now + '.log')
    formatter = logging.Formatter('%(asctime)s %(levelname)s %(name)s: %(message)s')
    handler.setFormatter(formatter)
    logger = logging.getLogger(log_name)
    logger.setLevel(""DEBUG"")
    if logger.hasHandlers():
        logger.handlers.clear()
    logger.addHandler(handler)
    logger.propagate = False
    return logger

I then call the logger in the following way in any views.py file or function, you can also pass the logger into a function you call in a views.py, keeping logs results of a function in the correct page file it was called from.
from app import site_logger
logger = site_logger('page_name')

I can then log events in the usual way. Two examples are below, and using f strings in logger is a great way to easily pass vars into log data.
logger.info(f'{user} logged in successfully')
logger.warning(f'{user} password failure!')

This will provide uniformly named log handlers and log files you can more easily trace back to the page or function call made. The date formatted files should auto rotate when the arrow.now('timezome') value rotates to the next day.
","I was able to work around this by removing the logging handler added by the Dash library.

my_dash = dash.Dash(__name__,
    server=app,
    url_base_pathname='/my_dash/',
    assets_folder=""../dashboards/foo/bar/assets""
)

if (my_dash.logger.hasHandlers()):
    my_dash.logger.handlers.clear()


There doesn't appear to be any downside to doing this, but it is possible I'm overlooking something and I'd be interested to see if anyone else has a better approach. I'll hold off on accepting my own answer for a few more days.

On a related note, I'd think this would be configurable via the Dash constructor. I'd also be curious to know if anyone has thoughts about that. I may open a ticket with the project to see what they think about the possibility.
","if not DashboardDAO.validate_create_title(dashboard_title, unique()):
      exceptions.append(DashboardTitleExistsValidationError())
   

"
Plotly unexpected behavior,https://stackoverflow.com/questions/65009862,Plotly.js with vue disappearing charts on resize and data change,"I'm using Ploty with Vue wrapper Vue-Plotly and I want to dynamically change the data on the plot
I use a function that changes the value of the data that is sent to the Ploty plot (which is responsive).
methods: {
    changelayout() {
      this.data[1].x = [10];
    },
  }

It works well, the data is updated and plot is also updated.
However there is some unexpected behavior happening, when I resize the window, the plot kind of collapses and other components cover it (it seems to collapse to the top, so when there are multiple plots they all collapse into one place).
For instance if I have two plots positioned one above the other, the second one will cover the first one.
You can see this behavior in action here:
https://codesandbox.io/s/vuetify-playground-xbgcv?file=/src/layout.vue
When you click on the button, and resize window this will happen ( or resize then click i think also works to reproduce the issue)
In the options I set the responsive to true for the plot and this is what seems to be breaking the chart:
  options: {
        displayModeBar: false,
        responsive: true,
      },

Is there a way to have the plot anchored so that it wont hide behind other components on window resize and still keeps being responsive?
",3,1014,"The SVGs have position: absolute which means they take up no space in their parent divs.
To compensate for that, their parent divs, .svg-container, have a fixed height of 90px (pixel).
After the button click .svg-container changes to height: 100% (percent), which is not fixed, so they shrink.
If you give them a fixed height (with !important) they will keep it:
.svg-container {
   height: 90px !important;
}

",,
Plotly unexpected behavior,https://stackoverflow.com/questions/36437879,Unexpected output using plotly and geom_tile,"I'm trying to print a heatmap using ggplot2's geom_tile and display it using the package plotly, but I get some unexpected behavior.

There is a reproducible example in plotly's website  https://plot.ly/ggplot2/geom_tile/

To make it easier, I paste the code here:

library(plotly)
library(reshape2)

p &lt;- volcano %&gt;%
  melt() %&gt;% 
  ggplot(aes(Var1, Var2, fill = value)) + geom_tile()

ggplotly(p)


According to the website, I'm supposed to get something like this:



But the result I get is the next one:



The funny thing is that if I print the ggplot object p with print(p) I get the outcome I was supposed to get:



which makes me think that the problem is plotly and not ggplot. 

I've run the code from other examples using ggplot and plotly and it all works perfectly, the issue seems to be with geom_tile.

My SessionInfo() is:

R version 3.2.4 Revised (2016-03-16 r70336)
Platform: x86_64-pc-linux-gnu (64-bit)
Running under: Ubuntu 15.10


And the versions of ggplot an plotly are:


ggplot2: 2.1.0 
plotly: 3.4.1


I also tested this code with a Mac and got the same unexpected results.
",1,693,"So, I found three possible solutions for this:


As Laterow said, downgrading to the 2.0.16 version works.
The people from plotly told me that installing the development version 3.4.13 with devtools::install_github('ropensci/plotly') should fix it. I tried it and it does work.
Transposing the original data and changing the order of variables in the aes in geom_tile also works.


The code for the third solution would be:

p &lt;- t(volcano) %&gt;%
melt() %&gt;% 
ggplot(aes(x=Var2, y=Var1)) + geom_tile(aes(fill=value))
ggplotly(p)

",,
Plotly unexpected behavior,https://stackoverflow.com/questions/71655326,R plotly waterfall with offset base: hovertext activated on hover at wrong position,"Recently, when using an offset (a non-zero base parameter) in a waterfall plot in plotly, I noticed that the boxes with hoverinfo were activated when the mouse pointer was in a completely wrong/unexpected place. Below I've pasted in screenshots to illustrate the problem.

Mouse pointer just to the left of the value in the base parameter:


Mouse pointer just to the right of the base:


Mouse pointer just to the left of the green box:


Mouse pointer inside the green box (same behavior to the right of the box):



Just found a similar question on the Plotly forum for the Python Dash users here, but it looks like it hasn't been answered.
Any ideas what might be wrong here?
I'm attaching the code below for reproducing the example shown in the pictures:
library(dplyr)
library(plotly)


set.seed(123)
test_x &lt;- rnorm(10)
test_y &lt;- c(LETTERS[1:10], ""all"") %&gt;% factor(., levels = .)
test_base &lt;- -15

test_data &lt;- tibble(
  x = c(test_x, 0),
  y = test_y,
  text = c(paste(""test"", 1:10), ""all""),
  measure = c(rep(""relative"", 10), ""total"")
)


plot_ly(
  test_data,
  type = ""waterfall"",
  orientation = ""h"",
  measure = ~measure,
  x = ~x,
  y = ~y,
  base = test_base,
  text = ~text,
  textposition = ""none"",
  hoverinfo = ""text"",
  decreasing = list(marker = list(color = ""orange""))
) %&gt;%
  layout(
    yaxis = list(autorange = ""reversed"", title = ""y""),
    xaxis = list(title = ""x"")
  )

",1,386,"I filed a detailed issue report on the plotly.js GitHub repo, and, unfortunately, it was recognized as a bug. As such, this doesn't seem to have much to do with SO, so I'm posting this answer in case anybody faces the same problem, and said bug hasn't been fixed yet.
",,
Plotly unexpected output,https://stackoverflow.com/questions/74132443,Select plotly charts via drop-down list,"I wrote a loop that made 10 graphs in R:
library(plotly)

for (i in 1:10)

{

d_i = data.frame(x = rnorm(100,100,100), y = rnorm(100,100,100))

title_i = paste0(""title_"",i)

p_i = plot_ly(data = d_i, x = ~x, y = ~y) %&gt;% layout(title = title_i)

htmlwidgets::saveWidget(as_widget(p_i), paste0(""plot_"",i, "".html""))

}

I have this code (Input menu contents do not overflow row box in flexdashboard) that makes a dashboard in R:
---
title: ""Test Dashboard""
output: 
  flexdashboard::flex_dashboard:
    orientation: rows
    vertical_layout: fill
runtime: shiny
---

```{r setup, include=FALSE}
library(flexdashboard)
library(shiny)
```

Column {data-width=100}
-----------------------------------------------------------------------

### Window 1

```{r}
selectInput(""project"", label = NULL, choices = c(""A"",""B"",""C"",""D""))
```


Column {data-width=400}
-----------------------------------------------------------------------

### Chart B

```{r}
renderPlot({
  plot(rnorm(1000), type = ""l"", main = paste(""Project:"",input$project, "" Data:"", input$data))
})
```

I would like to adapt this code so that the drop down menu allows the user to load the previously created graph/html file (e.g. from ""My Documents"") that is being searched for. For example, if the user searches for ""plot_7"", then plot_7 is displayed.
I tried the following code:
---
title: ""Test Dashboard""
output: 
  flexdashboard::flex_dashboard:
    orientation: rows
    vertical_layout: fill
runtime: shiny
---

```{r setup, include=FALSE}
library(flexdashboard)
library(shiny)
```

Column {data-width=100}
-----------------------------------------------------------------------

### Window 1

```{r}
plots = rep(""plot"", 10)
i = seq(1:100)
choice = paste0(plots, ""_"",i)
selectInput(""project"", label = NULL, choices = choice)
```


Column {data-width=400}
-----------------------------------------------------------------------

### Chart B

```{r}
renderPlot({
&lt;object class=""one"" type=""text/html"" data=""plot_i.html""&gt;&lt;/object&gt;
})
```

But this returns the following error:
Error: &lt;text&lt;:2:1 unexpected '&lt;' 
1: renderPlot({
2:&lt;
 ^

Can someone please show me how I can fix this? And is it possible to do this WITHOUT shiny? (i.e. only in flexdashboard)
Thank you!
",4,569,"This answers your next question:

Just a question: In the first answer you provided, you were able to ""type in"" which plot you wanted to see. In the second answer, you can only ""scroll"". Is there a way to add the ""type in"" which plot you want to see for the second answer?

Short answer: yes
... and how to do that?

I actually tried to use selectize.js in an ironic full circle of sorts, but it didn't work out...violence was considered...but it's an inanimate object...so...ya, I lost by default

This uses the JS library/package (whatever they call it for that language) select2.
flexdashboard is SUPER FUN! It really didn't want me to add this library with JS (that would have been too easy, ya know? So this puppy had to get added to the YAML.
The YAML to make this work.
---
title: ""Test Dashboard""
output: 
  flexdashboard::flex_dashboard:
    orientation: rows
    vertical_layout: fill
    extra_dependencies: !expr list(htmltools::htmlDependency('select2.min.js', '1.0', src = list(href = 'https://cdn.jsdelivr.net/npm/select2@4.1.0-rc.0/dist'), script='js/select2.min.js', all_files = FALSE, style = 'css/select2.min.css'))
---

By default, it will look like this.

I figured your very next question would be about appearance... so I jumped the gun.
As far as I understand it, (I'm new to select2), when widening the search box, you have to move the dropdown arrow, which accounts for the first 3 of the entries in this CSS.
The next two are for highlighting when you mouse over in the dropdown. By default, the previous selection is highlighted grey, and the currently hovered-over is highlighted light blue. I added these so that you could change the colors if you wanted to.
The final call in CSS is setting the font family. I chose the default family in Plotly (so they matched).
```{css}

.select2-container--default .select2-selection--single{ 
  min-height: 40px;
  padding: 6px 6px;
  width: 175px;
  position: relative;
}
.select2-container--default .select2-selection--single .select2-selection__arrow {
  right: 0px;
  width: 20px;
  min-height: 34px;   /* parent min-height, minus top padding 40 - 6 */
  position: absolute;
}
.select2-dropdown { /* the chunk requires 'important' */
  width: 175px !important; /* so they're the same width */
  top: 50%;
  padding: 6px; 12px;
}
.select2-container--default .select2-results__option--highlighted[aria-selected] {
  background-color: #F5F0E3;
  color: black;    /* in dropdown, item hovered on bg and text */
}                  /* default is background-color: #5897fb; default blue  */
.select2-container--default .select2-results__option--selected {
  background-color: #fbfaf5;
  color: black;    /* in dropdown, PREVIOUS selection bg and text  */
}                  /* default background-color: #ddd; yucky grey */
option {
  font-family: verdana;       /*  to match plotly  */
}

```

Creating the plot list, the dropdown, and rendering the plots in R code didn't change.
The JS didn't change that much.

/* doesn't catch that the first plot is default; set manually */
setTimeout(function(){
  $('select').select2();                /* connect to the select2 library */
  plt = document.querySelectorAll('div.plotly.html-widget');
  for(i = 0; i &lt; plt.length; i++) {
    if(i === 0) {
      plt[i].style.display = 'block';
    } else {
      plt[i].style.display = 'none';
    }
  }
}, 200) /* run once; allow for loading*/

/* goes with the dropdown; this shows/hides plots based on dropdown */
function getPlot(opt) {
  plt = document.querySelectorAll('div.plotly.html-widget');
  for(i = 0; i &lt; plt.length; i++) {      /* switched to plt from opt here */
    opti = opt.options[i];
    if(opti.selected) {
      plt[i].style.display = 'block';
    } else {
      plt[i].style.display = 'none'
    }
  }
}


That all gives you this.
 
All the code altogether.
---
title: ""Test Dashboard""
output: 
  flexdashboard::flex_dashboard:
    orientation: rows
    vertical_layout: fill
    extra_dependencies: !expr list(htmltools::htmlDependency('select2.min.js', '1.0', src = list(href = 'https://cdn.jsdelivr.net/npm/select2@4.1.0-rc.0/dist'), script='js/select2.min.js', all_files = FALSE, style = 'css/select2.min.css'))
---

```{r setup, include=FALSE}

library(flexdashboard)
library(plotly)
library(tidyverse)
library(htmltools)
library(shinyRPG)   #  devtools::install_github(""RinteRface/shinyRPG"")

plts &lt;- vector(mode = ""list"")   # store plot list

for (i in 1:10) {
  d_i = data.frame(x = rnorm(100,100,100), y = rnorm(100,100,100))
  title_i = paste0(""title_"",i)

  plts[[i]] &lt;- plot_ly( # make a list of objects; no .html
    data = d_i, x = ~x, y = ~y, height = 400, 
    mode = ""markers"", type = ""scatter"") %&gt;%
    layout(title = title_i)
  }

```


```{css}

.select2-container--default .select2-selection--single{ /* outer container of dropdown */
  min-height: 40px;
  padding: 6px 6px;
  width: 175px;
  position: relative;
}
.select2-container--default .select2-selection--single .select2-selection__arrow {
  right: 0px;
  width: 20px;
  min-height: 34px;   /* parent min-height, minus top padding 40 - 6 */
  position: absolute;
}
.select2-dropdown { /* the chunk requires 'important' */
  width: 175px !important; /* so they're the same width */
  top: 50%;
  padding: 6px; 12px;
}
.select2-container--default .select2-results__option--highlighted[aria-selected] {
  background-color: #F5F0E3;
  color: black;    /* in dropdown, item hovered on bg and text */
}                  /* default is background-color: #5897fb; default blue  */
.select2-container--default .select2-results__option--selected {
  background-color: #fbfaf5;
  color: black;    /* in dropdown, PREVIOUS selection bg and text  */
}                  /* default background-color: #ddd; yucky grey */
option {
  font-family: verdana;       /*  to match plotly  */
}

```


Column {data-width=100}
-----------------------------------------------------------------------

### Window 1 {data-height=500}

```{r makeGoodChoices}

opts &lt;- choice &lt;-  paste0(""plot_"", 1:100) # this line replaces last 3 lines
namedChoices = setNames(opts, choice)

newInput &lt;- rpgSelect(         # &lt;----- I'm new; the dropdown
  ""selectBox"",
  NULL,
  namedChoices,
  multiple = F)

newInput$children[[2]]$attribs$onchange &lt;- ""getPlot(this)""

newInput  # add dropdown to webpage

```

&lt;!--- make space between dropdown and plot ---&gt;
&lt;div id=""plots"" style=""margin-top:3rem; margin-bottom:3rem;""&gt;

```{r dynoPlots,results='asis'}

tagList(plts) # print every plot (so they're all in the HTML)

```

&lt;/div&gt;

```{r giveItUp,results='asis',engine='js'}

/* doesn't catch that the first plot is default, set manually */
setTimeout(function(){
  $('select').select2();                /* connect to the select2 library */
  plt = document.querySelectorAll('div.plotly.html-widget');
  for(i = 0; i &lt; plt.length; i++) {
    if(i === 0) {
      plt[i].style.display = 'block';
    } else {
      plt[i].style.display = 'none';
    }
  }
}, 200) /* run once; allow for loading*/

/* goes with the dropdown; this shows/hides plots based on dropdown */
function getPlot(opt) {
  plt = document.querySelectorAll('div.plotly.html-widget');
  for(i = 0; i &lt; plt.length; i++) {     /* switched to plt from opt here */
    opti = opt.options[i];
    if(opti.selected) {
      plt[i].style.display = 'block';
    } else {
      plt[i].style.display = 'none'
    }
  }
}

```

","I decided to make this an entirely different answer because it really is a different question.
This is based on the assumption that you won't import external files. This does not use Shiny runtime, but does the same thing as above.
BTW, I didn't check if selectInput would work, I went with shinyRPG because I knew it would work.
Here's a summary of changes from the answer to your original question:

dropped shiny: runtime from YAML
dropped library(shiny)
added library(shinyRPG)
dropped plot names (they're in a list now)
added list to store plots; sent plots to list when created
dropped .html from dropdown option names (they can be anything you want now)
rpgSelect replaced selectInput
added JS to connect plots to the dropdown

Here's what the bare bones looks like (almost exactly the same)

All of the code to make this happen with notes in the code for explanation. If anything is unclear, let me know.
---
title: ""Test Dashboard""
output: 
  flexdashboard::flex_dashboard:
    orientation: rows
    vertical_layout: fill
---

```{r setup, include=FALSE}

library(flexdashboard)
library(plotly)
library(tidyverse)
library(htmltools)
library(shinyRPG)   # devtools::install_github(""RinteRface/shinyRPG"")

plts &lt;- vector(mode = ""list"")   # stores plots

for (i in 1:10) {
  d_i = data.frame(x = rnorm(100,100,100), y = rnorm(100,100,100))
  title_i = paste0(""title_"",i)

  plts[[i]] &lt;- plot_ly(                  # make a list of objects
    data = d_i, x = ~x, y = ~y, height = 400, 
    mode = ""markers"", type = ""scatter"") %&gt;%
    layout(title = title_i)
  }

```

Column {data-width=100}
-----------------------------------------------------------------------

### Window 1 {data-height=500}

```{r makeGoodChoices}

opts &lt;- choice &lt;-  paste0(""plot_"", 1:100) # this line replaces last 3 lines
namedChoices = setNames(opts, choice)

newInput &lt;- rpgSelect(         # &lt;----- I'm new; the drop down (used same args)
  ""selectBox"",
  NULL,
  namedChoices,
  multiple = F)
newInput$children[[2]]$attribs$onchange &lt;- ""getPlot(this)""

newInput  # add dropdown to webpage
```

&lt;!--- make space between dropdown and plot ---&gt;

&lt;div id=""plots"" style=""margin-top:3rem; margin-bottom:3rem;""&gt;

```{r dynoPlots,results='asis'}

tagList(plts) # print every plot (so they're all in the HTML)

```

&lt;/div&gt;

```{r giveItUp,results='asis',engine='js'}

/* doesn't catch that the first plot is default, set manually */
setTimeout(function(){
  plt = document.querySelectorAll('div.plotly.html-widget');
  for(i = 0; i &lt; plt.length; i++) {
    if(i === 0) {
      plt[i].style.display = 'block';
    } else {
      plt[i].style.display = 'none';
    }
  }
}, 200) /* run once; allow for loading*/

/* goes with the drop down; this shows/hides plots based on drop down */
function getPlot(opt) {
  plt = document.querySelectorAll('div.plotly.html-widget');
  for(i = 0; i &lt; opt.length; i++) {
    opti = opt.options[i];
    if(opti.selected) {
      plt[i].style.display = 'block';
    } else {
      plt[i].style.display = 'none'
    }
  }
}

```

","This isn't exactly what you're looking for. This doesn't import the file. I'm still going to try to figure that part out.
I'm still trying to make the external file call work. Right now, it just wasn't to give me the literal HTML. I've tried a few approaches. I'm sure it's being a pain because this is probably not a good way to do this. For example, each plot will bring in the full HTML, which means that if there were 100 plots, you've got the entire plotly.js 100 times. (Whoa!)

If you're set on using external files and planning on rendering them in RMD, especially when using Shiny, you may want to consider an approach that keeps them R objects, like Rda or RData. That will use a LOT less memory.

In this version, I've only created the plots as objects (not saved, external files).
This is modified from your question. It creates an object for each for iteration.
---
title: ""Test Dashboard""
output: 
  flexdashboard::flex_dashboard:
    orientation: rows
    vertical_layout: fill
runtime: shiny
---

```{r setup, include=FALSE}
library(flexdashboard)
library(shiny)
library(plotly)
library(tidyverse)
library(htmltools)

for (i in 1:10) {
  d_i = data.frame(x = rnorm(100,100,100), y = rnorm(100,100,100))
  title_i = paste0(""title_"",i)
  # p_i = plot_ly(data = d_i, x = ~x, y = ~y) %&gt;% layout(title = title_i)
  assign(paste0(""plot_"", i, "".html""),   # name them plot_1.html, plot_2.html and so on
         plot_ly(data = d_i, x = ~x, y = ~y, height = 400) %&gt;% 
           layout(title = title_i))
        # not using right now!
  # htmlwidgets::saveWidget(as_widget(p_i), paste0(""plot_"",i, "".html"")) 
  }

# htmlwidgets::saveWidget(as_widget(plot_1.html), ""plot_1.html"") # created for testing
```

I've modified your called to selectInput, as well. I made this a named vector, so that you would have plot_1.html called when the user picked plot_1.
I've kept your code in there, so you can see what's changed.
```{r makeGoodChoices}

# plots = rep(""plot"", 10)
# i = seq(1:100)
# choice = paste0(plots, ""_"",i)

choice = paste0(""plot_"", 1:100) # this line replaces last 3 lines
opts &lt;- paste0(choice, "".html"")

namedChoices = setNames(opts, choice)

# selectInput(""project"", label = NULL, choices = choice) # originally
selectInput(""project"", label = NULL, choices = namedChoices)
```

Since this is an R object (not an external file), this is how you would call the plots from the dropdown.
```{r dynoPlots}

renderPlotly(get(input$project)) # show me!

```



The RMarkdown altogether
---
title: ""Test Dashboard""
output: 
  flexdashboard::flex_dashboard:
    orientation: rows
    vertical_layout: fill
runtime: shiny
---

```{r setup, include=FALSE}
library(flexdashboard)
library(shiny)
library(plotly)
library(tidyverse)
library(htmltools)

for (i in 1:10) {
  d_i = data.frame(x = rnorm(100,100,100), y = rnorm(100,100,100))
  title_i = paste0(""title_"",i)
  # p_i = plot_ly(data = d_i, x = ~x, y = ~y) %&gt;% layout(title = title_i)
  assign(paste0(""plot_"", i, "".html""),   # name them plot_1.html, plot_2.html and so on
         plot_ly(data = d_i, x = ~x, y = ~y, height = 400) %&gt;% 
           layout(title = title_i))
        # not using right now!
  # htmlwidgets::saveWidget(as_widget(p_i), paste0(""plot_"",i, "".html""))
  }

# htmlwidgets::saveWidget(as_widget(plot_1.html), ""plot_1.html"") # created for testing
```

Column {data-width=100}
-----------------------------------------------------------------------

### Window 1 {data-height=500}

```{r makeGoodChoices}

# plots = rep(""plot"", 10)
# i = seq(1:100)
# choice = paste0(plots, ""_"",i)

choice = paste0(""plot_"", 1:100) # this line replaces last 3 lines
opts &lt;- paste0(choice, "".html"")

namedChoices = setNames(opts, choice)

# selectInput(""project"", label = NULL, choices = choice) # originally
selectInput(""project"", label = NULL, choices = namedChoices)
```

```{r dynoPlots}

renderPlotly(get(input$project)) # show me!

```

"
Plotly unexpected output,https://stackoverflow.com/questions/50490887,set x and y range of plotly heatmap,"Plotly.js: Initial Zoom

Similar to the above question, is there a way to set the zoom for plotly heatmap produced with geom_tile() and ggplotly().
Say for instance, I wanted an x range of 5:10 and a y range of 20:30, how would I get a zoomed heatmap of that region? The whole goal of this is to zip quickly to important points of the heatmap in a large dataset in a shiny framework.
Here's some example code of a basic plotly heatmap:

p &lt;- t(volcano) %&gt;%
melt() %&gt;% 
ggplot(aes(x=Var2, y=Var1)) + geom_tile(aes(fill=value))
ggplotly(p)

reference: https://stackoverflow.com/questions/36437879/unexpected-output-using-plotly-and-geom-tile?utm_medium=organic&amp;utm_source=google_rich_qa&amp;utm_campaign=google_rich_qa).

",2,1857,"plotly bascially takes options through layout. This page might be helpful https://plot.ly/r/axes/#manual-ranges

ggplotly(p, dynamicTicks = T) %&gt;%
   layout(xaxis=list(autorange=F, range=c(5,10)), yaxis=list(autorange=F, range=c(20,30)))

",,
Plotly unexpected output,https://stackoverflow.com/questions/67072696,Python Dash how to show all rows in a data table?,"I have made an interactive table using dash and plotly in python, currently the table only gives the first few rows, is there a way to make the table have pages or be scrollable so all rows are interactively viewable?
Here is the code I'm using:
data = X_interactive

app = JupyterDash(__name__)

app.layout = html.Div([
    dash_table.DataTable(
        id='datatable-interactivity',
        columns=[
            {""name"": i, ""id"": i, ""deletable"": True, ""selectable"": True} for i in X_interactive.columns
        ],
        data=X_interactive.to_dict('records'),
        editable=True,
        filter_action=""native"",
        sort_action=""native"",
        sort_mode=""multi"",
        column_selectable=""single"",
        row_selectable=""multi"",
        row_deletable=True,
        selected_columns=[],
        selected_rows=[],
        page_action=""native"",
        page_current= 0,
        page_size= 10,
    ),
    html.Div(id='datatable-interactivity-container')
])

@app.callback(
    Output('datatable-interactivity', 'style_data_conditional'),
    Input('datatable-interactivity', 'selected_columns')
)
def update_styles(selected_columns):
    return [{
        'if': { 'column_id': i },
        'background_color': '#D2F3FF'
    } for i in selected_columns]

@app.callback(
    Output('datatable-interactivity-container', ""children""),
    Input('datatable-interactivity', ""derived_virtual_data""),
    Input('datatable-interactivity', ""derived_virtual_selected_rows""))
def update_graphs(rows, derived_virtual_selected_rows):

    if derived_virtual_selected_rows is None:
        derived_virtual_selected_rows = []

    dff = X_interactive if rows is None else pd.DataFrame(rows)

    colors = ['#7FDBFF' if i in derived_virtual_selected_rows else '#0074D9'
              for i in range(len(dff))]

    return [
        dcc.Graph(
            id=column,
            figure={
                ""data"": [
                    {
                        ""x"": dff[""Gene""],
                        ""y"": dff[column],
                        ""type"": ""bar"",
                        ""marker"": {""color"": colors},
                    }
                ],
                ""layout"": {
                    ""xaxis"": {""automargin"": True},
                    ""yaxis"": {
                        ""automargin"": True,
                        ""title"": {""text"": column}
                    },
                    ""height"": 250,
                    ""margin"": {""t"": 10, ""l"": 10, ""r"": 10},
                },
            },
        )
        # check if column exists - user may have deleted it
        # If `column.deletable=False`, then you don't
        # need to do this check.
        for column in [""col1"", ""col2"", ""col3""] if column in dff
    ]

    
app.run_server(mode='inline',port=8056)

From trying to find similar questions I've been trying to get pages with pagination_settings but I'm new to python and not sure how to properly get it into my current code without losing how the table currently is (interactive with being able to filter columns).
Just trying to add pagination_settings within html.Div()  gives me an error:
TypeError: The `dash_table.DataTable` component (version 4.11.2) with the ID ""datatable-interactivity"" received an unexpected keyword argument: `pagination_settings
",0,2789,"pagination-settings isn't a valid arguement as seen by the error below:
TypeError: The `dash_table.DataTable` component (version 4.10.1) with the ID 

""table"" received an unexpected keyword argument: `pagination_settings`
Allowed arguments: active_cell, cell_selectable, column_selectable, columns, css, data, data_previous, data_timestamp, derived_filter_query_structure, derived_viewport_data, derived_viewport_indices, derived_viewport_row_ids, derived_viewport_selected_columns, derived_viewport_selected_row_ids, derived_viewport_selected_rows, derived_virtual_data, derived_virtual_indices, derived_virtual_row_ids, derived_virtual_selected_row_ids, derived_virtual_selected_rows, dropdown, dropdown_conditional, dropdown_data, editable, end_cell, export_columns, export_format, export_headers, fill_width, filter_action, filter_query, fixed_columns, fixed_rows, hidden_columns, id, include_headers_on_copy_paste, is_focused, loading_state, locale_format, markdown_options, merge_duplicate_headers, page_action, page_count, page_current, page_size, persisted_props, persistence, persistence_type, row_deletable, row_selectable, selected_cells, selected_columns, selected_row_ids, selected_rows, sort_action, sort_as_null, sort_by, sort_mode, start_cell, style_as_list_view, style_cell, style_cell_conditional, style_data, style_data_conditional, style_filter, style_filter_conditional, style_header, style_header_conditional, style_table, tooltip, tooltip_conditional, tooltip_data, tooltip_delay, tooltip_duration, virtualization

And by looking at the official documentation to get pagination in a datatable
Code from the documentation:
import dash
from dash.dependencies import Input, Output
import dash_table
import pandas as pd


df = pd.read_csv('https://raw.githubusercontent.com/plotly/datasets/master/gapminder2007.csv')

df[' index'] = range(1, len(df) + 1)

app = dash.Dash(__name__)

PAGE_SIZE = 5

app.layout = dash_table.DataTable(
    id='datatable-paging',
    columns=[
        {""name"": i, ""id"": i} for i in sorted(df.columns)
    ],
    page_current=0,
    page_size=PAGE_SIZE,
    page_action='custom'
)
if __name__ == '__main__':
    app.run_server(debug=True)

You can see that the required and accepted arguments are page_current, page_size, page_action, page_count (from the error).
In your code, I think adding the page_count argument should end up being the fix.
","If you want to display all dataframe rows in your datatable set this option:
pd.set_option('display.max_rows',None)

",
Plotly unexpected output,https://stackoverflow.com/questions/66010312,R Shiny | evaluating user textInput with dynamic UI,"I want to safely allow my app users to be able to manipulate the dataset in my shiny app - by passing code to a data %&gt;% mutate (input$textInput1), and then updating a reactive value containing the manipulated data, v$data.
There are answers for how to use a single, pre-named input and parse it, but I can't extrapolate to how to define this for several text inputs.
e.g. 'input$textinput1','input$textinput2'..
Pressing the recode button with an activated input field causes the error:

Warning: Error in : Problem with `mutate()` input `..1`. x &lt;text&gt;:1:1: unexpected '[[' 1: [[ ^ ℹ Input `..1` is `eval.secure(parse(text = paste0(""[[input$recode_call"", &gt;i, ""]]"")))`. 94: &lt;Anonymous&gt;

library(ggplot2)
library(shiny)
library(DT)
library(dplyr)
library(plotly)
library(colourpicker)
library(RAppArmor)


server &lt;- shinyServer(function(input, output, session){

  #Tracks user changes to input
  v &lt;- reactiveValues(data=NULL, print_execute_complete=NULL)

  #For development, mtcars
  myData &lt;- reactive({
    return(mtcars)
  })

  #Count the number of recoding terms to render
  counter &lt;- reactiveValues(n = 0)

  observeEvent(input$add_recode, {counter$n &lt;-  counter$n + 1})
  observeEvent(input$rm_recode, {
    if(counter$n &gt; 0) counter$n &lt;-  counter$n - 1
  })


  #Recoding button functionality
  recoding_i &lt;- reactive({

    n &lt;- counter$n

    if(n&gt;0){
      isolate({
        lapply(seq_len(n),function(i){

          fluidRow(
            column(width=4,
                   textInput(inputId = paste0('recode_call',i),
                             label=paste0('Recode_',i)))
          )
        }
        )

      })
    }
  })

  #Render the dynamic UI
  output$recoding &lt;- renderUI({ recoding_i() })

  #Observes press of recode button.
  observeEvent(input$'execute_recode',{
    v[[""print_execute_complete""]] &lt;- TRUE
  })

  #Observes press of reset button.
  observeEvent(input$'reset_recode',{
    v[[""print_execute_complete""]] &lt;- FALSE
  })


  #Loop over recoding input boxes.
  observeEvent(v$print_execute_complete, {
    if(v[[""print_execute_complete""]] == TRUE){
      if(counter$n==0|is.null(counter$n)){
        return(myData())
      } else {
        lapply(seq_len(counter$n), function(i){
          if(is.null((v[[""data""]]))){
            v$data &lt;- myData() %&gt;% mutate(eval.secure(parse(text=paste0('[[input$recode_call',i,']]'))))
          } else {
            v$data &lt;- v[[""data""]] %&gt;%  mutate(eval.secure(parse(text=paste0('[[input$recode_call',i,']]'))))
          }
        }
        )
      }
    }
  })






  #Confirmation text
  output$execute_complete &lt;- renderText({
    req(v[[""print_execute_complete""]])
    if(v[[""print_execute_complete""]] == TRUE){
      ""Recoding Complete.""
    }

  })

  #Render recoded data table
  output$recoded_dt &lt;- DT::renderDataTable({
    req(v[[""print_execute_complete""]] == TRUE)
    if(!is.null(v[[""data""]])){
      return(DT::datatable(v[[""data""]], filter='top'))

    } else {
      return(iris)#DT::datatable(myData(),filter='top'))
    }
  })

}
)

ui &lt;- shinyUI(fluidPage(


  titlePanel(""Something is Wrong""),
  # Input: Select a file ----
  navlistPanel(
    tabPanel(""Recoding"",

             h3(""Instruction""),

             fluidRow(p(""Write a functional call in one of the action boxes below. A call takes the form of one of the following :""
                        ,style=""font-family: 'times'; font-si16pt"")
             ),

             fluidRow(actionButton('add_recode', 'Add recode term'),
                      actionButton('rm_recode', 'Remove recode term')),
             br(),
             br(),
             uiOutput('recoding'),
             br(),
             br(),
             fluidRow(actionButton('execute_recode', ""Recode"",icon = icon('angle-double-right')),
                      actionButton('reset_recode', ""Reset"", icon=icon('angle-double-right'))),
             textOutput('execute_complete'),
             br(),
             br(),
             br(),
             DT::dataTableOutput('recoded_dt')

    )
  )
)
)

shinyApp(ui, server)



",0,303,"The following code captures a dynamic number of textInputs and converts them to code matching: 'Variable name' 'Code call'. These must be handled separately by rlang evaluation since anything left of := must be a symbol. The chain of functions transform a textInput to actionable code.
I have tried to understand why this works (edits welcome by those who understand rlang/tidyeval!):

For each of the additional textInput boxes created, a counter allows an anonymous function to loop over and create and paste together valid input name, e.g. input$recode_call1. This is then evaluated as text, parsed into an expression, where it is evaluated and interpreted, and then turned into an expression.

Unanswered questions about this answer:

Error possibilities. Can certain inputs create bugs?
Security concerns. Can / should rlang::eval_tidy() be swapped out for, e.g. unix::eval.safe()?
Is there an easier / more secure way to handle the dynamic input?

library(ggplot2)
library(shiny)
library(DT)
library(dplyr)
library(plotly)
library(colourpicker)
library(RAppArmor)


server &lt;- shinyServer(function(input, output, session){

  #Tracks user changes to input
  v &lt;- reactiveValues(data=NULL, print_execute_complete=NULL)

  #For development, mtcars
  myData &lt;- reactive({
    return(mtcars)
  })

  #Count the number of recoding terms to render
  counter &lt;- reactiveValues(n = 0)

  observeEvent(input$add_recode, {counter$n &lt;-  counter$n + 1})
  observeEvent(input$rm_recode, {
    if(counter$n &gt; 0) counter$n &lt;-  counter$n - 1
  })


  #Recoding button functionality
  recoding_i &lt;- reactive({

    n &lt;- counter$n

    if(n&gt;0){
      isolate({
        lapply(seq_len(n),function(i){

          fluidRow(
            column(width=4,
                   textInput(inputId = paste0('recode_call',i),
                             label=paste0('Recode_',i)))
          )
        }
        )

      })
    }
  })

  #Render the dynamic UI
  output$recoding &lt;- renderUI({ recoding_i() })

  #Observes press of recode button.
  observeEvent(input$'execute_recode',{
    v[[""print_execute_complete""]] &lt;- TRUE
  })

  #Observes press of reset button.
  observeEvent(input$'reset_recode',{
    v[[""print_execute_complete""]] &lt;- FALSE
  })


   #Loop over recoding input boxes.
    observeEvent(v$print_execute_complete, {
      if(v[[""print_execute_complete""]] == TRUE){
        n &lt;- counter$n
        if(counter$n==0){
          v$data &lt;- myData()
         } else {
           v$data &lt;- myData()
           lapply(seq_len(n), function(i){
             recode_call_i &lt;- rlang::parse_expr(rlang::eval_tidy(rlang::parse_expr(eval(paste0(""input$recode_call"",i)))))

             var_name_i &lt;- rlang::sym(rlang::eval_tidy(rlang::parse_expr(paste0(""input$recode_name"",i))))

             v$data &lt;- mutate(v$data,!!var_name_i := !!recode_call_i)
           }
           )
             }
      }
    }
    )






  #Confirmation text
  output$execute_complete &lt;- renderText({
    req(v[[""print_execute_complete""]])
    if(v[[""print_execute_complete""]] == TRUE){
      ""Recoding Complete.""
    }

  })

  #Render recoded data table
  output$recoded_dt &lt;- DT::renderDataTable({
    req(v[[""print_execute_complete""]] == TRUE)
    if(!is.null(v[[""data""]])){
      return(DT::datatable(v[[""data""]], filter='top'))

    } else {
      return(iris)#DT::datatable(myData(),filter='top'))
    }
  })

}
)

ui &lt;- shinyUI(fluidPage(


  titlePanel(""This time it works""),
  # Input: Select a file ----
  navlistPanel(
    tabPanel(""Recoding"",

             h3(""Instruction""),

             fluidRow(p(""Write a functional call in one of the action boxes below. A call takes the form of one of the following :""
                        ,style=""font-family: 'times'; font-si16pt"")
             ),

             fluidRow(actionButton('add_recode', 'Add recode term'),
                      actionButton('rm_recode', 'Remove recode term')),
             br(),
             br(),
             uiOutput('recoding'),
             br(),
             br(),
             fluidRow(actionButton('execute_recode', ""Recode"",icon = icon('angle-double-right')),
                      actionButton('reset_recode', ""Reset"", icon=icon('angle-double-right'))),
             textOutput('execute_complete'),
             br(),
             br(),
             br(),
             DT::dataTableOutput('recoded_dt')

    )
  )
)
)

shinyApp(ui, server)


",,
Plotly unexpected output,https://stackoverflow.com/questions/50222396,error when displaying more details in plotly tooltip,"Im trying to display more data on hoover over a point, than default point coordinates. 
It works when I display only one extra information for example:

 output$myplot &lt;- renderPlotly({
    if (is.null(filtered())) {
      return()
    }
    ggplot(filtered(), aes_string(x=input$xvar, y=input$yvar, text=filtered()$Ep.name)) +
      geom_point()
  })


Works just fine and I get what I want to achieve (which is the data that i pass to text variable. But when I tried passing more variables, using paste:

 ggplot(filtered(), aes_string(x=input$xvar, y=input$yvar, text=paste(""name: "",filtered()$Ep.name, ""season: "", filtered()$Season, ""number: "", filtered()$Ep.Number))) +
      geom_point()


I get this error:

Warning: Error in parse: &lt;text&gt;:1:12: unexpected symbol
1: name:  The Kingsroad


Which means there is something wrong with the value when it is pasted.
However I have no idea how to inlcude all three variables from the filtered() dataframe onto aes_string so that they are all displayed in the tooltip.
Anyone has idea how to solve this?

EDIT: Here is code that allows you to reproduce the error, along with the sample of the dataset im using for this:

library(shiny)
library(ggplot2)
library(dplyr)
library(plotly)


shows &lt;- read.csv(""finalR1.csv"", header=TRUE)

ui &lt;- fluidPage(
  tabsetPanel(
    tabPanel(h1(""Plot""),
             plotlyOutput(""myplot""),
             hr()),
    tabPanel(h1(""Table""), tableOutput(""results""))
  ),

  fluidRow(
    column(3,
           h4(""Episode explorer""), 
           sliderInput(""voteInput"", ""Votes"", min = 0, max = 155000, value = c(2500, 40000)),
           br(),
           sliderInput(""lenInput"", ""Length"", min = 0, max = 110, value = c(0, 60)),
           br(),
           uiOutput(""ratingOutput"")

    ),

   column(4,offset = 0.5,
          h4('Axis display options'),
          selectInput('xvar', 'X', choice=c(""Length"", ""Ep.Rating"", ""Votes"", ""Year""), selected=""Ep.Rating""),
          selectInput('yvar', 'Y', choice=c(""Length"", ""Ep.Rating"", ""Votes"", ""Year""), selected=""Votes"")
          ))

)


server &lt;- function(input, output) {
  output$ratingOutput &lt;- renderUI({
    selectInput(""ratingInput"", ""Ratings"",
                c(""All"", as.character(sort(unique(shows$TV.Rating)))),
                selected = ""All"")
  })


  filtered&lt;-reactive({
    if (is.null(input$ratingInput)) {
      return(NULL)
    }

    shows %&gt;%
    filter(Votes &gt;= input$voteInput[1],
           Votes &lt;= input$voteInput[2],
           Length &gt;= input$lenInput[1],
           Length &lt;= input$lenInput[2],
           if (input$ratingInput != ""All"") {
             TV.Rating == input$ratingInput
           } else TRUE

    )

  })

  output$myplot &lt;- renderPlotly({
    if (is.null(filtered())) {
      return()
    }
    ggplot(filtered(), aes_string(x=input$xvar, y=input$yvar, text=paste(""name: "",filtered()$Ep.name, ""season: "", filtered()$Season, ""number: "", filtered()$Ep.Number))) +
      geom_point()
  })

  output$results &lt;- renderTable({
    filtered()
  })
}

shinyApp(ui = ui, server = server)


https://drive.google.com/file/d/15ZqzY_msBBlBnrrqsqeagZSpJxifKKjM/view?usp=sharing
",0,390,"Maybe you could try this:

  output$myplot &lt;- renderPlotly({
    if (is.null(filtered())) {
      return()
    }

  a &lt;- ggplot(filtered(), aes_string(x=input$xvar, y=input$yvar))  +
      geom_point(aes(text=paste('name:',filtered()$Ep.name, '&lt;br&gt;season:', filtered()$Season, '&lt;br&gt;number:', filtered()$Ep.Number)))

  ggplotly(a, tooltip = c(""x"", ""y"", ""text""))

  })

",,
Plotly unexpected result,https://stackoverflow.com/questions/74948525,FutureWarning: save is not part of the public API in Python,"I am using Python to convert Pandas df to .xlsx (in Plotly-Dash app.). All working well so far but with this warning tho:
""FutureWarning:
save is not part of the public API, usage can give unexpected results and will be removed in a future version""
How should I modify the code below in order to keep its functionality and stability in future? Thanks!
 writer = pd.ExcelWriter(""File.xlsx"", engine = ""xlsxwriter"")

 workbook  = writer.book

 df.to_excel(writer, sheet_name = 'Sheet', index = False)
  
 writer.save()

",22,15431,"just replace save with close.
 writer = pd.ExcelWriter(""File.xlsx"", engine = ""xlsxwriter"")

 workbook  = writer.book

 df.to_excel(writer, sheet_name = 'Sheet', index = False)
  
 writer.close()

","You can try any of these options, and it should work:
Replace:
writer.save()

by
    writer._save()
or
    writer.close()

",
Plotly unexpected result,https://stackoverflow.com/questions/67374558,Exporting Plotly charts with Kaleido not working?,"I have locally created plotly charts in Jupyter Notebook that I'm trying to save as png files on my local machine.
Example of charts:
for table in EG_SD_pivots:
    df = EG_SD_pivots[table].reset_index()
    fig = make_subplots(specs=[[{""secondary_y"": True}]])

    fig.add_trace(
            go.Scatter(x=df.iloc[:, 0], y=df['SD_Credibility'], name=""Credibility"", marker=dict(color=yellow)), secondary_y=True)

    fig.add_trace(
            go.Bar(x=df.iloc[:, 0], y=df['EG_LR_Rel'], name=""EG Relativity"", marker=dict(color=teal)), secondary_y=False)

    fig.add_trace(
            go.Bar(x=df.iloc[: ,0], y=df['SD_LR_Rel'], name=""SD Relativity"", marker=dict(color=indigo)), secondary_y=False)

    # Add figure title
    fig.update_layout(title_text=df.iloc[:0].columns[0], margin=dict(b=200))

    # Set x-axis title
    fig.update_xaxes(title_text=df.iloc[:0].columns[0])

    fig.update_yaxes(title_text=""&lt;b&gt;Relativity&lt;/b&gt;"", secondary_y=False)
    fig.update_yaxes(title_text=""&lt;b&gt;Credibility&lt;/b&gt;"", secondary_y=True)
    fig.show()

I've tried a number of different solutions, but they've all returned various errors...
Solutions:
fig.to_image(format=""png"") --&gt; returns
ValueError: 
The orca executable is required to export figures as static images,
but it could not be found on the system path.

plotly.orca.config.executable = 'C:/Program Files/Anaconda3/pkgs/plotly-4.0.0-py_0/site-packages/plotly/io/_orca.py' --&gt; returns
AttributeError: module 'chart_studio.plotly' has no attribute 'orca'

plotly.offline.iplot(fig, filename=(str(chart) + '.png')) --&gt; returns
AttributeError: module 'chart_studio.plotly' has no attribute 'offline'

fig.write_image(""EG_SD_images/"" + str(chart) + "".png"", engine=""kaleido"") --&gt; returns
`TypeError: write_image() got an unexpected keyword argument 'engine'`

plotly.image.save_as(fig, filename=(str(chart) + '.png')) --&gt; returns
Error                                     Traceback (most recent call last)
C:\Program Files\Anaconda3\lib\site-packages\urllib3\contrib\pyopenssl.py in wrap_socket(self, sock, server_side, do_handshake_on_connect, suppress_ragged_eofs, server_hostname)
    452             try:
--&gt; 453                 cnx.do_handshake()
    454             except OpenSSL.SSL.WantReadError:

C:\Program Files\Anaconda3\lib\site-packages\OpenSSL\SSL.py in do_handshake(self)
   1906         result = _lib.SSL_do_handshake(self._ssl)
-&gt; 1907         self._raise_ssl_error(self._ssl, result)
   1908 

C:\Program Files\Anaconda3\lib\site-packages\OpenSSL\SSL.py in _raise_ssl_error(self, ssl, result)
   1638         else:
-&gt; 1639             _raise_current_error()
   1640 

C:\Program Files\Anaconda3\lib\site-packages\OpenSSL\_util.py in exception_from_error_queue(exception_type)
     53 
---&gt; 54     raise exception_type(errors)
     55 

Error: [('SSL routines', 'tls_process_server_certificate', 'certificate verify failed')]

During handling of the above exception, another exception occurred:

SSLError                                  Traceback (most recent call last)
C:\Program Files\Anaconda3\lib\site-packages\urllib3\connectionpool.py in urlopen(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)
    599                                                   body=body, headers=headers,
--&gt; 600                                                   chunked=chunked)
    601 

C:\Program Files\Anaconda3\lib\site-packages\urllib3\connectionpool.py in _make_request(self, conn, method, url, timeout, chunked, **httplib_request_kw)
    342         try:
--&gt; 343             self._validate_conn(conn)
    344         except (SocketTimeout, BaseSSLError) as e:

C:\Program Files\Anaconda3\lib\site-packages\urllib3\connectionpool.py in _validate_conn(self, conn)
    838         if not getattr(conn, 'sock', None):  # AppEngine might not have  `.sock`
--&gt; 839             conn.connect()
    840 

C:\Program Files\Anaconda3\lib\site-packages\urllib3\connection.py in connect(self)
    343             server_hostname=server_hostname,
--&gt; 344             ssl_context=context)
    345 

C:\Program Files\Anaconda3\lib\site-packages\urllib3\util\ssl_.py in ssl_wrap_socket(sock, keyfile, certfile, cert_reqs, ca_certs, server_hostname, ssl_version, ciphers, ssl_context, ca_cert_dir)
    343         if HAS_SNI and server_hostname is not None:
--&gt; 344             return context.wrap_socket(sock, server_hostname=server_hostname)
    345 

C:\Program Files\Anaconda3\lib\site-packages\urllib3\contrib\pyopenssl.py in wrap_socket(self, sock, server_side, do_handshake_on_connect, suppress_ragged_eofs, server_hostname)
    458             except OpenSSL.SSL.Error as e:
--&gt; 459                 raise ssl.SSLError('bad handshake: %r' % e)
    460             break

SSLError: (""bad handshake: Error([('SSL routines', 'tls_process_server_certificate', 'certificate verify failed')])"",)

During handling of the above exception, another exception occurred:

MaxRetryError                             Traceback (most recent call last)
C:\Program Files\Anaconda3\lib\site-packages\requests\adapters.py in send(self, request, stream, timeout, verify, cert, proxies)
    448                     retries=self.max_retries,
--&gt; 449                     timeout=timeout
    450                 )

C:\Program Files\Anaconda3\lib\site-packages\urllib3\connectionpool.py in urlopen(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)
    637             retries = retries.increment(method, url, error=e, _pool=self,
--&gt; 638                                         _stacktrace=sys.exc_info()[2])
    639             retries.sleep()

C:\Program Files\Anaconda3\lib\site-packages\urllib3\util\retry.py in increment(self, method, url, response, error, _pool, _stacktrace)
    397         if new_retry.is_exhausted():
--&gt; 398             raise MaxRetryError(_pool, url, error or ResponseError(cause))
    399 

MaxRetryError: HTTPSConnectionPool(host='api.plotly.com', port=443): Max retries exceeded with url: /v2/images (Caused by SSLError(SSLError(""bad handshake: Error([('SSL routines', 'tls_process_server_certificate', 'certificate verify failed')])"")))

During handling of the above exception, another exception occurred:

SSLError                                  Traceback (most recent call last)
C:\Program Files\Anaconda3\lib\site-packages\chart_studio\api\v2\utils.py in request(method, url, **kwargs)
    171     try:
--&gt; 172         response = requests.request(method, url, **kwargs)
    173     except RequestException as e:

C:\Program Files\Anaconda3\lib\site-packages\requests\api.py in request(method, url, **kwargs)
     59     with sessions.Session() as session:
---&gt; 60         return session.request(method=method, url=url, **kwargs)
     61 

C:\Program Files\Anaconda3\lib\site-packages\requests\sessions.py in request(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)
    532         send_kwargs.update(settings)
--&gt; 533         resp = self.send(prep, **send_kwargs)
    534 

C:\Program Files\Anaconda3\lib\site-packages\requests\sessions.py in send(self, request, **kwargs)
    645         # Send the request
--&gt; 646         r = adapter.send(request, **kwargs)
    647 

C:\Program Files\Anaconda3\lib\site-packages\requests\adapters.py in send(self, request, stream, timeout, verify, cert, proxies)
    513                 # This branch is for urllib3 v1.22 and later.
--&gt; 514                 raise SSLError(e, request=request)
    515 

SSLError: HTTPSConnectionPool(host='api.plotly.com', port=443): Max retries exceeded with url: /v2/images (Caused by SSLError(SSLError(""bad handshake: Error([('SSL routines', 'tls_process_server_certificate', 'certificate verify failed')])"")))

During handling of the above exception, another exception occurred:

PlotlyRequestError                        Traceback (most recent call last)
&lt;ipython-input-39-fb5a62f7ab75&gt; in &lt;module&gt;
     69     fig.show()
     70     EG_SD_charts[table] = fig
---&gt; 71     plotly.image.save_as(fig, filename=(str(chart) + '.png'))
     72     #plotly.offline.iplot(fig, filename=(str(chart) + '.png'))
     73     #fig.write_image(""EG_SD_images/"" + str(chart) + "".png"", engine=""kaleido"")

C:\Program Files\Anaconda3\lib\site-packages\chart_studio\plotly\plotly.py in save_as(cls, figure_or_data, filename, format, width, height, scale)
    908             filename += ""."" + format
    909 
--&gt; 910         img = cls.get(figure_or_data, format, width, height, scale)
    911 
    912         f = open(filename, ""wb"")

C:\Program Files\Anaconda3\lib\site-packages\chart_studio\plotly\plotly.py in get(figure_or_data, format, width, height, scale)
    817             payload[""scale""] = scale
    818 
--&gt; 819         response = v2.images.create(payload)
    820 
    821         headers = response.headers

C:\Program Files\Anaconda3\lib\site-packages\chart_studio\api\v2\images.py in create(body)
     16     """"""
     17     url = build_url(RESOURCE)
---&gt; 18     return request(""post"", url, json=body)

C:\Program Files\Anaconda3\lib\site-packages\retrying.py in wrapped_f(*args, **kw)
     47             @six.wraps(f)
     48             def wrapped_f(*args, **kw):
---&gt; 49                 return Retrying(*dargs, **dkw).call(f, *args, **kw)
     50 
     51             return wrapped_f

C:\Program Files\Anaconda3\lib\site-packages\retrying.py in call(self, fn, *args, **kwargs)
    204 
    205             if not self.should_reject(attempt):
--&gt; 206                 return attempt.get(self._wrap_exception)
    207 
    208             delay_since_first_attempt_ms = int(round(time.time() * 1000)) - start_time

C:\Program Files\Anaconda3\lib\site-packages\retrying.py in get(self, wrap_exception)
    245                 raise RetryError(self)
    246             else:
--&gt; 247                 six.reraise(self.value[0], self.value[1], self.value[2])
    248         else:
    249             return self.value

C:\Program Files\Anaconda3\lib\site-packages\six.py in reraise(tp, value, tb)
    691             if value.__traceback__ is not tb:
    692                 raise value.with_traceback(tb)
--&gt; 693             raise value
    694         finally:
    695             value = None

C:\Program Files\Anaconda3\lib\site-packages\retrying.py in call(self, fn, *args, **kwargs)
    198         while True:
    199             try:
--&gt; 200                 attempt = Attempt(fn(*args, **kwargs), attempt_number, False)
    201             except:
    202                 tb = sys.exc_info()

C:\Program Files\Anaconda3\lib\site-packages\chart_studio\api\v2\utils.py in request(method, url, **kwargs)
    177         status_code = response.status_code if response else None
    178         content = response.content if response else ""No content""
--&gt; 179         raise exceptions.PlotlyRequestError(message, status_code, content)
    180     validate_response(response)
    181     return response

PlotlyRequestError: No message

",5,18080,"I also had the error: TypeError: write_image() got an unexpected keyword argument 'engine'. According to plotly/Kaleido manual:

Versions 4.9 and above of the Plotly Python library will automatically use Kaleido for static image export when Kaleido is installed.

Try to check the version by:
import plotly
plotly.__version__

You can then update your Plotly by:
pip install plotly==5.3.1

This will uninstall the old version and install the recent one.
","Personnaly, I use the GUI directy.

",
Plotly unexpected result,https://stackoverflow.com/questions/62825975,Plotly horizontal bar plots giving unexpected results,"I'm trying to create a horizontal bar plot similar to this one.
I can generate a vertical plot, but when I change it to horizontal the plot is a mess. I'm trying this two ways, using go and px. I don't understand the problem and am not sure how to proceed. Here is my code:
Using go:
import pandas as pd
import chart_studio.plotly as py
import plotly.graph_objects as go

dfs = pd.read_html('https://coronavirus.jhu.edu/data/mortality', header=0)
df = dfs[0]

df['Case-Fatality'] = list(map(lambda x: x[:-1], df['Case-Fatality'].values))

df['Case-Fatality'] = pd.to_numeric(df['Case-Fatality'], downcast='float')
df_sort = df.sort_values(by='Case-Fatality')
fig = go.Figure(go.Bar(
    x=df_sort['Country'],
    y=df_sort['Case-Fatality'],
    ))
fig.update_layout(
    title='',
    xaxis_title='',
    yaxis_title='')
fig.show()

Using px:
import plotly.express as px
fig = px.bar(df_sort, x='Country', y = 'Case-Fatality', orientation='h')
fig.show()

Following the plotly documentation, I should be able to add orientation='h' argument to either go.bar or px.bar. When I do, there is no error but the plot does not look correct.
Here is vertical, plotted properly but not the preferred orientation:

Here is with the horizontal orientation, it looks the same with either go or px:

Any help or guidance on why this is not working would be most appreciated.
",3,1201,"Change x and y values around in either go.bar or px.bar (still add orientation = 'h').
Both methods work identically well.
Output:

",,
Plotly unexpected result,https://stackoverflow.com/questions/68178930,Is there a way to create a set of images as plotly traces,"I'm attempting to create a series of Plotly traces where each trace is an image, thus allowing the use of of a slider to slide between them.
I've tried multiple methods and run into problems with each:
1.) Create an empty scatter plot and load background images into it. This just resulted in no images being shown and just a blank figure showing
2.) To create an go.Image figure, but all that results in 'Image has no add_trace attribute'
3.) Create a blank scatter and make each subsequent trace an Image, but i get an 'add_trace() got an unexpected keyword argument 'type'' error
Code(Method 3):
import plotly
import plotly.graph_objects as go
import plotly.express as px
import numpy as np
from PIL import Image
import os

img_width = 1600
img_height = 1600

imh=Image.open('/Users/######/Documents/Project_1/By_Hour/00_00.png',)
# Create figure
fig = go.Figure()

fig.add_trace(
    go.Scatter(
        x=[0, img_width],
        y=[0, img_height],
        mode=""markers"",
        marker_opacity=0
    )
)
# Add traces, one for each slider step
for step in np.arange(5):
    fig.add_trace(str(step),type='image',
                  visible = False,
                  source= imh)
                  
      



# Create and add slider
steps = []
for i in range(5):
    step = dict(
        method=""animate"",
        args=[{""visible"": [False] * (5)},
              {""title"": ""Slider switched to step: "" + str(i)}],  # layout attribute
    )
    step[""args""][0][""visible""][i] = True  # Toggle i'th trace to ""visible""
    steps.append(step)

sliders = [dict(
    active=0,
    visible=True,
    currentvalue={""prefix"": ""Time: ""},
    pad={""t"": 50},
    steps=steps
)]

fig.update_layout(
    sliders=sliders
)

fig.show()

Is there any way of loading images in such a way?
Thank you for any help.
",2,699,"I found this answer that uses an animation to simulate a discrete slider.
Basically you have to load all images and its labels before creating the figure. This is a partial answer as I have not found any working examples using traces.
",,
Plotly unexpected issue,https://stackoverflow.com/questions/60393253,Unselected entries displayed on axis - Crosstalk+Plotly bar-chart,"
EDIT
This seems to be an issue already known to the plotly community
github plotly issue #689
and there is an analogous question here on SO.
Unfortunately, it seems no solution is available yet. Any advice would be greatly appreciated.

I am trying to use Crosstalk and Plotly to create a dashboard and I have come across an unexpected behaviour.
When selecting through the Crosstalk filter, the Plotly bargraph leaves ""gaps"" for the unselected entries.
As a reproducible example, let's say I want to compare cities populations, what I am getting is this (code at the bottom):

It might very well be that I am missing something, is there a way to get rid of the gap? any advice on viable ways to do a similar comparison avoiding the issue?
Thanks in advance.
Code:
---
title: ""Crosstalk+Plotly bargraph selection""
---
 
```{r setup, include=FALSE}
options(stringsAsFactors = FALSE)
library(crosstalk) 
library(dplyr)
library(plotly)  

#data on cities' population
city_pop &lt;- data.frame(""City"" = c(""Florence"",  ""Milan"", ""Venice""),
                    ""Population"" = c(382258, 1352000, 261905))

#setting up Crosstalk shared data
sd &lt;- SharedData$new(city_pop, key = city_pop$city)

#filter for the cities
filt &lt;- filter_select(
  id = ""select_name"",
  label = ""Selected City"",
  sharedData = sd,
  group = ~City)

#barplot of cities' population
bars_pop &lt;- plot_ly(sd, x = ~City, y = ~Population) %&gt;% 
add_bars(width=0.2,
           x =  ~City,
       y =  ~Population,
       color = I(""#89CFF0""),
       name = """",
       opacity=.9,
       hoverinfo = 'y',
       hovertemplate = paste('%{x} &lt;br&gt; number of Residents: %{y}&lt;extra&gt;&lt;/extra&gt;')
       ) 
  
  

```

```{r, echo=FALSE}
filt

bars_pop
```

",4,693,"This worked for me - on the axis where it's happening, set categoryorder = ""trace"".
e.g.,
plot_ly(...) %&gt;% layout(yaxis = list(categoryorder = ""trace""))

",,
Plotly unexpected issue,https://stackoverflow.com/questions/69870017,Plotly Graphs - Conflict between Javascript and Jinja,"I've tried plotting graphs in my Flask web app using Plotly which didn't work for some reason so I started to simplify the issue to find the error.
It seems like there is an issue with the Jinja Syntax {{ myJSONfile | safe }} in javascript.
Aslong as I pass an empty string """" to the variable the graph renders but obviously without datapoints.
(Inspect Element Console gives an Unexpected token '{' Error referring to the first opening bracket of my Jinja variable)
According to this post I should have written the the syntax inside the javascript block correctly in order to pass a JSON-File to the javascript variable and I am out of ideas right now.
I'd appreciate if somebody has further ideas and can help me out here :)
Code example:
   {% extends ""layout.html"" %} {% block content %}

&lt;div class="" row p-4 ""&gt;
    &lt;div class=""card m-auto "" style=""width: 90%; "" data-aos=""fade-left ""&gt;
        &lt;div class=""card-body ""&gt;
            &lt;div id=""chart1 ""&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
&lt;/div&gt;


&lt;script src=""https://cdn.plot.ly/plotly-latest.min.js ""&gt;&lt;/script&gt;
&lt;script&gt;
    var graphs1 = {
        {
            graph1JSON | safe
        }
    };
    Plotly.plot(""chart1 "", graphs1, {});
&lt;/script&gt;

I've since tested out another test code from a tutorial github repository to eliminate the possibility that it may have been some typing errors in my code I wasn't able to identify.
But those copy paste examples gave me the same issue.
I've tried different things suggested in some posts I've found here on Stackoverflow

checked again if my data was saved to the JSON variable in app.py correctly (which it does)
using quotes like: var graph1 = '{{ graph1JSON | safe }}'
using block code syntax: var graph1 = {%block code %} {{ graph1JSON | safe }} {%endblock code %}
Several combinations of above syntax

During debug I clearly see that the JSON file is created successfully and holds the desired data, Flask/Jinja just doesnt want to communicate with Javascript
Here's an additional Screenshot of how the syntax highlighting looks in my VS Code. (Other than in this particular case my Flask app is running fine, being able to render pages dynamically etc.)

(Syntax not recognized in the javascript part)
",2,1199,"The problem here lies with VSCode, especially with Syntax Highlighting and Formatting Extensions.
There is some conflict between some of your enabled extensions which - on save - format the javascript code in above way.

Disable all enabled formatting extensions in VSCode
I only activated the following:


Python
Python Extension Pack by Jayamanne
Better Jinja by Samuel Colvin

This way on save the syntax doesn't get shredded anymore making the JavaScript recognize your JSON-File correctly.
","Please do not pull apart the jinja instruction. Jinja looks for double curly brackets in the template, which follow one another directly with no whitespace in between.
var graphs1 = {{ graph1JSON | safe }};
Plotly.plot('chart1', graphs1, {});

",
Plotly strange behavior,https://stackoverflow.com/questions/61484324,Callback error updating plot-div.children (Plotly Dash),"I ran into a strange behavior - I see similar questions on Plotly forums and on Stackoverflow, but no solution. Basically, I am trying to store intermediate value (to be reused in other callbacks) in a hidden div ‘data-storage-json’, but the callback which has it as Input does not seem to take place. There is no error on backend. On the front end I get ‘Callback error updating plot-div.children’ (which is the component specified as Output)

import dash
from dash.dependencies import Input, Output, State
import dash_core_components as dcc
import dash_html_components as html
import dash_table 
from dash.exceptions import PreventUpdate

########### Layout:
app = dash.Dash(__name__, external_stylesheets=external_stylesheets)

app.layout = html.Div(children=[
    html.Div(id='data-storage-json', style={'display': 'none'}),
    html.Div(children=[
                dash_table.DataTable(
                        id='event-table',
                        style_data={'whiteSpace': 'normal'}, #'border': '1px solid blue'},
                        style_cell={'textAlign': 'center'},
                        #style_header={ 'border': '1px solid pink' },
                        css=[{
                            'selector': '.dash-cell div.dash-cell-value',
                            'rule': 'display: inline; white-space: inherit; overflow: inherit; text-overflow: inherit;'
                        }],
                        columns=[{""name"": i, ""id"": i} for i in event_df.columns if i is not 'id'],
                        style_table={'overflowX': 'scroll'},
                        row_selectable='single',
                        selected_rows=[],
                        page_current=0,
                        page_size=PAGE_SIZE,
                        page_action='custom', 
                        filter_action='custom',
                        filter_query='',
                        sort_action='custom',
                        sort_mode='multi',
                        sort_by=[]                        
                  ),
                  html.Div(id='event-stats', style={'width': '80%', 'color': 'black', 'font-size': '9'})],
                  style={'width': '90%', 'margin-left': '20px', 'font-size': '9', 'horizontal-align': 'middle', 'vertical-align': 'middle'}),
    html.Div(children=[html.Br()]),
    html.Button('Plot', id='show-button'),
    html.Div(id='plot-div', children=[], style={'width': '95%', 'font-size': '9', 'vertical-align': 'middle'}),
])

########### Callbacks:

'''
Callback for sorting/filtering table
'''
@app.callback(
[Output('event-table', 'data'),
 Output('event-table', 'page_count'),
 Output('event-stats', 'children')],
[Input('event-table', 'sort_by'), 
 Input('event-table', 'filter_query'),
 Input('event-table', 'page_current'),
 Input('event-table', 'page_size')])
def update_event_selection(sort_by, filter_query,page_current, page_size):
    dff = sort_filter_table(event_df, filter_query, sort_by) 
    res = dff.iloc[page_current*page_size: (page_current + 1)*page_size]
    page_count = int(dff.shape[0]/page_size)+1
    stat_str = '{} events in the table. Displaying page {} of {}'.format(dff.shape[0], page_current+1, page_count)
    return res.to_dict('records'), page_count, stat_str

@app.callback(
Output('data-storage-json','children'),
[Input('show-button', 'n_clicks')],
[State('event-table','selected_row_ids')
])
def prepare_data(n_clicks,selected_id):
    duration=1
    print('Selected id: ',selected_id)
    if n_clicks is None or  selected_id is None or len(selected_id)==0:
        raise PreventUpdate
    duration=int(duration)
    selected_id=selected_id[0]
    row=event_df.loc[selected_id,:]
    print(row)
    event_time=pd.to_datetime(row['Start'],errors='ignore')

    # sensors to load:
    flist=['ip_m','vp_m','f','df']
    print('Duration {}'.format(duration))
    res_df=get_event_data(interconnect,event_time,duration, feature_list=flist)

    print(res_df.shape)
    js=res_df.to_json(date_format='iso', orient='split')
    print('In Prep: ',len(js))
    return js

@app.callback(
Output('plot-div','children'),
[Input('data-storage-json','children')],
[State('event-table','selected_row_ids')])
def generate_plots(data_storage,selected_id):
    if data_storage is None:
        print('None!!!')
        raise PreventUpdate
    else:
        print('InDisplay -storage: '+str(len(data_storage)))
        res_df = pd.read_json(data_storage, orient='split')

    print('InDisplay ',res_df.shape)
    selected_id=selected_id[0]
    row=event_df.loc[selected_id,:]
    event_time=pd.to_datetime(row['Start'],errors='ignore')
    event_type=row['Event']+': '+row['Cause']
    event_pid=''

    # columns sorted in reverse alphabetical
    flist=sorted(np.unique([c.split('__')[1] for c in res_df.columns]))[::-1]
    print('To plot: ',res_df.shape)
    # generate plots for each type of sensor:
    fig_list=[]
    for feature in flist:
        col_list = [c for c in res_df.columns if not c.startswith('_') and c.endswith('_'+feature)] 
        temp_df = res_df[col_list]
        # plot results
        print('Preparing figure '+feature)
        fig=temp_df.iplot(kind='scatter',mode='markers',size=3, title=""Plot {}: {} {} {}"".format(feature,event_time,event_type,event_pid), asFigure=True)
        #fig_list.append(fig)
        fig_list.append((html.Div(children=[dcc.Graph(id=feature+'-scatter',figure=fig)])))
    print('Figure done')
    return fig_list


########### Run the app:

if __name__ == '__main__':
    parser = argparse.ArgumentParser()
    parser.add_argument('--gpu', type=int, default=0, help='number of GPU to use for calculations')
    parser.add_argument('--port', type=int, default=8050, help='port on which to run (default: 8050)')
    options,_ = parser.parse_known_args()
    os.environ['CUDA_VISIBLE_DEVICES'] = str(options.gpu)

    app.run_server(debug=True, port = options.port)


UPD: event_df is smth like:

event_df = pd.DataFrame({""id"": [0,1,2],
    ""Start"": [""2016-01-01 14:33"",""2016-01-01 16:45"",""2016-01-01 17:46""], 
    ""Event"": [""Line Outage"",""Line Outage"",""Line Outage""],
     })


I also include a standalone code example in an answer below

Package versions: 

dash                      1.8.0                      py_0    conda-forge
dash-core-components      1.7.0                      py_0    conda-forge
dash-html-components      1.0.2                      py_0    conda-forge
dash-renderer             1.2.3                      py_0    conda-forge
dash-table                4.6.0                      py_0    conda-forge


UPDATE:
Ultimately the problem seemed to be due to size of the data frame. Hidden-div or Store could handle only few hundred rows. So I switched to using Flask Caching/Memoization: see https://dash.plotly.com/sharing-data-between-callbacks or https://dash.plotly.com/performance
",2,10070,"The below (simplified) code works for me. Because you haven't provided event_df its not possible to see what your exact problem is but I suspect the 'id' in event_df is not valid (for example doesn't start from 0) and you are addressing out of range here:

selected_id=selected_id[0]
row=event_df.loc[selected_id,:]


Although it could be any number of other issues. If you still have problems perhaps you can provide a sample event_df DataFrame?

Also included package versions for reference

import dash
import pandas as pd
from dash.dependencies import Input, Output, State
import dash_core_components as dcc
import dash_html_components as html
import dash_table 
from dash.exceptions import PreventUpdate

########### Layout:
app = dash.Dash(__name__)

event_df = pd.DataFrame({""id"": [0,1,2], ""a"": [11,21,31], ""b"": [41,51,61]})
PAGE_SIZE=1

app.layout = html.Div(children=[
    html.Div(id='data-storage-json', style={'display': 'none'}),
    html.Div(children=[
                dash_table.DataTable(
                        id='event-table',
                        style_data={'whiteSpace': 'normal'}, #'border': '1px solid blue'},
                        style_cell={'textAlign': 'center'},
                        #style_header={ 'border': '1px solid pink' },
                        css=[{
                            'selector': '.dash-cell div.dash-cell-value',
                            'rule': 'display: inline; white-space: inherit; overflow: inherit; text-overflow: inherit;'
                        }],
                        columns=[{""name"": i, ""id"": i} for i in event_df.columns if i is not 'id'],
                        style_table={'overflowX': 'scroll'},
                        row_selectable='single',
                        selected_rows=[],
                        page_current=0,
                        page_size=PAGE_SIZE,
                        page_action='custom', 
                        filter_action='custom',
                        filter_query='',
                        sort_action='custom',
                        sort_mode='multi',
                        sort_by=[]                        
                  ),
                  html.Div(id='event-stats', style={'width': '80%', 'color': 'black', 'font-size': '9'})],
                  style={'width': '90%', 'margin-left': '20px', 'font-size': '9', 'horizontal-align': 'middle', 'vertical-align': 'middle'}),
    html.Div(children=[html.Br()]),
    html.Button('Plot', id='show-button'),
    html.Div(id='plot-div', children=[], style={'width': '95%', 'font-size': '9', 'vertical-align': 'middle'}),
])

########### Callbacks:

'''
Callback for sorting/filtering table
'''
@app.callback(
Output('event-table', 'data'),
[Input('event-table', 'sort_by'), 
 Input('event-table', 'filter_query'),
 Input('event-table', 'page_current'),
 Input('event-table', 'page_size')])
def update_event_selection(sort_by, filter_query,page_current, page_size):

    return event_df.to_dict('records')

@app.callback(
Output('data-storage-json','children'),
[Input('show-button', 'n_clicks')],
[State('event-table','selected_row_ids')
])
def prepare_data(n_clicks,selected_id):
    duration=1

    print('Selected id: ',selected_id)

    if n_clicks is None or  selected_id is None or len(selected_id)==0:
        raise PreventUpdate

    duration=int(duration)
    selected_id=selected_id[0]
    row=event_df.loc[selected_id,:]
    print(row)

    res_df = pd.DataFrame({""id"": [0,1,2], ""a"": [11,21,31], ""b"": [41,51,61]})
    js=res_df.to_json(date_format='iso', orient='split')
    print('In Prep: ',len(js))
    return js

@app.callback(
Output('plot-div','children'),
[Input('data-storage-json','children')],
[State('event-table','selected_row_ids')])
def generate_plots(data_storage,selected_id):
    if data_storage is None:
        print('None!!!')
        raise PreventUpdate
    else:
        print('InDisplay -storage: '+str(len(data_storage)))
        res_df = pd.read_json(data_storage, orient='split')

    print('InDisplay ',res_df.shape)
    selected_id=selected_id[0]
    row=event_df.loc[selected_id,:]
    event_time=pd.to_datetime(row['Start'],errors='ignore')
    event_type=row['Event']+': '+row['Cause']
    event_pid=''

    # columns sorted in reverse alphabetical
    flist=sorted(np.unique([c.split('__')[1] for c in res_df.columns]))[::-1]
    print('To plot: ',res_df.shape)
    # generate plots for each type of sensor:
    fig_list=[]
    for feature in flist:
        col_list = [c for c in res_df.columns if not c.startswith('_') and c.endswith('_'+feature)] 
        temp_df = res_df[col_list]
        # plot results
        print('Preparing figure '+feature)
        fig=temp_df.iplot(kind='scatter',mode='markers',size=3, title=""Plot {}: {} {} {}"".format(feature,event_time,event_type,event_pid), asFigure=True)
        #fig_list.append(fig)
        fig_list.append((html.Div(children=[dcc.Graph(id=feature+'-scatter',figure=fig)])))
    print('Figure done')
    return fig_list


########### Run the app:

if __name__ == '__main__':

    app.run_server(debug=True)


Running on http://127.0.0.1:8050/
Debugger PIN: 361-595-854
Selected id:  None
Selected id:  [2]
id     2
a     31
b     61
Name: 2, dtype: int64
In Prep:  81
InDisplay -storage: 81
InDisplay  (3, 3)

# Name                    Version                   Build  Channel
dash                      1.4.0                      py_0    conda-forge
dash-bootstrap-components 0.8.1                    py36_0    conda-forge
dash-core-components      1.3.0                      py_0    conda-forge
dash-html-components      1.0.1                      py_0    conda-forge
dash-renderer             1.1.1                      py_0    conda-forge
dash-table                4.4.0                      py_0    conda-forge

","I pinned it down to the size of the dataframe I was trying to store in hidden-div. (It didn't take much to cause the error). I also tried using dcc.Store and observed the same behavior. So I switched to using Flask Caching/Memoization: see https://dash.plotly.com/sharing-data-between-callbacks or https://dash.plotly.com/performance 
","UPDATE: I include a complete example below. This example uses randomly generated data. It works if in line 38  five minutes of data is generated. If ten minutes is generated, I get the error. 

# -*- coding: utf-8 -*-
import dash
from dash.dependencies import Input, Output, State
import dash_core_components as dcc
import dash_html_components as html
import dash_table 
from dash.exceptions import PreventUpdate
external_stylesheets = ['https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css',
                                      'https://codepen.io/chriddyp/pen/bWLwgP.css',
                                      'https://cdnjs.cloudflare.com/ajax/libs/vis/4.21.0/vis.min.css',
                                      'https://codepen.io/chriddyp/pen/bWLwgP.css']

import numpy as np
import pandas as pd
from functools import reduce
import cufflinks as cf
from datetime import datetime as dt
import os
import sys
import argparse
#import plotly.offline

########### Prepare Data
PAGE_SIZE = 10

event_df = pd.DataFrame({""id"": [0,1,2],
    ""Start"": [""2016-01-01 14:33"",""2016-01-01 16:45"",""2016-01-01 17:46""], 
    ""Event"": [""Line Outage"",""Line Outage"",""Line Outage""],
    ""Cause"": ['','','']
     })

def list2dict(l):
    return [{'label': x, 'value':x} for x in l]


def make_random_data():#(useDates=True):
    #if useDates:
    date_rng = pd.date_range(start='1/01/2018 05:00:00', end='1/01/2018 05:05:00', freq='1S')
    #else:
    #    date_rng = pd.Series([10, 20, 30, 40, 50]) 
    df = pd.DataFrame(date_rng, columns=['date'])
    cols=['A__ip_m','B__ip_m','A__vp_m','B__vp_m']
    for c in cols:
        df[c] = np.random.randint(0,100,size=(len(date_rng)))
    df=df.set_index('date')
    return df

########### Layout:
app = dash.Dash(__name__, external_stylesheets=external_stylesheets)

app.layout = html.Div(children=[
    html.Div(id='data-storage-json', style={'display': 'none'}),
    html.Div(children=[
                dash_table.DataTable(
                        id='event-table',
                        data=event_df.to_dict('records'),
                        style_data={'whiteSpace': 'normal'},
                        style_cell={'textAlign': 'center'},
                        css=[{
                            'selector': '.dash-cell div.dash-cell-value',
                            'rule': 'display: inline; white-space: inherit; overflow: inherit; text-overflow: inherit;'
                        }],
                        columns=[{""name"": i, ""id"": i} for i in event_df.columns if i is not 'id'],
                        style_table={'overflowX': 'scroll'},
                        row_selectable='single',
                        selected_rows=[]
                  )]),
    html.Div(children=[html.Br()]),
    html.Button('Plot', id='show-button'),
    html.Div(id='plot-div', children=[], style={'width': '95%', 'font-size': '9', 'vertical-align': 'middle'}),
])

########### Callbacks:

#Output('data-storage-json','children'),
# Output('plot-div','children'),
@app.callback(
Output('data-storage-json','children'),
[Input('show-button', 'n_clicks')],
[State('event-table','selected_row_ids')])
def prepare_data(n_clicks,selected_id):
    if n_clicks is None or  selected_id is None or len(selected_id)==0:
        raise PreventUpdate
    duration=1
    selected_id=selected_id[0]
    row=event_df.loc[selected_id,:]
    print(row)
    event_time=pd.to_datetime(row['Start'],errors='ignore')

    res_df = make_random_data()#useDates=True)
    print(res_df.shape)
    print(res_df.head())
    js=res_df.to_json(date_format='iso', orient='split') #date_format='epoch'
    #res_df.to_json('epoch-sample.json',date_format='epoch', orient='split')
    #res_df.to_json('iso-sample.json',date_format='iso', orient='split')
    print('In Prep: ',len(js))
    return js

@app.callback(
Output('plot-div','children'),
[Input('data-storage-json','children')])
def generate_plots(data_storage):
    if data_storage is None:
        print('None!!!')
        raise PreventUpdate
    else:
        print('InDisplay -storage: '+str(len(data_storage)))
        res_df = pd.read_json(data_storage, orient='split')

    # columns sorted in reverse alphabetical
    flist=sorted(np.unique([c.split('__')[1] for c in res_df.columns]))[::-1]
    print('To plot: ',res_df.shape)
    # generate plots for each type of sensor:
    fig_list=[]
    for feature in flist:
        col_list = [c for c in res_df.columns if not c.startswith('_') and c.endswith('_'+feature)] 
        temp_df = res_df[col_list]
        # plot results
        print('Preparing figure '+feature)
        fig=temp_df.iplot(kind='scatter',mode='markers',size=3, title=""Plot"", asFigure=True)
        fig_list.append((html.Div(children=[dcc.Graph(id=feature+'-scatter',figure=fig)])))
    print('Figure done')
    return fig_list

########### Run the app:

if __name__ == '__main__':
    app.run_server(debug=True)

"
Plotly strange behavior,https://stackoverflow.com/questions/59379227,R Shiny ggplot2 line chart won&#39;t show lines when using &quot;key&quot; property and facet_grid,"I have an R shiny app that is displaying a line chart with a facet grid. I therefore use ggplot2 and plotly.

Now I want to make it possible to click a position on a line and retrieve the corresponding data row from the data frame.

I learned fetching the click event is possible by catching the ""plotly_click"" click event via event_data(""plotly_click""). This works; I get a curve number, x- and y- coordinates. Now to get a reference to my data row I learned I have to use a ""key"" property for ggplot like stated here: How can I grab the row of data from a ggplotly in shiny.

Now when I add the ""key"" property to my ggplot suddenly the lines won't show up anymore (it seems the chart stays empty). Interestingly, when I remove facet_grid, some lines will show up and clicking it provides the ""key""-information with the event like stated in the link above.

EDIT:

I reproduced the behavior with the mtcars exmaple:

library(shiny)
library(plotly)
library(tidyr)

mtcars$key &lt;- row.names(mtcars)

ui &lt;- fluidPage(
    plotlyOutput(""originalLinePlot""),
    plotlyOutput(""keyLinePlot""),
    verbatimTextOutput(""click""),
)

server &lt;- function(input, output) {

    output$originalLinePlot &lt;- renderPlotly({
        # here I want to add click event with data row selection - click doesn't return key info

        data_long &lt;- gather(mtcars, condition, measurement, c(drat, wt), factor_key=TRUE)

        g &lt;- ggplot(data_long, aes(x=mpg))
        # won't work
        # g &lt;- ggplot(data_long, aes(x=mpg, key=key))

        g &lt;- g + facet_grid(rows = vars(condition), scales=""free_y"")
        g &lt;- g + geom_line(aes(y=measurement))
        g
    })


    output$keyLinePlot &lt;- renderPlotly({
        data_long &lt;- gather(mtcars, condition, measurement, c(drat, wt), factor_key=TRUE)

        g &lt;- ggplot(data_long, aes(x=mpg, key=key))

        # won't work
        # g &lt;- g + facet_grid(rows = vars(condition), scales=""free_y"")
        g &lt;- g + geom_line(aes(y=measurement))
        g
    })

    output$click &lt;- renderPrint({
        d &lt;- event_data(""plotly_click"")
        if (is.null(d)) ""Click events appear here (double-click to clear)"" else data.frame(d) 
    })
}

shinyApp(ui = ui, server = server)


May it be that due to the key-property the line chart gets confused as how to draw its lines? The lines appearing when removing the facet_grid and having the ""key""-propery included look quite strange..

Any ideas how to resolve this? Can I solve my issue also in a different way than using the key-property?

Thanks and BR!
",1,432,"I've found a solution of how to solve this: by combining points and lines and adding the key information to the points instead of the lines - see second plot:

library(shiny)
library(plotly)
library(tidyr)

mtcars$key &lt;- row.names(mtcars)

ui &lt;- fluidPage(
    plotlyOutput(""originalLinePlot""),
    plotlyOutput(""keyLinePlot""),
    verbatimTextOutput(""click""),
)

server &lt;- function(input, output) {

    output$originalLinePlot &lt;- renderPlotly({
        # here I want to add click event with data row selection - click doesn't return key info

        data_long &lt;- gather(mtcars, condition, measurement, c(drat, wt), factor_key=TRUE)

        g &lt;- ggplot(data_long, aes(x=mpg))
        # won't work
        # g &lt;- ggplot(data_long, aes(x=mpg, key=key))

        g &lt;- g + facet_grid(rows = vars(condition), scales=""free_y"")
        g &lt;- g + geom_line(aes(y=measurement))
        g
    })


    output$keyLinePlot &lt;- renderPlotly({
        data_long &lt;- gather(mtcars, condition, measurement, c(drat, wt), factor_key=TRUE)

        g &lt;- ggplot(data_long, aes(x=mpg))

        g &lt;- g + facet_grid(rows = vars(condition), scales=""free_y"")
        g &lt;- g + geom_line(aes(y=measurement))
        g &lt;- g + geom_point(aes(y=measurement, key=key))
        g
    })

    output$click &lt;- renderPrint({
        d &lt;- event_data(""plotly_click"")
        if (is.null(d)) ""Click events appear here (double-click to clear)"" else data.frame(d) 
    })
}

shinyApp(ui = ui, server = server)



",,
Plotly strange behavior,https://stackoverflow.com/questions/66723520,Python: Data limit in Plotly frames?,"I have recently started to use Plotly to make 3D plots in python and I wanted to create an animation of what is going on in terms of column vectos of a 3 by 3 matrix when applying Gaussain elimination.
I wrote a function to get the row echelon form and the history of the matrix obtained at each step.
Then I wanted to plot the comuns vectors at each step of the algorithm.
At first I was able to get an animation of the the evolution of the three vectors by adpating this code : https://plotly.com/python/visualizing-mri-volume-slices/
But then I wanted to show on each frame the three row vectors of a given step and the three row vectors from the matrix of the previous step with opacity 0.2.
And when I added that part of the code I got a strange behavior from Plotly. It only showed me the three first vectors which are given to the frame and not all of them.
Here the code I have so far :
import numpy as np
import numpy.linalg as la
import plotly.graph_objects as go

v1 = np.array([5,2,1])
v2 = np.array([2,3,2])
v3 = np.array([3,-1,1])

A = np.transpose(np.vstack([v1,v2,v3]))
# G, H = pivot_Gauss(A)
H = [np.array([[ 5,  2,  3],[ 2,  3, -1],[ 1,  2,  1]]), np.array([[ 1,  0,  0],[ 2,  3, -1],[ 1,  2,  1]]),
    np.array([[ 1,  0,  0],[ 0,  3, -1],[ 1,  2,  1]]), np.array([[ 1,  0,  0],[ 0,  3, -1],[ 0,  2,  1]]), 
    np.array([[1, 0, 0],[0, 1, 0],[0, 2, 1]]), np.array([[1, 0, 0],[0, 1, 0],[0, 0, 1]]), 
    np.array([[1, 0, 0],[0, 1, 0],[0, 0, 1]]) ]

G = np.array([[1,0,0],[0,1,0],[0,0,1]]) # results obtained using the function pivot_Gauss(A)

nb_frames = len(H)

frames = []
v_norm = 5
colors = [""blue"",""red"",""green""]
for k in range(nb_frames): # go.Frame(data,name=str(k))
   dat = []
   for j in range(np.shape(A)[1]):
       v = H[k][:,j]
       if la.norm(v) != 0 :
           d1 = go.Scatter3d( x=[0,v[0]],y=[0,v[1]],z=[0,v[2]],name=""v""+str(k+j+1),hoverinfo='name',
                                              marker=dict(size=0), line=dict(color=colors[j], width=10 ))
           dat.append(d1)
           d2 = go.Cone(x=[v[0]],y=[v[1]],z=[v[2]],
                            u=[v[0]/v_norm],v=[v[1]/v_norm],w=[v[2]/v_norm],sizeref=1,
                            sizemode=""scaled"",anchor=""cm"",name=""v""+str(k+j+1),hoverinfo='x+y+z+name',
                            colorscale=[[0, colors[j]], [1,colors[j]]],showscale=False)
           dat.append(d2)
       if k&gt;0 : # add column vectors of previous Gaussain elimination step (causes some troubles, 
#if this if section is commented I get an animation of the three clumn vectors of current step)
           vk = H[k-1][:,j]
           if la.norm(v) != 0 :
               d3 =  go.Scatter3d( x=[0,vk[0]],y=[0,vk[1]],z=[0,vk[2]],name=""v""+str(k+j+1),hoverinfo='name',
                                                  marker=dict(size=0), line=dict(color=colors[j], width=10), opacity = 0.2 )
               dat.append(d3)
               d4 = go.Cone(x=[vk[0]],y=[vk[1]],z=[vk[2]],
                                u=[vk[0]/v_norm],v=[vk[1]/v_norm],w=[vk[2]/v_norm],sizeref=1,
                                sizemode=""scaled"",anchor=""cm"",name=""v""+str(k+j+1),hoverinfo='x+y+z+name',
                                colorscale=[[0, colors[j]], [1,colors[j]]],showscale=False,opacity=0.2)
               dat.append(d4)
   frames.append(go.Frame(data=dat,name=str(k)))
   

fig = go.Figure(frames=frames)
# Add data to be displayed before animation starts
for j in range(A.shape[1]):
   v = A[:,j]
   if la.norm(v) != 0 :
       fig.add_trace( go.Scatter3d( x=[0,v[0]],y=[0,v[1]],z=[0,v[2]],name=""v""+str(k+1),hoverinfo='name',
                                          marker=dict(size=0), line=dict(color=colors[j], width=10 ))  )
       fig.add_trace( go.Cone(x=[v[0]],y=[v[1]],z=[v[2]],
                        u=[v[0]/v_norm],v=[v[1]/v_norm],w=[v[2]/v_norm],sizeref=1,
                        sizemode=""scaled"",anchor=""cm"",name=""v""+str(k+1),hoverinfo='x+y+z+name',
                        colorscale=[[0, colors[j]], [1,colors[j]]],showscale=False) )

### This remained almost exactly as the Plotly example  
def frame_args(duration):
   return {
           ""frame"": {""duration"": duration},
           ""mode"": ""immediate"",
           ""fromcurrent"": True,
           ""transition"": {""duration"": duration, ""easing"": ""linear""},
       }

sliders = [
           {
               ""pad"": {""b"": 10, ""t"": 60},
               ""len"": 0.9,
               ""x"": 0.1,
               ""y"": 0,
               ""steps"": [
                   {
                       ""args"": [[f.name], frame_args(0)],
                       ""label"": str(k),
                       ""method"": ""animate"",
                   }
                   for k, f in enumerate(fig.frames)
               ],
           }
       ]

matrix_but = [
            {""buttons: [{},{},{},{},{},{}]""}
               ]

# Layout
fig.update_layout(
        title='Pivot de Gauss',
        width=600,
        height=400,
        scene=dict(xaxis=dict(autorange=True),
                   yaxis=dict(autorange=True),
                   zaxis=dict(autorange=True),
                   aspectratio=dict(x=1, y=1, z=1),
                   ),
        updatemenus = [
           {
               ""buttons"": [
                   {
                       ""args"": [None, frame_args(200)],
                       ""label"": ""&amp;#9654;"", # play symbol
                       ""method"": ""animate"",
                   },
                   {
                       ""args"": [[None], frame_args(0)],
                       ""label"": ""&amp;#9724;"", # pause symbol
                       ""method"": ""animate"",
                   },
               ],
               ""direction"": ""left"",
               ""pad"": {""r"": 10, ""t"": 70},
               ""type"": ""buttons"",
               ""x"": 0.1,
               ""y"": 0,
           }
        ],
        sliders=sliders
)

fig.show()

You will notice that for each vector I first draw a 3D line and then use cone to get the it arrow_shaped. It might not be the best way to do it, but I do not want to use cone alone as the apsect does not fit what I would like.
I stumbled across a (I think) similar question here : https://community.plotly.com/t/only-one-trace-showing-per-frame-in-animated-plot/25803
But I did not undestand the answer nor the example.
It seems from what I get that only the first six elemetns of the data contained in each frame is taken into account, but I do not understand why and I would like to show everything.
If someone has some insight (and a solution) on the subject, it would be warmly welcomed.
I can clarify things if needed.
Image of the two first column vectors of matrix from current step and first column vector of matrix from previous step
Image of the three column vectors of current matrix when part below if k&gt;0 is commented
",0,422,"
It seems from what I get that only the first six elemetns of the data contained in each frame is taken into account, but I do not understand why and I would like to show everything.

There's this paragraph under the heading 'Current Animation Limitations and Caveats':

Animations are designed to work well when each row of input is present across all animation frames, and when categorical values mapped to symbol, color and facet are constant across frames. Animations may be misleading or inconsistent if these constraints are not met.

Though in your first frame you have only three vectors (three lines plus three coneheads) to plot, it violates the above constraint when following frames contain six vectors. To overcome this restriction, we could insert the three vectors in the first frame (and also in the data to be displayed before animation starts) twice, i. e. to the
       if k&gt;0 : # add column vectors of previous Gaussain elimination step (causes some troubles, 

block add an
       else:
            dat.append(d1)
            dat.append(d2)

block, and in the
   if la.norm(v) != 0 :

block duplicate the two fig.add_trace calls.
",,
Plotly strange behavior,https://stackoverflow.com/questions/71399250,Dash+Plotly Synchronize zoom and pan between two plots using imshow,"I try to synchronize zoom and pan between two graphs in a dashboard (dash + plotly). I obtain strange behavior when I zoom on a graph, the second graph does not update. I need to zoom on the second graph to make both graphs update but not with the same zoom nor the same location on the graphs. Furthermore the shapes of the two graphs change.
Below is the code I am in. I do not see I am doing wrong.
import os

from dash import Dash, html, dcc, Input, Output, State
import plotly.express as px
import numpy as np
import rasterio as rio

app2 = Dash(__name__)

data_folder = r'.\data'
store = {}

for filename in os.listdir(data_folder):
    if os.path.isfile(os.path.join(data_folder, filename)):
        band_name = filename.replace('.', '_').split(sep='_')[-2]
        with rio.open(os.path.join(data_folder, filename)) as dataset:
            nb_band = dataset.count
            if nb_band == 1:
                data = dataset.read(1)
            else:
                data = dataset.read(tuple(range(1, nb_band + 1)))

            if band_name == 'triband':
                data = np.swapaxes(data, 2, 0)
                data = np.swapaxes(data, 0, 1)
                store[band_name] = data.astype(float)
            else:
                store[f'B{band_name}'] = data.astype(float)

fig1 = px.imshow(store['triband'])
fig1.update_xaxes(showticklabels=False, showgrid=False, zeroline=False)
fig1.update_yaxes(showticklabels=False, showgrid=False, zeroline=False)
fig1.update_layout(
    margin=dict(l=0, r=0, t=0, b=0),
    plot_bgcolor='rgba(0, 0, 0, 0)',
    paper_bgcolor='rgba(0, 0, 0, 0)',
)


# Application structure and content
app2.layout = html.Div(className='main', children=[
    html.H1(children='Hello Dash', style={'padding': 10}),

    html.Div(children=[

        html.Div(children=[
            dcc.Graph(
                id='graph1',
                figure=fig1,
                responsive=True
            )
        ], style={'padding': 5, 'flex': 1}),

        html.Div(children=[
            dcc.Graph(
                id='graph2',
                figure=fig1,
                responsive=True
            )
        ], style={'padding': 5, 'flex': 1})

    ], style={'display': 'flex', 'flex-direction': 'row'}),
])


@app2.callback(Output('graph2', 'figure'),
               Input('graph1', 'relayoutData'),
               State('graph2', 'figure'))
def graph_event1(select_data, fig):
    if select_data is not None:
        try:
            fig['layout']['xaxis']['range'] = [select_data['xaxis.range[0]'], select_data['xaxis.range[1]']],
            fig['layout']['yaxis']['range'] = [select_data['yaxis.range[0]'], select_data['yaxis.range[1]']]
        except KeyError:
            pass
    return fig


@app2.callback(Output('graph1', 'figure'),
               Input('graph2', 'relayoutData'),
               State('graph1', 'figure'))
def graph_event2(select_data,  fig):
    if select_data is not None:
        try:
            fig['layout']['xaxis']['range'] = [select_data['xaxis.range[0]'], select_data['xaxis.range[1]']],
            fig['layout']['yaxis']['range'] = [select_data['yaxis.range[0]'], select_data['yaxis.range[1]']]
        except KeyError:
            pass
    return fig


if __name__ == '__main__':
    app2.run_server(debug=True)

",0,2000,"I found a solution : rather than creating two graphs, I created a graph with several subplots and force zoom and pan between subplots.
fig = make_subplots(rows=1, cols=3, shared_xaxes=True, shared_yaxes=True)
fig.add_trace(
    px.imshow(store['triband']).data[0],
    row=1, col=1
)

fig.add_trace(
    px.imshow(index_store['NDVI']).data[0],
    row=1, col=2
)

fig.add_trace(
    px.imshow(np.where(index_store['NDVI'] &gt;= np.median(index_store['NDVI']),
                       0.8 * np.max(index_store['NDVI']),
                       0.8 * np.min(index_store['NDVI']))
              ).data[0],
    row=1, col=3
)

fig.update_xaxes(matches='x', showticklabels=False, showgrid=False, zeroline=False)
fig.update_yaxes(matches='y', showticklabels=False, showgrid=False, zeroline=False)

",,
Plotly strange output,https://stackoverflow.com/questions/62646432,Verbose response after install conda package,"I've encountered the following problem when I attempted to install a couple of packages. I'll use sqlite3 as an example.
When I entered conda install -c blaze sqlite3 in the Anaconda prompt (""base"" environment), I received the output below.
I'm uncertain exactly when this problem started. I checked my revisions using conda list -r. Revision #35 - conda  {4.8.3 (conda-forge/win-64) -&gt; 4.8.3 (anaconda/win-64)} - looks odd.
Therefore, I entered conda install revision 34. When I did so, I was informed that conda-forge/win-64 cannot be found, which seems strange. As mentioned earlier, I'm unclear if there's a correlation between revision 34 and this error.
The sqlite3 package listed in the ""base"" environment in the Anaconda Navigator.
How do I prevent conda from automatically running the additional code?
Collecting package metadata (current_repodata.json): done
Solving environment: /
Warning: 2 possible package resolutions (only showing differing packages):
  - anaconda/win-64::conda-4.8.3-py37_0
  - defaults/win-64::conda-4.8.3-py37done

## Package Plan ##

  environment location: C:\Users\morga\Anaconda3

  added / updated specs:
    - sqlite3


The following NEW packages will be INSTALLED:

  sqlite3            blaze/win-64::sqlite3-3.8.6-0


Proceed ([y]/n)? y

Preparing transaction: done
Verifying transaction: done
Executing transaction: done

C:\Users\morga&gt;SET DISTUTILS_USE_SDK=1

C:\Users\morga&gt;SET MSSdk=1

C:\Users\morga&gt;SET platform=

C:\Users\morga&gt;IF /I [AMD64] == [amd64] set ""platform=true""

C:\Users\morga&gt;IF /I [] == [amd64] set ""platform=true""

C:\Users\morga&gt;if defined platform (set ""VSREGKEY=HKEY_LOCAL_MACHINE\SOFTWARE\Wow6432Node\Microsoft\VisualStudio\14.0"" )  ELSE (set ""VSREGKEY=HKEY_LOCAL_MACHINE\SOFTWARE\Microsoft\VisualStudio\14.0"" )

C:\Users\morga&gt;for /F ""skip=2 tokens=2,*"" %A in ('reg query ""HKEY_LOCAL_MACHINE\SOFTWARE\Wow6432Node\Microsoft\VisualStudio\14.0"" /v InstallDir') do SET ""VSINSTALLDIR=%B""
ERROR: The system was unable to find the specified registry key or value.

C:\Users\morga&gt;if """" == """" (set ""VSINSTALLDIR="" )

C:\Users\morga&gt;if """" == """" (
ECHO ""WARNING: Did not find VS in registry or in VS140COMNTOOLS env var - your compiler may not work""
 GOTO End
)
""WARNING: Did not find VS in registry or in VS140COMNTOOLS env var - your compiler may not work""
The system cannot find the batch label specified - End

&lt;&lt;&lt;&lt;EDIT - ADDITIONAL COMMENT&gt;&gt;&gt;
I did more research it seems that the package  vs2015_win-64 is causing the problem. Others had similar experiences.
I executed conda uninstall  vs2015_win-64. The uninstallation was initiated then stopped and I received the error below.
The same thing occurred when I tried to install plotly using conda install plotly.
Error processing line 1 of C:\Users\morga\Anaconda3\lib\site-packages\matplotlib-3.1.0-py3.7-nspkg.pth:

  Traceback (most recent call last):
    File ""C:\Users\morga\Anaconda3\lib\site.py"", line 168, in addpackage
      exec(line)
    File ""&lt;string&gt;"", line 1, in &lt;module&gt;
    File ""&lt;frozen importlib._bootstrap&gt;"", line 580, in module_from_spec
  AttributeError: 'NoneType' object has no attribute 'loader'

",2,1505,"It is very unlikely this has anything to do with the change in the conda package in that revision (where it switched from sourcing at Conda Forge to Anaconda channel). Instead, some packages include additional installation scripts that must be run during installation for the package to function. There is currently not any way to prevent Conda from executing such scripts.
It might be worth reflecting here that this is a good example of why one should not install packages from channels that one does not explicitly trust. That is, searching Anaconda Cloud to find a package on a random user channel could lead to running arbitrary code that the user has included in the package build.
",,
Plotly strange output,https://stackoverflow.com/questions/63253895,Warning: Error in $: $ operator is invalid for atomic vectors in Shiny Web app,"I am trying to create an interactive map using Plotly in the Shiny app that allows the user to select the region by box select and lasso select on the map, then it can return a GoogleVis motion chart showing the statistics on the region selected within the Shiny app. However when it comes to the output function of GoogleVis:
output$motionChart &lt;- renderGvis({
           selected &lt;- event_data(event = ""plotly_selected"", source = ""countyMap"")
           selectedCountyCases &lt;- as.integer(unlist(selected[3]))
           selectedCounties &lt;- subset(totalComfirmed, totalComfirmed$cases %in% selectedCountyCases)
           gvisCasesDataSubset &lt;- subset(gvisCasesData, countyNames %in% c(selectedCounties$countyNames))
           motionChart &lt;- gvisMotionChart(gvisCasesDataSubset, ""countyNames"", ""Date"", options=list(width=800, height=400))
        })

It gives the error of:
Warning: Error in $: $ operator is invalid for atomic vectors
  96: renderText [/Users/b.w.h/Documents/JHU/Summer 1/COVID-19 Modeling Project/County Polygon Visualization with Shiny/USMapWithCountyPolygon/server.R#114]
  95: func
  82: origRenderFunc
  81: output$brush
   1: runApp

This is very strange because when I checked in the console, the data frame that I am trying to subset with ""$"" is not atomic.
is.atomic(totalComfirmed)
[1] FALSE

Why would that happen? Here is my full ui and server function:
library(shiny)
library(shinyWidgets)
library(plotly)
library(leaflet)

ui &lt;- fluidPage(
    
    titlePanel(""Johns Hopkins COVID-19 Modeling Visualization Map""),
    setBackgroundImage(
        src = ""https://brand.jhu.edu/assets/uploads/sites/5/2014/06/university.logo_.small_.horizontal.blue_.jpg""
    ),
    
    sidebarLayout(
        sidebarPanel(
            radioButtons(""countyFill"", ""Choose the County Map Type"", c(""Map by total confirmed"", ""Map by total death""), selected = ""Map by total confirmed""),
            checkboxGroupInput(""statesInput"", ""Choose the State(s)"", 
                               c(""AL"", ""MO"", ""AK"", ""MT"", ""AZ"", ""NE"", 
                                 ""AR"", ""NV"", ""CA"", ""NH"", ""CO"", ""NJ"", 
                                 ""CT"", ""NM"", ""DE"", ""NY"", ""DC"", ""NC"", 
                                 ""FL"", ""ND"", ""GA"", ""OH"", ""HI"", ""OK"", 
                                 ""ID"", ""OR"", ""IL"", ""PA"", ""IN"", ""RI"", 
                                 ""IA"", ""SC"", ""KS"", ""SD"", ""KY"", ""TN"", 
                                 ""LA"", ""TX"", ""ME"", ""UT"", ""MD"", ""VT"", 
                                 ""MA"", ""VA"", ""MI"", ""WA"", ""MN"", ""WV"", 
                                 ""MS"", ""WI"", ""WY""),
                               inline = TRUE),                       
            actionButton(""submit"", ""Submit (may take 30s to load)"")
        ), 
        
        mainPanel(
            tabsetPanel(type = ""tabs"", 
                        tabPanel(""County Level"", plotlyOutput(""countyPolygonMap""), 
                                 htmlOutput(""motionChart""), 
                                 verbatimTextOutput(""brush"")), 
                        tabPanel(""State Level"", leafletOutput(""statePolygonMap"")),
                        tags$div(
                            tags$p(
                                ""JHU.edu Copyright © 2020 by Johns Hopkins University &amp; Medicine. All rights reserved.""
                            ),
                            tags$p(
                                tags$a(href=""https://it.johnshopkins.edu/policies/privacystatement"",
                                       ""JHU Information Technology Privacy Statement for Websites and Mobile Applications"")
                            )
                        )
            )
        )
    )
)

library(shiny)
library(leaflet)
library(magrittr)
library(rgdal)
library(plotly)
library(rjson)
library(dplyr)
library(viridis) 
library(googleVis)
library(lubridate)
library(reshape2)
library(data.table)


server &lt;- function(input, output, session) {
    statepolygonZip &lt;- download.file(""https://www2.census.gov/geo/tiger/GENZ2018/shp/cb_2018_us_state_500k.zip"", 
                                     destfile = ""cb_2018_us_state_500k.zip"");
    unzip(""cb_2018_us_state_500k.zip"");
    statePolygonData &lt;- readOGR(""cb_2018_us_state_500k.shp"", layer = ""cb_2018_us_state_500k"", 
                                GDAL1_integer64_policy = TRUE);
    ## obtaning the state shape file data provided by cencus.gov 
    ## for more categories of region shape file: 
    ## https://www.census.gov/geographies/mapping-files/time-series/geo/carto-boundary-file.html
    
    url &lt;- 'https://raw.githubusercontent.com/plotly/datasets/master/geojson-counties-fips.json'
    countyGeo &lt;- rjson::fromJSON(file=url)
    ## Obtaining the geographical file for all U.S. counties
    
    url2&lt;- ""https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_confirmed_US.csv""
    covidCases &lt;- read.csv(url2, header = TRUE)
    fips &lt;- sprintf(""%05d"",covidCases$FIPS)
    colnames(covidCases)[6] &lt;- ""countyNames""
    totalComfirmed &lt;- covidCases[,ncol(covidCases)]
    names(totalComfirmed) &lt;- c(""countyNames"", ""cases"")
    
    destroyX = function(es) {
        f = es
        for (col in c(1:ncol(f))){ #for each column in dataframe
            if (startsWith(colnames(f)[col], ""X"") == TRUE)  { #if starts with 'X' ..
                colnames(f)[col] &lt;- substr(colnames(f)[col], 2, 100) #get rid of it
            }
        }
        assign(deparse(substitute(es)), f, inherits = TRUE) #assign corrected data to original name
    }
    destroyX(covidCases)
    
    gvisCasesData &lt;- cbind.data.frame(covidCases$countyNames, covidCases[11,ncol(covidCases)])
    gvisCasesData &lt;- melt(data = setDT(covidCases), id.vars = ""countyNames"",measure.vars = c(colnames(covidCases)[c(12:ncol(covidCases))]))
    colnames(gvisCasesData)[2:3] &lt;- c(""Date"", ""numCases"")
    gvisCasesData$Date &lt;- mdy(gvisCasesData$Date)
    
    
    url3 &lt;- ""https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_deaths_US.csv""
    covidDeath &lt;- read.csv(url3, header = TRUE)
    colnames(covidDeath)[6] &lt;- ""countyNames""
    totalDeath &lt;- covidDeath[,ncol(covidDeath)]
    
    v &lt;- reactiveValues(data = totalComfirmed)
    observeEvent(input$countyFill, {
        if (input$countyFill == ""Map by total confirmed"") {
            v$data &lt;-  totalComfirmed;
            v$zmin = 100;
            v$zmax = 12000;
            v$hover &lt;- with(covidCases, paste(countyNames));
        }
        if (input$countyFill == ""Map by total death"") {
            v$data &lt;-  totalDeath;
            v$zmin = 0;
            v$zmax = 1600;
            v$hover &lt;- with(covidDeath, paste(countyNames));
        }
    })
    
    observeEvent(input$submit, {
        req(input$submit)
        
        output$countyPolygonMap &lt;- renderPlotly({
            countyPolygonMap &lt;- plot_ly(source = ""countyMap"") %&gt;% add_trace(
                countyName &lt;- covidCases$countyNames,
                type=""choroplethmapbox"",
                geojson=countyGeo,
                locations=fips,
                z=v$data,
                colorscale=""Viridis"",
                zmin= v$zmin,
                zmax= v$zmax,
                text = ~v$hover,
                marker=list(line=list(width=0),opacity=0.5)
            ) %&gt;% layout(
                mapbox=list(
                    style=""carto-positron"",
                    zoom =2,
                    center=list(lon= -95.71, lat=37.09))
              %&gt;% event_register(event = ""plotly_selected"")
            );
            countyPolygonMap;
            ## generating the interactive plotly map
        })
        
        output$motionChart &lt;- renderGvis({
           selected &lt;- event_data(event = ""plotly_selected"", source = ""countyMap"")
           selectedCountyCases &lt;- as.integer(unlist(selected[3]))
           selectedCounties &lt;- subset(totalComfirmed, totalComfirmed$cases %in% selectedCountyCases)
           gvisCasesDataSubset &lt;- subset(gvisCasesData, countyNames %in% c(selectedCounties$countyNames))
           motionChart &lt;- gvisMotionChart(gvisCasesDataSubset, ""countyNames"", ""Date"", options=list(width=800, height=400))
        })
        
        #output$brush &lt;- renderText({
        #    selected &lt;- event_data(event = ""plotly_selected"", source = ""countyMap"")
        #    selectedCountyCases &lt;- as.integer(unlist(selected[3]))
        #    brush &lt;- selectedCounties
        #})

        
        output$statePolygonMap &lt;-renderLeaflet ({
            statesAbbr &lt;- subset(statePolygonData, input$statesInput %in% statePolygonData$STUSPS);
            ## subsetting the shape file with the selected states
            
            leaflet(statesAbbr) %&gt;%
                addPolygons(color = ""#444444"", weight = 1, smoothFactor = 0.5,
                            opacity = 1.0, fillOpacity = 0.5,
                            fillColor = ~colorQuantile(""YlOrRd"", ALAND)(ALAND),
                            highlightOptions = highlightOptions
                            (color = ""white"", weight = 2,bringToFront = TRUE))
        })
        ## producing the map with polygon boundary on the state level
    })
    
}

shinyApp(ui, server)

Thanks for your help!
",1,1675,"When I run
url2&lt;- ""https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_confirmed_US.csv""
covidCases &lt;- read.csv(url2, header = TRUE)
fips &lt;- sprintf(""%05d"",covidCases$FIPS)
colnames(covidCases)[6] &lt;- ""countyNames""
totalComfirmed &lt;- covidCases[,ncol(covidCases)]
names(totalComfirmed) &lt;- c(""countyNames"", ""cases"")

and then try is.atomic, i get
is.atomic(totalComfirmed)
# [1] TRUE

You've extracted a single column out of a data.frame which by default returns just an atomic vector. Then when you try to do
totalComfirmed$cases %in% selectedCountyCases

You'll get an error because totalComfirmed doesn't have any columns. Perhaps you meant
totalComfirmed &lt;- covidCases[,c(which(names(covidCases)==""countyNames""), ncol(covidCases))]

Also is totalComfirmed a typo? Should it be totalConfirmed?
",,
Plotly strange output,https://stackoverflow.com/questions/68628153,Y Ticks in Wrong Order,"I have a dataset that I'm plotting using plotly with python and for some reason the y-axis ticks are in the wrong order. When the value on the y-axis decreases the line on the chart is going higher.
Here is a small example of how the dataset is structured:
dfmeds = 

Start       Name                Medication  End           Dose
2020-12-09  Yosemite Sam        Lexapro     2021-06-30    5
2020-12-10  Yosemite Sam        Lexapro     2021-06-30    5
2020-12-11  Yosemite Sam        Lexapro     2021-06-30    5
2020-12-12  Yosemite Sam        Lexapro     2021-06-30    5
2020-12-13  Yosemite Sam        Lexapro     2021-06-30    5
2020-12-14  Yosemite Sam        Lexapro     2021-06-30    5
2020-12-15  Yosemite Sam        Lexapro     2021-06-30    4
2020-12-16  Yosemite Sam        Lexapro     2021-06-30    4
2020-12-17  Yosemite Sam        Lexapro     2021-06-30    4
2020-12-18  Yosemite Sam        Lexapro     2021-06-30    4
2020-12-19  Yosemite Sam        Lexapro     2021-06-30    4
2020-12-20  Yosemite Sam        Lexapro     2021-06-30    3
2020-12-21  Yosemite Sam        Lexapro     2021-06-30    3
2020-12-22  Yosemite Sam        Lexapro     2021-06-30    3
2020-12-23  Yosemite Sam        Lexapro     2021-06-30    3
2020-12-24  Yosemite Sam        Lexapro     2021-06-30    3
2020-12-25  Yosemite Sam        Lexapro     2021-06-30    2
2020-12-26  Yosemite Sam        Lexapro     2021-06-30    2
2020-12-27  Yosemite Sam        Lexapro     2021-06-30    2
2020-12-28  Yosemite Sam        Lexapro     2021-06-30    2

and the code I'm using to create the graph...
    fig2 = px.line(dfmeds, x='Start', y=""Dose"", color = ""Medication"",
        # labels={""Episode_Count"": tally + "" per Shift"",
        #         ""Target"":""Target"",
        #         ""Yr_Mnth"": ""Date"" },
        title=""Medication Dosages"")
    fig2.update_xaxes(tickangle=45,)
    fig2.update_yaxes(tickmode='linear')
    fig2.update_layout(template = 'plotly_white',hovermode=""x unified"")

and frustratingly this is my output:

Note the green trace in particular. Have any of you kind souls out there in the annals of the web came across this strange phenomenon?! I've looked at the docs for y-ticks and can't find any method of controlling the order...
############################# EDIT ##################################
As two people have pointed out in the comments the reason the for the lack of numerical order was that the ""Dose"" column was being passed in as an object and was therefore being handled as a categorical.
So I changed the dtype to numerical to address:
    dfmeds[""Dose""] = pd.to_numeric(dfmeds[""Dose""])

Though that introduced a new problem, due to the substantial range of dosage values the y-axis ticks is all bunched up:

I feel I should be able to fix that by formatting the y-ticks though the best case scenario would be to keep the categorical input and control the order as means the value of each trace can clearly be seen on the y-axis.
If anyone has any suggestions they would be greatly appreciated.
",0,423,"As pointed out in the comments part of the issue was that the y-axis values were being passed in as an object and therefore were treated categorically.
Easily addressed by converting dfmeds['Dose'] to numeric, in my case I also had to extract text from some cells also which is handled in the first line:
#strip non numeric characters
dfmeds['Dose'] = dfmeds['Dose'].str.extract('(\d+)', expand=False)
#convert to numeric
dfmeds['Dose'] = pd.to_numeric(dfmeds['Dose'])

The issue referred to in the ""edit"" was that I had ""tickmode"" set to linear. Which was also easily addressed by removing:
fig2.update_yaxes(tickmode='linear')

I hope this helps some other poor soul in need out there, happy programming!
",,
Plotly strange output,https://stackoverflow.com/questions/67749574,"Python: Plotly &amp; Dash | Real time graph dcc.Interval, function called twice... bug?","First post here, I hope you can help me with this weird situation… I’m working on some realtime plotting with plotly (dcc.Interval) and I just find one of the functions I’m using for getting my data is being called twice during the execution. This is very strange as I’m just calling this function once and it’s really messing with my code:
import dash
from dash.dependencies import Output, Input
import dash_core_components as dcc
import dash_html_components as html
import plotly
import plotly.graph_objs as go

import random

X = []; X.append(0)
Y = []; Y.append(1)

# THIS IS AN EXAMPLE OF A FUNCTION
def formula():
    er = 'text'
    return(er)

# CALLING THE FORMULA, JUST PRINTING THE TEXT
print(formula())

app = dash.Dash(__name__)

app.layout = html.Div(children=[
    html.H1(children='Ejemplo grafica'),
    dcc.Graph(id='live-graph', animate=True),
    dcc.Interval(
        id='graph-update',
        interval=1000
        ),
    ]
)

@app.callback(Output('live-graph', 'figure'),
              [Input('graph-update', 'n_intervals')])

def update_graph(input_data):
    X.append(X[-1]+1)
    Y.append(Y[-1]+Y[-1]*random.uniform(-0.1,0.1))

    data = plotly.graph_objs.Scatter(
            x=list(X),
            y=list(Y),
            name='Scatter',
            mode= 'lines'
            )

    return {'data' : [data],
            'layout' : go.Layout(
                xaxis=dict(range=[X[0],X[-1]]),
                yaxis=dict(range=[min(Y)-0.1,max(Y)+0.1]),)
            }

if __name__ == '__main__':
    app.run_server(host='127.0.0.1', port=8080 ,debug=True)

The output of the program is:
C:\Users\s3853339\Miniconda3\envs\spec\python.exe C:/pythonprojects/spectrometer/test_plotly.py
text # FUNCTION HAS BEEN CALLED ONCE
Dash is running on http://127.0.0.1:8080/

 * Serving Flask app 'test_plotly' (lazy loading)
 * Environment: production
   WARNING: This is a development server. Do not use it in a production deployment.
   Use a production WSGI server instead.
 * Debug mode: on
text # SECOND CALLING OF THE FUNCTION... WHAT HAPPENED HERE???

",0,485,"The code is run twice because you are in debug mode. Set debug=False, and it should execute only once.
",,
Plotly strange output,https://stackoverflow.com/questions/62684641,Plotly/Dash title of y-axes are not shown,"I want to create a dashboard with Plotly/Dash. If I just plot my figure with
figDashboard.show()everything is fine. But creating the Dashboard with
external_stylesheets = ['https://codepen.io/chriddyp/pen/bWLwgP.css']

app = dash.Dash(__name__, external_stylesheets=external_stylesheets)

app.layout = html.Div(children=[
    html.H1(
        children='Cake Lapis Dashboard',
        style={
            'textAlign': 'center'
        }
    ),
    html.Div(children='Graphical representation of pool transactions (Lapis entries)', style={
        'textAlign': 'center'
    }),
    dcc.Graph(
        id='Graph1',
        figure=figDashboard
    )
])

if __name__ == '__main__':
    app.run_server(debug=False) 

the title of the axes in the subplots are not shown and I don't know why. Here is the code of creating the figure and subplots
figDashboard = make_subplots(
    rows=2, cols=2,
    vertical_spacing=0.0,
    horizontal_spacing = 0.25,
    row_width=[0.5, 0.5], # from bottom to top
    specs=[[{""type"": ""table"",""rowspan"":2},{}],
          [None,{}]],
    shared_xaxes = True)

# Subplot for transaction entries (fees and gross return)
trace_GrossReturn = dict(type='bar',name='Gross return',x=aData.index, y=aData['GrossReturn'])
trace_EntryFee = dict(type='bar',name='Entry fee',x=aData.index, y=aData['EntryFee'])
trace_AddFee = dict(type='bar',name='Additional service fee',x=aData.index, y=aData['AdditionalFee'])
figDashboard.add_trace(trace_GrossReturn,1,2)
figDashboard.add_trace(trace_EntryFee,1,2)
figDashboard.add_trace(trace_AddFee,1,2)

figDashboard.update_yaxes(title_text=""Transactions in BTC"",ticks='',tickformat="".8f"", range=[0, 0.0003], row=1, col=2)

figDashboard.update_layout(
    barmode='stack',
    height=800,
    showlegend=False,
    title_text=""Cashflow"",
    yaxis=dict(title='Crude and Model'),
)

I hope someone of you can help me. I have no idea, what is wrong.
Today I have also create a small example, which is completely runnable (I have delete every unneeded code and put some example data inside)
import pandas as pd
import plotly.io as pio
from plotly.subplots import make_subplots

import dash
import dash_core_components as dcc
import dash_html_components as html

pio.renderers.default = ""browser""



Data = pd.DataFrame(index=[1, 2, 3, 4, 5])
Data['y1'] = [2.0, 3.0, 4.0, 3.0, 2.0]
Data['y1'] = Data['y1']*0.0001
Data['y2'] = Data['y1']*3
Data['y3'] = Data['y1']*0.8


figDashboard = make_subplots(
    rows=2, cols=2,
    vertical_spacing=0.0,
    horizontal_spacing = 0.25,
    row_width=[0.5, 0.5], # from bottom to top
    specs=[[{""type"": ""table"",""rowspan"":2},{}],
          [None,{}]],
    shared_xaxes = True)

# Subplot for transaction entries (fees and gross return)
trace_GrossReturn = dict(type='bar',name='Data1',x=Data.index, y=Data['y1'])
trace_EntryFee = dict(type='bar',name='Data2',x=Data.index, y=Data['y2'])
trace_AddFee = dict(type='bar',name='Data3',x=Data.index, y=Data['y3'])
figDashboard.add_trace(trace_GrossReturn,1,2)
figDashboard.add_trace(trace_EntryFee,1,2)
figDashboard.add_trace(trace_AddFee,1,2)


# Subplot for net return in coin and fiat
trace_netReturn = dict(type='scatter',name='Data1',x=Data.index, y=Data['y1'],
                       mode='lines',fill='tozeroy',line=dict(color='blue'),fillcolor='rgba(0, 0, 255, 0.2)')
figDashboard.add_trace(trace_netReturn,2,2)

figDashboard.update_yaxes(title_text=""Title of first y-axis"",ticks='',tickformat="".8f"",row=1, col=2)
figDashboard.update_yaxes(title_text='Title second y-axis',showgrid=False, tickformat="".8f"", row=2, col=2)

figDashboard.update_xaxes(title_text=""Title x-axis "", row=2, col=2)

figDashboard.update_layout(
    barmode='stack',
    height=800,
    showlegend=False,
    margin=dict(t=50),
)


#figDashboard.show()

app = dash.Dash(__name__)

app.layout = html.Div(children=[
    html.H1(
        children='Dashboard',
        style={
            'textAlign': 'center'
        }
    ),
    html.Div(children='Testframe for y-axes labeling', style={
        'textAlign': 'center'
    }),
    dcc.Graph(
        id='Graph1',
        figure=figDashboard
    )
])

if __name__ == '__main__':
    app.run_server(debug=False)

I now tried the code on a separate machine and there it works, really strange. There is a different console output on the working machine, which is
127.0.0.1 - - [02/Jul/2020 08:15:55] ""GET / HTTP/1.1"" 200 -
127.0.0.1 - - [02/Jul/2020 08:15:55] ""GET /_dash-component-suites/dash_renderer/react@16.v1_5_1m1593669821.13.0.min.js HTTP/1.1"" 200 -
127.0.0.1 - - [02/Jul/2020 08:15:55] ""GET /_dash-component-suites/dash_renderer/prop-types@15.v1_5_1m1593669821.7.2.min.js HTTP/1.1"" 200 -
127.0.0.1 - - [02/Jul/2020 08:15:55] ""GET /_dash-component-suites/dash_renderer/polyfill@7.v1_5_1m1593669821.8.7.min.js HTTP/1.1"" 200 -
127.0.0.1 - - [02/Jul/2020 08:15:55] ""GET /_dash-component-suites/dash_html_components/dash_html_components.v1_0_3m1593669825.min.js HTTP/1.1"" 200 -
127.0.0.1 - - [02/Jul/2020 08:15:55] ""GET /_dash-component-suites/dash_renderer/react-dom@16.v1_5_1m1593669821.13.0.min.js HTTP/1.1"" 200 -
127.0.0.1 - - [02/Jul/2020 08:15:55] ""GET /_dash-component-suites/dash_core_components/dash_core_components-shared.v1_10_1m1593669823.js HTTP/1.1"" 200 -
127.0.0.1 - - [02/Jul/2020 08:15:55] ""GET /_dash-component-suites/dash_core_components/dash_core_components.v1_10_1m1593669823.min.js HTTP/1.1"" 200 -
127.0.0.1 - - [02/Jul/2020 08:15:55] ""GET /_dash-component-suites/dash_renderer/dash_renderer.v1_5_1m1593669821.min.js HTTP/1.1"" 200 -
127.0.0.1 - - [02/Jul/2020 08:15:55] ""GET /_dash-layout HTTP/1.1"" 200 -
127.0.0.1 - - [02/Jul/2020 08:15:55] ""GET /_dash-dependencies HTTP/1.1"" 200 -
127.0.0.1 - - [02/Jul/2020 08:15:55] ""GET /_favicon.ico?v=1.13.4 HTTP/1.1"" 200 -

On the not working machine I only got this output
127.0.0.1 - - [02/Jul/2020 08:19:29] ""GET / HTTP/1.1"" 200 -
127.0.0.1 - - [02/Jul/2020 08:19:30] ""GET /_dash-layout HTTP/1.1"" 200 -
127.0.0.1 - - [02/Jul/2020 08:19:30] ""GET /_dash-dependencies HTTP/1.1"" 200 -

So, for me it looks like something is missing, but all packages are installed and I have also restarted the kernel.
",0,517,"After a long search journey, I found the problem.
On the not working machine, there was a really old dash version (0.21.1) installed. For package installation I used pip install dash. No idea, why the old version was installed.
With pip install dash==1.13.4 (version was installed on the working machine) I got the newer version and now it works. Maybe this answer help another person ;-)
",,
Plotly strange output,https://stackoverflow.com/questions/45947951,"plotly, not showing coordinates with np.array dataset","I wish to display a dataset of 1000 float, I have decided to do this with plotly, and I want to do it offline, I am getting in to a problem I really can't understand - I simply don't know what I am doing wrong at all.

Let's jump in to the code. First of I will show that the code should work, with a small np.array

import numpy as np
import plotly as py
import plotly.graph_objs as go

list = [1.2,2.3,3.3,4.4,5.4,6.4] 
x_data = np.array(list)
y_data = np.array(list)

#x_data = np.array(graph_test_q())
#y_data = np.array(graph_test_h())

trace = go.Scatter(
   x = x_data,
   y = y_data,
)

data = [trace]
fig = dict(data=data)
py.offline.plot(fig, filename='hejsa.html')
print data


The output of the above code:

Seems to work fine, but:
Below is the code, where I use a np.array created from a function, that extracts it from a postgrSQL db. I have checked, and it does print the data in the terminal.

def graph_test_q():
    conn = psycopg2.connect(""dbname='database1' user='postgres'        password='FFgg1905560' host='localhost' port='5432'"")
    cur  = conn.cursor()
    cur.execute(""SELECT q FROM pump_data_test WHERE pump_id = 1229"")
    rows = cur.fetchall()
    conn.commit()
    conn.close()
    return rows

def graph_test_h():
    conn = psycopg2.connect(""dbname='database1' user='postgres'    password='FFgg1905560' host='localhost' port='5432'"")
    cur  = conn.cursor()
    cur.execute(""SELECT h FROM pump_data_test WHERE pump_id = 1229"")
    rows = cur.fetchall()
    conn.commit()
    conn.close()
    return rows

#list = [1.2,2.3,3.3,4.4,5.4,6.4]
#x_data = np.array(list)
#y_data = np.array(list)

x_data = np.array(graph_test_q())
y_data = np.array(graph_test_h())

trace = go.Scatter(
    x = x_data,
    y = y_data,
)

data = [trace]
fig = dict(data=data)
py.offline.plot(fig, filename='hejsa.html')
print data


Now here is what I find strange, the output of these new np.arrays is this empty graph:


When I click on the link in the bottom right corner - ""export to plot.ly"" this is now the output I get:

Here I can see see the graph on the left, just as it's supposed to be. I would be very appreciated if anyone can help me find out what I'm doing wrong.

EDIT:
From comments: Code for showing dtypes of x_data &amp; y_data ( x_data = np.array(graph_test_q()) &amp; y_data = np.array(graph_test_h())):

print(x_data.dtype)
print(y_data.dtype)


output:



float64
float64
0:96: execution error: ""file://hejsa.html"" doesn’t understand the “open location” message. (-1708)
70:78: execution error: Can’t get application ""firefox"". (-1728)
 * Running on http://127.0.0.1:5000/ (Press CTRL+C to quit)
",0,1947,"fetchall returns tuples, so you end up with a Numpy array which has a shape of (n, 1) where n is the number of results. You could get the data in the correct format for Plotly using the following index:

np.array(graph_test_q())[:,0]


See below for a complete demo.

import plotly
import numpy as np
import random
import sqlite3

def graph_test_q():
    cur  = conn.cursor()
    cur.execute(""SELECT q FROM pump_data_test WHERE pump_id = 1229"")
    rows = cur.fetchall()
    return rows

def graph_test_h():
    cur  = conn.cursor()
    cur.execute(""SELECT h FROM pump_data_test WHERE pump_id = 1229"")
    rows = cur.fetchall()
    return rows

plotly.offline.init_notebook_mode()

# Create some mockup data
conn = sqlite3.connect(':memory:')
c = conn.cursor()
c.execute(""CREATE TABLE pump_data_test (h, q, pump_id)"")
for i in range(10):
    c.execute(""INSERT INTO pump_data_test VALUES ('{}','{}',1229)"".format(i, i + random.random()))
conn.commit()

trace1 = plotly.graph_objs.Scatter(
    name='works',
    x=np.array(graph_test_q())[:,0],
    y=np.array(graph_test_h())[:,0],
)
trace2 = plotly.graph_objs.Scatter(
    name='does not work',
    x=np.array(graph_test_q()),
    y=np.array(graph_test_h()),
)

data = [trace1, trace2]
fig = dict(data=data)
plotly.offline.iplot(fig)



",,
Plotly strange output,https://stackoverflow.com/questions/45707351,Shiny - Plotly Time Series and dateRangeInput,"I've got a Shiny dashboard with an Plotly time series, the range of which is adjusted via a reactive dateRangeInput I put together (please see code below). 

Everything was working fine, but then I updated my packages. Since then, the variables do not automatically show in the first instance, you have to choose the date ranges instead of the plot loading in with a default time range.

What's more the dateRangeInput is using strange language such as monate, tueate, wedate etc.

I need some help to establish what my issue may be.

Session info

Prior to the update I was using Plotly 3.6.0, shinyDashboard 0.5.1 and shiny 0.13.2.

After the update I am using Plotly 4.5.6, shinyDashboard 0.5.3 and shiny 0.14.2

Please see the respective code below

ui - plotlyOuput Time Series code

box(width = 8, 
    solidHeader = TRUE, 
    plotlyOutput(""Time_Ser"", height =""300px""))


ui - dateRangeInput code

dateRangeInput(""date"",""Date:"", 
              label = h4(""Time Series: select dates""),
              start = ""2017-05-02"",
              end = ""2017-07-30"", 
              min = ""2017-05-02"",
              max = ""2017-06-30"", 
              startview = ""2017-06-30"")


server - Reactive input code

        Time2 &lt;- Time
                 reactiveTime &lt;- reactive({
                 Time2 %&gt;% filter(Date.received&gt;=input$date[1] &amp; 
                 Date.received&lt;input$date[2])
                 })


server - output

       output$Time_Ser &lt;- renderPlotly({
                          Time_Ser &lt;- plot_ly(reactiveTime(), 
                          x = ~Date.received, 
                          y = ~n, 
                          type = ""scatter"", 
                          mode = ""lines"") %&gt;%
                          layout(title = ""Enquiries Time Series"")
                          })


Supporting images




",0,944,"Try using something like the following for your dateRangeInput.  I can't explain why your code worked before, but note that startview should be a categorical string, you can specify date format shown (to overwrite the fact that it seems to be defaulting to DD), and you can force language (but shouldn't actually need to). 

dateRangeInput(""date"", ""Date:"", 
          label = h4(""Time Series: select dates""),
          start = ""2016-05-02"",
          end = ""2016-12-31"",
          min = ""2016-01-01"",
          max = ""2016-12-31"",
          startview = ""year"",
          weekstart=0,
          language=""en"",
          format=""yyyy-mm-dd"")



Note that the dates in the above apply to some dummy data I created for testing purposes.

",,
Plotly strange result,https://stackoverflow.com/questions/72147771,Interactive Manhattan plot with string chromosome names,"I'm trying to generate ManhattanPlot using Dash-plotly library for python: https://dash.plotly.com/dash-bio/manhattanplot
I have SNP results data for plants like wheat which have chromosome names with letters e.g. 3A, 3B, 3D.
Is is possible to handle such identifiers (with letter suffixes) in plotly/python?
In the documentation there is the following remark about chromosome id:

chrm (string; default 'CHR'): A string denoting the column name for
the chromosome. This column must be float or integer. Minimum number
of chromosomes required is 1. If you have X, Y, or MT chromosomes, be
sure to renumber these 23, 24, 25, etc.

I have data for many plant genomes which contain letters in chromosome names.
It seems that CHR column can't have string values which is strange from the user perspective.
",2,683,"This looks like an unfortunate case where the developer of the dashbio.ManhattanPlot object made an assumption about the chrm parameter.
Looking through _manhattan.py in the dash-bio repository, you can start by commenting out these lines so that the CHRM column isn't required to be numerical.
EDIT: the other changes were updating self.ticksLabels to be strings instead of integers, and increasing the threshold from 10 to 20 ticks when deciding when to display every other ticklabel instead of all ticklabels. I've included my local version of _manhattan.py file below:
from __future__ import absolute_import

import numpy as np
import pandas as pd
from pandas.api.types import is_numeric_dtype

import plotly.graph_objects as go

from .utils import _get_hover_text

SUGGESTIVE_LINE_LABEL = ""suggestive line""
GENOMEWIDE_LINE_LABEL = ""genomewide line""


def ManhattanPlot(
        dataframe,
        chrm=""CHR"",
        bp=""BP"",
        p=""P"",
        snp=""SNP"",
        gene=""GENE"",
        annotation=None,
        logp=True,
        title=""Manhattan Plot"",
        showgrid=True,
        xlabel=None,
        ylabel='-log10(p)',
        point_size=5,
        showlegend=True,
        col=None,
        suggestiveline_value=-np.log10(1e-8),
        suggestiveline_color='#636efa',
        suggestiveline_width=1,
        genomewideline_value=-np.log10(5e-8),
        genomewideline_color='#EF553B',
        genomewideline_width=1,
        highlight=True,
        highlight_color=""red"",
):
    """"""Returns a figure for a manhattan plot.

Keyword arguments:
- dataframe (dataframe; required): A pandas dataframe which must contain at
    least the following three columns:
            - the chromosome number
            - genomic base-pair position
            - a numeric quantity to plot such as a p-value or zscore
- chrm (string; default 'CHR'): A string denoting the column name for
    the chromosome. This column must be float or integer. Minimum
    number of chromosomes required is 1. If you have X, Y, or MT
    chromosomes, be sure to renumber these 23, 24, 25, etc.
- bp (string; default 'BP'): A string denoting the column name for the
    chromosomal position.
- p (string; default 'P'): A string denoting the column name for the
    float quantity to be plotted on the y-axis. This column must be
    numeric. It does not have to be a p-value. It can be any numeric
    quantity such as peak heights, Bayes factors, test statistics. If
    it is not a p-value, make sure to set logp = False.
- snp (string; default 'SNP'): A string denoting the column name for
    the SNP names (e.g., rs number). More generally, this column could
    be anything that identifies each point being plotted. For example,
    in an Epigenomewide association study (EWAS), this could be the
    probe name or cg number. This column should be a character. This
    argument is optional, however it is necessary to specify it if you
    want to highlight points on the plot, using the highlight argument
    in the figure method.
- gene (string; default 'GENE'): A string denoting the column name for
    the GENE names. This column could be a string or a float. More
    generally, it could be any annotation information that you want
    to include in the plot.
- annotation (string; optional): A string denoting the column to use
    as annotations. This column could be a string or a float. It
    could be any annotation information that you want to include in
    the plot (e.g., zscore, effect size, minor allele frequency).
- logp (bool; optional): If True, the -log10 of the p-value is
    plotted. It isn't very useful to plot raw p-values; however,
    plotting the raw value could be useful for other genome-wide plots
    (e.g., peak heights, Bayes factors, test statistics, other
    ""scores"", etc.)
- title (string; default 'Manhattan Plot'): The title of the graph.
- showgrid (bool; default true): Boolean indicating whether gridlines
    should be shown.
- xlabel (string; optional): Label of the x axis.
- ylabel (string; default '-log10(p)'): Label of the y axis.
- point_size (number; default 5): Size of the points of the Scatter
    plot.
- showlegend (bool; default true): Boolean indicating whether legends
    should be shown.
- col (string; optional): A string representing the color of the
    points of the scatter plot. Can be in any color format accepted by
    plotly.graph_objects.
- suggestiveline_value (bool | float; default 8): A value which must
    be either False to deactivate the option, or a numerical value
    corresponding to the p-value at which the line should be drawn.
    The line has no influence on the data points.
- suggestiveline_color (string; default 'grey'): Color of the suggestive
  line.
- suggestiveline_width (number; default 2): Width of the suggestive
    line.
- genomewideline_value (bool | float; default -log10(5e-8)): A boolean
    which must be either False to deactivate the option, or a numerical value
    corresponding to the p-value above which the data points are
    considered significant.
- genomewideline_color (string; default 'red'): Color of the genome-wide
    line. Can be in any color format accepted by plotly.graph_objects.
- genomewideline_width (number; default 1): Width of the genome-wide
  line.
- highlight (bool; default True): turning on/off the highlighting of
    data points considered significant.
- highlight_color (string; default 'red'): Color of the data points
    highlighted because they are significant. Can be in any color
    format accepted by plotly.graph_objects.

    # ...
    Example 1: Random Manhattan Plot
    '''
    dataframe = pd.DataFrame(
        np.random.randint(0,100,size=(100, 3)),
        columns=['P', 'CHR', 'BP'])
    fig = create_manhattan(dataframe, title='XYZ Manhattan plot')

    plotly.offline.plot(fig, image='png')
    '''

    """"""

    mh = _ManhattanPlot(
        dataframe,
        chrm=chrm,
        bp=bp,
        p=p,
        snp=snp,
        gene=gene,
        annotation=annotation,
        logp=logp
    )

    return mh.figure(
        title=title,
        showgrid=showgrid,
        xlabel=xlabel,
        ylabel=ylabel,
        point_size=point_size,
        showlegend=showlegend,
        col=col,
        suggestiveline_value=suggestiveline_value,
        suggestiveline_color=suggestiveline_color,
        suggestiveline_width=suggestiveline_width,
        genomewideline_value=genomewideline_value,
        genomewideline_color=genomewideline_color,
        genomewideline_width=genomewideline_width,
        highlight=highlight,
        highlight_color=highlight_color
    )


class _ManhattanPlot():

    def __init__(
            self,
            x,
            chrm=""CHR"",
            bp=""BP"",
            p=""P"",
            snp=""SNP"",
            gene=""GENE"",
            annotation=None,
            logp=True
    ):
        """"""
        Keyword arguments:
        - dataframe (dataframe; required): A pandas dataframe which
        must contain at least the following three columns:
            - the chromosome number
            - genomic base-pair position
            - a numeric quantity to plot such as a p-value or zscore
        - chrm (string; default 'CHR'): A string denoting the column name for the
        chromosome.  This column must be float or integer.  Minimum number
        of chromosomes required is 1. If you have X, Y, or MT chromosomes,
        be sure to renumber these 23, 24, 25, etc.
        - bp (string; default 'BP'): A string denoting the column name for the
        chromosomal position.
        - p (string; default 'P'): A string denoting the column name for the
        float quantity to be plotted on the y-axis. This column must be
        numeric. This does not have to be a p-value. It can be any
        numeric quantity such as peak heights, bayes factors, test
        statistics. If it is not a p-value, make sure to set logp = FALSE.
        - snp (string; default 'SNP'): A string denoting the column name for the
        SNP names (e.g. rs number). More generally, this column could be
        anything that identifies each point being plotted. For example, in
        an Epigenomewide association study (EWAS) this could be the probe
        name or cg number. This column should be a character. This
        argument is optional, however it is necessary to specify if you
        want to highlight points on the plot using the highlight argument
        in the figure method.
        - gene (string; default 'GENE'): A string denoting the column name for the
        GENE names. This column could be a string or a float. More
        generally, it could be any annotation information that you want
        to include in the plot.
        - annotation (string; optional): A string denoting the column name for
        an annotation. This column could be a string or a float.  This
        could be any annotation information that you want to include in
        the plot (e.g. zscore, effect size, minor allele frequency).
        - logp (bool; default True): If True, the -log10 of the p-value is
        plotted.  It isn't very useful to plot raw p-values; however,
        plotting the raw value could be useful for other genome-wide plots
        (e.g., peak heights, Bayes factors, test statistics, other
        ""scores"", etc.).

        Returns:
        - A ManhattanPlot object.""""""

        # checking the validity of the arguments

        # Make sure you have chrm, bp and p columns and that they are of
        # numeric type
        if chrm not in x.columns.values:
            raise KeyError(""Column %s not found in 'x' data.frame"" % chrm)
        # else:
        #     if not is_numeric_dtype(x[chrm].dtype):
        #         raise TypeError(""%s column should be numeric. Do you have ""
        #                         ""'X', 'Y', 'MT', etc? If so change to ""
        #                         ""numbers and try again."" % chrm)

        if bp not in x.columns.values:
            raise KeyError(""Column %s not found in 'x' data.frame"" % bp)
        else:
            if not is_numeric_dtype(x[bp].dtype):
                raise TypeError(""%s column should be numeric type"" % bp)

        if p not in x.columns.values:
            raise KeyError(""Column %s not found in 'x' data.frame"" % p)
        else:
            if not is_numeric_dtype(x[p].dtype):
                raise TypeError(""%s column should be numeric type"" % p)

        # Create a new DataFrame with columns named after chrm, bp, and p.
        self.data = pd.DataFrame(data=x[[chrm, bp, p]])

        if snp is not None:
            if snp not in x.columns.values:
                # Warn if you don't have a snp column
                raise KeyError(
                    ""snp argument specified as %s but column not found in ""
                    ""'x' data.frame"" % snp)
            else:
                # If the input DataFrame has a snp column, add it to the new
                # DataFrame
                self.data[snp] = x[snp]

        if gene is not None:
            if gene not in x.columns.values:
                # Warn if you don't have a gene column
                raise KeyError(
                    ""gene argument specified as %s but column not found in ""
                    ""'x' data.frame"" % gene)
            else:
                # If the input DataFrame has a gene column, add it to the new
                # DataFrame
                self.data[gene] = x[gene]

        if annotation is not None:
            if annotation not in x.columns.values:
                # Warn if you don't have an annotation column
                raise KeyError(
                    ""annotation argument specified as %s but column not ""
                    ""found in 'x' data.frame"" % annotation
                )
            else:
                # If the input DataFrame has a gene column, add it to the new
                # DataFrame
                self.data[annotation] = x[annotation]

        self.xlabel = """"
        self.ticks = []
        self.ticksLabels = []
        self.nChr = len(x[chrm].unique())
        self.chrName = chrm
        self.pName = p
        self.snpName = snp
        self.geneName = gene
        self.annotationName = annotation
        self.logp = logp

        # Set positions, ticks, and labels for plotting

        self.index = 'INDEX'
        self.pos = 'POSITION'

        # Fixes the bug where one chromosome is missing by adding a sequential
        # index column.
        idx = 0
        for i in self.data[chrm].unique():
            idx = idx + 1
            self.data.loc[self.data[chrm] == i, self.index] = int(idx)
        # Set the type to be the same as provided for chrm column
        self.data[self.index] = \
            self.data[self.index].astype(self.data[chrm].dtype)

        # This section sets up positions and ticks. Ticks should be placed in
        # the middle of a chromosome. The new pos column is added that keeps
        # a running sum of the positions of each successive chromosome.
        # For example:
        # chrm bp pos
        # 1   1  1
        # 1   2  2
        # 2   1  3
        # 2   2  4
        # 3   1  5

        if self.nChr == 1:
            # For a single chromosome
            self.data[self.pos] = self.data[bp]
            self.ticks.append(int(len(self.data[self.pos]) / 2.) + 1)
            self.xlabel = ""Chromosome %s position"" % (self.data[chrm].unique())
            self.ticksLabels = self.ticks
        else:
            # For multiple chromosomes
            lastbase = 0
            for i in self.data[self.index].unique():
                if i == 1:
                    self.data.loc[self.data[self.index] == i, self.pos] = \
                        self.data.loc[self.data[self.index] == i, bp].values
                else:
                    prevbp = self.data.loc[self.data[self.index] == i - 1, bp]
                    # Shift the basepair position by the largest bp of the
                    # current chromosome
                    lastbase = lastbase + prevbp.iat[-1]

                    self.data.loc[self.data[self.index] == i, self.pos] = \
                        self.data.loc[self.data[self.index] == i, bp].values \
                        + lastbase

                tmin = min(self.data.loc[self.data[self.index] == i, self.pos])
                tmax = max(self.data.loc[self.data[self.index] == i, self.pos])
                self.ticks.append(int((tmin + tmax) / 2.) + 1)

            self.xlabel = 'Chromosome'
            self.data[self.pos] = self.data[self.pos].astype(
                self.data[bp].dtype)

            if self.nChr &gt; 20:  # To avoid crowded labels
                self.ticksLabels = [
                    chrm if np.mod(int(t+1), 2)  # Only every two ticks
                    else ''
                    for t,chrm in enumerate(self.data[chrm].unique())
                ]
            else:
                self.ticksLabels = self.data[chrm].unique()  # All the ticks

    def figure(
            self,
            title=""Manhattan Plot"",
            showgrid=True,
            xlabel=None,
            ylabel='-log10(p)',
            point_size=5,
            showlegend=True,
            col=None,
            suggestiveline_value=-np.log10(1e-8),
            suggestiveline_color='blue',
            suggestiveline_width=1,
            genomewideline_value=-np.log10(5e-8),
            genomewideline_color='red',
            genomewideline_width=1,
            highlight=True,
            highlight_color=""red"",
    ):
        """"""Keyword arguments:
    - title (string; default 'Manhattan Plot'): The title of the
        graph.
    - showgrid (bool; default True): Boolean indicating whether
        gridlines should be shown.
    - xlabel (string; optional): Label of the x axis.
    - ylabel (string; default '-log10(p)'): Label of the y axis.
    - point_size (number; default 5): Size of the points of the
        scatter plot.
    - showlegend (bool; default True): Boolean indicating whether
        legends should be shown.
    - col (string; optional): A string representing the color of the
        points of the Scatter plot. Can be in any color format
        accepted by plotly.graph_objects.
    - suggestiveline_value (bool | float; default 8): A value which
        must be either False to deactivate the option, or a numerical value
        corresponding to the p-value at which the line should be
        drawn. The line has no influence on the data points.
    - suggestiveline_color (string; default 'grey'): Color of the
        suggestive line.
    - suggestiveline_width (number; default 2): Width of the
        suggestive line.
    - genomewideline_value (bool | float; default -log10(5e-8)): A
        boolean which must be either False to deactivate the option, or a
        numerical value corresponding to the p-value above which the
        data points are considered significant.
    - genomewideline_color (string; default 'red'): Color of the
        genome-wide line. Can be in any color format accepted by
        plotly.graph_objects.
    - genomewideline_width (number; default 1): Width of the genome
      wide line.
    - highlight (bool; default True): Whether to turn on or off the
        highlighting of data points considered significant.
    - highlight_color (string; default 'red'): Color of the data
        points highlighted because they are significant. Can be in any
        color format accepted by plotly.graph_objects.

    Returns:
    - A figure formatted for plotly.graph_objects.

        """"""

        xmin = min(self.data[self.pos].values)
        xmax = max(self.data[self.pos].values)

        horizontallines = []

        if suggestiveline_value:
            suggestiveline = go.layout.Shape(
                name=SUGGESTIVE_LINE_LABEL,
                type=""line"",
                fillcolor=suggestiveline_color,
                line=dict(
                    color=suggestiveline_color,
                    width=suggestiveline_width
                ),
                x0=xmin, x1=xmax, xref=""x"",
                y0=suggestiveline_value, y1=suggestiveline_value, yref=""y""
            )
            horizontallines.append(suggestiveline)

        if genomewideline_value:
            genomewideline = go.layout.Shape(
                name=GENOMEWIDE_LINE_LABEL,
                type=""line"",
                fillcolor=genomewideline_color,
                line=dict(
                    color=genomewideline_color,
                    width=genomewideline_width
                ),
                x0=xmin, x1=xmax, xref=""x"",
                y0=genomewideline_value, y1=genomewideline_value, yref=""y""
            )
            horizontallines.append(genomewideline)

        data_to_plot = []  # To contain the data traces
        tmp = pd.DataFrame()  # Empty DataFrame to contain the highlighted data

        if highlight:
            if not isinstance(highlight, bool):
                if self.snpName not in self.data.columns.values:
                    raise KeyError(
                        ""snp argument specified for highlight as %s but ""
                        ""column not found in the data.frame"" % self.snpName
                    )
            else:
                if not genomewideline_value:
                    raise Warning(
                        ""The genomewideline_value you entered is not a ""
                        ""positive value, or False, you cannot set highlight ""
                        ""to True in that case."")
                tmp = self.data

                # Sort the p-values (or -log10(p-values) above the line
                if genomewideline_value:
                    if self.logp:
                        tmp = tmp.loc[-np.log10(tmp[self.pName])
                                      &gt; genomewideline_value]
                    else:
                        tmp = tmp.loc[tmp[self.pName] &gt; genomewideline_value]

                highlight_hover_text = _get_hover_text(
                    tmp,
                    snpname=self.snpName,
                    genename=self.geneName,
                    annotationname=self.annotationName
                )

                if not tmp.empty:
                    data_to_plot.append(
                        go.Scattergl(
                            x=tmp[self.pos].values,
                            y=-np.log10(tmp[self.pName].values) if self.logp
                            else tmp[self.pName].values,
                            mode=""markers"",
                            text=highlight_hover_text,
                            marker=dict(
                                color=highlight_color,
                                size=point_size
                            ),
                            name=""Point(s) of interest""
                        )
                    )

        # Remove the highlighted data from the DataFrame if not empty
        if tmp.empty:
            data = self.data
        else:
            data = self.data.drop(self.data.index[tmp.index])

        if self.nChr == 1:

            if col is None:
                col = ['black']

            # If single chromosome, ticks and labels automatic.
            layout = go.Layout(
                title=title,
                xaxis={
                    'title': self.xlabel if xlabel is None else xlabel,
                    'showgrid': showgrid,
                    'range': [xmin, xmax],
                },
                yaxis={'title': ylabel},
                hovermode='closest'
            )

            hover_text = _get_hover_text(
                data,
                snpname=self.snpName,
                genename=self.geneName,
                annotationname=self.annotationName
            )

            data_to_plot.append(
                go.Scattergl(
                    x=data[self.pos].values,
                    y=-np.log10(data[self.pName].values) if self.logp
                    else data[self.pName].values,
                    mode=""markers"",
                    showlegend=showlegend,
                    marker={
                        'color': col[0],
                        'size': point_size,
                        'name': ""chr%i"" % data[self.chrName].unique()
                    },
                    text=hover_text
                )
            )
        else:
            # if multiple chrms, use the ticks and labels you created above.
            layout = go.Layout(
                title=title,
                xaxis={
                    'title': self.xlabel if xlabel is None else xlabel,
                    'showgrid': showgrid,
                    'range': [xmin, xmax],
                    'tickmode': ""array"",
                    'tickvals': self.ticks,
                    'ticktext': self.ticksLabels,
                    'ticks': ""outside""
                },
                yaxis={'title': ylabel},
                hovermode='closest'
            )

            icol = 0
            if col is None:
                col = [
                    'black' if np.mod(i, 2)
                    else 'grey' for i in range(self.nChr)
                ]

            for i in data[self.index].unique():

                tmp = data[data[self.index] == i]
                
                chromo = tmp[self.chrName].unique()[0]  # Get chromosome name

                hover_text = _get_hover_text(
                    data,
                    snpname=self.snpName,
                    genename=self.geneName,
                    annotationname=self.annotationName
                )

                data_to_plot.append(
                    go.Scattergl(
                        x=tmp[self.pos].values,
                        y=-np.log10(tmp[self.pName].values) if self.logp
                        else tmp[self.pName].values,
                        mode=""markers"",
                        showlegend=showlegend,
                        name=f""Chr{chromo}"",
                        marker={
                            'color': col[icol],
                            'size': point_size
                        },
                        text=hover_text
                    )
                )

                icol = icol + 1

        layout.shapes = horizontallines

        return go.Figure(data=data_to_plot, layout=layout)

Then, I modified the plotly-dash example here to use a modified df where the ""CHR"" column has string values like ""1A"",""1B"",...""9A"",""9B"" instead of integers, and tested that the Dash app renders these strings correctly.
import numpy as np
import pandas as pd
import dash
from dash.dependencies import Input, Output
import dash_bio as dashbio
from dash import html, dcc

app = dash.Dash(__name__)

df = pd.read_csv('https://git.io/manhattan_data.csv')

## create some sample data where ""CHR"" column
## contains strings of the format ""{number}{letter}"" 
## where {letter} is one of ""A"",""B""

np.random.seed(42)
df_test = df[df[""CHR""] &lt; 10].copy()
for _, df_group in df_test.groupby(""CHR""):
    start, end = df_group.index[0], df_group.index[-1]
    midpt = (start + end) // 2
    df_test['CHR'].loc[start:midpt] = df_group['CHR'].loc[start:midpt].astype(str) + 'A'
    df_test['CHR'].loc[midpt:end] = df_group['CHR'].loc[midpt:end].astype(str) + 'B'

app.layout = html.Div([
    'Threshold value',
    dcc.Slider(
        id='default-manhattanplot-input',
        min=1,
        max=10,
        marks={
            i: {'label': str(i)} for i in range(10)
        },
        value=6
    ),
    html.Br(),
    html.Div(
        dcc.Graph(
            id='default-dashbio-manhattanplot',
            figure=dashbio.ManhattanPlot(
                dataframe=df_test
            )
        )
    )
])

@app.callback(
    Output('default-dashbio-manhattanplot', 'figure'),
    Input('default-manhattanplot-input', 'value')
)
def update_manhattanplot(threshold):

    return dashbio.ManhattanPlot(
        dataframe=df_test,
        genomewideline_value=threshold
    )

if __name__ == '__main__':
    app.run_server(debug=True)


",,
Plotly strange result,https://stackoverflow.com/questions/45992342,Plotly is blending palette colors in R,"When I'm trying to define a custom palette, say of blue and red colors, I get magenta instead of blue. Let's consider an example taken from plotly documentation:

library(plotly)
pal &lt;- c(""red"", ""blue"", ""green"")
p &lt;- plot_ly(data = iris, x = ~Sepal.Length, y = ~Petal.Length, 
      color = ~Species, colors = pal)
p


This works as expected. Each of three species get it's own color: red, blue or green.

Then I prepare a new data set, removing ""virginica"", and plotting the result:

library(dplyr)
pal &lt;- c(""red"", ""blue"", ""green"")
iris_new&lt;- filter(iris, Species == ""setosa"" | Species == ""versicolor"")
p &lt;- plot_ly(data = iris_new, x = ~Sepal.Length, y = ~Petal.Length, 
      color = ~Species, colors = pal)
p


Now ""setosa"" is red and ""versicolor"" is blue. We don't use green color, so it seems reasonable to remove it:

pal &lt;- c(""red"", ""blue"")
iris_new&lt;- filter(iris, Species == ""setosa"" | Species == ""versicolor"")
p &lt;- plot_ly(data = iris_new, x = ~Sepal.Length, y = ~Petal.Length, 
      color = ~Species, colors = pal)
p


The result, obviously, should be the same, as in previous chunk of code, but in fact it isn't: the blue color is replaced by magenta, and that is very strange.
",1,238,"I think this is probably just because you have a 2 colour palette mapped to a 3 level factor - until you drop the unused level from the Species factor, it's still considered an attribute of the Species variable and will affect how it is plotted.

Dropping the factor level makes the colours behave as you expected:

iris_new&lt;- filter(iris, Species == ""setosa"" | Species == ""versicolor"") %&gt;%
    mutate(Species = droplevels(Species))
p &lt;- plot_ly(data = iris_new, x = ~Sepal.Length, y = ~Petal.Length, 
             color = ~Species, colors = pal)
p

",,
Plotly strange result,https://stackoverflow.com/questions/59413487,How to Successfully Produce Mosaic Plots in Pyviz Panel Apps?,"I have created the following dataframe df:

Setup:

import pandas as pd
import numpy as np
import random
import copy
import feather
import matplotlib.pyplot as plt
from statsmodels.graphics.mosaicplot import mosaic
import plotly.graph_objects as go
import plotly.express as px
import panel as pn
import holoviews as hv
import geoviews as gv
import geoviews.feature as gf
import cartopy
import cartopy.feature as cf
from geoviews import opts
from cartopy import crs as ccrs
import hvplot.pandas
import colorcet as cc
from colorcet.plotting import swatch
#pn.extension() # commented out as this causes an intermittent javascript error
gv.extension(""bokeh"")


cols = {""name"":[""Jim"",""Alice"",""Bob"",""Julia"",""Fern"",""Bill"",""Jordan"",""Pip"",""Shelly"",""Mimi""], 
         ""age"":[19,26,37,45,56,71,20,36,37,55], 
         ""sex"":[""Male"",""Female"",""Male"",""Female"",""Female"",""Male"",""Male"",""Male"",""Female"",""Female""],
         ""age_band"":[""18-24"",""25-34"",""35-44"",""45-54"",""55-64"",""65-74"",""18-24"",""35-44"",""35-44"",""55-64""],
         ""insurance_renew_month"":[1,2,3,3,3,4,5,5,6,7],
         ""postcode_prefix"":[""EH"",""M"",""G"",""EH"",""EH"",""M"",""G"",""EH"",""M"",""EH""],
         ""postcode_order"":[3,2,1,3,3,2,1,3,2,3],
         ""local_authority_district"":[""S12000036"",""E08000003"",""S12000049"",""S12000036"",""S12000036"",""E08000003"",""S12000036"",""E08000003"",""S12000049"",""S12000036""],
         ""blah1"":[3,None,None,8,8,None,1,None,None,None],
         ""blah2"":[None,None,None,33,5,None,66,3,22,3],
         ""blah3"":[""A"",None,""A"",None,""C"",None,None,None,None,None],
         ""blah4"":[None,None,None,None,None,None,None,None,None,1]}
df = pd.DataFrame.from_dict(cols)
df


Out[2]: 
     name  age     sex age_band  ...  blah1 blah2  blah3 blah4
0     Jim   19    Male    18-24  ...    3.0   NaN      A   NaN
1   Alice   26  Female    25-34  ...    NaN   NaN   None   NaN
2     Bob   37    Male    35-44  ...    NaN   NaN      A   NaN
3   Julia   45  Female    45-54  ...    8.0  33.0   None   NaN
4    Fern   56  Female    55-64  ...    8.0   5.0      C   NaN
5    Bill   71    Male    65-74  ...    NaN   NaN   None   NaN
6  Jordan   20    Male    18-24  ...    1.0  66.0   None   NaN
7     Pip   36    Male    35-44  ...    NaN   3.0   None   NaN
8  Shelly   37  Female    35-44  ...    NaN  22.0   None   NaN
9    Mimi   55  Female    55-64  ...    NaN   3.0   None   1.0

[10 rows x 12 columns]


df[[""sex"",""age_band"",""postcode_prefix""]] = df[[""sex"",""age_band"",""postcode_prefix""]].astype(""category"")
df.info()


&lt;class 'pandas.core.frame.DataFrame'&gt;
RangeIndex: 10 entries, 0 to 9
Data columns (total 12 columns):
name                        10 non-null object
age                         10 non-null int64
sex                         10 non-null category
age_band                    10 non-null category
insurance_renew_month       10 non-null int64
postcode_prefix             10 non-null category
postcode_order              10 non-null int64
local_authority_district    10 non-null object
blah1                       4 non-null float64
blah2                       6 non-null float64
blah3                       3 non-null object
blah4                       1 non-null float64
dtypes: category(3), float64(3), int64(3), object(3)
memory usage: 1.3+ KB


The Problem:

I can successfully create a mosaic plot with the following code:

fig,ax = plt.subplots(figsize=(15,10))
mosaic(df,[""sex"", ""age_band""],ax=ax);




However, I am having issues when I try to create a corresponding app using pn.interact:

categoric_cols = df.select_dtypes(include=""category"")
cat_atts = categoric_cols.columns.tolist()
cat_atts


Out[4]: ['sex', 'age_band', 'postcode_prefix']


def bivar_cat(x=""sex"",y=""age_band""):
    if x in cat_atts and y in cat_atts:
        fig,ax = plt.subplots(figsize=(15,10))
        return mosaic(df,[x,y],ax=ax);

app_df_cat = pn.interact(bivar_cat,x=cat_atts,y=cat_atts)
app_df_cat


Which results in the following:



The above rendered mosaic plot seems to correspond to the default values of x &amp; y (ie sex &amp; age_band). When you select a new attribute for x or y from the dropdowns, the text above the mosaic plot changes (this text seems to be a string representation of the plot) however the mosaic plot itself does not.

Is my issue possibly related to having to comment out pn.extension()? I have found that when pn.extension() is not commented out, it results in an intermittent javascript error whereby sometimes there is no error raised, sometimes there is an error but my panel app still loads and sometimes there is an error and it crashes my browser. (I have omitted the javascript error here as it can be very large - if it is helpful I can add this to my post.) I would say that the error is raised significantly more often than it is not.

Strangely enough, I haven't observed any difference in other apps that I have created where I have omitted pn.extension() vs including it. 
However as the documentation always specifies that you include it, I would have expected that I would have to set my appropriate extensions for all my plots to work correctly? (I have plotly, hvplot, holoviews and geoviews plots successfully plotting in these other apps with and without pn.extension() and pn.extension(""plotly"") included).

Is it possible to produce panel apps based on mosaic plots?

Thanks



Software Info:

os x                      Catalina 
browser                   Firefox
python                    3.7.5
notebook                  6.0.2 
pandas                    0.25.3
panel                     0.7.0
plotly                    4.3.0 
plotly_express            0.4.1 
holoviews                 1.12.6
geoviews                  1.6.5 
hvplot                    0.5.2 

",1,387,"Statsmodels function mosaic() returns a tuple with a figure and rects. 

What you're seeing now via interact is that tuple. This tuple also gets updated  in your code when you use the dropdowns.

The figure you see below that is the figure that jupyter automatically plots one time. This one doesn't get updated.

The solution is two-fold:
1) only return the figure, not the tuple
2) prevent jupyter from automatically plotting your figure once with plt.close()

In code:

def bivar_cat(x='sex', y='age_band'):
    fig, ax = plt.subplots(figsize=(15,10))
    mosaic(df, [x,y], ax=ax)
    plt.close()
    return fig

app_df_cat = pn.interact(
    bivar_cat, 
    x=cat_atts, 
    y=cat_atts,
)

app_df_cat

",,
Plotly strange result,https://stackoverflow.com/questions/74954985,Adding Mapbox Layer to Plotly Map Positioned Over Data Layer,"I'm trying to understand how to situate a data layer within a stack of vector layers using Plotly's Mapbox functionality. I think this is possible with the following approach. Using Mapbox Studio, I've created two map styles: one which represents the base map that all data layers should be placed on top of and another which represents the remaining map layers that should be layered on above the data layers.
This example from the Plotly documentation seems to point to the viability of this approach.
Here's demo code that I put together to try and realize the result I'm after. The last update_layout statement is not adding the Mapbox layer. Not sure what I'm missing here. Any suggestions? Maybe the Mapbox style spec can't be passed for additional layers?
import plotly.graph_objects as go
import pandas as pd

us_cities = pd.read_csv(""https://raw.githubusercontent.com/plotly/datasets/master/us-cities-top-1k.csv"")

fig = go.Figure(go.Scattermapbox(lat=us_cities[""lat""], lon=us_cities[""lon""]))
fig.update_layout(mapbox_style=""mapbox://styles/diehl/cl9gh775w000014n2jwfncok7"", mapbox_accesstoken=MAPBOX_ACCESS_TOKEN)
fig.update_layout(mapbox_center_lat=38, mapbox_center_lon=-97, mapbox_zoom=3.25)
fig.update_layout(margin={""r"":0,""t"":0,""l"":0,""b"":0})

fig.update_layout(
    mapbox_layers=[
    {
        ""sourcetype"": ""vector"",
        ""source"": [""mapbox://styles/diehl/cl9gha8ta000914qm137qf52q""]
    }
])

fig.show()

UPDATE: Took a small step forward in that I got something to render. Replacing the last fig.update_layout statement with the following causes additional circles to be rendered instead of the vector tiles which is strange. The vector tiles referred to here correspond to the border for the state of New Mexico. Not complex by any means. Why that vector tile layer is showing up as additional circles I assume is due to the plot being a scatter plot. Any thoughts about how to get around this would be greatly appreciated!
fig.update_layout(
  mapbox_layers=[
  {
      ""sourcetype"": ""vector"",
      ""source"": ['https://api.mapbox.com/v4/diehl.3rvgqhks/{z}/{x}/{y}.mvt'],
      ""sourcelayer"": ""new_mexico_bnd""
  }  ])

For what it is worth, I noticed in the Chrome dev console the following message: ""Geometry exceeds allowed extent, reduce your vector tile buffer size""
",1,1845,"After much digging in the documentation, it became clear that my strategy is misguided. The more appropriate approach is to specify the full vector stack for the basemap and then use the below functionality to tell Plotly where to slot a particular trace into the basemap. Here's a working example.
import plotly.graph_objects as go
import pandas as pd

us_cities = pd.read_csv(""https://raw.githubusercontent.com/plotly/datasets/master/us-cities-top-1k.csv"")
token = &lt;MAPBOX_ACCESS_TOKEN&gt;

fig = go.Figure(go.Scattermapbox(lat=us_cities[""lat""], lon=us_cities[""lon""]))
fig.update_layout(mapbox_style=""mapbox://styles/mapbox/light-v10"", mapbox_accesstoken=token)
fig.update_layout(mapbox_center_lat=38, mapbox_center_lon=-97, mapbox_zoom=3.25)
fig.update_layout(margin={""r"":0,""t"":0,""l"":0,""b"":0})

# Statement that tells Plotly where to insert the trace into the basemap
fig.update_traces(below=""aeroway-polygon"", selector=dict(type='scattermapbox'))

fig.show()

","I am not sure if this answers your question in its entirety (because I am only rendering the small New Mexico sample layer on top of what you already have), but according to the plotly documentation, the default type of a mapbox layer will be ""circle"", so if we add the argument ""type"": ""line"", then the border of New Mexico renders as follows:
fig = go.Figure(go.Scattermapbox(lat=us_cities[""lat""], lon=us_cities[""lon""]))

fig.update_layout(mapbox_style=""mapbox://styles/diehl/cl9gh775w000014n2jwfncok7"", mapbox_accesstoken={MAPBOX_ACCESS_TOKEN})
fig.update_layout(mapbox_center_lat=38, mapbox_center_lon=-97, mapbox_zoom=3.25)
fig.update_layout(margin={""r"":0,""t"":0,""l"":0,""b"":0})

## update:
fig.update_layout(
    mapbox_layers=[
    {
        ""sourcetype"": ""vector"",
        ""source"": ['https://api.mapbox.com/v4/diehl.3rvgqhks/{z}/{x}/{y}.mvt'],
        ""sourcelayer"": ""new_mexico_bnd"",
        ""type"": ""line""
    }  
])

fig.show()


",
Plotly strange result,https://stackoverflow.com/questions/56233934,How to keep only time on plotly graph axis,"I'm trying to create a graph with plotly and cufflinks that has dates on the xaxis, some amounts on the primary y, and time on the secondary y.

The data looks like this:

        automatic   manual  time
2019-02-25  206.0   1206.0  2019-02-26 16:58:09
2019-02-26  225.0   136.0   2019-02-27 08:33:49
2019-02-27  213.0   554.0   2019-02-28 07:25:19
2019-02-28  244.0   103.0   2019-03-01 07:32:37
2019-03-01  102.0   119.0   2019-03-04 12:06:37


The setup for the figure is as follows:

fig = go.Figure(**cf.tools.merge_figures([
    df.figure(columns=[""automatic"", ""manual""], kind=""bar"", barmode=""stack""),
    df.figure(columns=[""time""])
])).set_axis([""time""], side=""right"")


The only way I've gotten the graph I want is by setting the date part of each date in the df.time column to the same arbitrary date, like so:

df.loc[:, ""time""] = df.time.apply(lambda d: d.replace(year=1967, month=1, day=1))


However, this way I get the wrong hover text and that arbitrary date is displayed at the bottom of the secondary y.

I've tried to set the range on yaxis2 manually like so:

sotimes = [d for d in df.time.tolist() if not pd.isnull(d)]
fig[""layout""].update({""yaxis2"": {""range"": [f""{min(sotimes):%H:%M:%S}"", f""{max(sotimes):%H:%M:%S}""]}})


Strangely, this results in yaxis2 being a date range as well.

I've also tried to convert df.time to time only, either as a string or as datetime.time like so:

df.loc[:, ""time""] = df.time.apply(
    lambda d: d.time() if not pd.isnull(d) else pd.NaT)
df.loc[:, ""time""] = df.time.apply(
    lambda d: f""{d:%H:%M:%S}"" if not pd.isnull(d) else """")


Both of these result in yaxis2 not being ordered at all, the times are displayed in the order in which they appear in df.time.



EDIT 1 - add complete code

import cufflinks as cf
import plotly.graph_objs as go
from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot
import pandas as pd


# plotly stuff
cf.go_offline()

dct = {
    ""date"": [""2019-02-25"", ""2019-02-26"", ""2019-02-27"", ""2019-02-28"", ""2019-03-01""],
    ""auto"": [206, 225, 213, 244, 102],
    ""manual"": [1206, 136, 554, 103, 119],
    'time': [pd.Timestamp(2019, 2, 26, 16, 58, 9), pd.Timestamp(2019, 2, 27, 8, 33, 49),
             pd.Timestamp(2019, 2, 28, 7, 25, 19), pd.Timestamp(2019, 3, 1, 7, 32, 37),
             pd.Timestamp(2019, 3, 4, 12, 6, 37)]
}
df = pd.DataFrame(dct).set_index(""date"")
df.loc[:, ""time""] = df.time.apply(lambda d: d.replace(year=1967, month=1, day=1))

fig = go.Figure(**cf.tools.merge_figures([
    df.figure(columns=[""auto"", ""manual""], kind=""bar"", barmode=""stack""),
    df.figure(columns=[""time""])
])).set_axis([""time""], side=""right"")
# sotimes = [d for d in df.time.tolist() if not pd.isnull(d)]
# fig[""layout""].update({""yaxis2"": {""range"": [f""{min(sotimes):%H:%M:%S}"", f""{max(sotimes):%H:%M:%S}""]}})
plot(fig)

",0,941,"This isn't an answer to the whole question, but creates a working graph the way you would want it to. Using the approach of setting every datetime instance to the same date, you could adapt the tick labels using:
    fig[""layout""].update({""yaxis2"": {""tickformat"": ""%H:%M""}})

",,
Plotly strange result,https://stackoverflow.com/questions/62654316,Why this problem when showing the heatmap with Plotly?,"I want to create a simple annoted heatmap with plotly.
But I get something strange when the y values contains years ('2017','2018','2019')
for example:
import plotly.figure_factory as ff

# create a x list (month)
x = ['January',
     'February',
     'March']

#create a y list (year)
y=['2017', '2018', '2019', 'Mean']

#create a z list (the values)
z = [[1,2,3],
    [4,5,6],
    [7,8,9],
    [4,5,6]]

fig = ff.create_annotated_heatmap(z,x=x,y=y)
fig.show()

this code above give me this result :
But, if I change the y list with other string. il's ok and I get what I want.
import plotly.figure_factory as ff

# create a x list (month)
x = ['January',
     'February',
     'March']

#create a y list (year)
y=['year 1', 'year 2', 'year 3', 'Mean']

#create a z list (the values)
z = [[1,2,3],
    [4,5,6],
    [7,8,9],
    [4,5,6]]

fig = ff.create_annotated_heatmap(z,x=x,y=y)
fig.show()



How could I solve this?
",0,384,"Try adding fig.update_layout(yaxis=dict(type='category')). This should ensure that the values on the y-axis are interpreted as strings.
import plotly.figure_factory as ff

# create a x list (month)
x = ['January',
     'February',
     'March']

#create a y list (year)
y=['2017', '2018', '2019', 'Mean']

#create a z list (the values)
z = [[1,2,3],
    [4,5,6],
    [7,8,9],
    [4,5,6]]

fig = ff.create_annotated_heatmap(z,x=x,y=y)

fig.update_layout(yaxis=dict(type='category'))

fig.show()

",,
Plotly strange result,https://stackoverflow.com/questions/67216705,How to create a bar plot with shared x-axis using plotly.express with sub-titles on top?,"I wonder how to correctly create a bar plot with shared x-axis in plotly? I created this bar plot and specified fig.update_xaxes(matches='x') which to my understanding is supposed to match the axes, but apparently this is not working. Does this look like a bug or am I doing something wrong? As you can see, it looks good except for one sub-figure, which is totally off. Sometimes, using facet_row instead of facet_col gives better results, but then the titles appear at the side of the plot. The titles are sometimes quite long and would not fit there. Therefore, I am using facet_col.
fig = px.bar(data_frame=df, x='RawFile', y=plot_column, 
             facet_col='Majority protein IDs', facet_col_wrap=1, color=color, 
             color_discrete_sequence=px.colors.qualitative.Dark24,
             color_continuous_scale=px.colors.sequential.Rainbow)

n_rows = len(df['Majority protein IDs'].drop_duplicates())

height = 300*(1+n_rows)

fig.update_layout(
        height=height,        
        margin=dict( l=50, r=10, b=200, t=50, pad=0 ),
        hovermode='closest')

fig.for_each_annotation(lambda a: a.update(text=a.text.split(""="")[-1]))
fig.update(layout_showlegend=True)
fig.update_xaxes(matches='x')


Strangely the y-axes are shared, but the x-axes are not. That is quite the opposite of what I am looking for.

# django-plotly-dash        1.6.3
# plotly                    4.14.3

I guess one solution to this would be to use facet_row however, then then I would need to move the titles from the righthand side to the top.
",0,345,"My understanding is that you'd like all plot title to appear right on top of all subplots. There does not seem to be any built-in functionality to do this for plotly express. You always have the possility to adjust the elements of the figure, but it's a bit tricky in this case since you would have to adjust the heights of all subplots to make room for the titles. Here's my best attempt so far with a sample dataset:

If this is something you can use, we can take a closer look at the details.
Complete code:
import plotly.express as px
df = px.data.tips()
fig = px.scatter(df, x=""total_bill"", y=""tip"", color=""smoker"", facet_row=""sex"")
f = fig.full_figure_for_development(warn=False)

yrefs = []
for i, elem in enumerate(fig.layout):
    if elem[:5] == 'yaxis':
        yrefs.append(fig.layout[elem].domain[1])

for i, title in enumerate(fig.layout.annotations):
    title.textangle = 0
    title.y = yrefs[i] - 0.03
#     title.x = 0.5 - ((len(title.text)+1) / 100)
    title.x = 0.40
    title.align = 'center'
    title.bgcolor='rgba(0,0,0,0.25)'
    
fig.show()

",,
Plotly strange result,https://stackoverflow.com/questions/72261594,Extract local max and min and Plot an envelope function,"My dataset:
I have the following dataset which when plotted using Plotly produces the following result (simplified):

My task:
I want to create an envelope function that extracts the local maxima and minima from data from the above dataset and plots an envelope curve. It would roughly look like this:

I have tried approaching the solutions provided here and the one for the envelope provided here. However, none of them works for me in this approach. For some strange reason, the scatter plots are producing the following results (for local min), which is not exactly what I need.
Here's my code for the initial plot:
import plotly.express as px
import plotly.graph_objects as go

fig= go.Figure()

fig.add_traces(go.Scatter(x= mem_df['time'], y=mem_df['volatge']))

fig.update_layout(xaxis_title = r'$\text{Time } T \text{ in s}  $',
                      yaxis_title = r'$\text{Deflection angle } \text{ in radians}$')
fig.update_layout(title= r'$\text{Deflection angles versus time for the damping sheet}$')
fig.show()

Any help, in this case, would be appreciated!
Thank you in advance!
",0,934,"The problem is that your data contains a bit of noise. One possible approach is to find an appropriate curve fit, then find the local maximum and minimum on the fit. scipy comes to help:
import plotly.express as px
import plotly.graph_objects as go
import pandas as pd
import numpy as np
from scipy.optimize import curve_fit
from scipy.signal import argrelextrema

mem_df = pd.read_csv(""path/to/file"")
x = mem_df['time'].values
y = mem_df['deflection'].values
# dampened oscillator
func = lambda x, xi, k, phi, a, b: a * np.sin(k * x - phi) * np.exp(-xi * x) + b
popt, pcov = curve_fit(func, x, y)

fig= go.Figure()
fig.add_traces(go.Scatter(x=x, y=y, name=""data""))

# plot the curve fit
xx = np.linspace(0, 20, 150)
yy = func(xx, *popt)
fig.add_traces(go.Scatter(x=xx, y=yy, name=""fit""))

idx_max = argrelextrema(yy, np.greater)
idx_min = argrelextrema(yy, np.less)
fig.add_traces(go.Scatter(x=xx[idx_max], y=yy[idx_max], name=""top""))
fig.add_traces(go.Scatter(x=xx[idx_min], y=yy[idx_min], name=""bottom""))

fig.update_layout(xaxis_title = r'$\text{Time } T \text{ in s}  $',
                      yaxis_title = r'$\text{Deflection angle } \text{ in radians}$')
fig.update_layout(title= r'$\text{Deflection angles versus time for the damping sheet}$')
fig.show()


",,
Plotly strange issue,https://stackoverflow.com/questions/68808298,"TypeError: scatter() got an unexpected keyword argument &#39;trendline_options&#39; (Plotly, Python)","I'm getting the error:
TypeError: scatter() got an unexpected keyword argument 'trendline_options'

When trying to adjust the smoothing of the lowess tendline using plotly express.
Here is my code for the graph:
fig = px.scatter(dfg, x=""Yr_Mnth"", y=""Episode_Count"", color = ""Target"",
                        labels={""Episode_Count"": tally + "" per Shift"",
                                ""Target"":""Target"",
                                ""Yr_Mnth"": ""Date"" },
                        trendline='lowess',trendline_options= dict(frac=0.1), title=""Aggregate Behavior Data: "" + patient + "" - "" + today)
        fig.update_xaxes(tickangle=45,)
        fig.update_layout(template = 'plotly_white',hovermode=""x unified"")

Dataset (dfg):
Yr_Mnth                         Target              Episode_Count
2020-08-01                     Aggression           0.09
2020-08-01                      Elopement           0.00
2020-08-01                    Self-injury           0.97
2020-09-01                     Aggression           0.65
2020-09-01                      Elopement           0.00
2020-09-01                    Self-injury           1.58
2020-10-01                     Aggression           0.24
2020-10-01                      Elopement           0.00
2020-10-01                    Self-injury           0.75
2020-11-01                     Aggression           0.03
2020-11-01                      Elopement           0.01
2020-11-01                    Self-injury           0.89
2020-12-01                     Aggression           0.14
2020-12-01                      Elopement           0.00
2020-12-01                    Self-injury           0.94
2021-01-01                     Aggression           0.05
2021-01-01                      Elopement           0.00
2021-01-01                    Self-injury           0.30
2021-02-01                    Self-injury           0.42
2021-02-01                      Elopement           0.03
2021-02-01                     Aggression           0.16
2021-03-01                      Elopement           0.00
2021-03-01                    Self-injury           0.68
2021-03-01                     Aggression           0.20
2021-04-01                     Aggression           0.10
2021-04-01                      Elopement           0.03
2021-04-01                    Self-injury           0.33
2021-05-01                      Elopement           0.20
2021-05-01                     Aggression           0.21
2021-05-01                    Self-injury           1.63
2021-06-01                    Self-injury           0.90
2021-06-01                     Aggression           0.29
2021-06-01                      Elopement           0.14

I find this strange as I'm directly following the documentation - https://plotly.com/python/linear-fits/
Is this a known issue? I can't find any examples with a google search...
",5,9294,"As Henry helpfully pointed out this was just a version problem, easily addressed by updating plotly using:
pip install plotly==5.2.1

",,
Plotly strange issue,https://stackoverflow.com/questions/55539395,Size of cones in 3d animated cone plot with variable animation speed,"The docstring of the property sizeref in plotly.graph_objs._cone.py says:

        Adjusts the cone size scaling. The size of the cones is
        determined by their u/v/w norm multiplied a factor and
        `sizeref`. This factor (computed internally) corresponds to the
        minimum ""time"" to travel across two successive x/y/z positions
        at the average velocity of those two successive positions. All
        cones in a given trace use the same factor. With `sizemode` set
        to ""scaled"", `sizeref` is unitless, its default value is 0.5
        With `sizemode` set to ""absolute"", `sizeref` has the same units
        as the u/v/w vector field, its the default value is half the
        sample's maximum vector norm.

        The 'sizeref' property is a number and may be specified as:
          - An int or float in the interval [0, inf]

        Returns
        -------
        int|float


where there is this mysterious factor that is computed  and for the life of me I cannot find where it is actually computed. Since I do not understand how this weird factor is computed I get very strange behaviour in my animations as follows:

import numpy as np
import plotly.graph_objs as go
import plotly.offline as pl

###np.around used because plotly.js doesn't like full precision float64s
t = np.linspace(0,2*np.pi,100)
x = np.around(np.vstack((np.cos(t), np.cos(t+np.pi))),decimals=6)
y = np.around(np.vstack((np.sin(t), np.sin(t+np.pi))),decimals=6)
z = np.around(np.vstack((np.ones(len(t)),np.ones(len(t)))),decimals=6)

v = np.around(np.vstack((np.cos(t), np.cos(t+np.pi))),decimals=6)
u = np.around(-np.vstack((np.sin(t), np.sin(t+np.pi))),decimals=6)
w = np.around(np.vstack((np.zeros(len(t)),np.ones(len(t)))),decimals=6)

fig3=go.Figure([dict(anchor=""cm"",showscale=False,sizemode=""scaled"",type=""cone"",x=x[:,0],y=y[:,0]
                                        ,z=z[:,0]
                                        ,u=u[:,0],v=v[:,0]
                                        ,w=w[:,0])],layout=go.Layout(
    scene=dict(aspectratio=dict(x=1,y=1,z=0.25),
                    xaxis=dict(range=[-2,2], tickmode=""linear""),
                    yaxis=dict(range=[-2,2], tickmode=""linear""),
                    zaxis=dict(range=[0,5]))))

fig3.frames= [go.Frame(data=[dict(type=""cone"",x=x[:,i],y=y[:,i],z=z[:,i],u=u[:,i],v=v[:,i],w=w[:,i])], 
                             layout=go.Layout(annotations=[dict(text=""frame {}"".format(i))]))for i in np.array(range(len(t)))]

pl.plot(fig3, animation_opts=""{frame: {duration: 1}}"")


Note that you must first either remove the animation_opts kwarg or use plotly from my repo since the official version doesn't support animation_opts yet (see the issue I raised here).



Where is this factor calculated? I have scoured the code but found nothing.
Thanks in advance!

P.S Yes I included a lot of extraneous information. I have never seen an example of 3d cone animations in plotly yet so I wanted to provide one and I worked on the animation interface in plotly.py which will definitely be useful to someone!

EDIT: see this issue on github
",3,902,"Found out there was an internal scaling factor in Plotly.js that was causing this mess. I have fixed this in my current repo version of plotly here.
",,
Plotly strange issue,https://stackoverflow.com/questions/55927668,Plotly histogram is missing data,"I'm trying to create a histogram which involves a lot of repeated values in one of the cases. One of the data points is not being represented in the graph. Here is the smallest, simplest subset I could find that still reproduced my issue.

cleanVar &lt;- c(rep(1,9),1.25,1.5)
plot_ly(data.table(cleanVar),
x = ~cleanVar,
type = ""histogram"")


The above graph shows only two bars. One centered at 1 of height 9, and one centered at 1.2 of height 1.
Also strangely, the hover-over shows ""1"" for the first bar, despite it covering the range [.9,1.1], and it shows ""1.25"" for the second bar, despite it covering the range [1.1,1.3].

If we change the 1 to only be repeated 8 times cleanVar &lt;- c(rep(1,8),1.25,1.5), so that there are 10 total values in the histogram, it works better, but still, the three bins it creates are .25 wide according to the hover-over, yet they are only .2 wide on the graph itself.

What is plotly doing? How can I properly show 3 bins of height 9,1,1 and width .25? binning options in layout() aren't working.
",2,793,"By default plotly uses the following procedure to define the bins:

start:


  Sets the starting value for the x axis bins. Defaults to the minimum data value, shifted down if necessary to make nice round values and to remove ambiguous bin edges. For example, if most of the data is integers we shift the bin edges 0.5 down, so a size of 5 would have a default start of -0.5, so it is clear that 0-4 are in the first bin, 5-9 in the second, but continuous data gets a start of 0 and bins [0,5), [5,10) etc. Dates behave similarly, and start should be a date string. For category data, start is based on the category serial numbers, and defaults to -0.5. If multiple non-overlaying histograms share a subplot, the first explicit start is used exactly and all others are shifted down (if necessary) to differ from that one by an integer number of bins.


end:


  Sets the end value for the x axis bins. The last bin may not end
  exactly at this value, we increment the bin edge by size from
  start until we reach or exceed end. Defaults to the maximum data
  value. Like start, for dates use a date string, and for category
  data end is based on the category serial numbers.


You can find this information via:

library(listviewer)
schema(jsonedit = interactive())


Navigate as follows: object ► traces ► histogram ► attributes ► xbins ► start

To avoid the default behaviour just make your x variable a factor:

library(plotly)
library(data.table)

cleanVar &lt;- c(rep(1, 9), 1.25, 1.5)
plot_ly(data.table(cleanVar),
        x = ~factor(cleanVar),
        type = ""histogram"")




Result:


",,
Plotly strange issue,https://stackoverflow.com/questions/54878982,Jupyter Notebook kernal is not connecting when lunching from power shell,"I am facing this strange issue, when I am directly launching Jupyter Notebook from CMD it is running properly but when I am launching it from powershell it is not able to connect to the Kernal. 

Also I have pip installed Plotly, when I am importing it in powershell, I am able to import properly, but the same this in jupyter, it is saying no module name Plotly (with proper casing)?

jupyter 4.4.0 and Python 3.6.5, iPython is already up to date

 
I am unable to find out what might be causing this issue.
or something to inspect the kernals.
Can someone help me with this issue?
",1,330,"according to github issue try downgrading prompt-toolkit to the version 1.0.15

pip3 install 'prompt-toolkit&lt;2.0.0,&gt;=1.0.15' --force-reinstall

Python 3.6.5
jupyter 1.0.0
jupyter-client 5.2.3
jupyter-console 5.2.0
jupyter-core 4.4.0
ipython 6.4.0

You can even try upgrading iPython

pip3 install --upgrade --user ipython
","Try this on powershell python -m notebook
",
Plotly strange issue,https://stackoverflow.com/questions/64781996,Issue with connecting points for 3d line plot in plotly,"I have a dataframe of the following format:
       x    y    z aa_num frame_num cluster
1   1.86 3.11 8.62      1         1       1
2   1.77 3.32 8.31      2         1       1
3   1.59 3.17 8.00      3         1       1
4   1.67 3.49 7.81      4         1       1
5   2.04 3.59 7.81      5         1       1
6   2.20 3.34 7.57      6         1       1
7   2.09 3.19 7.25      7         1       1
8   2.13 3.30 6.89      8         1       1
9   2.17 3.63 6.70      9         1       1
10  2.22 3.63 6.33     10         1       1
11  2.06 3.83 6.04     11         1       1
12  2.31 3.75 5.76     12         1       1
13  2.15 3.45 5.59     13         1       1
14  2.21 3.28 5.26     14         1       1
15  2.00 3.13 4.98     15         1       1
16  2.13 2.86 4.74     16         1       1
17  1.97 2.78 4.41     17         1       1
18  2.20 2.76 4.10     18         1       1
19  2.43 2.46 4.14     19         1       1
20  2.34 2.23 3.85     20         1       1
21  2.61 2.16 3.59     21         1       1
22  2.42 1.92 3.36     22         1       1
23  2.44 1.95 2.98     23         1       1
24  2.26 1.62 2.94     24         1       1
25  2.19 1.35 3.20     25         1       1
26  1.92 1.11 3.08     26         1       1
27  1.93 0.83 3.33     27         1       1
28  1.83 0.72 3.68     28         1       1
29  1.95 0.47 3.95     29         1       1
30  1.84 0.36 4.29     30         1       1
31  0.56 3.93 7.07      1         2       1
32  0.66 3.84 7.42      2         2       1
33  0.87 3.54 7.49      3         2       1
34  0.84 3.19 7.33      4         2       1
35  0.76 3.32 6.98      5         2       1
36  0.88 3.23 6.63      6         2       1
37  1.10 3.46 6.43      7         2       1
38  1.35 3.49 6.15      8         2       1
39  1.72 3.50 6.23      9         2       1
40  1.88 3.67 5.93     10         2       1
41  2.25 3.72 5.97     11         2       1
42  2.43 3.48 5.74     12         2       1
43  2.23 3.35 5.44     13         2       1
44  2.23 3.38 5.06     14         2       1
45  2.01 3.38 4.76     15         2       1
46  2.02 3.44 4.38     16         2       1
47  1.98 3.10 4.20     17         2       1
48  2.05 3.13 3.83     18         2       1
49  2.28 2.85 3.72     19         2       1
50  2.09 2.56 3.58     20         2       1
51  2.21 2.37 3.27     21         2       1
52  2.06 2.04 3.15     22         2       1
53  1.93 2.01 2.80     23         2       1
54  1.86 1.64 2.83     24         2       1
55  1.95 1.38 3.10     25         2       1
56  1.78 1.04 3.04     26         2       1
57  1.90 0.84 3.34     27         2       1
58  1.83 0.74 3.70     28         2       1
59  1.95 0.48 3.95     29         2       1
60  1.84 0.36 4.29     30         2       1
etc..

I'm trying to create a 3d line plot of this data, where a line consisting of 30 &lt;x,y,z&gt; points will be plotted for each frame_num and the points would be connected in the order of aa_num. The code to do this is as follows:
    fig = plot_ly(output_cl, x = ~x, y = ~y, z = ~z, type = 'scatter3d', mode = 'lines+markers',
                   opacity = 1, line = list(width = 1, color = ~frame_num, colorscale = 'Viridis'),
                  marker = list(size = 2, color = ~frame_num, colorscale = 'Viridis'))

When I plot a single frame, it works fine:

However, a strange issue arises when I try to plot multiple instances.

For some reason, when I try to plot frame 1 and 2, point 1 and 30 connect to each other for frame 2. However, this doesn't happen for frame 1. Any ideas why? Is there someway to specify the ordering of points in 3d in plotly?
",0,1294,"If you want to create two different traces based on column frame_num you need to pass it as a categorial variable by using factor. As an alternative you can use name = ~ frame_num or split = ~ frame_num to create multiple traces.
library(plotly)

output_cl &lt;- data.frame(
           x = c(1.86,1.77,1.59,1.67,2.04,2.2,2.09,
                 2.13,2.17,2.22,2.06,2.31,2.15,2.21,2,2.13,1.97,2.2,
                 2.43,2.34,2.61,2.42,2.44,2.26,2.19,1.92,1.93,1.83,1.95,
                 1.84,0.56,0.66,0.87,0.84,0.76,0.88,1.1,1.35,1.72,1.88,
                 2.25,2.43,2.23,2.23,2.01,2.02,1.98,2.05,2.28,2.09,
                 2.21,2.06,1.93,1.86,1.95,1.78,1.9,1.83,1.95,1.84),
           y = c(3.11,3.32,3.17,3.49,3.59,3.34,3.19,
                 3.3,3.63,3.63,3.83,3.75,3.45,3.28,3.13,2.86,2.78,2.76,
                 2.46,2.23,2.16,1.92,1.95,1.62,1.35,1.11,0.83,0.72,
                 0.47,0.36,3.93,3.84,3.54,3.19,3.32,3.23,3.46,3.49,3.5,
                 3.67,3.72,3.48,3.35,3.38,3.38,3.44,3.1,3.13,2.85,2.56,
                 2.37,2.04,2.01,1.64,1.38,1.04,0.84,0.74,0.48,0.36),
           z = c(8.62,8.31,8,7.81,7.81,7.57,7.25,6.89,
                 6.7,6.33,6.04,5.76,5.59,5.26,4.98,4.74,4.41,4.1,
                 4.14,3.85,3.59,3.36,2.98,2.94,3.2,3.08,3.33,3.68,3.95,
                 4.29,7.07,7.42,7.49,7.33,6.98,6.63,6.43,6.15,6.23,5.93,
                 5.97,5.74,5.44,5.06,4.76,4.38,4.2,3.83,3.72,3.58,
                 3.27,3.15,2.8,2.83,3.1,3.04,3.34,3.7,3.95,4.29),
      aa_num = c(1L,2L,3L,4L,5L,6L,7L,8L,9L,10L,
                 11L,12L,13L,14L,15L,16L,17L,18L,19L,20L,21L,22L,23L,
                 24L,25L,26L,27L,28L,29L,30L,1L,2L,3L,4L,5L,6L,7L,
                 8L,9L,10L,11L,12L,13L,14L,15L,16L,17L,18L,19L,20L,
                 21L,22L,23L,24L,25L,26L,27L,28L,29L,30L),
   frame_num = c(1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,
                 1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,
                 1L,1L,1L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,
                 2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,
                 2L,2L),
     cluster = c(1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,
                 1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,
                 1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,
                 1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,
                 1L,1L)
)

fig = plot_ly(output_cl, x = ~x, y = ~y, z = ~z, type = 'scatter3d', mode = 'lines+markers', color = ~factor(frame_num), # or name = ~ frame_num or split = ~ frame_num
              colors =""Set2"", opacity = 1, line = list(width = 1), marker = list(size = 2))
fig


",,
Plotly strange issue,https://stackoverflow.com/questions/61710209,ValueError: Invalid property specified for object of type plotly.graph_objs.layout.Geo: &#39;fitbounds&#39;,"I am trying to make a chloropleth map in Dash Plotly and since the data is bound to Europe I want the default zoom to be focused on there. There should be a bounds fitting feature in Plotly, as described in the tutorial documentation:

https://plotly.com/python/map-configuration/



However, I cannot get it to work, not on my map, but not even on this basic tutorial, I am always getting:

   ValueError: Invalid property specified for object of type plotly.graph_objs.layout.Geo: 'fitbounds'


It seems as if the fitbounds property is not defined for the geos, strange.

Code to replicate the issue:

fig = px.line_geo(lat=[0, 15, 20, 35], lon=[5, 10, 25, 30])
fig.update_geos(fitbounds=""locations"")
fig.update_layout(height=300, margin={""r"": 0, ""t"": 0, ""l"": 0, ""b"": 0})


What could this be caused by?
",0,793,"For me just trying to understand your problem, I had to install nbformat package. And it works perfectly after installing it. maybe your problem is just an update issue. **please note that I can NOT make any comments yet and that's why I'm posting an answer!

pip(3) install --upgrade nbformat
pip(3) install --upgrade plotly

",,
Plotly inconsistent output,https://stackoverflow.com/questions/56532527,"Customizing text (text size, text color, text angle) in Plotly for R","I'm working on making a donut chart/open pie chart in Plotly. Because of help from this question, Open Pie Chart/Donut Chart in R using Plotly with count and percentage, I was able to get fairly far with my plot. However, now I need help customizing the text size, angle, and color.

Here's the dataset:

library(dplyr)
testfile &lt;- tibble(personID = 1:10,
                   status = c(""bad"", ""good"", ""bad"", ""bad"", ""bad"", ""bad"", ""bad"", ""bad"", ""bad"", ""good""),
                   department = c(""sales"", ""sales"", ""marketing"", ""sales"", ""marketing"", ""management"", ""management"", ""sales"", ""sales"", ""sales""))


Here's the code to make the plot thus far. It has the count and percentage for each status, and in the center, it has the percentage of ""good"" status individuals.

library(plotly)

plot &lt;- plot_ly(values, labels = ~status, values = ~count, text = ~count, color = I(""white"")) %&gt;%
  add_pie(hole = 0.6) %&gt;%
  layout(title = ""Ratio of Good to Bad"",  showlegend = F, position = ""topcenter"", 
         xaxis = list(showgrid = FALSE, zeroline = FALSE, showticklabels = TRUE),
         yaxis = list(showgrid = FALSE, zeroline = FALSE, showticklabels = TRUE))

plot &lt;- layout(p, annotations=list(text=paste(good$count / sum(values$count) * 100, ""%"", sep=""""), ""showarrow""=F))
plot


This code yields these two plots. Surprisingly, it looks the first way when I click ""zoom"" on R Studio, but it looks the second (very wrong) way when I click export and open the PNG file.

Zoomed plot (mostly correct)


Exported plot (looks very different than how it should...)


This zoomed in plot is exactly what I want except for three issues:


I want the center text (20%) to be larger. I've tried p &lt;- layout(p, annotations=list(text=paste(good$count / sum(values$count) * 100, ""%"", sep=""""), ""showarrow""=F, size = 25)) but that did nothing.
I want both of the labels (2 20% and 8 80%) to be white, not just the 8 80%. I thought color = I(""white"") would do the trick, but the output is the same as when I don't specify the color at all.
I want the labels to be horizontal instead of curving and for them to fit within the bars.


Optional: Why does it look different when it is exported vs zoomed in? I need it to look like the zoomed in version. Also, is there also an easy way to get it to say n = 2 instead of just 2?

Thank you!

EDIT: This code makes the labels horizontal, but they are too small. When I add size = 15 or any number, it always makes the font the same large size but no longer horizontal. This also fixes the issue where the export and zoom are inconsistent. It also fixes the center text size. I just need the labels to both be white, larger in size, and horizontal.

plot &lt;- plot_ly(values, labels = ~status, values = ~count, text = ~count) %&gt;%
  add_pie(hole = 0.6) %&gt;%
  layout(title = ""Ratio of Good to Bad"",  showlegend = F, position = ""topcenter"", 
         xaxis = list(showgrid = FALSE, zeroline = FALSE, showticklabels = TRUE),
         yaxis = list(showgrid = FALSE, zeroline = FALSE, showticklabels = TRUE),
         annotations=list(text=paste(good$count / sum(values$count) * 100, ""%"", sep=""""), ""showarrow""=F, font=list(size = 40)))
plot



",2,7882,"Update, I cracked the code! Here's the code that worked

library(plotly)
library(dplyr)

values &lt;- testfile %&gt;%
  group_by(status) %&gt;%
  summarize(count = n())

t &lt;- list(size = 14, color = ""white"")

good &lt;- values %&gt;% filter(status == 'good')

plot &lt;- plot_ly(values, labels = ~status, values = ~count, text = ~count, textfont = list(color = ""white"", size = 40)) %&gt;%
  add_pie(hole = 0.6) %&gt;%
  layout(title = ""Ratio of Good to Bad"",  showlegend = TRUE, 
         xaxis = list(showgrid = FALSE, zeroline = FALSE, showticklabels = TRUE),
         yaxis = list(showgrid = FALSE, zeroline = FALSE, showticklabels = TRUE),
         annotations=list(text=paste(good$count / sum(values$count) * 100, ""%"", sep=""""), ""showarrow""=F, font=list(size = 180, color = ""black"")))
plot


Here's what the plot looks like now:


",,
Plotly inconsistent output,https://stackoverflow.com/questions/66205856,Sizing of ggiraph::girafe() output is not consistent with documentation on RStudio viewer after use with flexdashboard,"I have a number of plots using {ggplot2} and {ggiraph} that I have consolidated into a {flexdashboard}
Before attempting to display them with the dashboard, these plots displayed correctly on RStudio's viewer using the sizing option width = 0.7
However, they do not display on the dashboard (just a blank panel), and now produce a blank panel if run directly in RStudio. After some investigation it seems that some setting has changed and that the charts are not output because the size is inconsistent with the window.
If I change the width setting to 10 it displays correctly in the RStudio viewer which seems odd.
I have a small reprex here:
library(ggplot2)
library(ggiraph)

#dummy dataframe
df &lt;- data.frame(
  x = seq.Date(Sys.Date() - 30, Sys.Date() - 1, by = ""days""),
  y = runif(30, 0, 2) 
)

p &lt;- ggplot2::ggplot(df, (aes(x,y)))+
  geom_point_interactive(aes(tooltip = x, data_id = x))


plot &lt;- girafe(ggobj = p, 
                width =0.7)

print(plot)

This produces the following output:

However if I adjust width setting to 10
plot &lt;- girafe(ggobj = p, 
                width =10)

print(plot)

I get the following:

This seems inconsistent with the documentation for {ggiraph}, is there some graphics setting that has been modified by {flexdashboard}? I've tried interactive (js) charts with {plotly} and {echarts4r} on {flexdashboard} output which don't show the same behaviour.
I'm using
RStudio 1.4.1103
R version 4.0.2
System: x86_64-apple-darwin17.0
",0,589,"I've been experimenting and it seems that I can control the output by using width_svg consistently.
I still don't know why there is an issue but I suspect some underlying graphics setting.
",,
Plotly inconsistent result,https://stackoverflow.com/questions/64517809,Plotly: How to make data consistent across animation frames (i.e. avoiding vanishing data) in plotly express,"This is starting to bug me: In plotly express when using animation_frame, I know it's important to set ranges so data can be displayed consistently, otherwise data may vanish across frames. But for a column with categorical values (say 'US', 'Russia', 'Germany'), I cannot find any way to avoid disappearing data when not every frame contains all categories if I want that column to appear with different colors (in the code below, that column would be 'AnotherColumn'). Plotly documentation points out

Animations are designed to work well when each row of input is present across all animation frames, and when categorical values mapped to symbol, color and facet are constant across frames. Animations may be misleading or inconsistent if these constraints are not met.

but while I can easily set a range_color when I have a continuous color range, nothing of the sort seems to work for categorical data. I can somewhat workaround this by making my data numerical (e.g. 'US'-&gt; 1, 'Russia' -&gt; 2) bu that is both fiddly and the result visually unappealing.
import plotly.express as px

... 

fig = px.bar(data, x=""NameColumn"",
             y=""SomeColumn"",
             color=""AnotherColumn"",
             animation_frame=""AnimationColumn"",
             range_y=[0, max_y]
             )

Here is a simple reproducible example:
import pandas as pd
import plotly.express as px


data_dict = {'ColorColumn': ['p', 'p', 'p', 'q'],
             'xColumn': ['someName', 'someOtherName', 'someName', 'someOtherName'],
             'yColumn': [10, 20, 30, 40],
             'animationColumn': [1, 1, 2, 2]}

data = pd.DataFrame(data=data_dict)

fig = px.bar(data, x=""xColumn"",
             y=""yColumn"",
             color=""ColorColumn"",
             animation_frame=""animationColumn"",
             range_y=[0, 40]
             )
fig.update_layout(xaxis={'title': '',
                        'visible': True,
                        'showticklabels': True})



fig.show()


If you try it out, you'll notice the second frame is missing a bar. If the ColorColumn had numeric data, you could fix this by specifying range_color (similar to the specification of range_y in the code above); my question would be, how to handle this with categorical data?
Second edit: Some requested additional data or more a more reasonable example. This might be more appropriate:
import pandas as pd
import plotly.express as px


data_dict = {'Region': ['North America', 'Asia', 'Asia',
                        'North America', 'Asia', 'Europe',
                        'North America', 'Europe', 'Asia'],
             'Country': ['US', 'China', 'Korea',
                         'US', 'Phillipines', 'France',
                         'Canada', 'Germany', 'Thailand'],
             'GDP': [10, 20, 30,
                     40, 50, 60,
                     70, 80, 90],
             'Year': [2017, 2017, 2017,
                      2018, 2018, 2018,
                      2019, 2019, 2019]}

data = pd.DataFrame(data=data_dict)

fig = px.bar(data, x=""Country"",
             y=""GDP"",
             color=""Region"",
             animation_frame=""Year"",
             range_y=[0, 80]
             )
fig.update_layout(xaxis={'title': '',
                        'visible': True,
                        'showticklabels': True})



fig.show() 

",2,1359,"A similar question has been asked and answered under Plotly: How to specify categorical x-axis elements in a plotly express animation?. The necessary adjustments for your use case aren't exactly straight-forward though, so I'll might as well set it up for you.
It all boils down to this setup using, among other things:
df['key']=df.groupby(['Year','Country']).cumcount()
df1 = pd.pivot_table(df,index='Year',columns=['key', 'Country'],values='GDP')

And:
df1 = pd.merge(df1, data[['Country', 'Region']], how='left', on='Country').drop_duplicates()

Using some neat properties of pd.pivot_table, this will give you a dataset that has all years and all countries for all regions even though GDP from these have not been specified.
The two first animation frames will look like this:


Complete code:
import pandas as pd
import plotly.express as px


data_dict = {'Region': ['North America', 'Asia', 'Asia',
                        'North America', 'Asia', 'Europe',
                        'North America', 'Europe', 'Asia'],
             'Country': ['US', 'China', 'Korea',
                         'US', 'Phillipines', 'France',
                         'Canada', 'Germany', 'Thailand'],
             'GDP': [10, 20, 30,
                     40, 50, 60,
                     70, 80, 90],
             'Year': [2017, 2017, 2017,
                      2018, 2018, 2018,
                      2019, 2019, 2019]}

data = pd.DataFrame(data=data_dict)

# dat munging
df = data.copy()
df['key']=df.groupby(['Year','Country']).cumcount()

df1 = pd.pivot_table(df,index='Year',columns=['key', 'Country'],values='GDP')
df1 = df1.stack(level=[0,1],dropna=False).reset_index()

df1 = pd.merge(df1, data[['Country', 'Region']], how='left', on='Country').drop_duplicates()
df1.columns=['Year', 'Key', 'Country', 'GDP', 'Region']

fig = px.bar(df1, x=""Country"",
             y=""GDP"",
             color=""Region"",
             animation_frame=""Year"",
             range_y=[0, 80]
             )
fig.update_layout(xaxis={'title': '',
                        'visible': True,
                        'showticklabels': True})

fig.show()

","The following is not a direct answer to you question (as in what do i need to change in plotly), but rather focuses on consistent data in you DataFrame.
The basic idea is that the ""Primary Key"" of each of your rows in the second example is [""Year"", ""Country""]. plotly will now expect a value for ""GDP"" as well as ""Region"" for each combination of those. The following creates a DataFrame that looks just like so (be using a MultiIndex reindexing).
unqiue_years = data[""Year""].unique()
unqiue_countries = data[""Country""].unique()

# Let's first separate the region of a country
region_per_country = data[[""Country"", ""Region""]].drop_duplicates().set_index(""Country"") 

# Removing the region
data = data[[""Year"", ""Country"", ""GDP""]].set_index([""Year"", ""Country""]) 
 
# Creating all possible ""Year"" ""Country"" combinations
data = data.reindex(pd.MultiIndex.from_product([unqiue_years, unqiue_countries]))  

# Cleanup
data = data.reset_index().rename(columns={""level_0"": ""Year"", ""level_1"": ""Country""})  

# Re-adding the region
data = data.merge(region_per_country, left_on=""Country"", right_index=True)

Running this gives us the following DataFrame (shown without the .reset_index()):
                   GDP         Region
Year Country                         
2017 Canada        NaN  North America
     China        20.0           Asia
     France        NaN         Europe
     Germany       NaN         Europe
     Korea        30.0           Asia
     Phillipines   NaN           Asia
     Thailand      NaN           Asia
     US           10.0  North America
2018 Canada        NaN  North America
     China         NaN           Asia
     France       60.0         Europe
     Germany       NaN         Europe
     Korea         NaN           Asia
     Phillipines  50.0           Asia
     Thailand      NaN           Asia
     US           40.0  North America
2019 Canada       70.0  North America
     China         NaN           Asia
     France        NaN         Europe
     Germany      80.0         Europe
     Korea         NaN           Asia
     Phillipines   NaN           Asia
     Thailand     90.0           Asia
     US            NaN  North America

which plotly will then correctly plot.
",
Plotly inconsistent issue,https://stackoverflow.com/questions/63712393,Plotly Vertical Line Wrong X value,"I am trying to add a vertical line to a plotly line plot in python and it seems to work but plotly sometimes misplaces the vertical line and I do not know why. The x-values are string timestamps of the form '10:45:21.000000' and the y-values are just integers.
Here is my code:
import plotly.graph_objects as go
import plotly.express as px

vert_line = '10:45:49.983727'

fig = px.line(data, x=""time"", y=""y"")
fig.add_shape(
        dict(
            type=""line"",
            x0=vert_line,
            y0=data['y'].min(),
            x1=vert_line,
            y1=data['y'].max(),
            line=dict(
                color=""Red"",
                width=3,
                dash=""dot"",
            )
))
fig.show()


I can post some toy data, but i noticed the behaviour is super inconsistent depending on the data I feed it. Here are some examples where I sliced the data differently. Each plot is based on the above code just slicing the data by data[:100] , data[:200], and data[:300] respectively:



Notice the vertical line changes places and is never at what its actual value is. Why is this occurring? How can I get to plot where it should be?
EDIT: As requested, here's some toy data to get you started but this issue is dependent on the exact slice of data so it won't be reproducible with just this bit of data, the actual complete dataset is larger and I don't know a practical way to share that on stackoverflow.
[{'time': '10:42:21.000000', 'y': 342688},
 {'time': '10:42:22.000000', 'y': 342700},
 {'time': '10:42:23.000000', 'y': 342681},
 {'time': '10:42:24.000000', 'y': 342680},
 {'time': '10:42:25.000000', 'y': 342692},
 {'time': '10:42:26.000000', 'y': 342696},
 {'time': '10:42:27.000000', 'y': 342699},
 {'time': '10:42:28.000000', 'y': 342727},
 {'time': '10:42:29.000000', 'y': 342725},
 {'time': '10:42:30.000000', 'y': 342731},
 {'time': '10:42:31.000000', 'y': 342735},
 {'time': '10:42:32.000000', 'y': 342750},
 {'time': '10:42:33.000000', 'y': 342750},
 {'time': '10:42:34.000000', 'y': 342725},
 {'time': '10:42:35.000000', 'y': 342700},
 {'time': '10:42:36.000000', 'y': 342725},
 {'time': '10:42:37.000000', 'y': 342725},
 {'time': '10:42:38.000000', 'y': 342700},
 {'time': '10:42:39.000000', 'y': 342700}]

",2,631,"Complete snippet at the end

I've managed to reproduce your issue, and my preliminary conclusion has to be that this is caused by a bug. I'm basing this conclusion on an assumption that  the variable 'vert_line' has a value that falls outside the x-range for your figures. And, as I will show you, the specified shape seems to be put in the middle of the figure if x0 and x1 fall out of the range displayed on the x-axis. Below I have recreated a dataset with a time value that replicates your real world data. And I've set vert_line = '00:00:00.000044'.
This works fine for the first figure where '00:00:00.000044' is included in the x-axis range:

Now see what happens if I change vert_line = '00:00:00.000044' to a value outside the displayed range. Or, as in your case, make another subset of the data with data = data[:40] that also makes the specified vert_line fall out of the range:

For apparently no reason what so ever, the shape is placed right in the middle of the figure. Just as in all your provided figures. I can't possibly fix how these things work. But you can make sure to not produce the shape if vert_line falls out of the range displayed.
Complete code:
import plotly.graph_objects as go
import plotly.express as px
import random
import numpy as np
import pandas as pd

# data
np.random.seed(4)
n = 600
data = pd.DataFrame({'time':[t[11:28] for t in pd.date_range('2020', freq='U', periods=n).format()],
                      'y':np.random.uniform(low=-1, high=1, size=n).tolist()})
data['y']=data['y'].cumsum()

#vert_line = '10:45:49.983727'
#vert_line = random.choice(data['time'].to_list())
#vert_line = '00:00:00.000256'
vert_line = '00:00:00.000044'


data = data[:40]
fig = px.line(data, x=""time"", y=""y"")
fig.add_shape(
        dict(
            type=""line"",
            x0=vert_line,
            y0=data['y'].min(),
            x1=vert_line,
            y1=data['y'].max(),
            line=dict(
                color=""Red"",
                width=3,
                dash=""dot"",
            )
))

fig.update_layout(title=vert_line)
fig.update_xaxes(tickangle=90)

fig.show()

Edit: Test for different values of vert_line and a subset of the original data
The following snippet sets up a dataset with 100 observations, selects a random vert_line value from those observations, but splits the dataset in two before the figure is produced. This way, there will only be a 50% chance that vert_line stays in the range of the figure. Run it a few times, and you'll see that the shape is shown exactly as it's supposed to be as long as vert_line can be found on the x-axis. As soon as it can't, the shape is just placed there in the middle.
import plotly.graph_objects as go
import plotly.express as px
import random
import numpy as np
import pandas as pd

# data
np.random.seed(4)
n = 100
data = pd.DataFrame({'time':[t[11:28] for t in pd.date_range('2020', freq='U', periods=n).format()],
                      'y':np.random.uniform(low=-1, high=1, size=n).tolist()})
data['y']=data['y'].cumsum()

#vert_line = '10:45:49.983727'
vert_line = random.choice(data['time'].to_list())
#vert_line = '00:00:00.000256'
#vert_line = '00:00:00.000044'


data = data[:50]
fig = px.line(data, x=""time"", y=""y"")
fig.add_shape(
        dict(
            type=""line"",
            x0=vert_line,
            y0=data['y'].min(),
            x1=vert_line,
            y1=data['y'].max(),
            line=dict(
                color=""Red"",
                width=3,
                dash=""dot"",
            )
))

fig.update_layout(title=vert_line)
fig.update_xaxes(tickangle=90)

fig.show()

",,
Plotly inconsistent issue,https://stackoverflow.com/questions/74081386,Plotly graph display issue when embeded in HTML,"So there is this weird issue with Plotly displaying graphs in my HTML webpage that I'm not sure how to solve.
Basically, I have multiple graphs I wish to display on a webpage.
So what I did was:

come up with a Plotly graph
use fig.write_html to write that graph into a HTML file
embed it into my website using &lt;iframe&gt; and style it.

And it looks something like this
Now, I need to show multiple of these same graphs, so instead of creating many HTML files, I just used one HTML file and placed all my graphs into divs of their own category. To show the div, I toggle the display of the div using display:none and display:flex in my JavaScript.
Here's the issue and I think it's best if I demonstrate it as a gif
As you can see, the graphs in the second div aren't showing properly. So let's say I refresh the website: if I wait for the webpage to load completely and toggle the display on to show the second graphs, their resolution, positioning and size gets all messed up. And if, without waiting for the webpage to load completely, I toggle to show the second graphs, their positioning and resolution becomes normal, but the first graph gets all messed up.
I'm not sure if by disabling the div, I caused the graphs to not show properly, and I don't really know a workaround. I think this is a bad way to embed graphs into my webpage, but I don't exactly have a lot of time to change it, so let me know a quick fix if there is one. Thanks
Edit: the graphs displays very inconsistently. Sometimes it shows properly but other times it gets all messed up, so I'm not very sure what's going on.
",1,367,"Ok so I think this is more of a workaround but I got a solution.
I first set all the div's containing my graphs to visible. So it will look all messy and stuff initially.
Then I created a JS code using onLoad:
    window.onload = function () {
        mbsTopGraphContainer.style.display = ""none"";
        mbsBottomGraphContainer.style.display = ""none"";
        sglTopGraphContainer.style.display = ""none"";
        sglBottomGraphContainer.style.display = ""none"";
        fulTopGraphContainer.style.display = ""none"";
        fulBottomGraphContainer.style.display = ""none"";
    }

where I disable the respective div's that I wanted to hide initially. So all my graphs will be visible and would be loading together, and after it finished loading, the graphs that we don't want to see yet would be hidden.
",,
Bokeh unexpected output,https://stackoverflow.com/questions/47405628,Bokeh &#39;utf8&#39; codec can&#39;t decode byte 0xe9 : unexpected end of data,"Im using Bokeh to plot a pandas Dataframe. Following is the code:

map_options = GMapOptions(lat=19.075984, lng=72.877656, map_type=""roadmap"", zoom=11)
plot = GMapPlot(x_range=DataRange1d(), y_range=DataRange1d(), map_options=map_options)


plot.api_key = ""xxxxx""
source = ColumnDataSource(
    data=dict(
        lat=[float(i) for i in data.lat],
        lon=[float(i) for i in data.lon],
        size=[int(i)/1000 for i in data['count']],
        ID = [i for i in data.merchant_id],
        Merchant = [str(i) for i in data.merchant_name],
        count = [float(i) for i in data['count']]
    )
)
hover = HoverTool(tooltips=[
    (""(x,y)"", ""($lat, $lon)""),
    (""ID"", ""$ID""),
    (""Name"", ""@Merchant""),
    (""count"",""$count"")
])


# hover.renderers.append(circle_glyph)
plot.tools.append(hover)
circle = Circle(x=""lon"", y=""lat"", size='size', fill_color=""blue"", fill_alpha=0.8, line_color=None)
plot.add_glyph(source, circle)

# plot.add_layout(labels)
plot.add_tools(PanTool(), WheelZoomTool(), BoxSelectTool())
output_file(""gmap_plot.html"")
show(plot)


In the Hovertool using the ""Name"" field throws the following error:


  UnicodeDecodeError: 'utf8' codec can't decode byte 0xe9 in position 6:
  unexpected end of data


Also commenting the ""Name"" field still gives me the error but there is an output plot.

Following is the dataframe I'm using:

    lat lon merchant_id count   merchant_name
0   18.539971   73.893963   757 777 Portobello
1   18.565766   73.910980   745 10193   The Wok Box
2   18.815427   76.775143   1058    2354    Burrito Factory
3   18.914633   72.817916   87  1985    Flamboyante
4   18.915794   72.824370   94  1116    Butterfly Pond
5   18.916473   72.826868   145 1010    Leo's Boulangerie
6   18.918923   72.828325   115 517 Brijwasi Sweets
7   18.928063   72.832888   973 613 Pandora's Box
8   18.928562   72.832353   101 64  La Folie Patisserie
9   18.929516   72.831860   961 6673    Burma Burma


From my knowledge, the merchant name has characters that's causing the error, but i've tried encoding the column with 'utf-8', 'ascii', etc. But I get the following error:

data['merchant_name'] = data['merchant_name'].str.encode('utf-8')



  UnicodeDecodeError: 'ascii' codec can't decode byte 0xe9 in position 6: ordinal not in range(128)


Any Idea on how to proceed ?
",3,21214,"The byte 0xe9 is not in pure ascii, because it is 233 (in decadical system) and ascii has only 127 symbols. In UTF-8 it is a special byte, which introduces a charecter taking next two bytes. Thus the string is probably in another encoding. For example in latin1 and latin2 the byte 0xe9 symbolizes the letter é.

And remember, first you must decode the string. You tried encode the type str, (normal string) which does not make sense. Therefore Python tried his default decode('ascii') and you got the UnicodeDecodeError on encode method.

I didn't manage to replicate the error and also I don't see any special characters in the data you provided (especially I don't see the 0xe9 byte). So I can only guess. I would try something like this:

data['merchant_name'] = data['merchant_name'].str.decode('latin1').encode('utf-8')


And last but not least please please please, when you post your code, post the complete code with all imports and everything. I never used Bokeh, and now, when I tried to replicate your error, it was time consuming to reconstruct them. (But anyway -- at the end I managed to import everything, but I didn't get your error.) 
","I solved the problem like this
tsne_df['words'] = list(w2v_model.wv.vocab.keys())[:5000]

tsne_df['words'] = tsne_df['words'].str.decode('latin1').str.encode('utf-8')

plot_tfidf.scatter(x='x', y='y', source=tsne_df)
hover = plot_tfidf.select(dict(type=HoverTool))
hover.tooltips={""word"": ""@words""}
show(plot_tfidf)

",
Bokeh unexpected result,https://stackoverflow.com/questions/55642814,Bokeh uncheck checkbox,"For now, I am trying to simply uncheck a Bokeh checkbox using a custom button. 

But I'm getting unexpected results: The boxes are not being created in the checked state and the button does not clear the checkboxes.

Is this because I'm misunderstanding the active attribute?

from bokeh.models.widgets import CheckboxGroup, Button
from bokeh.layouts import column
from bokeh.io import curdoc
from bokeh.plotting import show

checkbox_group_1 = CheckboxGroup(labels=[""Group 1 Button""], active=[1])
checkbox_group_2 = CheckboxGroup(labels=[""Group 2 Button A"", ""Group 2 Button B""], active=[1,1])
checkbox_group_3 = CheckboxGroup(labels=[""Group 3 Button A"", ""Group 3 Button B""], active=[1,1])


button = Button(label=""Foo"", button_type=""success"")

def buttonclick():
    checkbox_group_1.active = [0]
    checkbox_group_2.active = [0,0]
    checkbox_group_3.active = [0,0]

button.on_click(buttonclick)


layout=column(checkbox_group_1,checkbox_group_2,checkbox_group_3, button)
curdoc().add_root(layout)


!powershell -command {'bokeh serve --show Buttoninteraction.ipynb'}
#I'm working within Jupyter notebook.


Ideally, I'd like to have boxes get unchecked when one from another group is checked. 
Appreciate any help. 
",0,562,"The active attribute is a list that specifies the index of the selected items. So to unselect them all simply use checkbox_group.active = []

from bokeh.models.widgets import CheckboxGroup, Button
from bokeh.layouts import column
from bokeh.io import curdoc
from bokeh.plotting import show

checkbox_group_1 = CheckboxGroup(labels = [""Group 1 Button""], active = [0])
checkbox_group_2 = CheckboxGroup(labels = [""Group 2 Button A"", ""Group 2 Button B""], active = [1])
checkbox_group_3 = CheckboxGroup(labels = [""Group 3 Button A"", ""Group 3 Button B""], active = [1])

button = Button(label = ""Foo"", button_type = ""success"")

def buttonclick():
    checkbox_group_1.active = []
    checkbox_group_2.active = []
    checkbox_group_3.active = []

button.on_click(buttonclick)

layout = column(checkbox_group_1, checkbox_group_2, checkbox_group_3, button)
curdoc().add_root(layout)


Example for active value for checkbox_group_2:

value      selected
[0]        first 
[1]        second
[0, 1]     both
[]         None

",,
Bokeh strange behavior,https://stackoverflow.com/questions/41172227,bokeh layout for plot and widget arrangement,"I have a specific design in mind for my bokeh app. I am using bokeh 0.12.3 and a bokeh server to keep everything in sync. Please, have a look at my mockup:



On the left-hand side, there is a static navigation bar, the right part of the view will consist of plots, that are manually added. The amount of plot columns on the right-hand side shall change w.r.t. window size. I am well aware of the bokeh layout documentation laying out plots and widgets, but it's a bit more complicated. This is the layout I currently have:

doc_layout = layout(children=[[column(radio_buttons,
                                      cbx_buttons,
                                      div,
                                      data_table,
                                      plot,
                                      button)]],
                    sizing_mode='scale_width')
curdoc().add_root(doc_layout)


In order to add new plots I use:

doc_layout.children[-1].children.append(plot)
# appends plot to layout children [[column(..), plot]]


But the behavior is very strange, and not at all what I actually want to achieve. New plots are added on top of the column (menu panel) instead.

Here, a short example where you can try out to see what I mean:

from bokeh.io import curdoc
from bokeh.plotting import figure
from bokeh.models.sources import ColumnDataSource
from bokeh.models.widgets import Button, DataTable, TableColumn
from bokeh.layouts import layout, widgetbox, column, row

WIDTH = 200
HEIGHT = 200

def add_plot():
    p = figure(width=WIDTH, height=HEIGHT, tools=[], toolbar_location=None)
    p.line([0, 1, 2, 3, 4, 5], [0, 1, 4, 9, 16, 25])
    doc_layout.children[-1].children.append(p)

src = ColumnDataSource(dict(x=[0, 1, 2, 3, 4, 5], y=[0, 1, 4, 9, 16, 25]))
t1 = DataTable(source=src, width=WIDTH, height=HEIGHT,
               columns=[TableColumn(field='x', title='x'),
                        TableColumn(field='y', title='y')])
b = Button(label='add plot')
b.on_click(add_plot)

doc_layout = layout([[widgetbox(b, t1)]], sizing_mode='scale_width')
curdoc().add_root(doc_layout)


I am not sure what is the best solution to overcome this problem. I've tried several things, from layout() with different sizing modes, gridplot(), column()/row() in different combinations. In my previous version, where the navigation menu was placed on the top of the page instead on the left-hand side, everything seemed to work:

layout(children=[[widgetbox(radio_button, cbx_button),
                  widgetbox(data_table),
                  widgetbox(div),
                  widgetbox(button)],
                  [Spacer()]],
       sizing_mode='scale_width')

",11,10698,"You can change the last line in your callback to:

doc_layout.children[0].children[-1].children.append(p)


And change your layout to:

doc_layout = layout(sizing_mode='scale_width') 
doc_layout.children.append(row(column(widgetbox(b, t1)), column()))


But then then don't propagate exactly as you'd like. I think for that you would need to do some custom css styling.

Assuming yours is a directory format app, one option would be to make a template/index.html file where you can add a style block in the header where you could try to override css to make your plots inline-blocks or something. 

&lt;style&gt;
  .bk-whatever-class {
    ...
  }
&lt;/style&gt;


Use a developer tool on your browser to find the appropriate classes and toy with them. But perhaps not the best solution....

For widgets there is a css_classes attribute, where you could specify a class for that widget to use, but that unfortunately doesn't help with the plot canvases.

mycol = column(css_classes=['myclass'])

",,
Bokeh strange behavior,https://stackoverflow.com/questions/51505436,loading bokehJS in jupyter lab,"I am having a strange behavior with bokeh library. 

So apparently, I am trying to create some beautiful plots with bokeh. When I try to load the bokeh in JupyterLab, I get constant, ""Loading bokehJS..."" message but if I try to plot with jupyter (from bokeh.io import output_notebook, show output_notebook()), I could plot it. It looks like lab has some issues? or am I not using lab properly?

comparative screenshot attached. Dark (JupyterLab), Light(Jupyter Notebook)


",0,954,"You likely need to install the jupyterlab_bokeh extension: 

https://docs.bokeh.org/en/latest/docs/user_guide/notebook.html#jupyterlab
",,
Bokeh strange output,https://stackoverflow.com/questions/37272310,Error- Plot PDF and CDF Bokeh : unsupported operand type(s) for /: &#39;list&#39; and &#39;int&#39;,"I am trying to read a csv and calculate the PDF and CDF with Bokeh. I am getting error. The input file is keyword and freq. The distribution of the frequency is to plotted. The input below are few rows from more than 50k rows. 

Input:

#sportsnews,8
#mashupradiomx,1
#arrestobama,2
#alemanha,1
#bizeskiden,1
#musicnews,4
#costumedesign,2
#champain,1
#pacer,1
#brunner,1
#fotoviajera,1
#itsjihadstupid,1
#lesdernierssurvivants,1
#sainsburycentre,1
#alanalwaysinourheart,1
#runinapp,1
#foroporlavida,1
#kidsday,1
#momentofart,2


Code:

# -*- coding: utf-8 -*-
import numpy as np
import scipy.special
import pandas as pd

from bokeh.plotting import figure, show, output_file, vplot

df = pd.read_csv('keyword.csv', header = None)

df.columns = ['keyword','freq']

p5 = figure(title=""Weibull Distribution (λ=1, k=1.25)"", tools=""save"",
            background_fill_color=""#E8DDCB"")

lam, k = 1, 1.25

#measured = lam*(-np.log(np.random.uniform(0, 1, 1000)))**(1/k)
#hist, edges = np.histogram(measured, density=True, bins=50)

x = df['freq']
pdf = (k/lam)*(x/lam)**(k-1) * np.exp(-(x/lam)**k)
cdf = 1 - np.exp(-(x/lam)**k)

p5.quad(top=hist, bottom=0, left=edges[:-1], right=edges[1:],
       fill_color=""#036564"", line_color=""#033649"")

p5.line(x, pdf, line_color=""#D95B43"", line_width=8, alpha=0.7, legend=""PDF"")
p5.line(x, cdf, line_color=""white"", line_width=2, alpha=0.7, legend=""CDF"")

p5.legend.location = ""top_left""
p5.xaxis.axis_label = 'x'
p5.yaxis.axis_label = 'Pr(x)'

output_file('histogram.html', title=""histogram.py example"")

show(vplot(p5))


I want to plot both line plots only. 

Error: 

Traceback (most recent call last):
  File ""pdf_bokeh.py"", line 21, in &lt;module&gt;
    pdf = (k/lam)*(x/lam)**(k-1) * np.exp(-(x/lam)**k)
TypeError: unsupported operand type(s) for /: 'list' and 'int'


Edit 1: After changing x=df['freq'] I am getting strange output. 
The full input file Dropbox  The data is discrete in nature but still distribution plots do not look like the below output.

The output: This is not really any where close to what it should be.


",0,591,"Your problem is the x-axis for the theoretical distribution, you do not want to sample this at x=df['freq'], but at some pretty looking coordinates. I downloaded your dataset and was able to get something sensible with:

x = np.linspace(0, 5, 100) # 100 points between 0 and 5


You could do some fancy statistics to figure out that most of the data is below 5.
",,
Bokeh strange output,https://stackoverflow.com/questions/58642600,Save gmap plot not including google tiles (the map),"Exporting a gmap plot to png (from the Bokeh SaveTool) does not include the map tiles. Just a grey background with lat and long axes and the plot title.
The bokeh tools are also missing.

Have tried on Brave, Chrome and Safari also tried adding a server side save using export_png which works albeit with a strange height and width (even when they are passed). The server side solution is not great as it does not reflect any client side zooms or pans, just the original plot.

I have simplified the plot to just a map with no overlays, still no joy.

from bokeh.models import (GMapPlot, GMapOptions, BoxSelectTool, SaveTool, Plot)
from bokeh.io import output_notebook, show
from bokeh.plotting import figure, gmap

output_notebook()

map_options = GMapOptions(lat=-33.79822854506091, lng=151.2562823223427, map_type=""roadmap"", zoom=13)


plot = gmap(""myGoogleKey"", map_options, height_policy=""max"", width_policy=""max"")
plot.title.text = ""Sample Plot""
plot.add_tools(BoxSelectTool(), SaveTool())
show(plot)


The expected output is what is shown on the screen, the actual output (to file) shows the lat/lng axes and tickers but the entire map area is grey.

This is after allowing plenty of time for the screen load (the screen is loaded)

There are no JavaScript errors in the browser console.

If I zoom (client side) the grid shown on the output png file reflects my location and zoom level - i.e. it knows what i have done but still no map tiles.
",0,326,"The save tool only saves the HTML canvas object. Neither the toolbar, which is a separate DOM object off the canvas, or the Google Map itself, which is a separate DOM object underneath the (transparent) canvas, are included in that. This is simply an intrinsic limitation of the SaveTool. It will not be able to save Google Map tiles. 

You might try the other non-Google tile renderer options in Bokeh. Those render raster tiles directly on to the HTML canvas. Otherwise, the only options is the export_png function. 
",,
Bokeh strange result,https://stackoverflow.com/questions/54987509,Bokeh remove space between elements in gridplot,"I have a Bokeh app (bokeh==1.0.1) where I use gridplot to render a slider widgetbox and several figures. I am using sizing_mode='stretch_both'. How can I eliminate the white space between the widget row and the rest of the figure panels? Here is a screenshot:

 

I create a layout for the widget as:

blank_button = bkm.widgets.RadioButtonGroup()
blank_wb = widgetbox(blank_button, width=50, height=100, sizing_mode='fixed') # placeholder for space
date_slider_wb = widgetbox(date_slider, width=400, height=100, sizing_mode='fixed')
apply_button = bkm.widgets.Button(label=""APPLY"")
apply_wb = widgetbox(apply_button, width=100, height=100, sizing_mode='fixed')

hor_layout_date_slider = (
    row(
      column(blank_wb),
      column(date_slider_wb),
      column(apply_wb),
    )
)


Then I include hor_layout_date_slider in a gridplot as:

grid = gridplot(
  children = [
    hor_layout_date_slider,
    airtemp_fig,
    windspeed_fig,
    winddir_fig,
    interval_fig,
    precip_fig,
    pressure_fig
    ],
    ncols=1,
    sizing_mode='stretch_both',
    merge_tools=False,
)

curdoc().add_root(grid)


I am using the bokeh server and rendering the bokeh document in a single div in my html template as:

  &lt;div class=""placeholderbokehapp rounded"" id=""tbc-id""&gt;

&lt;script src=""http://localhost:5858/stormtracker_bokehapp/autoload.js?bokeh-autoload-element=1000&amp;bokeh-app-path=/stormtracker_bokehapp&amp;bokeh-absolute-url=http://localhost:5858/stormtracker_bokehapp&amp;resources=none"" id=""1000""&gt;&lt;/script&gt;
  &lt;/div&gt;


As an attempted hack solution, I have been able to pass custom css rules to define the height of the div containing hor_layout_date_slider, but there is still a ""place holder"" for this element that persists (which I cannot access with inspect elements).

I have also tried using a simple widgetbox containing only the slider (without defining height and width and using sizing_mode='stretch_both'), instead of the full hor_layout_date_slider as defined above. However, this results in the same white space following the slider element.

Strangely enough, this problem does not occur with sizing_mode='scale_width' (the slider is tight in the layout).

Is there a Bokeh setting I am not aware of to control the spacing in this layout when using sizing_mode='stretch_both'?

UPDATE:

If I add the widget and the grid separately as:

curdoc().add_root(hor_layout_date_slider)
curdoc().add_root(grid)


The widget is then rendered underneath the first figure panel (you can see part of the slider widget showing through in the screenshot below).


",2,2031,"sizing_mode doesn't work well when nesting columns and rows in a grid or tabs. Just take the slider out of the grid and add separately to the root.

from bokeh.plotting import figure, curdoc
from bokeh.layouts import row, gridplot
from bokeh.models.widgets import Slider, RadioButtonGroup, Button

blank_button = RadioButtonGroup()
date_slider = Slider(start = 1, end = 10, value = 5)
apply_button = Button(label = ""APPLY"")

hor_layout_date_slider = row(blank_button, date_slider, apply_button)
airtemp_fig = figure()
windspeed_fig = figure()
grid = gridplot(children = [airtemp_fig, windspeed_fig],
                ncols = 1,
                sizing_mode = 'stretch_both',
                merge_tools = False)

curdoc().add_root(hor_layout_date_slider)
curdoc().add_root(grid)

",,
Bokeh strange result,https://stackoverflow.com/questions/57480666,"How to skew hexagonal tiles on an x,y graph, using bokeh HexTile","I am trying to create a hexmap of the UK parliamentary constituencies in python using the HexTile feature of the Bokeh library. I am attempting to emulate the Leeds ODI hexmap (https://odileeds.org/projects/hexmaps/constituencies/) and have used the q r coordinate system as supplied in their downloadable .hexjson file.

However, my plot looks skewed.

I am aware that the q r coordinate system that us used in the Bokeh HexTile feature may be different from the one that is used by the Leeds ODI. Hence why I have inverted the r value as before this plotted at a strange angle.

from bokeh.models import ColumnDataSource, Plot, LinearAxis, Grid, HoverTool
from bokeh.models.glyphs import HexTile
from bokeh.io import curdoc, show
import pandas as pd

df = pd.read_csv('/Users/georgefry/Documents/data_science/uk_pol/hex_map/cons_hex_coords.csv')

df['r'] = df['r'] * -1

df['q'] = df['q']

source = ColumnDataSource(df)

hover = HoverTool(tooltips=[('Code', '@code')])

plot = Plot(
    title=None, plot_width=300, plot_height=300,
    min_border=0, toolbar_location=None, tools=[hover])

glyph = HexTile(q=""q"", r=""r"", size=1, fill_color=""#fb9a99"", line_color=""white"")
plot.add_glyph(source, glyph)

xaxis = LinearAxis()
plot.add_layout(xaxis, 'below')

yaxis = LinearAxis()
plot.add_layout(yaxis, 'left')

plot.add_layout(Grid(dimension=1, ticker=xaxis.ticker))
plot.add_layout(Grid(dimension=0, ticker=yaxis.ticker))

curdoc().add_root(plot)

show(plot)


I appreciate that this is as much a geometry problem as a Bokeh-specific problem. I am also aware that the solution would be found by progressively skewing everything below the origin on the y-axis to the left, and everything above to the right. However, I have tried a number of transformations but none of them produced the desired result. 

The desired result is something similar to the Leeds ODI link.


",1,333,"If you look at this image from the Bokeh docs:



You can see the ""q=0"" axis goes up and to the left. There are unfortunately different conventions, and some systems have a ""q=0"" axis that goes up and to the right. I think to convert, you will need to successively add 1 to the q value for every row in your data i.e. q+=1 for all the hexes in the first row, then q+=2 for all the hexes in the second row, etc... including taking in to account and ""missing"" rows of data (though you don't appear to have any in your particular data set).

Assuming that works, please open an issue on GitHub, perhaps we can provide some sort of adapter for this situation.
",,
Bokeh strange result,https://stackoverflow.com/questions/54504904,How to handle overlaps in shapefile with Bokeh color map,"I would like to generate a chloropleth map with Bokeh. 
I have my dataset ready with France departments and their population. I also downloaded France departments shapefile.

After a first trial, I found my pallet being wrongly applied on departments (some being darker than others with a lower population).

I found this quite strange and set the same population to all departements just to check and I found that not all departments are having the same color! Find below my code

data = gdf.join(df)
# apply same population per department
data.population = 5678

geo_src = bm.GeoJSONDataSource(geojson=data.to_json())

# set up a log colormap
cmap = bm.LogColorMapper(
    palette=bokeh.palettes.Blues9[::-1], # reverse the palette
)


# define web tools
TOOLS = ""pan,wheel_zoom,box_zoom,reset,hover,save""

# set up bokeh figure
p = figure(
    title=""Population"", 
    tools=TOOLS,
    toolbar_location=""below"",
    x_axis_location=None, 
    y_axis_location=None, 
    width=800, 
    height=800
)

# remove the grid
p.grid.grid_line_color = None

# core part !
p.patches(
    'xs', 'ys', 
    fill_alpha=0.7, 
    fill_color={'field': 'population', 'transform': cmap},
    line_color='black', 
    line_width=0.5, 
    source=geo_src
)

# show plot
show(p)



See result,



My guess is that those darker departements are having overlap shapes and Bokeh applied twice the population making them darker...

I tried to find a way to remove overlaps from a shapefile (not being successful so far) but I'm wondering is there a way to configure Bokeh to ask it not to sum up overlaps?
",0,368,"I had the same problem and I have a solution.
The problem is a bug in Bokeh using p.patches when a patch is describing multiple polygons using nan separators in the x data list of boundaries. I'll have to figure out a way to report the bug later.
The solution is to use `p.multi_polygons' instead.  Their multiple polygons can be included in each entry without suffering from the bug.  https://docs.bokeh.org/en/latest/docs/reference/models/glyphs/multi_polygons.html
I haven't worked with geojson much but I have this working with GeoPandas.  The following is a working example using data from https://www.naturalearthdata.com/downloads/:
import geopandas as gpd
from shapely.geometry.polygon import Polygon
from shapely.geometry.multipolygon import MultiPolygon
from bokeh.io import show, output_file
from bokeh.models import  MultiPolygons, ColorMapper, LinearColorMapper
from bokeh.palettes import Inferno256 as palette
from bokeh.plotting import figure
from bokeh.models import ColumnDataSource
from bokeh.layouts import row, column

# Define the function I gave earlier
def get_MultiPoly(mpoly,coord_type='x'):
    """"""Returns the coordinates ('x' or 'y') for the exterior and interior of MultiPolygon digestible by multi_polygons in Bokeh""""""
    
    if coord_type == 'x':
        i=0
    elif coord_type == 'y':
        i=1
    
    # Get the x or y coordinates
    c = [] 
    if isinstance(mpoly,Polygon):
        mpoly = [mpoly]
    for poly in mpoly: # the polygon objects return arrays, it's important they be lists or Bokeh fails
        exterior_coords = poly.exterior.coords.xy[i].tolist()
        
        interior_coords = []
        for interior in poly.interiors:
            if isinstance(interior.coords.xy[i],list):
                interior_coords += [interior.coords.xy[i]]
            else:
                interior_coords += [interior.coords.xy[i].tolist()]
        c.append([exterior_coords, *interior_coords])
    return c

# Define input/output files and get data, index by name
output_file(""tmp.html"")

map_world_gpd = gpd.read_file('map_data/ne_10m_admin_0_countries/ne_10m_admin_0_countries.shp')
map_world_gpd.set_index('NAME_EN',inplace=True)

# Parse all countries into a ColumnDataSource
N=len(map_world_gpd) 
source3 = ColumnDataSource({ 
    'x': [get_MultiPoly(map_world_gpd.iloc[i]['geometry'],'x') for i in range(0,N)],
    'y': [get_MultiPoly(map_world_gpd.iloc[i]['geometry'],'y') for i in range(0,N)],
    'n': [i for i in range(0,N)],
    'name':[map_world_gpd.iloc[i]['NAME'] for i in range(0,N)],
    })

# Plot all the countries
p = figure(title=""map_exploration"",match_aspect=True,aspect_ratio=2,
            tooltips=[
                (""Name"",'@name')
                ],
            )

p.multi_polygons(xs='x',ys='y', source=source3,
          fill_color={'field': 'n', 'transform': LinearColorMapper(palette=palette,low=0,high=len(source3.data['x']))},
          fill_alpha=0.5, line_color=""black"", line_width=0.5)

show(row(p,sizing_mode = 'scale_width'))

If you are interested in the bug, then here is an example showing the bug:
map_states_gpd = gpd.read_file('map_data/cb_2019_us_state_20m.shp')
map_states_gpd.set_index('NAME',inplace=True)

map_world_gpd = gpd.read_file('map_data/ne_10m_admin_0_countries/ne_10m_admin_0_countries.shp')
map_world_gpd.set_index('NAME_EN',inplace=True)
map_world_gpd = map_world_gpd.to_crs(epsg=3395)
gsource = map_world_gpd.loc['Spain']['geometry']



output_file(""tmp.html"")

def get_PolyCoords(poly,coord_type='x'):
    """"""Returns the coordinates ('x' or 'y') of """"""
    
    if coord_type == 'x':
        # Get the x coordinates of the exterior
        return list( poly.exterior.coords.xy[0] )
    elif coord_type == 'y':
        # Get the y coordinates of the exterior
        return list( poly.exterior.coords.xy[1] )
    
def get_MultiPoly(mpoly,coord_type='x'):
    """"""Returns the coordinates ('x' or 'y') for the exterior and interior of MultiPolygon digestible by multi_polygons in Bokeh""""""
    
    if coord_type == 'x':
        i=0
    elif coord_type == 'y':
        i=1
    
    # Get the x or y coordinates
    c = [] 
    if isinstance(mpoly,Polygon):
        mpoly = [mpoly]
    for poly in mpoly: # the polygon objects return arrays, it's important they be lists or Bokeh fails
        exterior_coords = poly.exterior.coords.xy[i].tolist()
        
        interior_coords = []
        for interior in poly.interiors:
            if isinstance(interior.coords.xy[i],list):
                interior_coords += [interior.coords.xy[i]];
            else:
                interior_coords += [interior.coords.xy[i].tolist()]
        c.append([exterior_coords, *interior_coords])
    return c
    
    
def get_MultiPolyCords(mpoly,coord_type='x'):
    """"""Returns the coordinates ('x' or 'y') of edges of a MultiPolygon exterior""""""
    if isinstance(mpoly,Polygon):
        mpoly = [mpoly]
    if coord_type == 'x':
        # Get the x coordinates of the exterior
        x = [] 
        for poly in mpoly:
            x.append(get_PolyCoords(poly,coord_type))
        return x
    elif coord_type == 'y':
        # Get the y coordinates of the exterior
        y = [] 
        for poly in mpoly:
            y.append(get_PolyCoords(poly,coord_type))
        return y
    
def concat_MultiPolyCords(mpoly,coord_type='x'):
    """"""Returns the coordinates ('x' or 'y') of edges of a MultiPolygon exterior""""""
    if isinstance(mpoly,Polygon):
        mpoly = [mpoly]
    if coord_type == 'x':
        # Get the x coordinates of the exterior
        x = [] 
        for poly in mpoly:
            x.append(float('NaN'))
            x = x+ get_PolyCoords(poly,coord_type)
        return x
    elif coord_type == 'y':
        # Get the y coordinates of the exterior
        y = [] 
        for poly in mpoly:
            y.append(float('NaN'))
            y = y+ get_PolyCoords(poly,coord_type)
        return y
    
   
def plen(mp):
    if isinstance(mp,MultiPolygon):
        return len(mp)
    else:
        return 1



def plen(mpoly):
    if isinstance(mpoly,Polygon):
        return 1
    else:
        return len(mpoly)

# Plot the patches using the same color 
source1 = ColumnDataSource({
    'x': get_MultiPolyCords(gsource,'x'),
    'y': get_MultiPolyCords(gsource,'y'),
    'n': [3]*plen(gsource),
    })

# Plot the patches by combining them into one patch, separate sub patches with nan, all same color
# Note that the alpha outcome is faulty, it seems to redraw all the patches for each one
ys = [concat_MultiPolyCords(gsource,'y')]
source2 = ColumnDataSource({
    'x': [concat_MultiPolyCords(gsource,'x')],
    'y': [concat_MultiPolyCords(gsource,'y')],
    'n': [3],
    })

# Initialize our figure
p1 = figure(title=""map_exploration"",match_aspect=True,aspect_ratio=2)

# Plot grid
p1.patches('x', 'y', source=source1,
          fill_color={'field': 'n', 'transform': LinearColorMapper(palette=palette,low=0,high=plen(gsource))},
          fill_alpha=0.5, line_color=""black"", line_width=0.5)
p1.title.text = ""Map has ""+str(plen(gsource))+"" polygons, patched using ""+str(plen(source1.data['x']))+"" patches""

# Initialize our figure
p2 = figure(title=""map_exploration"",match_aspect=True,aspect_ratio=2)

# Plot grid
p2.patches('x', 'y', source=source2,
          fill_color={'field': 'n', 'transform': LinearColorMapper(palette=palette,low=0,high=plen(gsource))},
          fill_alpha=0.5, line_color=""black"", line_width=0.5)
p2.title.text = ""Map has ""+str(plen(gsource))+"" polygons, patched using ""+str(plen(source2.data['x']))+"" patches using NaN separators""

show(row(p1,p2,sizing_mode = 'scale_width',))

So don't use the second code, use the first to get the desired results.
","All right, I finally managed to understand what I did wrong.

It is not about overlap or something like that (I used QGIS to confirm there is no overlap).
Instead I noticed the departments darker than others are actually the one being splitted in several pieces!

And here is the thing; when applying the Bokeh patches, I was using a fill_alpha smaller than 1. I just had to set this param to 1 so that whatever pieces the department is made of, the color will be the same!

p.patches(
    'xs', 'ys', 
    fill_alpha=1, 
    fill_color={'field': 'population', 'transform': cmap},
    line_color='black', 
    line_width=0.5, 
    source=geo_src
)

",
matplotlib unexpected behavior,https://stackoverflow.com/questions/38800189,Why does matplotlib require setting log scale before plt.scatter() but not plt.plot()?,"I found out in this helpful answer that plt.scatter() and  plt.plot() behave differently when a logrithmic scale is used on the y axis. 

With plot, I can change to log any time before I use plt.show(), but log has to be set up-front, before the scatter method is used.

Is this just a historical and irreversible artifact in matplotlib, or is this in the 'unexpected behavior' category?



import matplotlib.pyplot as plt

X = [0.997, 2.643, 0.354, 0.075, 1.0, 0.03, 2.39, 0.364, 0.221, 0.437]
Y = [15.487507, 2.320735, 0.085742, 0.303032, 1.0, 0.025435, 4.436435,
     0.025435, 0.000503, 2.320735]

plt.figure()

plt.subplot(2,2,1)
plt.scatter(X, Y)
plt.xscale('log')
plt.yscale('log')
plt.title('scatter - scale last')   

plt.subplot(2,2,2)
plt.plot(X, Y)
plt.xscale('log')
plt.yscale('log')
plt.title('plot - scale last')   

plt.subplot(2,2,3)
plt.xscale('log')
plt.yscale('log')
plt.scatter(X, Y)
plt.title('scatter - scale first')   


plt.subplot(2,2,4)
plt.xscale('log')
plt.yscale('log')
plt.plot(X, Y)
plt.title('plot - scale first')   


plt.show()

",22,4304,"This somehow has to do with the the display area (axes limits) calculated by matplotlib. 

This behaviour is fixed by manually editing the axes range by using set_xlim and set_ylim methods.

plt.figure()
plt.scatter(X, Y)
plt.yscale('log')
plt.xscale('log')
axes = plt.gca()
axes.set_xlim([min(X),max(X)])
axes.set_ylim([min(Y),max(Y)])
plt.show()




The exact reason of this behavior is however not yet figured out by me. Suggestions are welcomed.

EDIT

As mentioned in comments section, apparently Matplotlib has identified Autoscaling has fundamental problems as a Release Critical Issue on their official Github repo, which would be fixed in upcoming versions. Thanks.
",,
matplotlib unexpected behavior,https://stackoverflow.com/questions/13132194,Type 1 fonts with log graphs,"I'm trying to use Matplotlib graphs as part of a camera-ready
submission, and the publishing house requires the use of Type 1 fonts
only.

I'm finding that the PDF backend happily outputs Type-1 fonts for
simple graphs with linear Y axes, but outputs Type-3 fonts for
logarithmic Y axes.

Using a logarithmic yscale incurs the use of mathtext, which seems to
use Type 3 fonts, presumably because of the default use of exponential
notation.  I can use an ugly hack to get around this - using
pyplot.yticks() to force the axis ticks to not use exponents - but
this would require moving the plot region to accommodate large labels
(like 10 ^ 6) or writing the axes as 10, 100, 1K, etc. so they fit.

I've tested the example below with the
matplotlib master branch as of today, as well as 1.1.1, which produces
the same behavior, so I don't know that this is a bug, probably just
unexpected behavior.

#!/usr/bin/env python
# Simple program to test for type 1 fonts. 
# Generate a line graph w/linear and log Y axes.

from matplotlib import rc, rcParams

rc('font',**{'family':'sans-serif','sans-serif':['Helvetica']})
#rc('font',**{'family':'sans-serif','sans-serif':['computer modern sans serif']})

# These lines are needed to get type-1 results:
# http://nerdjusttyped.blogspot.com/2010/07/type-1-fonts-and-matplotlib-figures.html
rcParams['ps.useafm'] = True
rcParams['pdf.use14corefonts'] = True
rcParams['text.usetex'] = False

import matplotlib.pyplot as plt

YSCALES = ['linear', 'log']

def plot(filename, yscale):
    plt.figure(1)
    xvals = range(1, 2)
    yvals = xvals
    plt.plot(xvals, yvals)
    plt.yscale(yscale)
    plt.savefig(filename + '.pdf')

if __name__ == '__main__':
    for yscale in YSCALES:
        plot('linegraph-' + yscale, yscale)


Does anyone know a clean way to get Type 1 fonts with log axes?

Thanks!
",16,5432,"This is the code I use for camera-ready submissions:

from matplotlib import pyplot as plt

def SetPlotRC():
    #If fonttype = 1 doesn't work with LaTeX, try fonttype 42.
    plt.rc('pdf',fonttype = 1)
    plt.rc('ps',fonttype = 1)

def ApplyFont(ax):

    ticks = ax.get_xticklabels() + ax.get_yticklabels()

    text_size = 14.0

    for t in ticks:
        t.set_fontname('Times New Roman')
        t.set_fontsize(text_size)

    txt = ax.get_xlabel()
    txt_obj = ax.set_xlabel(txt)
    txt_obj.set_fontname('Times New Roman')
    txt_obj.set_fontsize(text_size)

    txt = ax.get_ylabel()
    txt_obj = ax.set_ylabel(txt)
    txt_obj.set_fontname('Times New Roman')
    txt_obj.set_fontsize(text_size)

    txt = ax.get_title()
    txt_obj = ax.set_title(txt)
    txt_obj.set_fontname('Times New Roman')
    txt_obj.set_fontsize(text_size)


The fonts won't appear until you run savefig

Example:

import numpy as np

SetPlotRC()

t = np.arange(0, 2*np.pi, 0.01)
y = np.sin(t)

plt.plot(t,y)
plt.xlabel(""Time"")
plt.ylabel(""Signal"")
plt.title(""Sine Wave"")

ApplyFont(plt.gca())
plt.savefig(""sine.pdf"")

","The preferred method to get Type 1 fonts via matplotlib seems to be to use TeX for typesetting. Doing so results in all axis being typeset in the default math font, which is typically undesired but which can be avoided by using TeX-commands.

Long story short, I found this solution:

import matplotlib.pyplot as mp
import numpy as np

mp.rcParams['text.usetex'] = True #Let TeX do the typsetting
mp.rcParams['text.latex.preamble'] = [r'\usepackage{sansmath}', r'\sansmath'] #Force sans-serif math mode (for axes labels)
mp.rcParams['font.family'] = 'sans-serif' # ... for regular text
mp.rcParams['font.sans-serif'] = 'Helvetica, Avant Garde, Computer Modern Sans serif' # Choose a nice font here

fig = mp.figure()
dim = [0.1, 0.1, 0.8, 0.8]

ax = fig.add_axes(dim)
ax.text(0.001, 0.1, 'Sample Text')
ax.set_xlim(10**-4, 10**0)
ax.set_ylim(10**-2, 10**2)
ax.set_xscale(""log"")
ax.set_yscale(""log"")
ax.set_xlabel('$\mu_0$ (mA)')
ax.set_ylabel('R (m)')
t = np.arange(10**-4, 10**0, 10**-4)
y = 10*t

mp.plot(t,y)

mp.savefig('tmp.png', dpi=300)


Then results in this


Inspired by:
https://stackoverflow.com/a/20709149/4189024 and
http://wiki.scipy.org/Cookbook/Matplotlib/UsingTex
","The other answers did not work for me, but the following did:
import matplotlib.pylab as pylab

params = {'pdf.fonttype': 42}
pylab.rcParams.update(params)

"
matplotlib unexpected behavior,https://stackoverflow.com/questions/18572234,matplotlib axes.set_aspect(&#39;equal&#39;) doesn&#39;t behave like expected,"I need a figure in matplotlib where both axes are always the same length. For this I am using the option 'equal'. In most cases it works quite well and I get the expected results  (see figure 1), but when the values of the y-axis are much higher than x, the figure shows an unexpected behaviour (see figure 2). Does anyone know this behavior of matplotlib?

Danke, Jörg

        host = SubplotHost(fig, 111)
        try:
            min_x_val = min(x for x in self.x_values if x is not None)
            max_x_val = max(self.x_values)
        except ValueError:
            return

        max_y_val = list()
        for n, evaluator in enumerate(self.cleaned_evaluators):
            max_y_val.append(max(self.y_values[n]))

        # axis settings
        host.axis['left'].label.set_fontsize('small')
        host.axis['left'].major_ticklabels.set_fontsize('small')
        host.axis['bottom'].label.set_fontsize('small')
        host.axis['bottom'].major_ticklabels.set_fontsize('small')
        host.axis['bottom'].major_ticklabels.set_rotation(0)
        host.set_ylabel(y_label)
        host.set_xlabel(x_label)


        host.set_xlim(0, max_x_val)
        host.set_ylim(0, max_y_val)

        host.minorticks_on()
        host.toggle_axisline(False)
        host.axes.set_aspect('equal')
        host.grid(True, alpha=0.4)

        return fig


Figure 1:



Figure 2:


",10,15893,"equal means that the x and y dimensions are the same length in data coordinates. To obtain square axis you can set manually the aspect ratio:

ax.set_aspect(1./ax.get_data_ratio())

",,
matplotlib unexpected behavior,https://stackoverflow.com/questions/34660617,Matplotlib Python: How to add panel button,"I am using matplotlib to create a simple interactive plot where the user will be able to place markers on the plot. For that matter everything works fine.

Now I want to add a button that when pressed a certain function will be executed. For that I followed that example. But using the button causes unexpected behavior. With the button included instead of being able to add markers all markers are placed inside the button area and are not displayed at all in the graph. Which doesn't make much sense.

I am looking for a way to add a panel button like those that exist by default in every matplotlib window. Do you have any suggestion? Any other example that I could take a look into? I have seen a lot of examples but I find to it hard to navigate through the documentation to find exactly what I need. Thanks in advance. 

update

The code I use right now looks like this:

import numpy as np
import matplotlib.pyplot as plt
from matplotlib.widgets import Button

dataX = np.array([1,2,3,4,5,6,7,8,9,10])
dataY = np.array([1193,1225,1125,1644,1255,13676,2007,2008,12359,1210])

def on_click(event):
    if event.dblclick:
        plt.plot((event.xdata, event.xdata),(mean-standardDeviation, mean+standardDeviation), 'r-')
        plt.show()

def _yes(event):
    print ""yolo""

global mean, standardDeviation

# mean and standard deviation
mean = np.mean(dataY)
standardDeviation = np.std(dataY)

# plot data
plt.plot(dataX, dataY, linewidth=0.5)

plt.connect('button_press_event', on_click)

# button
axcut = plt.axes([0.9, 0.0, 0.1, 0.075])
bcut = Button(axcut, 'YES', color='red', hovercolor='green')
bcut.on_clicked(_yes)

plt.show()


When the button is not added everything works as expected. With the button I can only place markers inside the button's area. Any idea?
",2,17204,"You need to separate those two.

Let's try with subplot:

import numpy as np
import matplotlib.pyplot as plt
from matplotlib.widgets import Button

dataX = np.array([1,2,3,4,5,6,7,8,9,10])
dataY = np.array([1193,1225,1125,1644,1255,13676,2007,2008,12359,1210])

ax = plt.subplot(111)
def on_click(event):
    if event.dblclick:
       ax.plot((event.xdata, event.xdata), (mean-standardDeviation, mean+standardDeviation), 'r-')
       plt.show()

def _yes(event):
    print(""yolo"")

mean = np.mean(dataY)
standardDeviation = np.std(dataY)

ax.plot(dataX, dataY, linewidth=0.5)
plt.connect('button_press_event', on_click)

axcut = plt.axes([0.9, 0.0, 0.1, 0.075])
bcut = Button(axcut, 'YES', color='red', hovercolor='green')
bcut.on_clicked(_yes)

plt.show()


Now it should work.

But, if you accidentally double click on yes it will draw a line on a graph. So, if you change button action to right-click:

def _yes(event):
    if event.button == 3:
        print(""yolo"")


Now it's fine :)
",,
matplotlib unexpected behavior,https://stackoverflow.com/questions/69516844,Plotting two pandas time-series on the same axes with matplotlib - unexpected behavior,"I'm working with two timeseries (df1 and df2) and when I try to plot the on the same x-axis, with different y axis I get unexpected behavior.
Below the code and data.
dates1 = ['2021-08-26', '2021-08-27', '2021-08-30', '2021-08-31',
               '2021-09-01', '2021-09-02', '2021-09-03', '2021-09-07',
               '2021-09-08', '2021-09-09', '2021-09-10', '2021-09-13',
               '2021-09-14', '2021-09-15', '2021-09-16', '2021-09-17',
               '2021-09-20', '2021-09-21', '2021-09-22', '2021-09-23',
               '2021-09-24', '2021-09-27', '2021-09-28', '2021-09-29',
               '2021-09-30', '2021-10-01', '2021-10-04', '2021-10-05',
               '2021-10-06', '2021-10-07', '2021-10-08']


dates2 = ['2021-08-29', '2021-09-05', '2021-09-12', '2021-09-19',
               '2021-09-26']

y1 = np.random.randn(len(dates1)).cumsum()
y2 = np.random.randn(len(dates2)).cumsum()

df1 = pd.DataFrame({'date':pd.to_datetime(dates1), 'y1':y1})
df1.set_index('date', inplace=True)

df2 = pd.DataFrame({'date':pd.to_datetime(dates2), 'y2':y2})
df2.set_index('date', inplace=True)

When plotting the two datasets together either I see no plot (first plot) or I see the y data resampled in some way I don't understand (second plot). If I plot the data separately there is no issue (third &amp; fourth plots).
fig, axs = plt.subplots(1,4, figsize=[12,4])

df1.plot(ax=axs[0])
df2.plot(ax=axs[0], secondary_y=True)

df2.plot(ax=axs[1])
df1.plot(ax=axs[1], secondary_y=True)

df1.y1.plot(ax=axs[2])
df2.y2.plot(ax=axs[3])

plt.tight_layout()


",2,2155,"
pandas bug: #43972
The issue is how pandas deals with the xticks for different spans of datetimes.

Currently dates2 is less than one month. As you can see on the plots with pandas.DataFrame.plot, when the span is less than a month, the format is different. If dates2 spans at least a month, the issue doesn't occur. (e.g. dates2 = ['2021-08-29', '2021-09-05', '2021-09-12', '2021-09-19', '2021-09-26', '2021-09-29']).


Using secondary_y=True affects how pandas manages the ticks, because axs[0] plots correctly if secondary_y=True is removed.

I don't know why df1 will work if df2 is first as in axs[1], but df2 won't work when df1 is first.



fig, axs = plt.subplots(1, 4, figsize=[15, 6], sharey=False, sharex=False)
axs = axs.flatten()

df1.plot(ax=axs[0])
print(f'axs[0]: {axs[0].get_xticks()}')
ax4 = axs[0].twiny() 
df2.plot(ax=ax4, color='tab:orange')
print(f'ax4: {ax4.get_xticks()}')

df2.plot(ax=axs[1], color='tab:orange')
print(f'axs[1]: {axs[1].get_xticks()}')
df1.plot(ax=axs[1], secondary_y=True)
print(f'axs[1]: {axs[1].get_xticks()}')

df1.y1.plot(ax=axs[2])
print(f'axs[2]: {axs[2].get_xticks()}')

df2.y2.plot(ax=axs[3])
print(f'axs[3]: {axs[3].get_xticks()}')

plt.tight_layout()

[output]:
axs[0]: [18871. 18878. 18885. 18892. 18901. 18908.]
ax4: [2696 2697 2700]
axs[1]: [2696 2697 2700]  # after plotting df2
axs[1]: [2696 2697 2701 2702]  # after plotting df1
axs[2]: [18871. 18878. 18885. 18892. 18901. 18908.]
axs[3]: [2696 2697 2700]


Note the difference in the printed xticks, which are the locations on the axis for each tick.




Plotting with matplotlib.pyplot.plot treats the dataframe datetime index the same.

fig, axs = plt.subplots(2, 2, figsize=[20, 12], sharey=False, sharex=False)
axs = axs.flatten()

axs[0].plot(df1.index, df1.y1, marker='.', color='tab:blue')
print(f'axs[0]: {axs[0].get_xticks()}')
ax4 = axs[0].twinx()
ax4.plot(df2.index, df2.y2, marker='.', color='tab:orange')
print(f'ax4: {ax4.get_xticks()}')

axs[1].plot(df2.index, df2.y2, marker='.', color='tab:orange')
print(f'axs[1]: {axs[1].get_xticks()}')
ax5 = axs[1].twinx()
ax5.plot(df1.index, df1.y1, marker='.', color='tab:blue')
print(f'ax5: {ax5.get_xticks()}')

axs[2].plot(df1.index, df1.y1, marker='.', color='tab:blue')
print(f'axs[2]: {axs[2].get_xticks()}')
axs[3].plot(df2.index, df2.y2, marker='.', color='tab:orange')
print(f'axs[3]: {axs[3].get_xticks()}')

[output]:
axs[0]: [18871. 18878. 18885. 18892. 18901. 18908.]
ax4: [18871. 18878. 18885. 18892. 18901. 18908.]
axs[1]: [18868. 18871. 18875. 18879. 18883. 18887. 18891. 18895.]
ax5: [18871. 18878. 18885. 18892. 18901. 18908.]
axs[2]: [18871. 18878. 18885. 18892. 18901. 18908.]
axs[3]: [18868. 18871. 18875. 18879. 18883. 18887. 18891. 18895.]


",,
matplotlib unexpected behavior,https://stackoverflow.com/questions/22721060,matplotlib: Unexpected gridspec behavior,"I try to plot two plots vertically within one figure using gridspec. The upper one is supposed to be twice as high as the lower one (i.e. the ratio is 2:1 with 3 equal vertical parts) and they have to share the x-axis scale. I have the following minimal example, which produces the correct 3-part plot, but it does not span the upper subplot twice as high as the lower one. The first plot is at the top third, the second is in the bottom third, and the middle is emptyWhere is the error?

import matplotlib.gridspec as gridspec
x = y = [1,2,3]
gs = gridspec.GridSpec(3, 1)
ax1 = plt.subplot(gs[0:1, 0])
ax1.plot(x, y)
ax2 = plt.subplot(gs[2, 0], sharex=ax1)
ax2.plot(x,y)
plt.show()

",1,4592,"When using gridspec the indexing acts similarly to numpy arrays. As such you need gs[0:2, 0] to get the output that you want.

import matplotlib.pyplot as plt
from matplotlib import gridspec

x = y = [1,2,3]

gs = gridspec.GridSpec(3, 1)

ax1 = plt.subplot(gs[0:2, 0])
ax1.plot(x, y)

ax2 = plt.subplot(gs[2, 0], sharex=ax1)
ax2.plot(x,y)

plt.show()


Note that you could use plt.tight_layout() to remove the overlap of axis


",,
matplotlib unexpected behavior,https://stackoverflow.com/questions/42302672,Requests.get in multiprocessing pool blocked by a Matplotlib figure?,"See the code example below.  The symptom is that the requests.get(url) within a multiprocessing Pool seems to be somehow blocked by a matplotlib figure().

The ingredients needed to reproduce this rather unexpected behavior are:


Use the multiprocessing Pool to apply a function to a list; a plain request.get(url) after plt.figure() runs fluently.
The function for the Pool map contains requests.get; using other ""simpler"" function such as the identity function (e.g. f in the code sample) runs fluently.
Before starting the Pool, create a matplotlib figure; without this figure, the code runs fluently.


Code example:

# matplotlib.__version__: 1.4.3
# requests.__version__: 2.9.1
# Python version:
#   2.7.12 |Anaconda 2.3.0 (x86_64)| (default, Jul  2 2016, 17:43:17)
#   [GCC 4.2.1 (Based on Apple Inc. build 5658) (LLVM build 2336.11.00)]
# uname -a:
#   Darwin myMBP 16.4.0 Darwin Kernel Version 16.4.0:
#   Thu Dec 22 22:53:21 PST 2016;
#   root:xnu-3789.41.3~3/RELEASE_X86_64 x86_64 i386 MacBookPro11,3 Darwin


from matplotlib import pyplot as plt
from multiprocessing import Pool
import requests

urls = ['http://stackoverflow.com']

# Runs as expected:
p = Pool(processes=1)
print 1, p.map(requests.get, urls)

# Runs as expected:
def f(x):
    return x
fig = plt.figure()
p = Pool(processes=1)
print 2, p.map(f, urls)

# Will not run (the p.map takes forever to run):
fig = plt.figure()
p = Pool(processes=1)
print 3, p.map(requests.get, urls)

# REPLACING the previous block with the following
# will again runs as expected:
fig = plt.figure()
print 4, requests.get(urls[0])


Output of the code sample:

1 [&lt;Response [200]&gt;]
2 ['http://stackoverflow.com']
3

",1,471,"If you are running this in windows, you will need to move all your code below def f() into a:

if __name__ == ""__main__"":


block.
",,
matplotlib unexpected behavior,https://stackoverflow.com/questions/64078805,Colab matplotlib plot style unexpected behaivor,"I'm getting some unexpected behavior when setting the matplotlib style in Google Colab. Whether or not plt.style.use is called in the first cell (which also has the import matplotlib.pyplot as plt import) or if it's called in a subsequent cell determines whether or not all of the style settings are correctly set.
NOTE: I'm referring to a fresh kernel, so in order to reproduce this behavior restart the kernel for each condition. Also, this behavior is not occurring in JupyterLab on my local machine (the style is set correctly regardless of where plt.style.use is called).
If the plt.style.use is called in a cell after the imports then everythign is set correctly:

However, if plt.style.use is called in the first cell with the imports then only some of the style settings are set and others are not:

I like to have all my imports and global settings together in the first cell, so it would be great to figure out how to get this working correctly. I'd appreciate any insights anybody might have. Thanks!
",0,394,"I tried this, and I can confirm that I got the same results. I also tried with different styles and plotting with a pandas df and seaborn. Strangely though, if you put all the code into the first cell block it all runs as expected.
Probably a bug -- post an issue on thir github.
",,
matplotlib unexpected behavior,https://stackoverflow.com/questions/68194345,Unexpected behavior from matplotlib using bar plots for floating values,"My code:
import matplotlib.pyplot as plt
a = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]
b = [8.8913, 3.9714, 2.3416, 1.5099, 1.0078, 0.6577, 0.4198, 0.2523, 0.1141]
plt.bar(a,b)
plt.show()

I can't find a problem in this code but the graph that this outputs does not show the true values.
The output:

There are several mistakes like ,the x axis should have 0.1,0.2,0.3.,.. but it starts from -0.3? also for 0.2 the height should be around 3.9 but it is around 8 here?
Question : where have I gone wrong?
",0,965,"You need to adjust the width of the bars (by default, the width is set to 0.8; with values as small as yours you can see why this becomes a problem).
Your data plotted with width set to 0.01 (plt.bar(a,b, width = 0.01)) looks like:

Both issues you mention are due to this width parameter. The bars in barplot are centered around their x-values, thus 0.1 is the center of the first bar (with witdh of 0.8, it spans from -0.3 to 0.5). Also, since this first bar is the tallest, it covers anything within that range (so all except the last bar are at least partially covered, due to overlap).
",,
matplotlib unexpected output,https://stackoverflow.com/questions/64517366,"Python, Error while installing matplotlib","OS: Windows 10
Python ver: 3.9.0
Error code:
ERROR: Command errored out with exit status 1: python setup.py egg_info Check the logs for full command output. 

I tried:
python -m pip install -U pip
python -m pip install -U matplotlib

didn't work.
and then I tried:
pip install --upgrade setuptools

didn't solve the problem.
I read on SO that maybe if I open the shell in administrator mode it could solve the problem but it didn't work too.
I saw someone mentioning ez-setup for this error code. I installed it but that didn't work too.
I don't know if it has something to do but my C directory looks like this:
C:\Users\METİNUSTA

It has an uppercase i character which sometimes can cause problems with applications. I can't change it because I am using my school's Windows key and it don't let me do any change. Because of this I installed python on D: .
Also here my pip list for extra information:
ez-setup    0.9
flake8      3.8.4
mccabe      0.6.1
pip         20.2.4
pycodestyle 2.6.0
pyflakes    2.2.0
setuptools  50.3.2
wheel       0.35.1

and finally whole error log that I get on windows powershell:
    ERROR: Command errored out with exit status 1:
     command: 'd:\python\python39\python.exe' -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'C:\\Users\\METİNUSTA\\AppData\\Local\\Temp\\pip-install-8iv10tb_\\matplotlib\\setup.py'""'""'; __file__='""'""'C:\\Users\\METİNUSTA\\AppData\\Local\\Temp\\pip-install-8iv10tb_\\matplotlib\\setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' egg_info --egg-base 'C:\Users\METİNUSTA\AppData\Local\Temp\pip-pip-egg-info-elosrn6m'
         cwd: C:\Users\METİNUSTA\AppData\Local\Temp\pip-install-8iv10tb_\matplotlib\
    Complete output (99 lines):
      WARNING: Missing build requirements in pyproject.toml for numpy&gt;=1.15 from https://files.pythonhosted.org/packages/bf/e8/15aea783ea72e2d4e51e3ec365e8dc4a1a32c9e5eb3a6d695b0d58e67cdd/numpy-1.19.2.zip#sha256=0d310730e1e793527065ad7dde736197b705d0e4c9999775f212b03c44a8484c.
      WARNING: The project does not specify a build backend, and pip cannot fall back to setuptools without 'setuptools&gt;=40.8.0' and 'wheel'.
        ERROR: Command errored out with exit status 1:
         command: 'd:\python\python39\python.exe' 'd:\python\python39\lib\site-packages\pip\_vendor\pep517\_in_process.py' prepare_metadata_for_build_wheel 'C:\Users\METNUS~1\AppData\Local\Temp\tmpqz3brme_'
             cwd: C:\Users\METİNUSTA\AppData\Local\Temp\pip-wheel-l2wpf1i8\numpy
        Complete output (49 lines):
        Error in sitecustomize; set PYTHONVERBOSE for traceback:
        SyntaxError: (unicode error) 'utf-8' codec can't decode byte 0xdd in position 0: unexpected end of data (sitecustomize.py, line 21)
        Running from numpy source directory.
        setup.py:470: UserWarning: Unrecognized setuptools command, proceeding with generating Cython sources and expanding templates
          run_build = parse_setuppy_commands()
        Error in sitecustomize; set PYTHONVERBOSE for traceback:
        SyntaxError: (unicode error) 'utf-8' codec can't decode byte 0xdd in position 0: unexpected end of data (sitecustomize.py, line 21)
        Processing numpy/random\_bounded_integers.pxd.in
        Processing numpy/random\bit_generator.pyx
        Traceback (most recent call last):
          File ""C:\Users\METİNUSTA\AppData\Local\Temp\pip-wheel-l2wpf1i8\numpy\tools\cythonize.py"", line 59, in process_pyx
            from Cython.Compiler.Version import version as cython_version
        ModuleNotFoundError: No module named 'Cython'

        During handling of the above exception, another exception occurred:

        Traceback (most recent call last):
          File ""C:\Users\METİNUSTA\AppData\Local\Temp\pip-wheel-l2wpf1i8\numpy\tools\cythonize.py"", line 235, in &lt;module&gt;
            main()
          File ""C:\Users\METİNUSTA\AppData\Local\Temp\pip-wheel-l2wpf1i8\numpy\tools\cythonize.py"", line 231, in main
            find_process_files(root_dir)
          File ""C:\Users\METİNUSTA\AppData\Local\Temp\pip-wheel-l2wpf1i8\numpy\tools\cythonize.py"", line 222, in find_process_files
            process(root_dir, fromfile, tofile, function, hash_db)
          File ""C:\Users\METİNUSTA\AppData\Local\Temp\pip-wheel-l2wpf1i8\numpy\tools\cythonize.py"", line 188, in process
            processor_function(fromfile, tofile)
          File ""C:\Users\METİNUSTA\AppData\Local\Temp\pip-wheel-l2wpf1i8\numpy\tools\cythonize.py"", line 64, in process_pyx
            raise OSError('Cython needs to be installed in Python as a module')
        OSError: Cython needs to be installed in Python as a module
        Cythonizing sources
        Traceback (most recent call last):
          File ""d:\python\python39\lib\site-packages\pip\_vendor\pep517\_in_process.py"", line 280, in &lt;module&gt;
            main()
          File ""d:\python\python39\lib\site-packages\pip\_vendor\pep517\_in_process.py"", line 263, in main
            json_out['return_val'] = hook(**hook_input['kwargs'])
          File ""d:\python\python39\lib\site-packages\pip\_vendor\pep517\_in_process.py"", line 133, in prepare_metadata_for_build_wheel
            return hook(metadata_directory, config_settings)
          File ""d:\python\python39\lib\site-packages\setuptools\build_meta.py"", line 161, in prepare_metadata_for_build_wheel
            self.run_setup()
          File ""d:\python\python39\lib\site-packages\setuptools\build_meta.py"", line 253, in run_setup
            super(_BuildMetaLegacyBackend,
          File ""d:\python\python39\lib\site-packages\setuptools\build_meta.py"", line 145, in run_setup
            exec(compile(code, __file__, 'exec'), locals())
          File ""setup.py"", line 499, in &lt;module&gt;
            setup_package()
          File ""setup.py"", line 479, in setup_package
            generate_cython()
          File ""setup.py"", line 274, in generate_cython
            raise RuntimeError(""Running cythonize failed!"")
        RuntimeError: Running cythonize failed!
        ----------------------------------------
    ERROR: Command errored out with exit status 1: 'd:\python\python39\python.exe' 'd:\python\python39\lib\site-packages\pip\_vendor\pep517\_in_process.py' prepare_metadata_for_build_wheel 'C:\Users\METNUS~1\AppData\Local\Temp\tmpqz3brme_' Check the logs for full command output.
    Traceback (most recent call last):
      File ""d:\python\python39\lib\site-packages\setuptools\installer.py"", line 126, in fetch_build_egg
        subprocess.check_call(cmd)
      File ""d:\python\python39\lib\subprocess.py"", line 373, in check_call
        raise CalledProcessError(retcode, cmd)
    subprocess.CalledProcessError: Command '['d:\\python\\python39\\python.exe', '-m', 'pip', '--disable-pip-version-check', 'wheel', '--no-deps', '-w', 'C:\\Users\\METNUS~1\\AppData\\Local\\Temp\\tmppoh8r2c9', '--quiet', 'numpy&gt;=1.15']' returned non-zero exit status 1.

    The above exception was the direct cause of the following exception:

    Traceback (most recent call last):
      File ""&lt;string&gt;"", line 1, in &lt;module&gt;
      File ""C:\Users\METİNUSTA\AppData\Local\Temp\pip-install-8iv10tb_\matplotlib\setup.py"", line 242, in &lt;module&gt;
        setup(  # Finally, pass this all along to distutils to do the heavy lifting.
      File ""d:\python\python39\lib\site-packages\setuptools\__init__.py"", line 152, in setup
        _install_setup_requires(attrs)
      File ""d:\python\python39\lib\site-packages\setuptools\__init__.py"", line 147, in _install_setup_requires
        dist.fetch_build_eggs(dist.setup_requires)
      File ""d:\python\python39\lib\site-packages\setuptools\dist.py"", line 673, in fetch_build_eggs
        resolved_dists = pkg_resources.working_set.resolve(
      File ""d:\python\python39\lib\site-packages\pkg_resources\__init__.py"", line 764, in resolve
        dist = best[req.key] = env.best_match(
      File ""d:\python\python39\lib\site-packages\pkg_resources\__init__.py"", line 1049, in best_match
        return self.obtain(req, installer)
      File ""d:\python\python39\lib\site-packages\pkg_resources\__init__.py"", line 1061, in obtain
        return installer(requirement)
      File ""d:\python\python39\lib\site-packages\setuptools\dist.py"", line 732, in fetch_build_egg
        return fetch_build_egg(self, req)
      File ""d:\python\python39\lib\site-packages\setuptools\installer.py"", line 128, in fetch_build_egg
        raise DistutilsError(str(e)) from e
    distutils.errors.DistutilsError: Command '['d:\\python\\python39\\python.exe', '-m', 'pip', '--disable-pip-version-check', 'wheel', '--no-deps', '-w', 'C:\\Users\\METNUS~1\\AppData\\Local\\Temp\\tmppoh8r2c9', '--quiet', 'numpy&gt;=1.15']' returned non-zero exit status 1.

    Edit setup.cfg to change the build options; suppress output with --quiet.

    BUILDING MATPLOTLIB
      matplotlib: yes [3.3.2]
          python: yes [3.9.0 (tags/v3.9.0:9cf6752, Oct  5 2020, 15:34:40) [MSC
                      v.1927 64 bit (AMD64)]]
        platform: yes [win32]
     sample_data: yes [installing]
           tests: no  [skipping due to configuration]
          macosx: no  [Mac OS-X only]

    ----------------------------------------
ERROR: Command errored out with exit status 1: python setup.py egg_info Check the logs for full command output.

",7,20743,"
python -m pip install --upgrade pip

pip install matplotlib==3.3.1


Worked for me.
","edit: matplotlib has now released wheels for python 3.9 so pip install --upgrade matplotlib should work.
original answer
matplotlib hasn't made a wheel yet for version 3.9 so your python attempted to build it from source. You should downgrade to python 3.8 and then everything should work
","This is because matplotlib has no 3.9 wheels ..
For convenience in the use of matplotlib, you can install anaconda.
In anaconda environment matplotlib, numpy and pandas are available by default for Python installed in Anaconda.
Or you can use lower versions of Python.
"
matplotlib unexpected output,https://stackoverflow.com/questions/37139099,iPython notebook not working in Pycharm,"I did exactly as the official guide told.

For me I didn't have this window pop up when I click the Run button

However, when I click the Run button it was just not working and showed a * in the bracket.


In terminal:

ipython notebook
[I 21:25:20.940 NotebookApp] Serving notebooks from local directory: /Users/yangyy/GitHub/Signal&amp;System
[I 21:25:20.941 NotebookApp] 0 active kernels
[I 21:25:20.941 NotebookApp] The IPython Notebook is running at: http://localhost:8888/
[I 21:25:20.941 NotebookApp] Use Control-C to stop this server and shut down all kernels (twice to skip confirmation).
[W 21:25:24.439 NotebookApp] 404 GET /api/kernels/e905abbe-caad-458e-a50b-a48b7dbcc6ab/channels?session_id=16F0F228133A41AC84E1A2A71F1064F2 (::1): Kernel does not exist: e905abbe-caad-458e-a50b-a48b7dbcc6ab
[W 21:25:24.446 NotebookApp] 404 GET /api/kernels/e905abbe-caad-458e-a50b-a48b7dbcc6ab/channels?session_id=16F0F228133A41AC84E1A2A71F1064F2 (::1) 14.19ms referer=None
[E 21:25:30.370 NotebookApp] Notebook JSON is not valid v3: Additional properties are not allowed (u'cells' was unexpected)

    Failed validating u'additionalProperties' in schema:
        {u'$schema': u'http://json-schema.org/draft-04/schema#',
         u'additionalProperties': False,
         u'definitions': {u'code_cell': {u'additionalProperties': False,
                                         u'description': u'Notebook code cell.',
                                         u'properties': {u'cell_type': {u'description': u'String identifying the type of cell.',
                                                                        u'enum': [u'code']},
                                                         u'collapsed': {u'description': u'Whether the cell is collapsed/expanded.',
                                                                        u'type': u'boolean'},
                                                         u'input': {u'$ref': u'#/definitions/misc/source'},
                                                         u'language': {u'description': u""The cell's language (always Python)"",
                                                                       u'type': u'string'},
                                                         u'metadata': {u'additionalProperties': True,
                                                                       u'description': u'Cell-level metadata.',
                                                                       u'type': u'object'},
                                                         u'outputs': {u'description': u'Execution, display, or stream outputs.',
                                                                      u'items': {u'$ref': u'#/definitions/output'},
                                                                      u'type': u'array'},
                                                         u'prompt_number': {u'description': u""The code cell's prompt number. Will be null if the cell has not been run."",
                                                                            u'minimum': 0,
                                                                            u'type': [u'integer',
                                                                                      u'null']}},
                                         u'required': [u'cell_type',
                                                       u'input',
                                                       u'outputs',
                                                       u'language'],
                                         u'type': u'object'},
                          u'display_data': {u'additionalProperties': False,
                                            u'description': u'Data displayed as a result of code cell execution.',
                                            u'patternProperties': {u'[a-zA-Z0-9]+/[a-zA-Z0-9\\-\\+\\.]+$': {u'$ref': u'#/definitions/misc/multiline_string',
                                                                                                            u'description': u'mimetype output (e.g. text/plain), represented as either an array of strings or a string.'}},
                                            u'properties': {u'html': {u'$ref': u'#/definitions/misc/multiline_string'},
                                                            u'javascript': {u'$ref': u'#/definitions/misc/multiline_string'},
                                                            u'jpeg': {u'$ref': u'#/definitions/misc/multiline_string'},
                                                            u'json': {u'$ref': u'#/definitions/misc/multiline_string'},
                                                            u'latex': {u'$ref': u'#/definitions/misc/multiline_string'},
                                                            u'metadata': {u'$ref': u'#/definitions/misc/output_metadata'},
                                                            u'output_type': {u'description': u'Type of cell output.',
                                                                             u'enum': [u'display_data']},
                                                            u'pdf': {u'$ref': u'#/definitions/misc/multiline_string'},
                                                            u'png': {u'$ref': u'#/definitions/misc/multiline_string'},
                                                            u'svg': {u'$ref': u'#/definitions/misc/multiline_string'},
                                                            u'text': {u'$ref': u'#/definitions/misc/multiline_string'}},
                                            u'required': [u'output_type'],
                                            u'type': u'object'},
                          u'heading_cell': {u'additionalProperties': False,
                                            u'description': u'Notebook heading cell.',
                                            u'properties': {u'cell_type': {u'description': u'String identifying the type of cell.',
                                                                           u'enum': [u'heading']},
                                                            u'level': {u'description': u'Level of heading cells.',
                                                                       u'minimum': 1,
                                                                       u'type': u'integer'},
                                                            u'metadata': {u'additionalProperties': True,
                                                                          u'description': u'Cell-level metadata.',
                                                                          u'type': u'object'},
                                                            u'source': {u'$ref': u'#/definitions/misc/source'}},
                                            u'required': [u'cell_type',
                                                          u'source',
                                                          u'level'],
                                            u'type': u'object'},
                          u'markdown_cell': {u'additionalProperties': False,
                                             u'description': u'Notebook markdown cell.',
                                             u'properties': {u'cell_type': {u'description': u'String identifying the type of cell.',
                                                                            u'enum': [u'markdown',
                                                                                      u'html']},
                                                             u'metadata': {u'additionalProperties': True,
                                                                           u'description': u'Cell-level metadata.',
                                                                           u'properties': {u'name': {u'$ref': u'#/definitions/misc/metadata_name'},
                                                                                           u'tags': {u'$ref': u'#/definitions/misc/metadata_tags'}},
                                                                           u'type': u'object'},
                                                             u'source': {u'$ref': u'#/definitions/misc/source'}},
                                             u'required': [u'cell_type',
                                                           u'source'],
                                             u'type': u'object'},
                          u'misc': {u'metadata_name': {u'description': u""The cell's name. If present, must be a non-empty string."",
                                                       u'pattern': u'^.+$',
                                                       u'type': u'string'},
                                    u'metadata_tags': {u'description': u""The cell's tags. Tags must be unique, and must not contain commas."",
                                                       u'items': {u'pattern': u'^[^,]+$',
                                                                  u'type': u'string'},
                                                       u'type': u'array',
                                                       u'uniqueItems': True},
                                    u'mimetype': {u'patternProperties': {u'^[a-zA-Z0-9\\-\\+]+/[a-zA-Z0-9\\-\\+]+': {u'$ref': u'#/definitions/misc/multiline_string',
                                                                                                                     u'description': u""The cell's mimetype output (e.g. text/plain), represented as either an array of strings or a string.""}}},
                                    u'multiline_string': {u'oneOf': [{u'type': u'string'},
                                                                     {u'items': {u'type': u'string'},
                                                                      u'type': u'array'}]},
                                    u'output_metadata': {u'additionalProperties': True,
                                                         u'description': u'Cell output metadata.',
                                                         u'type': u'object'},
                                    u'prompt_number': {u'description': u""The code cell's prompt number. Will be null if the cell has not been run."",
                                                       u'minimum': 0,
                                                       u'type': [u'integer',
                                                                 u'null']},
                                    u'source': {u'$ref': u'#/definitions/misc/multiline_string',
                                                u'description': u'Contents of the cell, represented as an array of lines.'}},
                          u'output': {u'oneOf': [{u'$ref': u'#/definitions/pyout'},
                                                 {u'$ref': u'#/definitions/display_data'},
                                                 {u'$ref': u'#/definitions/stream'},
                                                 {u'$ref': u'#/definitions/pyerr'}],
                                      u'type': u'object'},
                          u'pyerr': {u'additionalProperties': False,
                                     u'description': u'Output of an error that occurred during code cell execution.',
                                     u'properties': {u'ename': {u'description': u'The name of the error.',
                                                                u'type': u'string'},
                                                     u'evalue': {u'description': u'The value, or message, of the error.',
                                                                 u'type': u'string'},
                                                     u'output_type': {u'description': u'Type of cell output.',
                                                                      u'enum': [u'pyerr']},
                                                     u'traceback': {u'description': u""The error's traceback, represented as an array of strings."",
                                                                    u'items': {u'type': u'string'},
                                                                    u'type': u'array'}},
                                     u'required': [u'output_type',
                                                   u'ename',
                                                   u'evalue',
                                                   u'traceback'],
                                     u'type': u'object'},
                          u'pyout': {u'additionalProperties': False,
                                     u'description': u'Result of executing a code cell.',
                                     u'patternProperties': {u'^[a-zA-Z0-9]+/[a-zA-Z0-9\\-\\+\\.]+$': {u'$ref': u'#/definitions/misc/multiline_string',
                                                                                                      u'description': u'mimetype output (e.g. text/plain), represented as either an array of strings or a string.'}},
                                     u'properties': {u'html': {u'$ref': u'#/definitions/misc/multiline_string'},
                                                     u'javascript': {u'$ref': u'#/definitions/misc/multiline_string'},
                                                     u'jpeg': {u'$ref': u'#/definitions/misc/multiline_string'},
                                                     u'json': {u'$ref': u'#/definitions/misc/multiline_string'},
                                                     u'latex': {u'$ref': u'#/definitions/misc/multiline_string'},
                                                     u'metadata': {u'$ref': u'#/definitions/misc/output_metadata'},
                                                     u'output_type': {u'description': u'Type of cell output.',
                                                                      u'enum': [u'pyout']},
                                                     u'pdf': {u'$ref': u'#/definitions/misc/multiline_string'},
                                                     u'png': {u'$ref': u'#/definitions/misc/multiline_string'},
                                                     u'prompt_number': {u'description': u""A result's prompt number."",
                                                                        u'minimum': 0,
                                                                        u'type': [u'integer']},
                                                     u'svg': {u'$ref': u'#/definitions/misc/multiline_string'},
                                                     u'text': {u'$ref': u'#/definitions/misc/multiline_string'}},
                                     u'required': [u'output_type',
                                                   u'prompt_number'],
                                     u'type': u'object'},
                          u'raw_cell': {u'additionalProperties': False,
                                        u'description': u'Notebook raw nbconvert cell.',
                                        u'properties': {u'cell_type': {u'description': u'String identifying the type of cell.',
                                                                       u'enum': [u'raw']},
                                                        u'metadata': {u'additionalProperties': True,
                                                                      u'description': u'Cell-level metadata.',
                                                                      u'properties': {u'format': {u'description': u'Raw cell metadata format for nbconvert.',
                                                                                                  u'type': u'string'},
                                                                                      u'name': {u'$ref': u'#/definitions/misc/metadata_name'},
                                                                                      u'tags': {u'$ref': u'#/definitions/misc/metadata_tags'}},
                                                                      u'type': u'object'},
                                                        u'source': {u'$ref': u'#/definitions/misc/source'}},
                                        u'required': [u'cell_type',
                                                      u'source'],
                                        u'type': u'object'},
                          u'stream': {u'additionalProperties': False,
                                      u'description': u'Stream output from a code cell.',
                                      u'properties': {u'output_type': {u'description': u'Type of cell output.',
                                                                       u'enum': [u'stream']},
                                                      u'stream': {u'description': u'The stream type/destination.',
                                                                  u'type': u'string'},
                                                      u'text': {u'$ref': u'#/definitions/misc/multiline_string',
                                                                u'description': u""The stream's text output, represented as an array of strings.""}},
                                      u'required': [u'output_type',
                                                    u'stream',
                                                    u'text'],
                                      u'type': u'object'},
                          u'worksheet': {u'additionalProperties': False,
                                         u'properties': {u'cells': {u'description': u'Array of cells of the current notebook.',
                                                                    u'items': {u'oneOf': [{u'$ref': u'#/definitions/raw_cell'},
                                                                                          {u'$ref': u'#/definitions/markdown_cell'},
                                                                                          {u'$ref': u'#/definitions/heading_cell'},
                                                                                          {u'$ref': u'#/definitions/code_cell'}],
                                                                               u'type': u'object'},
                                                                    u'type': u'array'},
                                                         u'metadata': {u'description': u'metadata of the current worksheet',
                                                                       u'type': u'object'}},
                                         u'required': [u'cells']}},
         u'description': u'IPython Notebook v3.0 JSON schema.',
         u'properties': {u'metadata': {u'additionalProperties': True,
                                       u'description': u'Notebook root-level metadata.',
                                       u'properties': {u'kernel_info': {u'description': u'Kernel information.',
                                                                        u'properties': {u'codemirror_mode': {u'description': u'The codemirror mode to use for code in this language.',
                                                                                                             u'type': u'string'},
                                                                                        u'language': {u'description': u'The programming language which this kernel runs.',
                                                                                                      u'type': u'string'},
                                                                                        u'name': {u'description': u'Name of the kernel specification.',
                                                                                                  u'type': u'string'}},
                                                                        u'required': [u'name',
                                                                                      u'language'],
                                                                        u'type': u'object'},
                                                       u'signature': {u'description': u'Hash of the notebook.',
                                                                      u'type': u'string'}},
                                       u'type': u'object'},
                         u'nbformat': {u'description': u'Notebook format (major number). Incremented between backwards incompatible changes to the notebook format.',
                                       u'maximum': 3,
                                       u'minimum': 3,
                                       u'type': u'integer'},
                         u'nbformat_minor': {u'description': u'Notebook format (minor number). Incremented for backward compatible changes to the notebook format.',
                                             u'minimum': 0,
                                             u'type': u'integer'},
                         u'orig_nbformat': {u'description': u'Original notebook format (major number) before converting the notebook between versions.',
                                            u'minimum': 1,
                                            u'type': u'integer'},
                         u'orig_nbformat_minor': {u'description': u'Original notebook format (minor number) before converting the notebook between versions.',
                                                  u'minimum': 0,
                                                  u'type': u'integer'},
                         u'worksheets': {u'description': u'Array of worksheets',
                                         u'items': {u'$ref': u'#/definitions/worksheet'},
                                         u'type': u'array'}},
         u'required': [u'metadata',
                       u'nbformat_minor',
                       u'nbformat',
                       u'worksheets'],
         u'type': u'object'}

    On instance:
        {u'cells': [],
         u'metadata': {},
         u'nbformat': 3,
         u'nbformat_minor': 0,
         u'worksheets': [{u'cells': [{u'cell_type': u'code',
                                      u'execution_count': None,
                                      u'input': u'%matplotlib inline',
                                      u'language': u'python',
                                      u'metadata': {},
                                      u'outputs': []},
                                     {u'cell_type': u'code',
                                      u'execution_count': None,
                                      u'input': u'',
                                      u'language': u'python',
                                      u'metadata': {},
                                      u'outputs': []},
                                     {u'cell_type': u'code',
                                      u'execution_count': None,
                                      u'input': u'',
                                      u'language': u'python',
                                      u'metadata': {},
                                      u'outputs': []}]}]}
[W 21:25:30.393 NotebookApp] Notebook test.ipynb is not trusted


In Pycharm

/System/Library/Frameworks/Python.framework/Versions/2.7/bin/python2.7 /usr/local/bin/ipython notebook --no-browser --ip localhost --port 8889
[I 21:18:54.055 NotebookApp] Serving notebooks from local directory: /Users/yangyy/GitHub/Signal&amp;System
[I 21:18:54.055 NotebookApp] 0 active kernels 
[I 21:18:54.055 NotebookApp] The IPython Notebook is running at: http://localhost:8889/
[I 21:18:54.056 NotebookApp] Use Control-C to stop this server and shut down all kernels (twice to skip confirmation).
[W 21:18:56.558 NotebookApp] 404 GET /api (127.0.0.1) 27.47ms referer=None
[I 21:18:56.589 NotebookApp] Kernel started: 2ff0bf8f-4351-4423-a867-23a698f4b820
[W 21:18:56.594 NotebookApp] 404 GET /api/kernels/2ff0bf8f-4351-4423-a867-23a698f4b820/iopub (127.0.0.1) 1.52ms referer=None
[W 21:18:56.596 NotebookApp] 404 GET /api/kernels/2ff0bf8f-4351-4423-a867-23a698f4b820/shell (127.0.0.1) 1.16ms referer=None
[W 21:18:56.690 NotebookApp] 404 GET /api (127.0.0.1) 2.05ms referer=None
[I 21:18:56.703 NotebookApp] Kernel started: d1e16c68-352d-4d1a-89ed-9548a666f9ab
[W 21:18:56.712 NotebookApp] 404 GET /api/kernels/d1e16c68-352d-4d1a-89ed-9548a666f9ab/shell (127.0.0.1) 2.61ms referer=None
[W 21:18:56.715 NotebookApp] 404 GET /api/kernels/d1e16c68-352d-4d1a-89ed-9548a666f9ab/iopub (127.0.0.1) 1.78ms referer=None
[I 21:20:29.566 NotebookApp] Kernel restarted: d1e16c68-352d-4d1a-89ed-9548a666f9ab
[W 21:20:38.117 NotebookApp] 404 GET /api (127.0.0.1) 2.99ms referer=None
[I 21:20:38.132 NotebookApp] Kernel started: 792636bb-8fda-4d26-9f4f-c92a799ef23c
[W 21:20:38.138 NotebookApp] 404 GET /api/kernels/792636bb-8fda-4d26-9f4f-c92a799ef23c/shell (127.0.0.1) 2.28ms referer=None
[W 21:20:38.140 NotebookApp] 404 GET /api/kernels/792636bb-8fda-4d26-9f4f-c92a799ef23c/iopub (127.0.0.1) 1.26ms referer=None


Things are working fine when I use the browser


Anyone's run into the same problem?
",7,11264,"I found that it is because Pycharm doesn't recognize the new api of IPython 4.2.0 and I just need to uninstall the latest version IPython and install IPython 3.2.3.

Run

$ pip uninstall IPython
$ pip install 'ipython&lt;4'


in terminal and use sudo if permission denied.
",,
matplotlib unexpected output,https://stackoverflow.com/questions/57560017,Stuck when setting up to use anaconda with VS Code and Integrated Git terminal,"I want to learn Data Science and so have used some really popular Python modules likes Pandas, Matplotlib, Numpy, etc. So I clean installed Anaconda and am now using it as my default Python interpreter and also using Conda for installing packages and making virtual environments. I use VS Code as my daily text editor. But I have run into some issues when using the integrated Git terminal in VS Code with the Anaconda Python interpreter.

There are a couple of issues that I am facing. One of the first issues that I see is when I am using CMD to run Python. If I type and enter python in cmd, the Python interpreter provided by anaconda comes up. But I also get a warning:


  Warning:
  This Python interpreter is in a conda environment, but the environment has not been activated. Libraries may fail to load. To activate this environment please see https://conda.io/activation


I didn't expect to get this output. Anyway, there's another problem in VS code. But first I would like to mention that I have checked ""Add to PATH"" when installing Anaconda so no issues there. Now, when I open a new Terminal in VS Code, automatically C:/Users/User/Anaconda3/Scripts/activate is run and then conda activate base is run. But when conda activate base is run, automatically, as mentioned, I get a CommandNotFoundError. It states Your shell has not been properly configured to use 'conda activate'.
If using 'conda activate' from a batch script, change your
invocation to 'CALL conda.bat activate'

And then I am told to initialize my shell, so I did conda init bash but still no luck. And this brings me to talk about .bash_profile. I think it has to do something with this bash profile. Anyway, this is what is in my bash profile


# &gt;&gt;&gt; conda initialize &gt;&gt;&gt;
# !! Contents within this block are managed by 'conda init' !!
eval ""$('/C/Users/User/Anaconda3/Scripts/conda.exe' 'shell.bash' 'hook')""
# &lt;&lt;&lt; conda initialize &lt;&lt;&lt;



Just a summary of the problem:


Unexpected warning in CMD when running Anaconda Python interpreter
Automatically run Anaconda Scripts and conda activate base when opening new Terminal in VS Code
Conda init bash not helping


P.S I have tried using conda activate [env_name] in CMD and also in Git Bash and they work without any issues. In other words, Anaconda and Conda work perfectly outside of VS Code terminal.
",5,6015,"I have figured out the answer myself and would like to share it here. First of all at the time of writing the question I was using Git Bash as my Terminal in VS Code (am still using it). So the issue was that when I ran the command conda init bash in Git Bash or the VS Code Terminal, Conda just basically put the command used for activating Conda environments in the .bash_profile since it is sourced during logging into Bash. But the integrated Terminal in VS Code is a subshell of a Git Bash session. That is why .bash_profile is NOT sourced in VS Code since .bash_profile is only sourced during the main Bash session. The .bashrc file is the file that is sourced when creating a Terminal session in VS Code. So what you actually need to do is take the code that is put into .bash_profile by conda init bash and paste it into your .bashrc file and make the .bash_profile source that .bashrc file automatically.

So, to sum up, using conda init bash will put the conda command in the .bash_profile and it is usually sourced by Git Bash, but VS Code Git Bash terminal will use .bashrc.

So you can just cut and paste the code from .bash_profile to .bashrc (as already mentioned) or if you want, just simply follow this: put this code in your .bash_profile:

if [ -f ~/.bashrc ]; then
    source ~/.bashrc
fi


And in your .bashrc, put this code:

# &gt;&gt;&gt; conda initialize &gt;&gt;&gt;
# !! Contents within this block are managed by 'conda init' !!
eval ""$('{path_to_your_conda.exe}' 'shell.bash' 'hook')""
# &lt;&lt;&lt; conda initialize &lt;&lt;&lt;
# You can get conda.exe path by using `which conda` in Git Bash


Now, when you open up a Git Bash session in VS Code Terminal, you can use conda activate env_name to activate any environments you have.

Everything is now supposed to work as expected in VS Code terminal but I would like to further elaborate about something. If you want, you can skip the conda init bash process (NOT recommended, just read on for additional knowledge but please follow the above steps only). This is a feature that was introduced in conda 4.4.0. Till then the way of activating conda environments was by using source activate but that command was NOT cross-platform, meaning that the command could not be used in OSes like Windows.
So they made this change by introducing commands like: conda activate env_name so that conda environments become much easier to use despite the OS platform. 

conda activate also has other advantages. This is directly from their release docs:


  conda activate: The logic and mechanisms underlying environment activation have been reworked. With conda 4.4, conda activate and conda deactivate are now the preferred commands for activating and deactivating environments. You'll find they are much more snappy than the source activate and source deactivate commands from previous conda versions. The conda activate command also has advantages of (1) being universal across all OSes, shells, and platforms, and (2) not having path collisions with scripts from other packages like Python virtualenv's activate script.


I used this question as a reference. Check it out to learn more.

Having said that, using source activate env_name will still work if you are using Git Bash, even in VS Code Git Bash terminal. source activate env_name requires no prior set up or config. But it is highly recommended that you only use conda init to set everything up and then use conda activate env_name.

[NOTE]: Locating and modifying the said .bashrc and .bash_profile on Windows is usually not as simple as it is on Linux. But can be done fairly easily like this:

It goes without saying but, you should have Git Bash installed. Having Git Bash installed should, as far as I know, automatically create .bashrc or .bash_profile or maybe both. These files are called ""dotfiles"" (since they start with a dot) and these are by default hidden on most OSes and definitely on Windows. If they were auto-created by Git Bash on your system, it is most likely that they are placed in your home directory. Home directory on Windows is C:\Users\&lt;you&gt;\. With that said, follow this:


Open Git Bash and go to your home directory with: cd. Just type this and you will be in your home directory
Enter this command: ls -a and you will see all your files, even hidden ones. Look for .bash_profile and .bashrc. Both should be present. If they are, you are ready to follow the above instructions. But if one is not there or if both are missing create them using: touch .bashrc &amp;&amp; touch .bash_profile.  You should now see these files when you again type: ls -a
That's it. Now that you have your .bashrc and .bash_profile, you can follow the above instructions. Also, to access these two files quicker, open them like this with VS Code: code ~/.bashrc or code ~/.bash_profile. Now, modify these two files as per the instructions.


In the question, I have also talked about VS Code activating Conda environments automatically. There's no issue with VS Code doing that since this is the default behavior. 
I misinterpreted that as something that's an issue. But if anyone was looking to stop VS Code from automatically doing that, I would recommend trying to set this in the user settings: 

""python.terminal.activateEnvironment"": false
","EDIT: A better solution than using source activate to get conda activate commands to work in the git bash terminal in VS Code:


Run conda init in the Git Bash Terminal in VS Code
Type in bash -l in VS Code's Git Bash terminal to launch your configured shell as a login shell
You should now be able to run conda activate commands per normal!


More info: bash -l runs your ~/.profile/~/.bash_profile/~/.zprofile scripts where the conda executable is actually referenced (but in which Git Bash as a integrated terminal does not run by default and refers to). Hence, git bash does not know where to search for conda when running conda activate commands and per Arafat's explanation above, running conda init changes the git bash PATHs in this .bash_profile file, but is ineffectual as the git bash terminal in VS Code doesn't actually refer to this file! Further info in VS Code's official docs.



Supplementing the explanation of the accepted answer, I've posted a solution that worked for me here that might possibly help others (changing user settings did not solve the issue for me). Link could also point to other working solutions if the below or accepted answer above doesn't work.

NOTE: Please read Arafat's answer before attempting the source activate method below to understand why it's not normally recommended. That said leaving it up as it still solves the problem.


  Here's what worked for me using the Git Bash terminal in VS Code on
  windows in succinct steps:
  
  
  source activate env-name - You should see your line appended by the (base) tag now.
  After calling on source activate, I've found following conda activate commands to work: i.e. conda activate env2-name
  
  
  What didn't work for Git Bash (as a VS Code terminal) for me: activate
  env-name and conda activate env-name.

","A year later I am still running into this issue. The following is a streamlined and updated approach based on Arafat's answer.

Install Git Bash

Configure Git Bash to be used in VSC (see How do I use Bash on Windows from the Visual Studio Code integrated terminal?)

Open the git bash Terminal from VSC

If conda activate is run successfully, skip the rest

run
conda init bash

Check for the exiting bash dot files:
ls -al ~/.bash*

Likely only one of '.bashrc' and '.bash_profile' exist

Check the existing dot file for conda initialization code
e.g.
cat ~/.bash_profile


This included in my case '&gt;&gt;&gt; conda initialize &gt;&gt;&gt; ...' code
(But, and this is the source of the problem, it is not executed when the terminal is opened. To check which of the files is executed simple add 'echo hello-X' to each of them.)

To fix the problem, we must create the missing dot file and make it execute the OTHER previously existing one:
tee -a ~/.bashrc &lt;&lt; END
if [ -f ~/.bash_profile ]; then
source ~/.bash_profile
fi
END

Reopen the Terminal in VSC


"
matplotlib unexpected output,https://stackoverflow.com/questions/21678394,Updates of pyOpenSSL and pandas using &#39;pip&#39; fail with &quot;TypeError: resolve() got an unexpected keyword argument &#39;replace_conflicting&#39;&quot;,"When I attempt to update or install any version of pandas or pyOpenSSL(with any instance of sudo pip --[un]install|update [--no-use-wheel] [pandas|pyOpenSSL|xattr|stevedore], using pip 1.5.4) I get:

Command python setup.py egg_info failed with error code 1 in /private/tmp/pip_build_root/pandas
Storing debug log for failure in /Users/Rax/Library/Logs/pip.log
...
TypeError: resolve() got an unexpected keyword argument 'replace_conflicting'


Why am I getting this error and what can I do to avoid it?



OSX 10.9.2; Python 2.7.5; setuptools 3.4.3.

More detail on the error report (for pandas):

Downloading/unpacking pandas
  Downloading pandas-0.13.1.tar.gz (6.1MB): 6.1MB downloaded
  Running setup.py (path:/private/tmp/pip_build_root/pandas/setup.py) egg_info for package pandas
    Traceback (most recent call last):
      File ""&lt;string&gt;"", line 17, in &lt;module&gt;
      File ""/private/tmp/pip_build_root/pandas/setup.py"", line 590, in &lt;module&gt;
        **setuptools_kwargs)
      File ""/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/distutils/core.py"", line 112, in setup
        _setup_distribution = dist = klass(attrs)
      File ""/Library/Python/2.7/site-packages/setuptools/dist.py"", line 239, in __init__
        self.fetch_build_eggs(attrs.pop('setup_requires'))
      File ""/Library/Python/2.7/site-packages/setuptools/dist.py"", line 264, in fetch_build_eggs
        replace_conflicting=True
    TypeError: resolve() got an unexpected keyword argument 'replace_conflicting'
    Complete output from command python setup.py egg_info:
    Traceback (most recent call last):

  File ""&lt;string&gt;"", line 17, in &lt;module&gt;

  File ""/private/tmp/pip_build_root/pandas/setup.py"", line 590, in &lt;module&gt;

    **setuptools_kwargs)

  File ""/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/distutils/core.py"", line 112, in setup

    _setup_distribution = dist = klass(attrs)

  File ""/Library/Python/2.7/site-packages/setuptools/dist.py"", line 239, in __init__

    self.fetch_build_eggs(attrs.pop('setup_requires'))

  File ""/Library/Python/2.7/site-packages/setuptools/dist.py"", line 264, in fetch_build_eggs

    replace_conflicting=True

TypeError: resolve() got an unexpected keyword argument 'replace_conflicting'


Contents of /Users/Rax/Library/Logs/pip.log:

Exception information:
Traceback (most recent call last):
  File ""/Library/Python/2.7/site-packages/pip/basecommand.py"", line 122, in main
    status = self.run(options, args)
  File ""/Library/Python/2.7/site-packages/pip/commands/install.py"", line 274, in run
    requirement_set.prepare_files(finder, force_root_egg_info=self.bundle, bundle=self.bundle)
  File ""/Library/Python/2.7/site-packages/pip/req.py"", line 1215, in prepare_files
    req_to_install.run_egg_info()
  File ""/Library/Python/2.7/site-packages/pip/req.py"", line 321, in run_egg_info
    command_desc='python setup.py egg_info')
  File ""/Library/Python/2.7/site-packages/pip/util.py"", line 697, in call_subprocess
    % (command_desc, proc.returncode, cwd))
InstallationError: Command python setup.py egg_info failed with error code 1 in /private/tmp/pip_build_root/pandas




Cython          - 0.20.1       - active 
Flask           - 0.10.1       - active 
Jinja2          - 2.7.2        - active 
MarkupSafe      - 0.19         - active 
PyRSS2Gen       - 1.1          - active 
Pygments        - 1.6          - active 
Python          - 2.7.5        - active development (/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/lib-dynload)
Sphinx          - 1.2.2        - active 
Twisted         - 13.2.0       - active 
Werkzeug        - 0.9.4        - active 
altgraph        - 0.11         - active 
astropy         - 0.3          - active 
backports.ssl-match-hostname - 3.4.0.2      - active 
bdist-mpkg      - 0.5.0        - active 
brewer2mpl      - 1.3.2        - active 
cffi            - 0.8.2        - active 
colorama        - 0.2.7        - active 
configobj       - 5.0.1        - active 
dill            - 0.2b1        - active 
distribute      - 0.7.3        - active 
docutils        - 0.11         - active 
ggplot          - 0.4.7        - active 
ipython         - 1.2.1        - active 
itsdangerous    - 0.23         - active 
macholib        - 1.6          - active 
matplotlib      - 1.3.1        - active 
modulegraph     - 0.11         - active 
mpltools        - 0.1          - active 
nose            - 1.3.0        - active 
numexpr         - 2.3.1        - active 
numpy           - 1.8.0        - active 
numpydoc        - 0.4          - active 
pandas          - 0.13.1       - active 
patsy           - 0.2.1        - active 
pika            - 0.9.13       - active 
pip             - 1.5.4        - active 
prettytable     - 0.7.2        - active 
progressbar     - 2.2          - active 
py2app          - 0.8          - active 
pycparser       - 2.10         - active 
pyparsing       - 2.0.1        - active 
python-dateutil - 2.2          - active 
pytz            - 2014.1.1     - active 
pyzmq           - 14.0.1       - active 
readline        - 6.2.4.1      - active 
rpy2            - 2.3.9        - active 
scikit-learn    - 0.14.1       - active 
scipy           - 0.13.3       - active 
setuptools      - 3.4.3        - active 
sphinx-argparse - 0.1.9        - active 
sphinxcontrib-napoleon - 0.2.6        - active 
sphinxcontrib-programoutput - 0.8          - active 
statsmodels     - 0.5.0        - active 
stevedore       - 0.14.1       - active 
sympy           - 0.7.5        - active 
tornado         - 3.2          - active 
virtualenv-clone - 0.2.4        - active 
virtualenv      - 1.11.4       - active 
virtualenvwrapper - 4.2          - active 
websocket-client - 0.12.0       - active 
wsgiref         - 0.1.2        - active development (/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7)
xattr           - 0.7.4        - active 
yhat            - 0.6.14       - active 
yolk            - 0.4.3        - active 
zope.interface  - 4.0.5        - active 




FWIW, I can install pandas successfully in a virtual environment. If I copy it from there back into the environment where I get this error and run nosetests pandas I get:

..SS..SS..SS..SS/Library/Python/2.7/site-packages/pandas/core/index.py:910: RuntimeWarning: tp_compare didn't return -1 or -2 for exception
  result.sort()
..SS..SS..SS..SS..SSSSSSSSSS..SS..SS..SSSS.S.S...........SS..SS..SS..........................SSSSSSSSSS.SSSS.....SSSSSSSSSSSSSS.SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS.SSSS..................S.....S........................................................................................................................................................................................................................................................................SS......................................................................................................SSSS.......................SSS..............................................................................................................................................S.......................SSSSSSS...........................................................................................S......................................................................................................................................................S.........................................S..S..S....S........................................................S......S.S.......S...S..S............S............................................................................................................................................................................................S............................................................................S........................................./Library/Python/2.7/site-packages/numpy/core/_methods.py:55: RuntimeWarning: Mean of empty slice.
  warnings.warn(""Mean of empty slice."", RuntimeWarning)
...................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................S...............................................
----------------------------------------------------------------------
Ran 4466 tests in 230.555s

OK (SKIP=330)


This is also what I get if I run the tests in the virtual environment.
",3,3700,,,
matplotlib unexpected output,https://stackoverflow.com/questions/69997327,"Tensorflow: ValueError: Input 0 is incompatible with layer model: expected shape=(None, 99), found shape=(None, 3)","I am trying to predict with a ANN classification model made in Tensorflow to classify pose keypoints with MediaPipe. The mediapipe pose tracker has 33 keypoints for x y and z coordinates for a total of 99 data points.
I am training for 4 classes.
This is running the pose embedding
import mediapipe as mp
import numpy as np
import tensorflow as tf
from tensorflow import keras
mp_pose = mp.solutions.pose


def get_center_point(landmarks, left_bodypart, right_bodypart):
  """"""Calculates the center point of the two given landmarks.""""""

  left = tf.gather(landmarks, left_bodypart.value, axis=1)
  right = tf.gather(landmarks, right_bodypart.value, axis=1)
  center = left * 0.5 + right * 0.5
  return center


def get_pose_size(landmarks, torso_size_multiplier=2.5):
  """"""Calculates pose size.

  It is the maximum of two values:
    * Torso size multiplied by `torso_size_multiplier`
    * Maximum distance from pose center to any pose landmark
  """"""
  # Hips center
  hips_center = get_center_point(landmarks, mp_pose.PoseLandmark.LEFT_HIP, 
                                 mp_pose.PoseLandmark.RIGHT_HIP)

  # Shoulders center
  shoulders_center = get_center_point(landmarks,mp_pose.PoseLandmark.LEFT_SHOULDER,
                                      mp_pose.PoseLandmark.RIGHT_SHOULDER)

  # Torso size as the minimum body size
  torso_size = tf.linalg.norm(shoulders_center - hips_center)

  # Pose center
  pose_center_new = get_center_point(landmarks,mp_pose.PoseLandmark.LEFT_HIP, 
                                     mp_pose.PoseLandmark.RIGHT_HIP)
  pose_center_new = tf.expand_dims(pose_center_new, axis=1)
  # Broadcast the pose center to the same size as the landmark vector to
  # perform substraction
  pose_center_new = tf.broadcast_to(pose_center_new,
                                    [tf.size(landmarks) // (33*3), 33, 3])

  # Dist to pose center
  d = tf.gather(landmarks - pose_center_new, 0, axis=0,
                name=""dist_to_pose_center"")
  # Max dist to pose center
  max_dist = tf.reduce_max(tf.linalg.norm(d, axis=0))

  # Normalize scale
  pose_size = tf.maximum(torso_size * torso_size_multiplier, max_dist)

  return pose_size


def normalize_pose_landmarks(landmarks):
  """"""Normalizes the landmarks translation by moving the pose center to (0,0) and
  scaling it to a constant pose size.
  """"""
  # Move landmarks so that the pose center becomes (0,0)
  pose_center = get_center_point(landmarks, mp_pose.PoseLandmark.LEFT_HIP, 
                                 mp_pose.PoseLandmark.RIGHT_HIP)
  pose_center = tf.expand_dims(pose_center, axis=1)
  # Broadcast the pose center to the same size as the landmark vector to perform
  # substraction
  pose_center = tf.broadcast_to(pose_center, 
                                [tf.size(landmarks) // (33*3), 33, 3])
  landmarks = landmarks - pose_center

  # Scale the landmarks to a constant pose size
  pose_size = get_pose_size(landmarks)
  landmarks /= pose_size

  return landmarks


def landmarks_to_embedding(landmarks_and_scores):
  """"""Converts the input landmarks into a pose embedding.""""""
  # Reshape the flat input into a matrix with shape=(33, 3)
  reshaped_inputs = keras.layers.Reshape((33, 3))(landmarks_and_scores)

  # Normalize landmarks 3D
  landmarks = normalize_pose_landmarks(reshaped_inputs[:, :, :3])

  # Flatten the normalized landmark coordinates into a vector
  embedding = keras.layers.Flatten()(landmarks)

  return embedding

Then I create the model and feed the embedding inputs to it
import csv
import cv2
import itertools
import numpy as np
import pandas as pd
import os
import sys
import tempfile
import tqdm
import mediapipe as mp
from matplotlib import pyplot as plt
from matplotlib.collections import LineCollection
import tensorflow as tf
from tensorflow import keras
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from poseEmbedding import get_center_point, get_pose_size, normalize_pose_landmarks, landmarks_to_embedding


def load_pose_landmarks(csv_path):
    #load CSV file
    dataframe = pd.read_csv(csv_path)
    df_to_process = dataframe.copy()
    
    #extract the list of class names
    classes = df_to_process.pop('class_name').unique()
    
    #extract the labels
    y = df_to_process.pop('class_no')
    
    #convert the input features and labels into float64 format for training
    X = df_to_process.astype('float64')
    y = keras.utils.to_categorical(y)
    
    return X,y, classes, dataframe
csvs_out_train_path = 'train_data.csv'
csvs_out_test_path = 'test_data.csv'

#Load training data

X, y, class_names, _ = load_pose_landmarks(csvs_out_train_path)

#split training data(X,y) into (X_train, y_train) and (X_val, y_val)
X_train, X_val, y_train, y_val = train_test_split(X,y, test_size=0.15)

X_test, y_test, _, df_test = load_pose_landmarks(csvs_out_test_path)

mp_pose = mp.solutions.pose

inputs = tf.keras.Input(shape=(99))
embedding = landmarks_to_embedding(inputs)

layer = keras.layers.Dense(128, activation=tf.nn.relu6)(embedding)
layer = keras.layers.Dropout(0.5)(layer)
layer = keras.layers.Dense(64, activation=tf.nn.relu6)(layer)
layer = keras.layers.Dropout(0.5)(layer)
outputs = keras.layers.Dense(4, activation=""softmax"")(layer)

model = keras.Model(inputs, outputs)
#model.summary()


model.compile(
    optimizer = 'adam',
    loss = 'categorical_crossentropy',
    metrics=['accuracy']
)




# Start training
history = model.fit(X_train, y_train,
                    epochs=200,
                    batch_size=16,
                    validation_data=(X_val, y_val))
model.save(""complete_epoch_model"")
                    
# Visualize the training history to see whether you're overfitting.
plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.title('Model accuracy')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['TRAIN', 'VAL'], loc='lower right')
plt.show()
loss, accuracy = model.evaluate(X_test, y_test)

The model summary prints this out:
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_18 (InputLayer)          [(None, 99)]         0           []                               
                                                                                                  
 reshape_17 (Reshape)           (None, 33, 3)        0           ['input_18[0][0]']               
                                                                                                  
 tf.__operators__.getitem_10 (S  (None, 33, 3)       0           ['reshape_17[0][0]']             
 licingOpLambda)                                                                                  
                                                                                                  
 tf.compat.v1.gather_69 (TFOpLa  (None, 3)           0           ['tf.__operators__.getitem_10[0][
 mbda)                                                           0]']                             
                                                                                                  
 tf.compat.v1.gather_70 (TFOpLa  (None, 3)           0           ['tf.__operators__.getitem_10[0][
 mbda)                                                           0]']                             
                                                                                                  
 tf.math.multiply_69 (TFOpLambd  (None, 3)           0           ['tf.compat.v1.gather_69[0][0]'] 
 a)                                                                                               
                                                                                                  
 tf.math.multiply_70 (TFOpLambd  (None, 3)           0           ['tf.compat.v1.gather_70[0][0]'] 
 a)                                                                                               
                                                                                                  
 tf.__operators__.add_31 (TFOpL  (None, 3)           0           ['tf.math.multiply_69[0][0]',    
 ambda)                                                           'tf.math.multiply_70[0][0]']    
                                                                                                  
 tf.compat.v1.size_17 (TFOpLamb  ()                  0           ['tf.__operators__.getitem_10[0][
 da)                                                             0]']                             
                                                                                                  
 tf.expand_dims_17 (TFOpLambda)  (None, 1, 3)        0           ['tf.__operators__.add_31[0][0]']
                                                                                                  
 tf.compat.v1.floor_div_17 (TFO  ()                  0           ['tf.compat.v1.size_17[0][0]']   
 pLambda)                                                                                         
                                                                                                  
 tf.broadcast_to_17 (TFOpLambda  (None, 33, 3)       0           ['tf.expand_dims_17[0][0]',      
 )                                                                'tf.compat.v1.floor_div_17[0][0]
                                                                 ']                               
                                                                                                  
 tf.math.subtract_23 (TFOpLambd  (None, 33, 3)       0           ['tf.__operators__.getitem_10[0][
 a)                                                              0]',                             
                                                                  'tf.broadcast_to_17[0][0]']     
                                                                                                  
 tf.compat.v1.gather_75 (TFOpLa  (None, 3)           0           ['tf.math.subtract_23[0][0]']    
 mbda)                                                                                            
                                                                                                  
 tf.compat.v1.gather_76 (TFOpLa  (None, 3)           0           ['tf.math.subtract_23[0][0]']    
 mbda)                                                                                            
                                                                                                  
 tf.math.multiply_75 (TFOpLambd  (None, 3)           0           ['tf.compat.v1.gather_75[0][0]'] 
 a)                                                                                               
                                                                                                  
 tf.math.multiply_76 (TFOpLambd  (None, 3)           0           ['tf.compat.v1.gather_76[0][0]'] 
 a)                                                                                               
                                                                                                  
 tf.__operators__.add_34 (TFOpL  (None, 3)           0           ['tf.math.multiply_75[0][0]',    
 ambda)                                                           'tf.math.multiply_76[0][0]']    
                                                                                                  
 tf.compat.v1.size_18 (TFOpLamb  ()                  0           ['tf.math.subtract_23[0][0]']    
 da)                                                                                              
                                                                                                  
 tf.compat.v1.gather_73 (TFOpLa  (None, 3)           0           ['tf.math.subtract_23[0][0]']    
 mbda)                                                                                            
                                                                                                  
 tf.compat.v1.gather_74 (TFOpLa  (None, 3)           0           ['tf.math.subtract_23[0][0]']    
 mbda)                                                                                            
                                                                                                  
 tf.compat.v1.gather_71 (TFOpLa  (None, 3)           0           ['tf.math.subtract_23[0][0]']    
 mbda)                                                                                            
                                                                                                  
 tf.compat.v1.gather_72 (TFOpLa  (None, 3)           0           ['tf.math.subtract_23[0][0]']    
 mbda)                                                                                            
                                                                                                  
 tf.expand_dims_18 (TFOpLambda)  (None, 1, 3)        0           ['tf.__operators__.add_34[0][0]']
                                                                                                  
 tf.compat.v1.floor_div_18 (TFO  ()                  0           ['tf.compat.v1.size_18[0][0]']   
 pLambda)                                                                                         
                                                                                                  
 tf.math.multiply_73 (TFOpLambd  (None, 3)           0           ['tf.compat.v1.gather_73[0][0]'] 
 a)                                                                                               
                                                                                                  
 tf.math.multiply_74 (TFOpLambd  (None, 3)           0           ['tf.compat.v1.gather_74[0][0]'] 
 a)                                                                                               
                                                                                                  
 tf.math.multiply_71 (TFOpLambd  (None, 3)           0           ['tf.compat.v1.gather_71[0][0]'] 
 a)                                                                                               
                                                                                                  
 tf.math.multiply_72 (TFOpLambd  (None, 3)           0           ['tf.compat.v1.gather_72[0][0]'] 
 a)                                                                                               
                                                                                                  
 tf.broadcast_to_18 (TFOpLambda  (None, 33, 3)       0           ['tf.expand_dims_18[0][0]',      
 )                                                                'tf.compat.v1.floor_div_18[0][0]
                                                                 ']                               
                                                                                                  
 tf.__operators__.add_33 (TFOpL  (None, 3)           0           ['tf.math.multiply_73[0][0]',    
 ambda)                                                           'tf.math.multiply_74[0][0]']    
                                                                                                  
 tf.__operators__.add_32 (TFOpL  (None, 3)           0           ['tf.math.multiply_71[0][0]',    
 ambda)                                                           'tf.math.multiply_72[0][0]']    
                                                                                                  
 tf.math.subtract_25 (TFOpLambd  (None, 33, 3)       0           ['tf.math.subtract_23[0][0]',    
 a)                                                               'tf.broadcast_to_18[0][0]']     
                                                                                                  
 tf.math.subtract_24 (TFOpLambd  (None, 3)           0           ['tf.__operators__.add_33[0][0]',
 a)                                                               'tf.__operators__.add_32[0][0]']
                                                                                                  
 tf.compat.v1.gather_77 (TFOpLa  (33, 3)             0           ['tf.math.subtract_25[0][0]']    
 mbda)                                                                                            
                                                                                                  
 tf.compat.v1.norm_14 (TFOpLamb  ()                  0           ['tf.math.subtract_24[0][0]']    
 da)                                                                                              
                                                                                                  
 tf.compat.v1.norm_15 (TFOpLamb  (3,)                0           ['tf.compat.v1.gather_77[0][0]'] 
 da)                                                                                              
                                                                                                  
 tf.math.multiply_77 (TFOpLambd  ()                  0           ['tf.compat.v1.norm_14[0][0]']   
 a)                                                                                               
                                                                                                  
 tf.math.reduce_max_7 (TFOpLamb  ()                  0           ['tf.compat.v1.norm_15[0][0]']   
 da)                                                                                              
                                                                                                  
 tf.math.maximum_7 (TFOpLambda)  ()                  0           ['tf.math.multiply_77[0][0]',    
                                                                  'tf.math.reduce_max_7[0][0]']   
                                                                                                  
 tf.math.truediv_7 (TFOpLambda)  (None, 33, 3)       0           ['tf.math.subtract_23[0][0]',    
                                                                  'tf.math.maximum_7[0][0]']      
                                                                                                  
 flatten_7 (Flatten)            (None, 99)           0           ['tf.math.truediv_7[0][0]']      
                                                                                                  
 dense_21 (Dense)               (None, 128)          12800       ['flatten_7[0][0]']              
                                                                                                  
 dropout_14 (Dropout)           (None, 128)          0           ['dense_21[0][0]']               
                                                                                                  
 dense_22 (Dense)               (None, 64)           8256        ['dropout_14[0][0]']             
                                                                                                  
 dropout_15 (Dropout)           (None, 64)           0           ['dense_22[0][0]']               
                                                                                                  
 dense_23 (Dense)               (None, 4)            260         ['dropout_15[0][0]']             
                                                                                                  
==================================================================================================
Total params: 21,316
Trainable params: 21,316
Non-trainable params: 0
__________________________________________________________________________________________________

Now when I try to run inference on my webcam, I get the following error from mediapipe and Tensorflow:
ValueError: Input 0 is incompatible with layer model: expected shape=(None, 99), found shape=(None, 3)

I am not sure how to fix this error as I could only train with shape of 99 as TF was giving me errors for using a shape of 3 when trying to compile. How do I fix this?
This is my inference code:
import cv2
import os
import tqdm
import numpy as np
import logging
from mediapipe.python.solutions import pose as mp_pose
from mediapipe.python.solutions import drawing_utils as mp_drawing
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import backend as K
from tensorflow.keras.utils import CustomObjectScope


def relu6(x):
  return K.relu(x, max_value=6)

logging.getLogger().setLevel(logging.CRITICAL)



cap = cv2.VideoCapture(0)

model = tf.keras.models.load_model('weights_best.hdf5', compile = True,
        custom_objects = {""relu6"": relu6})


with mp_pose.Pose() as pose_tracker:
  while cap.isOpened():
    # Get next frame of the video.
    ret, frame = cap.read()


    # Run pose tracker.
    imagefirst = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
    image = cv2.flip(imagefirst,1)

    result = pose_tracker.process(image)
    pose_landmarks = result.pose_landmarks

    # Draw pose prediction.
    if pose_landmarks is not None:
      mp_drawing.draw_landmarks(
          image,
          landmark_list=pose_landmarks,
          connections=mp_pose.POSE_CONNECTIONS)

    if pose_landmarks is not None:
      # Get landmarks.
      frame_height, frame_width = frame.shape[0], frame.shape[1]
      pose_landmarks = np.array([[lmk.x * frame_width, lmk.y * frame_height, lmk.z * frame_width]
                                 for lmk in pose_landmarks.landmark], dtype=np.float32)
      assert pose_landmarks.shape == (33, 3), 'Unexpected landmarks shape: {}'.format(pose_landmarks.shape)
      prediction = model.predict(pose_landmarks)



    # Save the output frame.
    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)

    cv2.imshow('Raw Webcam Feed', image)
    if cv2.waitKey(10) &amp; 0xFF == ord('q'):
      break

# Close output video.
cap.release()
cv2.destroyAllWindows()

# Release MediaPipe resources.
pose_tracker.close()

",2,3407,"Maybe try changing the shape of pose_landmarks from (33, 3) to (1, 99) after your assertion and before you make a prediction:
import tensorflow as tf

pose_landmarks = tf.random.normal((33, 3))
assert pose_landmarks.shape == (33, 3), 'Unexpected landmarks shape: {}'.format(pose_landmarks.shape)

pose_landmarks = tf.expand_dims(pose_landmarks, axis=0)
shape = tf.shape(pose_landmarks)
pose_landmarks = tf.reshape(pose_landmarks, (shape[0], shape[1] * shape[2]))

tf.print(pose_landmarks.shape)

TensorShape([1, 99])

",,
matplotlib unexpected output,https://stackoverflow.com/questions/55912710,Error while initializing Ray on an EC2 master node,"I am using Ray to run a parallel loop on an Ubuntu 14.04 cluster on AWS EC2. The following Python 3 script works well on my local machine with just 4 workers (imports and local initializations left out):-

ray.init()           #initialize Ray

@ray.remote
def test_loop(n):
    c=tests[n,0]                            
    tout=100                
    rc=-1   

    with tmp.TemporaryDirectory() as path: #Create a temporary directory        
        for files in filelist:        #then copy in all of the 
            sh.copy(filelist,path)    #files
        txtfile=path+'/inputf.txt'    #create the external
        fileId=open(txtfile,'w')      #data input text file,
        s='Number = '+str(c)+""\n""     #write test number,           
        fileId.write(s)
        fileId.close()                #close external parameter file,
        os.chdir(path)                #and change working directory

        try:                                    #Try running simulation:
            rc=sp.call('./simulation.run',timeout=tout,stdout=sp.DEVNULL,\
        stderr=sp.DEVNULL,shell=True)           #(must use .call for timeout)
            outdat=sio.loadmat('outputf.dat')   #get the output data struct
            rt_Data=outdat.get('rt_Data')       #extract simulation output
            err=float(rt_Data[-1])              #use final value of error
        except:                                 #If system fails to execute,
            err=deferr                          #use failure default 
        #end try

        if (err&lt;=0) or (err&gt;deferr) or (rc!=0): 
            err=deferr                          #Catch other types of failure
    return err 

if __name__=='__main__':
    result=ray.get([test_loop.remote(n) for n in range(0,ntest)])
    print(result)


The unusual bit here is that the simulation.run has to read in a different test number from an external text file when it runs. The file name is the same for all iterations of the loop, but the test number is different.  

I launched an EC2 cluster using Ray, with the number of CPUs available equal to n (I am trusting that Ray will not default to multi-threading).  Then I had to copy the filelist (which includes the Python script) from my local machine to the master node using rsync, because I couldn't do this from the config (see recent question: ""Workers not being launched on EC2 by Ray"").  Then ssh into that node, and run the script. The result is a file-finding error:-

~$ python3 test_small.py
2019-04-29 23:39:27,065 WARNING worker.py:1337 -- WARNING: Not updating worker name since `setproctitle` is not installed. Install this with `pip install setproctitle` (or ray[debug]) to enable monitoring of worker processes.
2019-04-29 23:39:27,065 INFO node.py:469 -- Process STDOUT and STDERR is being redirected to /tmp/ray/session_2019-04-29_23-39-27_3897/logs.
2019-04-29 23:39:27,172 INFO services.py:407 -- Waiting for redis server at 127.0.0.1:42930 to respond...
2019-04-29 23:39:27,281 INFO services.py:407 -- Waiting for redis server at 127.0.0.1:47779 to respond...
2019-04-29 23:39:27,282 INFO services.py:804 -- Starting Redis shard with 0.21 GB max memory.
2019-04-29 23:39:27,296 INFO node.py:483 -- Process STDOUT and STDERR is being redirected to /tmp/ray/session_2019-04-29_23-39-27_3897/logs.
2019-04-29 23:39:27,296 INFO services.py:1427 -- Starting the Plasma object store with 0.31 GB memory using /dev/shm.
(pid=3917) sh: 0: getcwd() failed: No such file or directory
    2019-04-29 23:39:44,960 ERROR worker.py:1672 -- Traceback (most recent call last):
File ""/home/ubuntu/anaconda3/lib/python3.6/site-packages/ray/worker.py"", line 909, in _process_task
self._store_outputs_in_object_store(return_object_ids, outputs)
  File ""/home/ubuntu/anaconda3/lib/python3.6/site-packages/ray/worker.py"", line 820, in _store_outputs_in_object_store
self.put_object(object_ids[i], outputs[i])
  File ""/home/ubuntu/anaconda3/lib/python3.6/site-packages/ray/worker.py"", line 375, in put_object
self.store_and_register(object_id, value)
  File ""/home/ubuntu/anaconda3/lib/python3.6/site-packages/ray/worker.py"", line 309, in store_and_register
self.task_driver_id))
  File ""/home/ubuntu/anaconda3/lib/python3.6/site-packages/ray/worker.py"", line 238, in get_serialization_context
_initialize_serialization(driver_id)
  File ""/home/ubuntu/anaconda3/lib/python3.6/site-packages/ray/worker.py"", line 1148, in _initialize_serialization
serialization_context = pyarrow.default_serialization_context()
  File ""/home/ubuntu/anaconda3/lib/python3.6/site-packages/ray/pyarrow_files/pyarrow/serialization.py"", line 326, in default_serialization_context
register_default_serialization_handlers(context)
  File ""/home/ubuntu/anaconda3/lib/python3.6/site-packages/ray/pyarrow_files/pyarrow/serialization.py"", line 321, in register_default_serialization_handlers
_register_custom_pandas_handlers(serialization_context)
  File ""/home/ubuntu/anaconda3/lib/python3.6/site-packages/ray/pyarrow_files/pyarrow/serialization.py"", line 129, in _register_custom_pandas_handlers
import pandas as pd
  File ""/home/ubuntu/anaconda3/lib/python3.6/site-packages/pandas/__init__.py"", line 42, in &lt;module&gt;
from pandas.core.api import *
  File ""/home/ubuntu/anaconda3/lib/python3.6/site-packages/pandas/core/api.py"", line 10, in &lt;module&gt;
from pandas.core.groupby import Grouper
  File ""/home/ubuntu/anaconda3/lib/python3.6/site-packages/pandas/core/groupby.py"", line 49, in &lt;module&gt;
from pandas.core.frame import DataFrame
  File ""/home/ubuntu/anaconda3/lib/python3.6/site-packages/pandas/core/frame.py"", line 74, in &lt;module&gt;
from pandas.core.series import Series
  File ""/home/ubuntu/anaconda3/lib/python3.6/site-packages/pandas/core/series.py"", line 3042, in &lt;module&gt;
import pandas.plotting._core as _gfx  # noqa
  File ""/home/ubuntu/anaconda3/lib/python3.6/site-packages/pandas/plotting/__init__.py"", line 8, in &lt;module&gt;
from pandas.plotting import _converter
  File ""/home/ubuntu/anaconda3/lib/python3.6/site-packages/pandas/plotting/_converter.py"", line 7, in &lt;module&gt;
import matplotlib.units as units
  File ""/home/ubuntu/anaconda3/lib/python3.6/site-packages/matplotlib/__init__.py"", line 1060, in &lt;module&gt;
rcParams = rc_params()
  File ""/home/ubuntu/anaconda3/lib/python3.6/site-packages/matplotlib/__init__.py"", line 892, in rc_params
fname = matplotlib_fname()
  File ""/home/ubuntu/anaconda3/lib/python3.6/site-packages/matplotlib/__init__.py"", line 736, in matplotlib_fname
for fname in gen_candidates():
  File ""/home/ubuntu/anaconda3/lib/python3.6/site-packages/matplotlib/__init__.py"", line 725, in gen_candidates
yield os.path.join(six.moves.getcwd(), 'matplotlibrc')
FileNotFoundError: [Errno 2] No such file or directory

During handling of the above exception, another exception occurred:


The problem then seems to repeat for all the other workers and finally gives up:-

AttributeError: module 'pandas' has no attribute 'core'

  This error is unexpected and should not have happened. Somehow a worker
  crashed in an unanticipated way causing the main_loop to throw an exception,
  which is being caught in ""python/ray/workers/default_worker.py"".

2019-04-29 23:44:08,489 ERROR worker.py:1672 -- A worker died or was killed while executing task 000000002d95245f833cdbf259672412d8455d89.
Traceback (most recent call last):
  File ""test_small.py"", line 82, in &lt;module&gt;
result=ray.get([test_loop.remote(n) for n in range(0,ntest)])
  File ""/home/ubuntu/anaconda3/lib/python3.6/site-packages/ray/worker.py"", line 2184, in get
raise value
ray.exceptions.RayWorkerError: The worker died unexpectedly while executing this task.


I suspect that I am not initializing Ray correctly. I tried with ray.init(redis_address=""172.31.50.149:6379"") - which was the redis address given when the cluster was formed, but the error was more or less the same. I also tried starting Ray on the master (in case it needed starting):-

~$ ray start --redis-address 172.31.50.149:6379 #Start Ray
2019-04-29 23:46:20,774 INFO services.py:407 -- Waiting for redis server at 172.31.50.149:6379 to respond...
2019-04-29 23:48:29,076 INFO services.py:412 -- Failed to connect to the redis server, retrying.


....etc.
",2,1567,"The installation of pandas and matplotlib on the master node seems to have solved the problem. Ray now initializes successfully.
",,
matplotlib unexpected output,https://stackoverflow.com/questions/49970209,Why are my histogram bars all displaying frequencies of 1,"I have a series (114 rows) with indexed timestamps and percentages (astype float). 

testseries.head()
Out[100]: 
Timestamps
2018-04-19 13:23:57-04:00    0.000161238
2018-04-06 13:59:50-04:00     -0.0169348
2018-04-04 11:39:41-04:00      0.0475188
2018-04-03 14:53:37-04:00    -0.00231244
2018-03-29 14:09:57-04:00      0.0209815
Name: Change, dtype: object


I'm trying to create a histogram of the distribution of these, as I've done several times before, but am getting an unexpected result when I call 

testseries.hist()


link to image of output hist

I've tried various options, like setting density=True, changing the number of bins, or plotting in matplotlib vs. pandas, but the result is always a series of thin bars with height equal to the maximum on the y-axis. 

What's causing this? 
",2,507,"The histogram is correctly showing you that each value appears once. In order to show something smoother, you might want to group counts by quantiles and count, displaying the histogram of the result:

testseries.groupby(pd.cut(testseries.astype(float), 10)).sum().hist()


Example

import pandas as pd
import numpy as np

testseries = pd.Series(np.random.randn(100000))

testseries.groupby(pd.cut(testseries.astype(float), 10)).sum().hist();



",,
matplotlib unexpected output,https://stackoverflow.com/questions/64001199,Python from RMarkdown - Matplotlib problems,"I'm trying to use reticulate to run some simple Python code in an RMarkdown document.  I've found that if Matplotlib is in the conda environment, I get errors when trying to run a python code chunk, but I can run Python from R directly.  Here's a simple example of what I see:
---
title: ""Reticulate Test""
date: ""9/21/2020""
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(reticulate)
use_condaenv('Toy_MPL') # this environment contain matplotlib and produces the error
#use_condaenv('Toy') # this environment does not contain matplotlib and no error
```

```{r}
# this works regardless of which environment I use
pysys &lt;- import('sys')
pysys$version
```

[1] ""3.8.5 (default, Sep  4 2020, 02:23:17) \n[Clang 10.0.0 ]""

```{python, engine.path = '/opt/miniconda2/envs/Toy_MPL/bin/python'}
# if Toy_MPL conda environment is used, the error is generated
# if Toy conda environment is used, I get the same output as above
import sys
print(sys.version)
```
Error in py_call_impl(callable, dots$args, dots$keywords) : 
  TypeError: use() got an unexpected keyword argument 'warn'

My first thought was that reticulate was not seeing the various system libraries that are installed in the conda environment lib/ folder when Matplotlib is installed - there are a LOT of dependencies that come along with Matplotlib.  I tried the following, but none worked:

Set LD_LIBRARY_PATH in .Renviron to point to the correct library path.
Call use_python() in addition to or instead of use_condaenv()
set engine.path in the Python code chunk
I tried downgrading matplotlib to v3.2 (suggested here), but that caused a new set of errors:

Error in if (has_compatible_arch &amp;&amp; has_preferred_numpy) valid_python_versions &lt;- c(valid_python_versions, : missing value where TRUE/FALSE needed

Checking NumPy, I see I have v1.19.1 (other errors suggest needing &gt;1.6).  And, reinstalling matplotlib v3.3.1 does not prevent the error.  After this ""fix"" I end up having to rebuild the entire environment.

traceback() gives me a CPP stack trace from the reticulate.so which is not interpretable.
My interpretation is that the environment created for RMarkdown does not point to the correct library locations, but I cannot determine how to set it correctly.
System info:

Mac OS Catalina 10.15.6
RStudio v1.3.1073
reticulate v1.16
conda v4.8.4
Python in conda environments v3.8.5
Matplotlib in Toy_MPL environment v3.3.1

",1,545,"I ran into a similar issue as well. I'm new to python so I used the Anaconda Navigator to manage my python environments and packages. I fixed my problem by doing the following.

Remove the reticulate package from R
Open the Anaconda Navigator and remove the r-reticulate environment from environments page. I also removed an additional conda environment that I had created with reticulate.
Re install the reticulate package and run your code

This won't help too much if you still want to use matplotlib with reticulate, but it should at least get your script running again if it doesn't use matplotlib.
","In my original question, I referred to this question in which there was a suggestion to downgrade matplotlib to version 3.2.0 because reticulate was not up to date with changes in matplotlib.  I followed up further on that suggestion and have found a resolution (for now).
TL;DR
Removing pip and conda installed versions of matplotlib, and then installation of matplotlib version 3.2.2 with conda (NOT pip) resolves the problem.  Installing matplotlib with pip leads to other errors.
Details
In the response to the other question, the suggestion was to do:
pip install matplotlib==3.2  

I tried this and ended up with other errors that I also could not track down.  So, I uninstalled matplotlib and then reinstalled it with
pip install matplotlib==3.3.1

in hopes of getting back to where I was.  This also did not work and the new errors persisted.  I then removed matplotlib completely and with pip and reinstalled version 3.3.1 with conda:
pip uninstall matplolib
conda install matplotlib

This got me back to matplotlib version 3.3.1 and the original error I mention in my question.  I then tried installing matplotlib version 3.2 with conda:
conda install matplotplib==3.2

The installed version is 3.2.2 and not 3.2.0, as suggested in the response, but when I did this, the original problem seems to be resolved.
There is clearly a difference in dependency resolution between pip and conda in this case, and conda provides a version of matplotlib that plays nicely with reticulate.  I do not at this point know what the difference is, however.
",
matplotlib unexpected output,https://stackoverflow.com/questions/39814364,tensorflow lstm model for time series,"I am trying to use tensorflow LSTMs for Time-Series predictions. I am using the modified version of lstm-for-epf.py from the repo 

import numpy as np
import pandas as pd
from matplotlib import pyplot as plt

from tensorflow.contrib import learn
from sklearn.metrics import mean_squared_error, mean_absolute_error
from lstm_predictor import generate_data, load_csvdata, lstm_model


LOG_DIR = './ops_logs'
TIMESTEPS = 10
RNN_LAYERS = [{'steps': TIMESTEPS}]
DENSE_LAYERS = [10, 10]
TRAINING_STEPS = 100000
BATCH_SIZE = 100
PRINT_STEPS = TRAINING_STEPS / 100

dateparse = lambda dates: pd.datetime.strptime(dates, '%d/%m/%Y %H:%M')
rawdata = pd.read_csv(""RealMarketPriceDataPT.csv"", 
                   parse_dates={'timeline': ['date', '(UTC)']}, 
                   index_col='timeline', date_parser=dateparse)


X, y = load_csvdata(rawdata, TIMESTEPS, seperate=False)


regressor = learn.TensorFlowEstimator(model_fn=lstm_model(TIMESTEPS, RNN_LAYERS, DENSE_LAYERS), 
                                      n_classes=0,
                                      verbose=1,  
                                      steps=TRAINING_STEPS, 
                                      optimizer='Adagrad',
                                      learning_rate=0.03,
                                      batch_size=BATCH_SIZE )




validation_monitor = learn.monitors.ValidationMonitor(X['val'], y['val'],
                                                      every_n_steps=PRINT_STEPS,
                                                      early_stopping_rounds=1000,
                                                      batch_size=BATCH_SIZE )

regressor.fit(X['train'], y['train'], monitors=[validation_monitor], logdir=LOG_DIR)


predicted = regressor.predict(X['test'])
mse = mean_absolute_error(y['test'], predicted)
print (""Error: %f"" % mse)

# plot_predicted, = plt.plot(predicted, label='predicted')
# plot_test, = plt.plot(y['test'], label='test')
# plt.legend(handles=[plot_predicted, plot_test])


It is giving the error.

Traceback (most recent call last):
  File ""lstm-for-epf.py"", line 43, in &lt;module&gt;
    regressor.fit(X['train'], y['train'], monitors=[validation_monitor], logdir=LOG_DIR)
  File ""/home/tensorflow/anaconda3/lib/python3.5/site-packages/tensorflow/contrib/learn/python/learn/estimators/base.py"", line 166, in fit
    monitors=monitors)
  File ""/home/tensorflow/anaconda3/lib/python3.5/site-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py"", line 578, in _train_model
    max_steps=max_steps)
  File ""/home/tensorflow/anaconda3/lib/python3.5/site-packages/tensorflow/contrib/learn/python/learn/graph_actions.py"", line 280, in _supervised_train
    None)
  File ""/home/tensorflow/anaconda3/lib/python3.5/site-packages/tensorflow/contrib/learn/python/learn/supervised_session.py"", line 270, in run
    run_metadata=run_metadata)
  File ""/home/tensorflow/anaconda3/lib/python3.5/site-packages/tensorflow/contrib/learn/python/learn/recoverable_session.py"", line 54, in run
    run_metadata=run_metadata)
  File ""/home/tensorflow/anaconda3/lib/python3.5/site-packages/tensorflow/contrib/learn/python/learn/coordinated_session.py"", line 70, in run
    self._coord.join(self._coordinated_threads_to_join)
  File ""/home/tensorflow/anaconda3/lib/python3.5/site-packages/tensorflow/python/training/coordinator.py"", line 357, in join
    six.reraise(*self._exc_info_to_raise)
  File ""/home/tensorflow/anaconda3/lib/python3.5/site-packages/six.py"", line 686, in reraise
    raise value
  File ""/home/tensorflow/anaconda3/lib/python3.5/site-packages/tensorflow/contrib/learn/python/learn/coordinated_session.py"", line 66, in run
    return self._sess.run(*args, **kwargs)
  File ""/home/tensorflow/anaconda3/lib/python3.5/site-packages/tensorflow/contrib/learn/python/learn/monitored_session.py"", line 107, in run
    induce_stop = monitor.step_end(monitors_step, monitor_outputs)
  File ""/home/tensorflow/anaconda3/lib/python3.5/site-packages/tensorflow/contrib/learn/python/learn/monitors.py"", line 396, in step_end
    return self.every_n_step_end(step, output)
  File ""/home/tensorflow/anaconda3/lib/python3.5/site-packages/tensorflow/contrib/learn/python/learn/monitors.py"", line 687, in every_n_step_end
    steps=self.eval_steps, metrics=self.metrics, name=self.name)
TypeError: evaluate() got an unexpected keyword argument 'batch_size'

",1,909,"Have you tried moving the ""batch_size=BATCH_SIZE"" into the fit call like: regressor.fit(...batch_size=BATCH_SIZE...)

Source with similar code:
https://github.com/tgjeon/TensorFlow-Tutorials-for-Time-Series/blob/master/lstm-for-sine-wave.ipynb
",,
matplotlib unexpected output,https://stackoverflow.com/questions/76564629,Cannot import protobuf builder when deploying Azure function,"The Problem
I have an Azure function built using the Python v1 programming model which builds a basic Keras model and trains it using some data from an Azure file share.
Tested locally and everything works.
I'm deploying to Azure using Azure Pipelines. It's invoking correctly, but then failing. Checking the detailed invocation history reveals the following error:
Result: Failure Exception: ImportError: cannot import name 'builder' from 'google.protobuf.internal' (/azure-functions-host/workers/python/3.9/LINUX/X64/google/protobuf/internal/__init__.py). 
Please check the requirements.txt file for the missing module. For more info, please refer the troubleshooting guide: https://aka.ms/functions-modulenotfound
Stack: File ""/azure-functions-host/workers/python/3.9/LINUX/X64/azure_functions_worker/dispatcher.py"", line 380, in _handle__function_load_request func = loader.load_function( 
File ""/azure-functions-host/workers/python/3.9/LINUX/X64/azure_functions_worker/utils/wrappers.py"", line 48, in call raise extend_exception_message(e, message) File ""/azure-functions-host/workers/python/3.9/LINUX/X64/azure_functions_worker/utils/wrappers.py"", line 44, in call return func(*args, **kwargs)
File ""/azure-functions-host/workers/python/3.9/LINUX/X64/azure_functions_worker/loader.py"", line 132, in load_function mod = importlib.import_module(fullmodname)
File ""/usr/local/lib/python3.9/importlib/__init__.py"", line 127, in import_module return _bootstrap._gcd_import(name[level:], package, level) File ""&lt;frozen importlib._bootstrap&gt;"", line 1030, in _gcd_import
File ""&lt;frozen importlib._bootstrap&gt;"", line 1007, in _find_and_load
File ""&lt;frozen importlib._bootstrap&gt;"", line 986, in _find_and_load_unlocked 
File ""&lt;frozen importlib._bootstrap&gt;"", line 680, in _load_unlocked
File ""&lt;frozen importlib._bootstrap_external&gt;"", line 850, in exec_module
File ""&lt;frozen importlib._bootstrap&gt;"", line 228, in _call_with_frames_removed 
File ""/home/site/wwwroot/func-gc-imgorientation-train/__init__.py"", line 20, in &lt;module&gt; from core.data.training_image_filestore import TrainingImageFilestore
File ""/home/site/wwwroot/core/data/training_image_filestore.py"", line 4, in &lt;module&gt; import tensorflow as tf
File ""/home/site/wwwroot/.python_packages/lib/site-packages/tensorflow/__init__.py"", line 37, in &lt;module&gt; from tensorflow.python.tools import module_util as _module_util
File ""/home/site/wwwroot/.python_packages/lib/site-packages/tensorflow/python/__init__.py"", line 37, in &lt;module&gt; from tensorflow.python.eager import context
File ""/home/site/wwwroot/.python_packages/lib/site-packages/tensorflow/python/eager/context.py"", line 28, in &lt;module&gt; from tensorflow.core.framework import function_pb2
File ""/home/site/wwwroot/.python_packages/lib/site-packages/tensorflow/core/framework/function_pb2.py"", line 5, in &lt;module&gt; from google.protobuf.internal import builder as _builder

Well Documented Error with a Solution that Works Outside of Azure
This error is well documented and understood as in the following version:
ImportError: cannot import name 'builder' from 'google.protobuf.internal'
In general, arising from API changes in v3.20:
https://stackoverflow.com/a/71984564/1928761
TF adopted these changes, but there were bugs when using TF with protobuf &gt;= v3.20 which were resolved in the most recent Tensorflow release so protobuf 4.23.3 should work with TF 2.12.0 as here:
https://github.com/tensorflow/tensorflow/issues/59221
Problem is resolved locally
I resolved this issue locally and have tested my function. All works fine. The problem only occurs in Azure.
Suspected Root Cause in Azure
Looking through the error above I noticed that all my custom modules, and Tensorflow, are installed to the same root: /home/site/wwwroot/
For instance, my custom core.data module is installed at /home/site/wwwroot/core/data/
Tensorflow is installed at /home/site/wwwroot/.python_packages/lib/site-packages/tensorflow/
I've confirmed that all modules in my requirements.txt are being installed as expected in /home/site/wwwroot/.python_packages/lib/site-packages/ - including the latest protobuf with the builder module as expected.
However, protobuf is being imported from /azure-functions-host/workers/python/3.9/LINUX/X64/google/protobuf/internal/init.py
Presumably the implication is that TF is using the version of protobuf that's bundled with Azure's python distribution on the host, rather than the version in my site packages.
To test this theory I added the following code to the top of my init.py for the function:
import sys
print(sys.path)
import google.protobuf
print(google.protobuf.__version__)
print(google.protobuf.__path__)

This confirmed that protobuf version 3.19 was being loaded from the following path and not from site packages: /azure-functions-host/workers/python/3.9/LINUX/X64/google/protobuf/
The Question
The question therefore is can I upgrade the version of protobuf that's included in this python bundle?
Alternatively, can I force TF to use the version of protobuf in my site packages rather than the ones in the Azure python bundle?
Attempted Solutions
ADO is building and installing the correct versions of both protobuf and TF, and the builder file is definitely in the installed site-packages. I've confirmed this by downloading the package from my storage account and unzipping it.
To resolve the issue, I've tried the following:

Confirmed that I can import other modules and that this is specific to importing the protobuf builder
Confirmed that I can import google.protobuf.internal.
Tried clearing the protobuf pycache by adding an rm -rvf command to my bash script immediately after the pip install --target....
Using subprocess to run pip install --upgrade protobuf==4.23.3 from within my function and before importing TF (which runs without error, but does not do the job).

At this point, I'm all out of ideas.
requirements.txt
# DO NOT include azure-functions-worker in this file
# The Python Worker is managed by Azure Functions platform
# Manually managing azure-functions-worker may cause unexpected issues

# Protobuf comes first to force the very latest version
protobuf==4.23.3

# Tensorflow, Dotenv, Pillow
absl-py==1.4.0
astunparse==1.6.3
cachetools==5.3.1
certifi==2023.5.7
charset-normalizer==3.1.0
contourpy==1.1.0
cycler==0.11.0
flatbuffers==23.5.26
fonttools==4.40.0
gast==0.4.0
google-auth==2.20.0
google-auth-oauthlib==1.0.0
google-pasta==0.2.0
graphviz==0.20.1
grpcio==1.54.2
h5py==3.8.0
idna==3.4
jax==0.4.12
keras==2.12.0
kiwisolver==1.4.4
libclang==16.0.0
Markdown==3.4.3
MarkupSafe==2.1.3
matplotlib==3.7.1
ml-dtypes==0.2.0
numpy==1.23.5
oauthlib==3.2.2
opt-einsum==3.3.0
packaging==23.1
Pillow==9.5.0
pyasn1==0.5.0
pyasn1-modules==0.3.0
pydot==1.4.2
pyparsing==3.0.9
python-dateutil==2.8.2
python-dotenv
requests==2.31.0
requests-oauthlib==1.3.1
rsa==4.9
scipy==1.10.1
six==1.16.0
tensorboard==2.12.3
tensorboard-data-server==0.7.1
tensorflow==2.12.0
tensorflow-estimator==2.12.0
termcolor==2.3.0
typing_extensions==4.6.3
urllib3==1.26.16
Werkzeug==2.3.6
wrapt==1.14.1

# Azure libraries
azure-functions
azure-identity
azure-keyvault

Deployment pipeline YAML
# Python Function App to Linux on Azure
# Build a Python function app and deploy it to Azure as a Linux function app.
# Add steps that analyze code, save build artifacts, deploy, and more:
# https://docs.microsoft.com/azure/devops/pipelines/languages/python

trigger:
- main

variables:
  # Azure Resource Manager connection created during pipeline creation
  azureSubscription: '4ae24131-0b22-421c-8e3e-6d766e891ece'

  # Function app name
  functionAppName: 'func-xxxx-dev'

  # Agent VM image name
  vmImageName: 'ubuntu-latest'

  # Working Directory
  workingDirectory: '$(System.DefaultWorkingDirectory)'

stages:
- stage: Build
  displayName: Build stage

  jobs:
  - job: Build
    displayName: Build
    pool:
      vmImage: $(vmImageName)

    steps:
    - bash: |
        if [ -f extensions.csproj ]
        then
            dotnet build extensions.csproj --runtime ubuntu.16.04-x64 --output ./bin
        fi
      workingDirectory: $(workingDirectory)
      displayName: 'Build extensions'

    - task: UsePythonVersion@0
      displayName: 'Use Python 3.9'
      inputs:
        versionSpec: 3.9 # Functions V2 supports Python 3.6 as of today

    - bash: |
        python -m pip install --upgrade pip
        pip install --target=""./.python_packages/lib/site-packages"" -r ./requirements.txt
      workingDirectory: $(workingDirectory)
      displayName: 'Install application dependencies'

    - task: ArchiveFiles@2
      displayName: 'Archive files'
      inputs:
        rootFolderOrFile: '$(workingDirectory)'
        includeRootFolder: false
        archiveType: zip
        archiveFile: $(Build.ArtifactStagingDirectory)/$(Build.BuildId).zip
        replaceExistingArchive: true

    - publish: $(Build.ArtifactStagingDirectory)/$(Build.BuildId).zip
      artifact: drop

- stage: Deploy
  displayName: Deploy stage
  dependsOn: Build
  condition: succeeded()

  jobs:
  - deployment: Deploy
    displayName: Deploy
    environment: 'development'
    pool:
      vmImage: $(vmImageName)

    strategy:
      runOnce:
        deploy:

          steps:
          - task: AzureFunctionApp@1
            displayName: 'Azure functions app deploy'
            inputs:
              azureSubscription: '$(azureSubscription)'
              appType: functionAppLinux
              appName: $(functionAppName)
              package: '$(Pipeline.Workspace)/drop/$(Build.BuildId).zip'

Log stream
2023-06-27T11:50:43Z   [Information]   Host Status: {
  ""id"": ""func-xxxx-dev"",
  ""state"": ""Running"",
  ""version"": ""4.21.3.3"",
  ""versionDetails"": ""4.21.3+2e42e3beb40b89d4f5d3dd962f3a5d420d376d71"",
  ""platformVersion"": """",
  ""instanceId"": ""9A4EF22A-638234606489642041"",
  ""computerName"": """",
  ""processUptime"": 2778401,
  ""functionAppContentEditingState"": ""NotAllowed"",
  ""extensionBundle"": {
    ""id"": ""Microsoft.Azure.Functions.ExtensionBundle"",
    ""version"": ""4.5.0""
  }
}
2023-06-27T11:50:43Z   [Information]   Host Status: {
  ""id"": ""func-xxxx-dev"",
  ""state"": ""Running"",
  ""version"": ""4.21.3.3"",
  ""versionDetails"": ""4.21.3+2e42e3beb40b89d4f5d3dd962f3a5d420d376d71"",
  ""platformVersion"": """",
  ""instanceId"": ""9A4EF22A-638234606489642041"",
  ""computerName"": """",
  ""processUptime"": 2778404,
  ""functionAppContentEditingState"": ""NotAllowed"",
  ""extensionBundle"": {
    ""id"": ""Microsoft.Azure.Functions.ExtensionBundle"",
    ""version"": ""4.5.0""
  }
}
2023-06-27T11:50:43Z   [Information]   Host Status: {
  ""id"": ""func-xxxx-dev"",
  ""state"": ""Running"",
  ""version"": ""4.21.3.3"",
  ""versionDetails"": ""4.21.3+2e42e3beb40b89d4f5d3dd962f3a5d420d376d71"",
  ""platformVersion"": """",
  ""instanceId"": ""9A4EF22A-638234606489642041"",
  ""computerName"": """",
  ""processUptime"": 2778801,
  ""functionAppContentEditingState"": ""NotAllowed"",
  ""extensionBundle"": {
    ""id"": ""Microsoft.Azure.Functions.ExtensionBundle"",
    ""version"": ""4.5.0""
  }
}
2023-06-27T11:51:00Z   [Information]   Executing 'Functions.func-xxxx-train' (Reason='Timer fired at 2023-06-27T11:51:00.0016150+00:00', Id=3795600b-379e-423c-b29f-65fec390289a)
2023-06-27T11:51:00Z   [Verbose]   Sending invocation id: '3795600b-379e-423c-b29f-65fec390289a
2023-06-27T11:51:00Z   [Verbose]   Posting invocation id:3795600b-379e-423c-b29f-65fec390289a on workerId:250ae2e1-4416-41ef-b355-0684a59d0a91
2023-06-27T11:51:00Z   [Error]   Executed 'Functions.func-xxxx-train' (Failed, Id=3795600b-379e-423c-b29f-65fec390289a, Duration=2ms)
2023-06-27T11:51:00Z   [Verbose]   Function 'func-xxx-train' updated status: Last='2023-06-27T11:51:00.0015724+00:00', Next='2023-06-27T11:52:00.0000000+00:00', LastUpdated='2023-06-27T11:51:00.0015724+00:00'
2023-06-27T11:51:00Z   [Verbose]   Timer for 'func-gc-imgorientation-train' started with interval '00:00:59.9674482'.
2023-06-27T11:51:05Z   [Information]   Host Status: {
  ""id"": ""func-gc-imgorientation-dev"",
  ""state"": ""Running"",
  ""version"": ""4.21.3.3"",
  ""versionDetails"": ""4.21.3+2e42e3beb40b89d4f5d3dd962f3a5d420d376d71"",
  ""platformVersion"": """",
  ""instanceId"": ""9A4EF22A-638234606489642041"",
  ""computerName"": """",
  ""processUptime"": 2800545,
  ""functionAppContentEditingState"": ""NotAllowed"",
  ""extensionBundle"": {
    ""id"": ""Microsoft.Azure.Functions.ExtensionBundle"",
    ""version"": ""4.5.0""
  }
}

",0,1494,"I had a similar issue and found a different solution. There is an app setting that isolates the application's dependencies from the azure function runtime dependencies:
https://learn.microsoft.com/en-us/azure/azure-functions/functions-app-settings#python_isolate_worker_dependencies
Adding the PYTHON_ISOLATE_WORKER_DEPENDENCIES setting with a value of 1 to my function in azure fixed the issue for me.
","The root cause of the problem, as highlighted by @SiddheshDesai, turned out to be that the Azure Functions host was loading a version of protobuf (3.19.6) into the cache which was earlier than the version that the latest Tensorflow needed (4.23.3).
Even when placing my protobuf/TF imports at the top of my function app module, the same error arose.
I was, however, able to reload protobuf from my site-packages using importlib.
This cause was proven by adding the following code to my function:
import logging
import importlib
import google.protobuf

logging.info(google.protobuf.__version__)
logging.info(google.protobuf.__file__)

importlib.reload(google.protobuf)

logging.info(google.protobuf.__version__)
logging.info(google.protobuf.__file__)

However, the reload then interfered with the Azure Functions library and host causing a heap of other exceptions.
My conclusion therefore is that this is an issue with Azure Functions rather than an issue with my code.
Rather than trying to work around these issues, I've now containerised my function and re-deployed. This has worked as expected.
The answer, therefore, turns out to be - as SiddheshDesai suggested above - that any application needing to use versions of protobuf &gt;= 3.20.0 need to be containerised rather than deployed as code/run from packages.
","If you want to use Protobuf module in Azure Functions, You need to downgrade it to 3.20.* and add it in your requirements.txt.
I added protobuf==3.20.* in my requirements.txt and the Http Trigger got deployed successfully in Azure Functions via DevOps YAML pipeline, Refer below:-
My requirements.txt:-
azure-functions
protobuf==3.20.*

My init.py:-
import logging

import azure.functions as func


def main(req: func.HttpRequest) -&gt; func.HttpResponse:
    logging.info('Python HTTP trigger function processed a request.')

    name = req.params.get('name')
    if not name:
        try:
            req_body = req.get_json()
        except ValueError:
            pass
        else:
            name = req_body.get('name')

    if name:
        return func.HttpResponse(f""Hello, {name}. This HTTP triggered function executed successfully."")
    else:
        return func.HttpResponse(
             ""This HTTP triggered function executed successfully. Pass a name in the query string or in the request body for a personalized response."",
             status_code=200
        )


My YAML pipeline:-
trigger:
- master

variables:
  
  azureSubscription: 'xxxxxxxx-xxxxx-xxx9bbd4354dd'

 
  functionAppName: 'valleyfunc541'

 
  vmImageName: 'ubuntu-latest'

 
  workingDirectory: '$(System.DefaultWorkingDirectory)'

stages:
- stage: Build
  displayName: Build stage

  jobs:
  - job: Build
    displayName: Build
    pool:
      vmImage: $(vmImageName)

    steps:
    - bash: |
        if [ -f extensions.csproj ]
        then
            dotnet build extensions.csproj --runtime ubuntu.16.04-x64 --output ./bin
        fi
      workingDirectory: $(workingDirectory)
      displayName: 'Build extensions'

    - task: UsePythonVersion@0
      displayName: 'Use Python 3.10'
      inputs:
        versionSpec: 3.10 # Functions V2 supports Python 3.6 as of today

    - bash: |
        pip install --target=""./.python_packages/lib/site-packages"" -r ./requirements.txt
      workingDirectory: $(workingDirectory)
      displayName: 'Install application dependencies'

    - task: ArchiveFiles@2
      displayName: 'Archive files'
      inputs:
        rootFolderOrFile: '$(workingDirectory)'
        includeRootFolder: false
        archiveType: zip
        archiveFile: $(Build.ArtifactStagingDirectory)/$(Build.BuildId).zip
        replaceExistingArchive: true

    - publish: $(Build.ArtifactStagingDirectory)/$(Build.BuildId).zip
      artifact: drop

- stage: Deploy
  displayName: Deploy stage
  dependsOn: Build
  condition: succeeded()

  jobs:
  - deployment: Deploy
    displayName: Deploy
    environment: 'development'
    pool:
      vmImage: $(vmImageName)

    strategy:
      runOnce:
        deploy:

          steps:
          - task: AzureFunctionApp@1
            displayName: 'Azure functions app deploy'
            inputs:
              azureSubscription: '$(azureSubscription)'
              appType: functionAppLinux
              appName: $(functionAppName)
              package: '$(Pipeline.Workspace)/drop/$(Build.BuildId).zip'

Output:-

The HTTP Trigger got deployed successfully:-


Reference:- My SO thread answer
"
matplotlib unexpected output,https://stackoverflow.com/questions/61254317,Tkinter GUI Crashing on Button Click,"I am currently doing a machine learning project and I am trying to make a basic Tkinter UI that calls function from a file that performs gesture recognition.

Currently I have a button on my GUI that calls for the 'main' function in the gesture recognition file, every time I click that, it does run the function but then the GUI becomes unresponsive so I can't click anything else. I'm wondering if this is to do with nothing being returned perhaps? which is stopping the ui mainloop.

The functionality i'm aiming to build is for the UI to run the 'main function' from Camera.py (i'll link below) , and then have a button assigned to each of the three options from that main function ""press 1 to train the model, 2 to detect hand gesture"" etc. Rather than typing them in manually.

My current UI code is 

import numpy as np
import matplotlib as plt
from sys import exit
from tkinter import *
from tkinter import ttk
import tkinter.simpledialog as sd
import camera as cam

modelobject = cam.Yolo_Model()

def runTraining():
    modelobject.main()

def getUserInput():
    options = {'minvalue':1, 'maxvalue':4}
    userInput = sd.askinteger('User Input', 'Select an Option', **options)
    print((f'You entered {userInput}'))

window=Tk()
window.title(""Gesture Recognition"")
window.geometry('600x350')

btn1 = ttk.Button(window, text=""Train the Model"", command=lambda: runTraining())
btn1.grid(column=0, row=0)


btn2 = ttk.Button(window, text=""Detect Hand Gesture"", command=lambda: getUserInput())
btn2.grid(column=1, row=0)


btn3 = ttk.Button(window, text=""Clear Images Stored From Training"")
btn3.grid(column=2, row=0)

def close_window():
    exit(0)
    window.destroy()

exitbtn = ttk.Button(window, text=""Exit"")
exitbtn.grid(column=0, row=4)
exitbtn.config(command=close_window)

window.mainloop()


The code for the file I want to run and call functions from within the UI is

import numpy as np
import os
from numpy import expand_dims
import cv2
from trainer import predicter
from keras.preprocessing.image import img_to_array
import time
from im_to_txt import m
from random import random
import shutil


class Yolo_Model:
    # Sets the intial parameters used within the definition of the class.
    def __init__(self):
        # Enables the video capture
        self.cap = cv2.VideoCapture(0)
        # Sets the temporary directory used for the captured frame
        self.filename = ""img.jpg""
        # Sets the destination of the training directory
        self.train_dir = ""new_training""
        try:
            os.mkdir(self.train_dir)
        except:
            print(""directory exists"")

    def func(self, curr):
        boo = """"
        image = cv2.imread(self.filename)
        os.remove(self.filename)
        print(""Enter 'T' if the label is correct or 'F' is the label is incorrect!"")
        boo = input(curr[0] + ""\n"")
        try:
            if boo == 'T':
                name = str(random()) + '.jpg'
                name = os.path.join(self.train_dir, name)
                try:
                    cv2.imwrite(name, image)
                except:
                    print(""Could not save the image to "" + str(name))
                m(name, curr)
            image = cv2
        except ValueError as e:
            print(e)
            print(""Value not recognised"")

    def model_detector(self):
        while (self.cap.isOpened()):
            ret, frame = self.cap.read()
            if ret == True:
                # time.sleep(1)
                frame = cv2.flip(frame, +1)
                cv2.imshow('window', frame)
                try:
                    cv2.imwrite(self.filename, frame)
                except:
                    print(""Could not save the image to img.jpg"")
                output = predicter(0.6, self.filename)

                os.remove(self.filename)
                # os.remove(photo_filename)
                while (len(output) &gt;= 5):
                    curr = output[0:6]
                    output = output[6:]
                    label = curr[0]
                    try:
                        image = cv2.rectangle(frame, (curr[1], curr[4]), (curr[3], curr[2]), (0, 0, 255), 3)
                        image = cv2.putText(frame, label, (curr[1], (curr[2] - 10)), cv2.FONT_HERSHEY_SIMPLEX, 1,
                                            (0, 255, 0), 2)
                    except:
                        print(""Could not annote the image. Please ensure the co-ordinates are correct"")
                    cv2.imshow('window', image)

            # Used to detstroy the image capture window
            if cv2.waitKey(1) &amp; 0xFF == ord('q'):
                break
            else:
                break
                # This function is used to train the system. It detects a gesture being shown by the user and applies

    # a label. It then prompts the user, asking if the label identified was correct. If yes, the yolo format
    # text file is created, needed for training, and the image is saved into the training directory. The user
    # is also shown the detected image with the label and bounding box.
    def model_trainer(self):
        # Constant loop for capturing frames for the webcam.
        while (True):
            # Captures the image and frame number.
            ret, frame = self.cap.read()
            if ret == True:
                # Additional sleep timer can be used for waiting in between capturing frames. Turned
                # off for real time detection.
                # time.sleep(1)
                # Flips the frame so it appears the right way for the user.
                frame = cv2.flip(frame, +1)
                # Shows the user the frame that has been captured by the camera. This is constantly
                # updated each time a new frame is captured.
                cv2.imshow('window', frame)
                # Try and except block used incase the system cannot write the captured frame. The frame
                # is saved so it can be used by the prediction file.
                try:
                    cv2.imwrite(self.filename, frame)
                except:
                    print(""Could not save the image to img.jpg"")
                # The predict method from trainer.py is called. This will return the prediction of the captured
                # image along with the the parameters needed for the boudnding box and prediction score. The parameter
                # 0.6 corrosponds to threshold of how confident the model is about the label applied. Any lower and the
                # prediction will be discounted. 0.6 is lower than normal to allow for additonal classifcation to be made
                # during training.
                output = predicter(0.6, self.filename)
                # If the length of the array returned from the model prediction is greater than 5,i.e. a prediction was made
                # then the first 6 values, corrosponding to the length of one prediction, will be upacked until there are no more
                # predictions from that frame.
                while (len(output) &gt;= 5):
                    # The first 6 values corrosponding to the first prediction are extracted
                    curr = output[0:6]
                    output = output[6:]
                    # The predicted label is the first value within the returned array
                    label = curr[0]
                    # A try block is used incase the given parameters for the bounding box are incorrect or cannot be labeled
                    # within the image.
                    try:
                        image = cv2.rectangle(frame, (curr[1], curr[4]), (curr[3], curr[2]), (0, 0, 255), 3)
                        image = cv2.putText(frame, label, (curr[1], (curr[2] - 10)), cv2.FONT_HERSHEY_SIMPLEX, 1,
                                            (0, 255, 0), 2)
                    except:
                        print(""Could not annote the image. Please ensure the co-ordinates are correct"")
                    cv2.imshow('window', image)
                    # This tunction is called to check if the predicted label was correct and if so use the image for training.
                    self.func(curr)
                    # Used to destroy the image capture window
            if cv2.waitKey(1) &amp; 0xFF == ord('q'):
                break
            else:
                break

    # Adds additional ethics and security in removing the training directory
    # from the system.
    def removeFiles(self):
        # Try and except used incase the training directory could not be located.
        try:
            # Removes the training directory.
            shutil.rmtree(self.train_dir, ignore_errors=True)
        # Throws an OS error as the file cannot be found, printing user instructions on the error.
        except OSError as e:
            print(""Error: "")
            print(e)

    # The main menu for the user to select the options for the system. This menu calls the functions
    # used for training, detecting, and removing stored images.
    def main(self):
        menu = input(
            ""Press 1 to train the model: \nWarning training will store images of you!!\nPress 2 to detect the hand gesture: \nPress 3 to remove any images stored during training\n"")
        if menu == '1':
            while (True):
                self.model_trainer()
        elif menu == '2':
            while (True):
                self.model_detector()
        elif menu == '3':
            self.removeFiles()
        # The final else statement catches the program should a unexpected value be entered.
        else:
            print(""Error. Correct option not selected"")


# Automatically calls the main method within the Yolo_Model class
if __name__ == ""__main__"":
    model = Yolo_Model()
    # Call the main method
    model.main()
    print(""Closing application"")
    # Try and except should the camera not have been used within the system.
    try:
        # Stop the frame capture
        cap.release()
        # Destroy the all the CV2 windows
        cv2.destroyAllWindows()
    except:
        print(""Image capture not used"")


To clarify, when running the UI, the window pops up just fine, then as soon as I click the button that is currently assigned to run 'main' from camera.py, it runs but the UI becomes unresponsive, so I have to complete the process from the console, which is what i'm trying to replace with UI functionality.

I hope this is clear, and there is an obvious fix that i'm missing, as i'm not very experienced.
Cheers
",0,828,"Your UI will be as long unresponsive as long as your runTraining() function is running.
You have to run your long running task in a separate Thread or Process. If you want to change UI elements from your long running task you have to set-up a Queue which is polled in your main loop and can change the UI in context of the main loop.

Here is an example: http://zetcode.com/articles/tkinterlongruntask/

See also Python Tkinter: How do I make my GUI responsive as long as a thread runs? (there are some links to related questions in the comments)

And here is an example which uses PySimpleGui instead of TKinter directly: https://github.com/PySimpleGUI/PySimpleGUI/blob/master/DemoPrograms/Demo_Multithreaded_Long_Tasks.py
",,
matplotlib unexpected output,https://stackoverflow.com/questions/56534950,Using skimage.color.gray2rgb produces a wrong output image,"Hello I was just trying to make a code to set up a grayscale image as a three channel image format and then display it. I am using skimage.color  function gray2rgb which does change the shape of the mono scale image matrix. However when wanting to plot the result of that conversion I get an unexpected image as output.

Also I am having a hard time to create an empty 3 channel image to fill it with whatever monoscale information I want. Apparently because of nature of image as uint16 there is some problem with matplotlib

I have tried np.vstack, skimage.color.gray2rgb. All of them produce the same output.
I have tried using np.uint8 to convert from 65535 max pixel value to 255 but it is not working. It seems that division works

from skimage.io import imread, imshow
import matplotlib.pyplot as plt
import os
import numpy as np
img = imread('516.jpg')
img_uint8 = np.uint8(img)
from skimage.color import gray2rgb
from skimage import img_as_float
img_rgb = gray2rgb(img_uint8, alpha=None)
print(img_rgb.shape)
plt.imshow(img_rgb)


Image shown is not what it would be expected!!!!

I would expect an image similar to original but probably with a different color since it should be the same image in all 3 channels.
I think the problem is with np.uint8 for my image. I tested with cameraman image and had no problem
",0,1805,"I don't know why but it seems that using img_as_ubyte makes possible to copy the gray image in the three channels and be able to display an image similar to original

from skimage import img_as_ubyte
i = imread('516.jpg')
i_8 = img_as_ubyte(i)
imshow(i_8)
print(i_8)
i_8rgb = gray2rgb(i_8)
plt.imshow(i_8rgb, cmap='gray')


Don't know why yet np.uint8 produces a result altered. Somehow also using openCV as a read seems to always put image in a uint8 range, which could be handy.
I wait for your comments

Original Image used for testing
This is what np.uint8 seems to be producing
",,
matplotlib unexpected output,https://stackoverflow.com/questions/23024787,Getting unexpected output when plotting with Matplotlib - Cmap - Python,"I have a method in my project in which I verify if a pixel has the desired reliability (in terms of its classification as edge or not) and I plot the pixels in the following scheme:

White -&gt; pixel doesn't have the required reliability
Blue -&gt; pixel has the required reliability and it was classified as not edge
Red -&gt; pixel has the required reliability and it was classified as an edge


This is my code:

def generate_data_reliability(classification_mean, data_uncertainty, x_axis_label, y_axis_label, plot_title,
                                  file_path, reliability):
        """"""
        :classification_mean : given a set of images, how was the mean classification for each pixel
        :param data_uncertainty : the uncertainty about the classification
        :param x_axis_label : the x axis label of the data
        :param y_axis_label : the y axis label of the data
        :param plot_title : the title of the data
        :param file_path : the name of the file
        """"""
        plt.figure()
        # 0 -&gt; certainty
        # 1 -&gt; uncertainty
        r = 0
        b = 0
        w = 0
        has_reliability = numpy.zeros((data_uncertainty.rows, data_uncertainty.cols), float)
        for x, y in product(range(data_uncertainty.rows), range(data_uncertainty.cols)):
            # I the uncertainty is &gt; then the required reliability, doesn't show it
            if data_uncertainty.data[x][y] &gt; (1.0 - reliability):
                has_reliability[x][y] = 0.5
                w += 1
            else:
                has_reliability[x][y] = classification_mean.data[x][y]
                if has_reliability[x][y] == 1.0:
                    r += 1
                if has_reliability[x][y] == 0.0:
                    b += 1

        print reliability, w+r+b, w, r, b

        plt.title(plot_title)
        plt.imshow(has_reliability, extent=[0, classification_mean.cols, classification_mean.rows, 0], cmap='bwr')
        plt.xlabel(x_axis_label)
        plt.ylabel(y_axis_label)
        plt.savefig(file_path + '.png')
        plt.close()


And this is the print that I got:

&gt;&gt;&gt;&gt; Prewitt
0.8 95100 10329 0 84771
0.9 95100 12380 0 82720
0.99 95100 18577 0 76523


As can be seen, as the required reliability get higher, less pixels have this reliability (more of then will be plot white and none of them are red).

But this is the plots that I get:







I don't know why, if I have less pixels with the desired reliability, I don't get more white pixels, but these red ones. I'm not changing my objects, to mess with them. Oo

I'm stucked in this problem at about 3 hours with no clue about what is wrong.

EDIT:

In this cmap 0 is blue, 0.5 is white and 1 is red, isn't it? I'm pretty sure that the problem is 'cause I am using a diverging color map and sometimes and don't have a central value. E.g., in the situation that I posted here, I don't have red values, so my values vary between 0.5 and 1. Then, matplotlib automatically set my min value to be red and my max value to be blue. But how could I do that? I choose this 'cause would like to represent colors in the scheme: 0=blue, 0.5=white and 1=red (My values will always be 0, 0.5 or 1).

Any help would be very, very much appreciated.

Thank you in advance.
",0,531,,,
matplotlib unexpected output,https://stackoverflow.com/questions/7170631,What can cause delay in my worker thread Queue.get()?,"Note: This is closely related to my other topic here and here, but is an independent question and you needn't read them if you're not interested to..

I have some unexpected delays in processing items from the queue.  It seems to be related to the matplotlib GUI thread, because I can make the delay arbitrarily long by avoiding generating any GUI events, but for example if I create some mouse move events, I can get at the queue.  What is the cause of this delay and how to fix it?

# ~/repo/wim/mpl_q.py
import threading, Queue, time, random, sys

t0 = time.time()
t = lambda : time.time() - t0

def worker():
  while True:
    thing = queue.get()
    sys.stdout.write('({0}) hello {1}!\n'.format(t(), thing))
    queue.task_done()

queue = Queue.Queue()
thread = threading.Thread(target=worker)
thread.daemon = True
thread.start()

def say_hello(thing='world'):
  print '({0}) --&gt; say_hello({1})'.format(t(), thing)
  queue.put(thing)

say_hello('world')
say_hello('cruel world')
say_hello('stack overflow')

import matplotlib.pyplot as plt
fig = plt.figure()

def event_handler(event):
  world = random.choice(['foo', 'bar', 'baz'])
  say_hello(world)

event_handler_ = lambda x: event_handler(x)
cid0 = fig.canvas.mpl_connect('key_press_event', event_handler)
cid1 = fig.canvas.mpl_connect('button_press_event', event_handler_)

plt.show()


Example of typical output , you can see the scripted items happen on time, but the user generated ones can come much later..

wim@wim-acer:~/repo/wim$ python mpl_q.py
(0.000166893005371) --&gt; say_hello(world)
(0.000200986862183) --&gt; say_hello(cruel world)
(0.000216960906982) hello world!
(0.000231027603149) --&gt; say_hello(stack overflow)
(0.000247955322266) hello cruel world!
(0.00425505638123) hello stack overflow!
(7.80911588669) --&gt; say_hello(foo)
(7.84842705727) hello foo!
(9.41998004913) --&gt; say_hello(baz)
(11.4023530483) hello baz!
(14.3315930367) --&gt; say_hello(foo)
(19.4317750931) hello foo!
(20.96124506) --&gt; say_hello(bar)
(23.2277729511) --&gt; say_hello(baz)
(23.2278220654) hello bar!
(29.0094120502) hello baz!


edit: I am using GTKAgg backend, and matplotlib version 1.1.0
",0,250,,,
matplotlib unexpected output,https://stackoverflow.com/questions/70954411,ValueError: Found unexpected losses or metrics that do not correspond to any Model output,,-1,3428,,,
matplotlib unexpected result,https://stackoverflow.com/questions/69148495,TypeError: import_optional_dependency() got an unexpected keyword argument &#39;errors&#39;,"I am trying to work with Featuretools to develop an automated feature engineering workflow for the customer churn dataset. The end outcome is a function that takes in a dataset and label times for customers and builds a feature matrix that can be used to train a machine learning model.
As part of this exercise I am trying to  execute the below code for plotting a histogram and got ""TypeError: import_optional_dependency() got an unexpected keyword argument 'errors' "". Please help resolve this TypeError.
import matplotlib.pyplot as plt
%matplotlib inline
plt.style.use('fivethirtyeight')
plt.rcParams['figure.figsize'] = (10, 6)

trans.loc[trans['actual_amount_paid'] &lt; 250, 'actual_amount_paid'].dropna().plot.hist(bins = 30)
plt.title('Distribution of Actual Amount Paid')

Below is the full error I received:
    ---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
&lt;ipython-input-32-7e19affd5fc1&gt; in &lt;module&gt;
      4 plt.rcParams['figure.figsize'] = (10, 6)
      5 
----&gt; 6 trans.loc[trans['actual_amount_paid'] &lt; 250, 'actual_amount_paid'].dropna().plot.hist(bins = 30)
      7 plt.title('Distribution of Actual Amount Paid')

~\anaconda3\lib\site-packages\pandas\core\ops\common.py in new_method(self, other)
     63                     break
     64                 if isinstance(other, cls):
---&gt; 65                     return NotImplemented
     66 
     67         other = item_from_zerodim(other)

~\anaconda3\lib\site-packages\pandas\core\arraylike.py in __lt__(self, other)
     35     def __ne__(self, other):
     36         return self._cmp_method(other, operator.ne)
---&gt; 37 
     38     @unpack_zerodim_and_defer(""__lt__"")
     39     def __lt__(self, other):

~\anaconda3\lib\site-packages\pandas\core\series.py in _cmp_method(self, other, op)  
   4937         --------
   4938         &gt;&gt;&gt; s = pd.Series(range(3))
-&gt; 4939         &gt;&gt;&gt; s.memory_usage()
   4940         152
   4941 

~\anaconda3\lib\site-packages\pandas\core\ops\array_ops.py in comparison_op(left, right, op)
    248     lvalues = ensure_wrapped_if_datetimelike(left)
    249     rvalues = ensure_wrapped_if_datetimelike(right)
--&gt; 250 
    251     rvalues = lib.item_from_zerodim(rvalues)
    252     if isinstance(rvalues, list):

~\anaconda3\lib\site-packages\pandas\core\ops\array_ops.py in _na_arithmetic_op(left, right, op, is_cmp)
    137 
    138 def _na_arithmetic_op(left, right, op, is_cmp: bool = False):
--&gt; 139     
    140     Return the result of evaluating op on the passed in values.
    141 

~\anaconda3\lib\site-packages\pandas\core\computation\expressions.py in &lt;module&gt;
     17 from pandas._typing import FuncType
     18 
---&gt; 19 from pandas.core.computation.check import NUMEXPR_INSTALLED
     20 from pandas.core.ops import roperator
     21 

~\anaconda3\lib\site-packages\pandas\core\computation\check.py in &lt;module&gt;
      1 from pandas.compat._optional import import_optional_dependency
      2 
----&gt; 3 ne = import_optional_dependency(""numexpr"", errors=""warn"")
      4 NUMEXPR_INSTALLED = ne is not None
      5 if NUMEXPR_INSTALLED:

TypeError: import_optional_dependency() got an unexpected keyword argument 'errors'

",9,13481,"Try to upgrade pandas:
pip install pandas --upgrade

",,
matplotlib unexpected result,https://stackoverflow.com/questions/57381514,Make matplotlib export figure svg without stroke,"I am trying to generate an svg file with Python and matplotlib. A simpler version of the code that I use is the following :
import matplotlib.pyplot as plt


plt.figure()
plt.fill([0,1,0.5],[0,0,1],color = ""r"")
plt.fill([1.5,1,0.5],[1,0,1],color = ""b"")
plt.show()
plt.savefig(""soedgesvg.svg"")

So far so good the result is as expected :

But when I open it in Inkscape I get an unexpected ghost edge with no color specified. This can be seen in the following picture :

Is there a way to remove this edge in Python before exporting the svg ?
Edit
This occurs only when a color is specified from my experience.
Inkscape 0.92.3 and matplotlib 3.1.1
",3,395,"The way too go is to add linewidth=0 when calling plt.fill like so :

plt.fill([0,1,0.5],[0,0,1],color = ""r"", linewidth=0)
plt.fill([1.5,1,0.5],[1,0,1],color = ""b"", linewidth=0)


This answer has been provided by @ImportanceOfBeingErnest in the comments with the following explanation :


  Because the linewidth is defined in points. When you zoom inside matplotlib, the lines always keep their physical size, e.g. 1/72. of an inch. When you zoom in in Inkscape you zoom into the actual figure, up to the point where maybe your screen only shows 1/72. of an inch and hence the line will be as large as the screen.

",,
matplotlib unexpected result,https://stackoverflow.com/questions/42216865,Python Matplotlib x-axis improperly labels timedelta64 object,"I am trying to generate a plot from a Pandas dataframe in Python with Matplotlib. Here is a summary of the dataframe. 

import pandas as pd
import datetime
import matplotlib.pyplot as plt

# Summarize data frame.
&gt;&gt;&gt; df.shape
(40, 4)

&gt;&gt;&gt; df.dtypes
ID                         object
relative_time     timedelta64[ns]
value                     float64
relative_value            float64
dtype: object

&gt;&gt;&gt; df.head()
    ID     relative_time  value  relative_value
0  001 -1 days +18:08:04    4.5            -1.0
1  001 -1 days +18:18:03    4.5            -1.0
2  001 -1 days +18:28:03    4.5            -1.0
3  001 -1 days +18:38:04    4.5            -1.0
4  001 -1 days +18:48:03    4.5            -1.0

&gt;&gt;&gt; df.tail()
     ID     relative_time  value  relative_value
35  001 -1 days +23:58:03    5.5             0.0
36  001          00:08:03    5.5             0.0
37  001          00:18:03    5.5             0.0
38  001          00:28:02    5.5             0.0
39  001          00:38:04    5.5             0.0


I am trying to plot relative_time on the x-axis and relative_value on the y-axis. However, the code below produces an unexpected result, where I cannot what tell what units the x-axis is in. 

# Plot the desired plot.
plt.plot(test['relative_time'], test['relative_value'], marker='.')




Note, the x-axis in the plot above is not in units of hours (relative to time 0). Such a plot would look like the following. 

plt.plot(test['relative_time'] / np.timedelta64(1, 'h'), test['relative_value'], marker='.')




How can I plot the x-axis so that it displays time in the same format as the relative_time column? For example, if the x-axis were to have tick marks every hour, they would be labeled as, -1 days +18:00:00, -1 days +19:00:00, ...,  00:00:00, and 01:00:00. 
",3,1159,,,
matplotlib unexpected result,https://stackoverflow.com/questions/28101851,ipython pandas TypeError: read_csv() got an unexpected keyword argument &#39;delim-whitespace&#39;&#39;,,3,73955,,,
matplotlib unexpected result,https://stackoverflow.com/questions/64952236,matplotlib: how to put picture to a specific point of data-coordinate system,"I'm trying to add picture at the plot in specific place in it by a given coordinates in data-coordinate system. However it runs a little bit unpredicted. Here is a code snippet I wrote:
import matplotlib.pyplot as plt
import numpy as np

fig, ax = plt.subplots(figsize=(5,5))

ax.set_xlim(0, 1)
ax.set_ylim(0, 1)
ax.grid('on')

x = np.arange(0, 1, 0.005)
y = np.exp(-x/2.) * np.sin(2*np.pi*x)
ax.plot(x, y)

im_orange = plt.imread('./test100x100_orange.png')
im_blue = plt.imread('./test100x100_blue.png')

x0, y0 = ax.transData.transform((0,0))
print('transData(0,0) = {}'.format(ax.transData.transform((0,0))))

ax.figure.figimage(im_orange, alpha=0.5)
ax.figure.figimage(im_blue, x0, y0, alpha=0.5)

ax.transData.transform((0,0)) returns transData(0,0) = [45. 45.] which is unexpected since doesn't represent actual position of (0,0) on the plot. Here is result image as well:

My base question is how to put picture left bottom corner exactly at (0,0) point in data-coordinates? And if possible please explain such behaviour of matplotlib.
Update. A few experiments on top. I've run a slightly modified script (provided below by Iammuratc but with plt.savefig() instead) in 3 modes:

From python console (copy and paste):

Python script (something like python test.py:
Result is the same as before.

ipython from Jupiter Notebook:
Suboption A: plt.show()


Suboption B: plt.savefig()

Now it's even more confusing..
",2,2043,"It works for me how you did it. You might check the image arrays in case they are shifted.
import numpy as np
import matplotlib.pyplot as plt


im_orange  = np.zeros((128,128,3),'uint8')
im_orange[:,:,0] = 255

im_blue = np.zeros((128,128,3),'uint8')
im_blue[:,:,2] = 255


fig, ax = plt.subplots(figsize=(5,5))

ax.set_xlim(0, 1)
ax.set_ylim(0, 1)
ax.grid('on')

x = np.arange(0, 1, 0.005)
y = np.exp(-x/2.) * np.sin(2*np.pi*x)
ax.plot(x, y)

# im_orange = plt.imread('./test100x100_orange.png')
# im_blue = plt.imread('./test100x100_blue.png')

x0, y0 = ax.transData.transform((0,0))
print('transData(0,0) = {}'.format(ax.transData.transform((0,0))))

ax.figure.figimage(im_orange,x0,y0, alpha=0.5)
ax.figure.figimage(im_blue, x0, y0, alpha=0.5)

plt.show()


",,
matplotlib unexpected result,https://stackoverflow.com/questions/59798684,How to change axis labels in matplotlib?,"I have the following piece of code which creates a simple plot with matplotlib (python 3.6.9, matplotlib 3.1.2, Mac Mojave):

import numpy as np
import matplotlib.pyplot as plt

plt.imshow(np.random.random((50,50)))
plt.show()


The created plot is as expected:



Now, in order to relabel the xtick/ytick labels I am using the following code

import numpy as np
import matplotlib.pyplot as plt

plt.imshow(np.random.random((50,50)));

ticks, labels = plt.xticks()
labels[1] = '22'
plt.xticks(ticks, labels)

plt.show()


where I expect the second label to be replaced by '22', but everything else stays the same. However, I get the following plot instead:




There is some unexpected white area in the left part of the plot
All the other labels have vanished. 


How to do it correctly? 

Just as a reminder: I want to get the exact same result as the original image (first plot), ONLY with one of the lables changed.

This question has been asked before, one example is here. But the answer does not seem to work. Here is the code

import numpy as np
import matplotlib.pyplot as plt

fig, ax = plt.subplots()
plt.imshow(np.random.random((50,50)))

labels = [item.get_text() for item in ax.get_xticklabels()]
labels[1] = 'Test'
ax.set_xticklabels(labels)

plt.show()


which creates an image as follows:



which does not show the white area anymore, but still the other labels are not shown. 
",2,11703,"Create axes using subplots, so that you can have set_xticklabels method, so you can update the labels.

You need to use, canvas.draw() to get the values.

import numpy as np
import matplotlib.pyplot as plt

fig,ax = plt.subplots()
ax.imshow(np.random.random((50,50)));
fig.canvas.draw()
#labels = ['-10','0','22','20','30','40'] or
labels[2]=22
ax.set_xticklabels(labels)

plt.show()


Output:



Hope this is what you need!
",,
matplotlib unexpected result,https://stackoverflow.com/questions/44273021,Matplotlib: Formatting time on x-axis of stacked horizontal bar graph,"I am trying to graph the amounts of time a user spent on different tasks in a stacked horizontal bar graph using matplotlib. The x-axis is time. Each portion of the bar represents the amount of time spent on the task. However, I am getting an unexpected graph with incorrect formatting. The formatting of the graph requires me to zoom very far on the graph in order to see all of the bars.

The graph does resemble what I need, but at first it looks like this: original graph

And I have to zoom in very far in order to get this: zoomed-in graph

The zoomed-in graph seems to have the correct proportions of the bars except for the first and last data points.

My data is a list of timedelta objects so they may be added together. I am converting them to datetime objects to graph the data.

The code that gives me undesired results is:

import matplotlib.pyplot as plt
from datetime import datetime
from datetime import timedelta
import matplotlib.dates as dt

def sum_times(data, N):
    '''sums the first N datetime elements in a list
       changes from timedelta object to datetime object
    '''
    # initialize
    summ = datetime(1900, 1, 1, 0, 0, 0)
    for i in range(len(data[0:N])):
        summ += data[i]
    return summ

start_date = datetime(1900, 1, 1, 0, 0, 0)
times = [timedelta(0, 737), 
         timedelta(0, 110), 
         timedelta(0, 356), 
         timedelta(0, 171), 
         timedelta(0, 306)]

fig, ax1 = plt.subplots(1,1, sharex=True, sharey=True)

ax1.set_title(""Bug being fixed"")
ax1.set_xlabel('time')
ax1.xaxis_date()

# the graph uses datetime objects
ax1.barh(1, times[0] + start_date)
ax1.barh(1, times[1] + start_date, left=sum_times(times, 1))
ax1.barh(1, times[2] + start_date, left=sum_times(times, 2))
ax1.barh(1, times[3] + start_date, left=sum_times(times, 3))
ax1.barh(1, times[4] + start_date, left=sum_times(times, 4))

ax1.set_xticks(range(1,10))   # arbitrary 

plt.show()


What can I do to change the formatting?

I have tried using ax1.set_xticklabels() with either datetime or timedelta objects, but I get the exception: ValueError: ordinal must be &gt;= 1. 

When I use

ax1.xaxis.set_major_locator(dt.MinuteLocator(interval=60))
ax1.xaxis.set_major_formatter(dt.DateFormatter('%M:%S'))


the formatting is still incorrect.

I am using python 3.6, matplotlib, PyDev IDE, and Windows 8.

If there are any libraries that are better suited to display this sort of data or can supplement matplotlib/pyplot, I would appreciate a solution using one.

Also, I have tried using a loop to automate the calls to ax1.barh(...) but the graph window doesn't display and becomes unresponsive. Since I would like to graph larger data sets, I could use a more elegant solution.
",2,1907,"Building on the suggestion @rvd provided, I have come up with a somewhat better solution using pandas. It's still not ideal in terms of formatting, but I do not need to zoom in ridiculously far in order to see my graph and the data is proportional.
Here is the code that got it working a little better using pandas:
import pandas as pd
import matplotlib.pyplot as plt
from datetime import datetime
from datetime import timedelta
import matplotlib.dates as dt

times = [timedelta(0, 737), 
         timedelta(0, 110), 
         timedelta(0, 356), 
         timedelta(0, 171), 
         timedelta(0, 306)]

start_date = datetime(1900, 1, 1, 0, 0, 0)
times_datetime = [start_date + times[i] for i in range(len(times))]
# pandas requires numerical data on dependent axis
times_num = dt.date2num(times_datetime)
# to make times_num proportionally correct
for i in range(len(times_num)):
    times_num[i] -= dt.date2num(start_date)
    
df = pd.DataFrame([times_num], index=['bugs'])
fig, ax1 = plt.subplots(1,1, sharex=True, sharey=True)
df.plot(kind='barh', ax=ax1, stacked=True)
plt.show()

And this produces: 
The x-axis ticks may not be times, but the data visualization is more along the lines of what I'm looking for.
Thanks!
",,
matplotlib unexpected result,https://stackoverflow.com/questions/28398660,"Matplotlib parametric surface plot unexpected results, why?","I am trying to plot an ellipsoid so, I thought I would amend the example code for a sphere from the matplotlib 3D plotting page.

from mpl_toolkits.mplot3d import Axes3D
import matplotlib.pyplot as plt
from matplotlib import cm
import numpy as np

fig = plt.figure()
ax = fig.add_subplot(111, projection='3d')

# Ellipsoid
u = np.linspace(-np.pi/2.0,np.pi/2.0,100)
v = np.linspace(-np.pi,np.pi,100)
x = 10 * np.outer(np.cos(u), np.cos(v))
y = 10 * np.outer(np.cos(u), np.sin(v))
z = 10 * np.outer(np.ones(np.size(u)), np.sin(v))


# Sphere
#u = np.linspace(0, 2 * np.pi, 100)
#v = np.linspace(0, np.pi, 100)
#x = 10 * np.outer(np.cos(u), np.sin(v))
#y = 10 * np.outer(np.sin(u), np.sin(v))
#z = 10 * np.outer(np.ones(np.size(u)), np.cos(v))

ax.plot_surface(x, y, z,  rstride=4, cstride=4, cmap = cm.copper)
ax.set_xlabel('x-axis')
ax.set_ylabel('y-axis')
ax.set_zlabel('z-axis')
plt.show()


If you run the code you will see that the plot returns an aesthetically pleasing half inside out boat like surface but sadly not an ellipsoid.

Have included the sphere code (commented out) for comparison.

Is there something obvious here that I'm missing?
",2,1174,,,
matplotlib unexpected result,https://stackoverflow.com/questions/24641179,Pandas: Pivoting and plotting workflow,,2,381,,,
matplotlib unexpected result,https://stackoverflow.com/questions/17066313,Pandas plotting two graphs on one scale,"I have a line graph that I am highligting at end with a marker(shown as large red diamond here).

I am using two two pandas plot commands to create it.   Problem is that I get unexpected results.   depending on the length of the data and whether I put the plot for the red diamond first or second I get different results.   There does not seem to be a pattern that I can discern. The Correct / Expected result shown below



sometimes i get:



and most of the time with big data sets I get the following warning:

/Users/xxxxx/.virtualenvs/test2/lib/python2.7/site-packages/matplotlib/axes.py:2542: UserWarning: Attempting to set identical left==right results
in singular transformations; automatically expanding.
left=15727, right=15727
  + 'left=%s, right=%s') % (left, right))

The Warning only shows 1st time it happens.   Obviously pandas does not like support the plotting of 2 different series with different x scales on the same axis? 

Can try code below to generate the graphs, can play around by passing, Series or Dataframes for plot can also reverse the order of the plotting of the red diamond.   Can also change the number of data points.   One error that I could not recreate here is with the red diamond in middle and blue line only going of to left.

Code:

plot_with_series = False
reverse_order = False
import pandas as pd
dates = pd.date_range('20101115', periods=800)
df =  pd.DataFrame(randn(len(dates)), index = dates, columns = ['A'])
ds = pd.Series(randn(len(dates)), index = dates)
clf()
if plot_with_series:
    if reverse_order: ds.plot()
    ds.tail(1).plot(style='rD', markersize=20)
    if not reverse_order: ds.plot()
else:
    if reverse_order: df.plot(legend=False)
    df.A.tail(1).plot(style='rD', markersize=20,legend=False)
    if not reverse_order: df.plot(legend=False)


The errors/warnings happen from both within IPython or from running the as script from command line.  Also constant across 2 latest versions of pandas.  Any ideas or obvious problems?
",2,6618,,,
matplotlib unexpected result,https://stackoverflow.com/questions/76667318,conda environment.yaml package conflict,"When I'm trying to build a git project, the environment,yml won't work properly. It seems like some package conflict, but other user doesn't seem to run into this issue, and I couldn't figure out a solution. My conda version is conda 4.8.2,and down below is the environment.yaml, requirment.txt and error
environment.yml:
name: oneposeplus
channels:
  - pytorch
  - conda-forge
  - defaults
dependencies:
  - python=3.7
  - pytorch=1.8.0
  - torchvision=0.9.1
  - cudatoolkit=10.1
  - ipython
  - tqdm
  - matplotlib
  - pylint
  - conda-forge::jupyterlab
  - conda-forge::h5py=3.1.0
  - conda-forge::loguru=0.5.3
  - conda-forge::scipy
  - conda-forge::numba
  - conda-forge::ipdb
  - conda-forge::albumentations=0.5.1
  - pip
  - pip:
    - -r requirements.txt

requirment.txt:
pytorch-lightning==1.5.10
ray==1.13.0
aiohttp==3.7
aioredis==1.3.1
pydegensac==0.1.2
opencv_python==4.4.0.46
yacs&gt;=0.1.8
pytorch_memlab
joblib
pytorch3d
open3d
einops==0.3.0
kornia==0.4.1
autopep8
pickle5==0.0.11
timm&gt;=0.3.2
hydra-core
omegaconf
pycocotools
wandb
rich
transforms3d
natsort
plyfile
pycolmap==0.3.0

error:
rvl224@eervl224:~/OnePose_Plus_Plus-main$ conda env create -f environment.yaml
Collecting package metadata (repodata.json): done
Solving environment: \ 
Found conflicts! Looking for incompatible packages.
This can take several minutes.  Press CTRL-C to abort.
Examining conflict for pylint numba python matplotlib jupyterlab ipython tqdm scExamining conflict for pylint numba python matplotlib torchvision jupyterlab ipyExamining conflict for pylint numba python matplotlib torchvision jupyterlab ipyExamining conflict for pylint jupyterlab ipython: : 20it [06:56, 20.25s/it]     Examining conflict for albumentations numba matplotlib torchvision h5py scipy pyExamining conflict for albumentations numba matplotlib torchvision h5py scipy pyExamining conflict for albumentations torchvision: : 24it [08:51,  8.89s/it]    Examining conflict for albumentations pytorch torchvision: : 25it [09:21, 37.46sExamining conflict for albumentations pytorch torchvision: : 26it [09:21, 35.40sExamining conflict for albumentations h5py: : 26it [09:35, 35.40s/it]           Examining conflict for albumentations matplotlib torchvision: : 27it [09:41, 28.Examining conflict for albumentations matplotlib torchvision: : 28it [09:41, 21.Examining conflict for albumentations matplotlib: : 28it [10:08, 21.91s/it]     Examining conflict for albumentations numba matplotlib torchvision scipy: : 29itExamining conflict for albumentations numba matplotlib torchvision scipy: : 30itExamining conflict for albumentations numba python matplotlib torchvision h5py cExamining conflict for albumentations numba python matplotlib torchvision h5py cExamining conflict for albumentations pytorch: : 31it [11:39, 25.74s/it]        failed                                                                  

# &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; ERROR REPORT &lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;

    Traceback (most recent call last):
      File ""/home/rvl224/anaconda3/lib/python3.7/site-packages/conda/exceptions.py"", line 1079, in __call__
        return func(*args, **kwargs)
      File ""/home/rvl224/anaconda3/lib/python3.7/site-packages/conda_env/cli/main.py"", line 80, in do_call
        exit_code = getattr(module, func_name)(args, parser)
      File ""/home/rvl224/anaconda3/lib/python3.7/site-packages/conda_env/cli/main_create.py"", line 111, in execute
        result[installer_type] = installer.install(prefix, pkg_specs, args, env)
      File ""/home/rvl224/anaconda3/lib/python3.7/site-packages/conda_env/installers/conda.py"", line 32, in install
        prune=getattr(args, 'prune', False), update_modifier=UpdateModifier.FREEZE_INSTALLED)
      File ""/home/rvl224/anaconda3/lib/python3.7/site-packages/conda/core/solve.py"", line 117, in solve_for_transaction
        should_retry_solve)
      File ""/home/rvl224/anaconda3/lib/python3.7/site-packages/conda/core/solve.py"", line 158, in solve_for_diff
        force_remove, should_retry_solve)
      File ""/home/rvl224/anaconda3/lib/python3.7/site-packages/conda/core/solve.py"", line 281, in solve_final_state
        ssc = self._run_sat(ssc)
      File ""/home/rvl224/anaconda3/lib/python3.7/site-packages/conda/common/io.py"", line 88, in decorated
        return f(*args, **kwds)
      File ""/home/rvl224/anaconda3/lib/python3.7/site-packages/conda/core/solve.py"", line 808, in _run_sat
        should_retry_solve=ssc.should_retry_solve
      File ""/home/rvl224/anaconda3/lib/python3.7/site-packages/conda/common/io.py"", line 88, in decorated
        return f(*args, **kwds)
      File ""/home/rvl224/anaconda3/lib/python3.7/site-packages/conda/resolve.py"", line 1318, in solve
        self.find_conflicts(specs, specs_to_add, history_specs)
      File ""/home/rvl224/anaconda3/lib/python3.7/site-packages/conda/resolve.py"", line 347, in find_conflicts
        bad_deps = self.build_conflict_map(specs, specs_to_add, history_specs)
      File ""/home/rvl224/anaconda3/lib/python3.7/site-packages/conda/resolve.py"", line 507, in build_conflict_map
        root, search_node, dep_graph, num_occurances)
      File ""/home/rvl224/anaconda3/lib/python3.7/site-packages/conda/resolve.py"", line 369, in breadth_first_search_for_dep_graph
        last_spec = MatchSpec.union((path[-1], target_paths[-1][-1]))[0]
      File ""/home/rvl224/anaconda3/lib/python3.7/site-packages/conda/models/match_spec.py"", line 481, in union
        return cls.merge(match_specs, union=True)
      File ""/home/rvl224/anaconda3/lib/python3.7/site-packages/conda/models/match_spec.py"", line 475, in merge
        reduce(lambda x, y: x._merge(y, union), group) if len(group) &gt; 1 else group[0]
      File ""/home/rvl224/anaconda3/lib/python3.7/site-packages/conda/models/match_spec.py"", line 475, in &lt;lambda&gt;
        reduce(lambda x, y: x._merge(y, union), group) if len(group) &gt; 1 else group[0]
      File ""/home/rvl224/anaconda3/lib/python3.7/site-packages/conda/models/match_spec.py"", line 502, in _merge
        final = this_component.union(that_component)
      File ""/home/rvl224/anaconda3/lib/python3.7/site-packages/conda/models/match_spec.py"", line 764, in union
        return '|'.join(options)
    TypeError: sequence item 0: expected str instance, Channel found

`$ /home/rvl224/anaconda3/bin/conda-env create -f environment.yaml`

  environment variables:
                 CIO_TEST=&lt;not set&gt;
        CMAKE_PREFIX_PATH=/opt/ros/noetic
  CONDA_AUTO_UPDATE_CONDA=false
                CONDA_EXE=/home/rvl224/anaconda3/bin/conda
         CONDA_PYTHON_EXE=/home/rvl224/anaconda3/bin/python
               CONDA_ROOT=/home/rvl224/anaconda3
              CONDA_SHLVL=0
            DEFAULTS_PATH=/usr/share/gconf/ubuntu.default.path
          LD_LIBRARY_PATH=/opt/ros/noetic/lib:/opt/ros/noetic/lib/x86_64-linux-
                          gnu:/usr/local/cuda/lib64:
           MANDATORY_PATH=/usr/share/gconf/ubuntu.mandatory.path
                     PATH=/home/rvl224/anaconda3/bin:/home/rvl224/anaconda3/condabin:/opt/ros/no
                          etic/bin:/usr/local/cuda/bin:/home/rvl224/.local/bin:/home/rvl224/.loc
                          al/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/u
                          sr/games:/usr/local/games:/snap/bin
          PKG_CONFIG_PATH=/opt/ros/noetic/lib/pkgconfig:/opt/ros/noetic/lib/x86_64-linux-
                          gnu/pkgconfig
               PYTHONPATH=/opt/ros/noetic/lib/python3/dist-packages
       REQUESTS_CA_BUNDLE=&lt;not set&gt;
         ROS_PACKAGE_PATH=/opt/ros/noetic/share
            SSL_CERT_FILE=&lt;not set&gt;
               WINDOWPATH=2

     active environment : None
            shell level : 0
       user config file : /home/rvl224/.condarc
 populated config files : /home/rvl224/.condarc
          conda version : 4.8.2
    conda-build version : 3.18.11
         python version : 3.7.6.final.0
       virtual packages : __cuda=12.0
                          __glibc=2.31
       base environment : /home/rvl224/anaconda3  (writable)
           channel URLs : https://conda.anaconda.org/conda-forge/linux-64
                          https://conda.anaconda.org/conda-forge/noarch
                          https://repo.anaconda.com/pkgs/main/linux-64
                          https://repo.anaconda.com/pkgs/main/noarch
                          https://repo.anaconda.com/pkgs/r/linux-64
                          https://repo.anaconda.com/pkgs/r/noarch
          package cache : /home/rvl224/anaconda3/pkgs
                          /home/rvl224/.conda/pkgs
       envs directories : /home/rvl224/anaconda3/envs
                          /home/rvl224/.conda/envs
               platform : linux-64
             user-agent : conda/4.8.2 requests/2.22.0 CPython/3.7.6 Linux/5.15.0-76-generic ubuntu/20.04.4 glibc/2.31
                UID:GID : 1000:1000
             netrc file : None
           offline mode : False


An unexpected error has occurred. Conda has prepared the above report.

If submitted, this report will be used by core maintainers to improve
future releases of conda.
Would you like conda to send this report to the core maintainers?

[y/N]: 
Timeout reached. No report sent.

down below are attempt updates to fix the error:
installing micromamba:
## install micromamba
curl micro.mamba.pm/install.sh | bash
export MAMBA_ROOT_PREFIX=/home/rvl224/anaconda3
micromamba update -n base conda

micromamba create -n oneposeplus -f environment.yaml

stuck at:
pkgs/main/linux-64                                            No change
pkgs/r/linux-64                                               No change
pkgs/r/noarch                                                 No change
pkgs/main/noarch                                              No change
pytorch/linux-64                                              No change
pytorch/noarch                                                No change
conda-forge/noarch                                  11.7MB @   2.3MB/s  5.2s
conda-forge/linux-64                                29.2MB @   4.4MB/s  6.9s

install mamba
#install mamba
micromamba install -c conda-forge mamba --root-prefix=/home/rvl224/anaconda3 -n base

mamba create -n oneposeplus -f environment.yaml

error:
Looking for: ['environment.yaml']

warning  libmamba Could not parse mod/etag header
warning  libmamba Could not parse mod/etag header
warning  libmamba Could not parse mod/etag header
warning  libmamba Could not parse mod/etag header
warning  libmamba Could not parse mod/etag header
warning  libmamba Could not parse mod/etag header
pkgs/main/noarch                                   851.4kB @   1.3MB/s  0.7s
pkgs/r/linux-64                                      1.4MB @   1.6MB/s  0.9s
pkgs/r/noarch                                        1.3MB @   1.0MB/s  0.6s
pkgs/main/linux-64                                 @   2.5MB/s  2.4s
conda-forge/noarch                                 @   3.0MB/s  4.6s
conda-forge/linux-64                               @   4.8MB/s  7.4s
Encountered problems while solving:
  - nothing provides requested environment.yaml

tried installing packages one by one in a test environmemt,down below are some error while installing:
mamba install -n test_environment cudatoolkit=10.1 --yes

Encountered problems while solving:
  - nothing provides pytorch 1.10.2 cpu_py37h76afcab_0 needed by pytorch-cpu-1.10.2-cpu_py37h718b53a_0


mamba install -n test_environment ""conda-forge::h5py=3.1.0"" --yes

Encountered problems while solving:
  - package pulseaudio-daemon-16.1-ha8d29e2_3 requires openssl &gt;=3.1.0,&lt;4.0a0, but none of the providers can be installed

mamba install -n test_environment -c conda-forge -c pytorch -c defaults --file requirements.txt --yes


Encountered problems while solving:
  - nothing provides requested ray 1.13.0
  - nothing provides requested pydegensac 0.1.2
  - nothing provides requested opencv_python 4.4.0.46
  - nothing provides requested pytorch_memlab
  - nothing provides requested pytorch3d
  - nothing provides requested open3d
  - nothing provides requested kornia 0.4.1
  - package pycolmap-0.3.0-cpu_py39h5202583_1 requires python &gt;=3.9,&lt;3.10.0a0, but none of the providers can be installed

change into:
cudatoolkit=10.2
h5py (without version)

still figuring what to do with requirement.txt
",1,1059,"Had to manually install every package,idk why environment.yml doesn't work
mamba create -n oneposeplus python=3.7 --yes
conda activate oneposeplus

pip install torch==1.8.0 --no-input
pip install torchvision==0.9.1 --no-input
pip install matplotlib --no-input
pip install ipython --no-input
pip install tqdm --no-input
pip install pylint --no-input
pip install jupyterlab --no-input
pip install ""h5py==3.1.0"" --no-input
pip install loguru==0.5.3 --no-input
pip install scipy --no-input
pip install numba --no-input
pip install ipdb --no-input
pip install ""albumentations==0.5.1"" --no-input

pip install pytorch-lightning==1.5.10 --no-input
pip install ray==1.13.0 --no-input
pip install aiohttp==3.7 --no-input
pip install aioredis==1.3.1 --no-input
pip install pydegensac==0.1.2 --no-input
pip install opencv-python==4.4.0.46 --no-input
pip install ""yacs&gt;=0.1.8"" --no-input
pip install pytorch_memlab --no-input
pip install joblib --no-input
pip install pytorch3d --no-input
pip install open3d --no-input
pip install ""einops==0.3.0"" --no-input
pip install ""kornia==0.4.1"" --no-input
pip install autopep8 --no-input
pip install ""pickle5==0.0.11"" --no-input
pip install ""timm&gt;=0.3.2"" --no-input
pip install hydra-core --no-input
pip install omegaconf --no-input
pip install pycocotools --no-input
pip install wandb --no-input
pip install rich --no-input
pip install transforms3d --no-input
pip install natsort --no-input
pip install plyfile --no-input
pip install ""pycolmap==0.3.0"" --no-input

Thanks for @merv for helping.
",,
matplotlib unexpected result,https://stackoverflow.com/questions/75172467,"Extrude a concave, complex polygon in PyVista","I wish to take a concave and complex (containing holes) polygon and extrude it 'vertically' into a polyhedron, purely for visualisation. I begin with a shapely Polygon, like below:
poly = Polygon(
    [(0,0), (10,0), (10,10), (5,8), (0,10), (1,7), (0,5), (1,3)], 
    holes=[
        [(2,2),(4,2),(4,4),(2,4)],
        [(6,6), (7,6), (6.5,6.5), (7,7), (6,7), (6.2,6.5)]])

which I correctly plot (reorientating the exterior coordinates to be clockwise, and the hole coordinates to be counterclockwise) in matplotlib as:

I then seek to render this polygon extruded out-of-the-page (along z), using PyVista. There are a few hurdles; PyVista doesn't directly support concave (nor complex) input to its PolyData type. So we first create an extrusion of simple (hole-free) concave polygons, as per this discussion.
def extrude_simple_polygon(xy, z0, z1):

    # force counter-clockwise ordering, so PyVista interprets polygon correctly
    xy = _reorient_coords(xy, clockwise=False)

    # remove duplication of first &amp; last vertex
    xyz0 = [(x,y,z0) for x,y in xy]
    if (xyz0[0] == xyz0[-1]):
        xyz0.pop()

    # explicitly set edge_source
    base_vert = [len(xyz0)] + list(range(len(xyz0)))
    base_data = pyvista.PolyData(xyz0, base_vert)
    base_mesh = base_data.delaunay_2d(edge_source=base_data)
    vol_mesh  = base_mesh.extrude((0, 0, z1-z0), capping=True)

    # force triangulation, so PyVista allows boolean_difference
    return vol_mesh.triangulate()

Observe this works when extruding the outer polygon and each of its internal polygons in-turn:
extrude_simple_polygon(list(poly.exterior.coords), 0, 5).plot()


extrude_simple_polygon(list(poly.interiors[0].coords), 0, 5).plot()
extrude_simple_polygon(list(poly.interiors[1].coords), 0, 5).plot()



I reasoned that to create an extrusion of the original complex polygon, I could compute the boolean_difference. Alas, the result of
outer_vol = extrude_simple_polygon(list(poly.exterior.coords), 0, 5)
for hole in poly.interiors:
    hole_vol = extrude_simple_polygon(list(hole.coords), 0, 5)
    outer_vol = outer_vol.boolean_difference(hole_vol)

outer_vol.plot()

is erroneous:

The doc advises to inspect the normals via plot_normals, revealing that all extruded volumes have inward-pointing (or else, unexpected) normals:



The extrude doc mentions nothing of the extruded surface normals nor the original object (in this case, a polygon) orientation.
We could be forgiven for expecting our polygons must be clockwise, so we set clockwise=True in the first line of extrude_simple_polygon and try again. Alas, PolyData now  misinterprets our base polygon; calling base_mesh.plot() reveals (what should look like our original blue outer polygon):

with extrusion


Does PyVista always expect counter-clockwise polygons?
Why does extrude create volumes with inward-pointing surface normals?
How can I correct the extruded surface normals?
Otherwise, how can I make PyVista correctly visualise what should be an incredibly simply-extruded concave complex polygon??

",1,788,"You're very close. What you have to do is use a single call to delaunay_2d() with all three polygons (i.e., the enclosing one and the two holes) as edge source (loop source?). It's also important to have faces (rather than lines) from each polygon; this is what makes it possible to enforce the holeyness of the holes.
Here's a complete example for your input (where I manually flipped the orientation of the holes; you seem to have a _reorient_coords() helper that you should use instead):
import pyvista as pv

# coordinates of enclosing polygon
poly_points = [
    (0, 0), (10, 0), (10, 10), (5, 8), (0, 10), (1, 7), (0, 5), (1, 3),
]
# hole point order hard-coded here; use your _reorient_coords() function
holes = [
    [(2, 2), (4, 2), (4, 4), (2, 4)][::-1],
    [(6, 6), (7, 6), (6.5, 6.5), (7, 7), (6, 7), (6.2, 6.5)][::-1],
]

z0, z1 = 0.0, 5.0

def points_2d_to_poly(points, z):
    """"""Convert a sequence of 2d coordinates to a polydata with a polygon.""""""
    faces = [len(points), *range(len(points))]
    poly = pv.PolyData([p + (z,) for p in points], faces=faces)
    return poly

# bounding polygon
polygon = points_2d_to_poly(poly_points, z0)

# add all holes
for hole_points in holes:
    polygon += points_2d_to_poly(hole_points, z0)

# triangulate poly with all three subpolygons supplying edges
# (relative face orientation is critical here)
polygon_with_holes = polygon.delaunay_2d(edge_source=polygon)

# extrude
holey_solid = polygon_with_holes.extrude((0, 0, z1 - z0), capping=True)
holey_solid.plot()


Here's the top view of the polygon pre-extrusion:
plotter = pv.Plotter()
plotter.add_mesh(polygon_with_holes, show_edges=True, color='cyan')
plotter.view_xy()
plotter.show()


",,
matplotlib unexpected result,https://stackoverflow.com/questions/70915345,Why did I get unexpected field names: [&#39;is_dynamic_op&#39;]?,"I am working on a low light video processing project where I am getting some errors in some areas,
For this code.
params = trt.DEFAULT_TRT_CONVERSION_PARAMS._replace(
    precision_mode='FP16',
    is_dynamic_op = True)

I am getting this error.
&gt; --------------------------------------------------------------------------- ValueError                                Traceback (most recent call
&gt; last) &lt;ipython-input-8-326230ed5373&gt; in &lt;module&gt;()
&gt;       2 params = trt.DEFAULT_TRT_CONVERSION_PARAMS._replace(
&gt;       3     precision_mode='FP16',
&gt; ----&gt; 4     is_dynamic_op = True)
&gt;       5 
&gt;       6 # Convert the model
&gt; 
&gt; /usr/lib/python3.7/collections/__init__.py in _replace(_self, **kwds)
&gt;     414         result = _self._make(map(kwds.pop, field_names, _self))
&gt;     415         if kwds:
&gt; --&gt; 416             raise ValueError(f'Got unexpected field names: {list(kwds)!r}')
&gt;     417         return result
&gt;     418 
&gt; 
&gt; ValueError: Got unexpected field names: ['is_dynamic_op']

I have used these libraries,
from glob import glob
from PIL import Image
from matplotlib import pyplot as plt
from mirnet.inference import Inferer
from mirnet.utils import download_dataset, plot_result
from tensorflow.python.compiler.tensorrt import trt_convert as trt

import tensorflow as tf
import numpy as np
import time

I have imported all the libraries but am still stuck.
",1,304,"Firstly, it seems that trt.DEFAULT_TRT_CONVERSION_PARAMS doesn't have the field 'is_dynamic_op'. My guess would be that, some documentation exists for the library, and there you can probably see whether the field is settable or not.
Secondly, it seems you are using TensorFlow and the TensorRT API. (but again, it's a guess...). In the source-code of the library, they use ""is_dynamic_op"" on some ""rewrite_config"". Maybe this helps you reformat your code to work: https://github.com/tensorflow/tensorflow/blob/dd38449b8ac3fd9ffaa98349d39d36ec26e72dfe/tensorflow/python/compiler/tensorrt/trt_convert.py#L1136
",,
matplotlib unexpected result,https://stackoverflow.com/questions/51152563,Not able to plot the series type when using python3,"I am using Python3 and I got the problem when I have been working on it.
I coded as below and I found the right results that I expected. And then I want to see the result as diagram. So I tried to plot it but I got the unexpected error indicating that I need the matplotlib. Since I imported the matplotlib at the first code, I have no idea solving this issue.

import matplotlib.pyplot as plt
%matplotlib inline
train.Survived[train['Name']=='Mr'].value_counts()

#Result
0    436
1     81
Name: Survived, dtype: int64


I typed the below code to see them as pie-like diagram.

train.Survived[train['Name']=='Mr'].value_counts().plot(kind='pie')


The error message is as below.

---------------------------------------------------------------------------
ImportError                               Traceback (most recent call last)
&lt;ipython-input-191-7f60a15206ad&gt; in &lt;module&gt;()
      1 import pandas.plotting
----&gt; 2 train.Survived[train['Name']=='Mr'].value_counts().plot(kind='pie')

/usr/local/lib/python3.6/site-packages/pandas/plotting/_core.py in __call__(self, kind, ax, figsize, use_index, title, grid, legend, style, logx, logy, loglog, xticks, yticks, xlim, ylim, rot, fontsize, colormap, table, yerr, xerr, label, secondary_y, **kwds)
   2739                            colormap=colormap, table=table, yerr=yerr,
   2740                            xerr=xerr, label=label, secondary_y=secondary_y,
-&gt; 2741                            **kwds)
   2742     __call__.__doc__ = plot_series.__doc__
   2743 

/usr/local/lib/python3.6/site-packages/pandas/plotting/_core.py in plot_series(data, kind, ax, figsize, use_index, title, grid, legend, style, logx, logy, loglog, xticks, yticks, xlim, ylim, rot, fontsize, colormap, table, yerr, xerr, label, secondary_y, **kwds)
   2000                  yerr=yerr, xerr=xerr,
   2001                  label=label, secondary_y=secondary_y,
-&gt; 2002                  **kwds)
   2003 
   2004 

/usr/local/lib/python3.6/site-packages/pandas/plotting/_core.py in _plot(data, x, y, subplots, ax, kind, **kwds)
   1757                 data = data[y].copy()
   1758                 data.index.name = y
-&gt; 1759         plot_obj = klass(data, subplots=subplots, ax=ax, kind=kind, **kwds)
   1760     else:
   1761         if isinstance(data, ABCDataFrame):

/usr/local/lib/python3.6/site-packages/pandas/plotting/_core.py in __init__(self, data, kind, **kwargs)
   1498         if (data &lt; 0).any().any():
   1499             raise ValueError(""{0} doesn't allow negative values"".format(kind))
-&gt; 1500         MPLPlot.__init__(self, data, kind=kind, **kwargs)
   1501 
   1502     def _args_adjust(self):

/usr/local/lib/python3.6/site-packages/pandas/plotting/_core.py in __init__(self, data, kind, by, subplots, sharex, sharey, use_index, figsize, grid, legend, rot, ax, fig, title, xlim, ylim, xticks, yticks, sort_columns, fontsize, secondary_y, colormap, table, layout, **kwds)
    105                  table=False, layout=None, **kwds):
    106 
--&gt; 107         _raise_if_no_mpl()
    108         _converter._WARN = False
    109         self.data = data

/usr/local/lib/python3.6/site-packages/pandas/plotting/_core.py in _raise_if_no_mpl()
     55     # TODO(mpl_converter): remove once converter is explicit
     56     if not _HAS_MPL:
---&gt; 57         raise ImportError(""matplotlib is required for plotting."")
     58 
     59 

ImportError: matplotlib is required for plotting.


I really appreciate it if you could resolve my issue.
Thanks.
",1,997,"If you are using Jupyter (I was) and you have matplotlib installed and imported correctly, try to restart the kernel in your notebook:

Menu &gt; Kernel &gt; Restart

Then re-run all the needed code. This solved my problem. I got the idea from here. It looks like other problem with the same solution.

Hope it helps.
",,
matplotlib unexpected result,https://stackoverflow.com/questions/65747436,problems using local jupyter with google colab,"I am using jupyter notebook with google colab, on Ubuntu 20.04LTS with Python 3.8.5.
Problems I face are:

connection drops suddenly. I mean, to say, colab never executes a cell, instead it just keeps waiting. By waiting, I mean, the cell I run always looks like this

and never turns into this: . But, colab does say that I am connected to an instance: .
everytime I write something and wait for autocomplete to show suggestions, I receive a TypeError that says, there was unexpected keyword argument 'column'. Here's the complete error message right from the launch of jupyter:

(piu_venv) ubuntu@ip-xxx-xxx-xxx-xxx:~/pump_it_up$ jupyter notebook --NotebookApp.allow_origin='https://colab.research.google.com' --port=8888 --NotebookApp.port_retries=0
jupyter_http_over_ws extension initialized. Listening on /http_over_websocket
[I 07:14:12.316 NotebookApp] Serving notebooks from local directory: /home/ubuntu/pump_it_up
[I 07:14:12.316 NotebookApp] Jupyter Notebook 6.2.0 is running at:
[I 07:14:12.316 NotebookApp] http://localhost:8888/?token=c5d00b63744c7a6cd9229df8340c46bc18506d3994c208e1
[I 07:14:12.316 NotebookApp]  or http://127.0.0.1:8888/?token=c5d00b63744c7a6cd9229df8340c46bc18506d3994c208e1
[I 07:14:12.316 NotebookApp] Use Control-C to stop this server and shut down all kernels (twice to skip confirmation).
[W 07:14:12.319 NotebookApp] No web browser found: could not locate runnable browser.
[C 07:14:12.319 NotebookApp] 
    
    To access the notebook, open this file in a browser:
        file:///home/ubuntu/.local/share/jupyter/runtime/nbserver-11870-open.html
    Or copy and paste one of these URLs:
        http://localhost:8888/?token=c5d00b63744c7a6cd9229df8340c46bc18506d3994c208e1
     or http://127.0.0.1:8888/?token=c5d00b63744c7a6cd9229df8340c46bc18506d3994c208e1
[I 07:14:19.632 NotebookApp] 302 GET /?token=0b923e1d3d46db65bf3fe322c20bc0e4f5515f243873e2f7 (127.0.0.1) 0.250000ms
[I 07:14:19.634 NotebookApp] 302 GET /tree?token=0b923e1d3d46db65bf3fe322c20bc0e4f5515f243873e2f7 (127.0.0.1) 0.330000ms
[W 07:14:19.657 NotebookApp] Forbidden
[W 07:14:19.657 NotebookApp] 403 GET /api/kernelspecs (127.0.0.1) 0.420000ms referer=None
[I 07:14:23.317 NotebookApp] 302 GET /?token=c5d00b63744c7a6cd9229df8340c46bc18506d3994c208e1 (127.0.0.1) 0.240000ms
[I 07:14:23.461 NotebookApp] 302 GET /?token=c5d00b63744c7a6cd9229df8340c46bc18506d3994c208e1 (127.0.0.1) 0.230000ms
[I 07:14:23.478 NotebookApp] Kernel started: 2442098f-efe2-44fa-8916-2b8081a4904b, name: python3
[I 07:14:23.968 NotebookApp] 302 GET /?token=c5d00b63744c7a6cd9229df8340c46bc18506d3994c208e1 (127.0.0.1) 0.270000ms
[I 07:14:23.971 NotebookApp] proxying WebSocket connection to: ws://localhost:8888/api/kernels/2442098f-efe2-44fa-8916-2b8081a4904b/channels?session_id=dd75a32fa4b4426587f78beb823c6122&amp;jupyter_http_over_ws_auth_url=http%3A%2F%2Flocalhost%3A8888%2F%3Ftoken%3Dc5d00b63744c7a6cd9229df8340c46bc18506d3994c208e1
[I 07:14:26.402 NotebookApp] 302 GET /?token=c5d00b63744c7a6cd9229df8340c46bc18506d3994c208e1 (127.0.0.1) 0.260000ms
[I 07:14:45.852 NotebookApp] 302 GET /?token=c5d00b63744c7a6cd9229df8340c46bc18506d3994c208e1 (127.0.0.1) 0.280000ms
[I 07:14:47.477 NotebookApp] 302 GET /?token=c5d00b63744c7a6cd9229df8340c46bc18506d3994c208e1 (127.0.0.1) 0.230000ms
[IPKernelApp] ERROR | Exception in message handler:
Traceback (most recent call last):
  File ""/home/ubuntu/pump_it_up/piu_venv/lib/python3.8/site-packages/ipykernel/kernelbase.py"", line 265, in dispatch_shell
    yield gen.maybe_future(handler(stream, idents, msg))
  File ""/home/ubuntu/pump_it_up/piu_venv/lib/python3.8/site-packages/tornado/gen.py"", line 762, in run
    value = future.result()
  File ""/home/ubuntu/pump_it_up/piu_venv/lib/python3.8/site-packages/tornado/gen.py"", line 234, in wrapper
    yielded = ctx_run(next, result)
  File ""/home/ubuntu/pump_it_up/piu_venv/lib/python3.8/site-packages/ipykernel/kernelbase.py"", line 580, in complete_request
    matches = yield gen.maybe_future(self.do_complete(code, cursor_pos))
  File ""/home/ubuntu/pump_it_up/piu_venv/lib/python3.8/site-packages/ipykernel/ipkernel.py"", line 356, in do_complete
    return self._experimental_do_complete(code, cursor_pos)
  File ""/home/ubuntu/pump_it_up/piu_venv/lib/python3.8/site-packages/ipykernel/ipkernel.py"", line 381, in _experimental_do_complete
    completions = list(_rectify_completions(code, raw_completions))
  File ""/home/ubuntu/pump_it_up/piu_venv/lib/python3.8/site-packages/IPython/core/completer.py"", line 484, in rectify_completions
    completions = list(completions)
  File ""/home/ubuntu/pump_it_up/piu_venv/lib/python3.8/site-packages/IPython/core/completer.py"", line 1818, in completions
    for c in self._completions(text, offset, _timeout=self.jedi_compute_type_timeout/1000):
  File ""/home/ubuntu/pump_it_up/piu_venv/lib/python3.8/site-packages/IPython/core/completer.py"", line 1861, in _completions
    matched_text, matches, matches_origin, jedi_matches = self._complete(
  File ""/home/ubuntu/pump_it_up/piu_venv/lib/python3.8/site-packages/IPython/core/completer.py"", line 2029, in _complete
    completions = self._jedi_matches(
  File ""/home/ubuntu/pump_it_up/piu_venv/lib/python3.8/site-packages/IPython/core/completer.py"", line 1373, in _jedi_matches
    interpreter = jedi.Interpreter(
  File ""/home/ubuntu/pump_it_up/piu_venv/lib/python3.8/site-packages/jedi/api/__init__.py"", line 725, in __init__
    super().__init__(code, environment=environment,
TypeError: __init__() got an unexpected keyword argument 'column'


Details about computer:
This is actually an AWS EC2 instance(c5a.4xlarge) I have port mapped to my local laptop, to be able to use jupyter notebook on colab. Both my laptop and EC2 instance are running Ubuntu 20.04LTS.
Package versions:
I have installed jupyter on 16-Jan-2021 by doing pip3 install jupyter. So, I believe I got the latest version available and here are the package versions along with some dependencies of jupyter:
ipykernel==5.4.3
ipython==7.19.0
ipython-genutils==0.2.0
ipywidgets==7.6.3
jedi==0.18.0
Jinja2==2.11.2
jsonschema==3.2.0
jupyter==1.0.0
jupyter-client==6.1.11
jupyter-console==6.2.0
jupyter-core==4.7.0
jupyter-http-over-ws==0.0.8
jupyterlab-pygments==0.1.2
jupyterlab-widgets==1.0.0
kiwisolver==1.3.1
MarkupSafe==1.1.1
matplotlib==3.3.3
mistune==0.8.4
mpmath==1.1.0
nbclient==0.5.1
nbconvert==6.0.7
nbformat==5.1.2
nest-asyncio==1.4.3
notebook==6.2.0

How do I fix these issues?
",0,1314,"The second issue is caused by your jedi version. You can fix it with:
pip install --upgrade 'jedi&lt;0.18.0'

Related issue &amp; discussion: https://github.com/ipython/ipython/issues/12745#issuecomment-751892538
",,
matplotlib unexpected result,https://stackoverflow.com/questions/55995209,How to make matplotlib handle custom class &quot;units&quot;,"I am trying to make my class compatible with matplotlib's units and facing an unexpected behaviour.

Here is a simplified version of my custom class, that is not a subclass of numpy's ndarray :

import numpy as np
import matplotlib
import matplotlib.units as units

class Toto:
    def __init__(self, value_like, unit_like):
        self.value_like = value_like # typically a scalar or array
        self.unit_like = unit_like # a string describing the unit

    def __array__(self, *args, **kwargs):
        return np.array(self.value_like, *args, **kwargs)

# To test if plot as expected, without units handling
arr_x = Toto(np.arange(5), ""meter"")
arr_y = Toto(np.arange(5), ""second"")
plt.plot(arr_x, arr_y)


Notice I added a __array__ method in order to make it ""plottable"" with matplotlib (if not, I get a TypeError: float() argument must be a string or a number, not 'Toto' exception, when numpy tries to cast Toto with array(toto_instance, float)). I suspect my problem actually come from this method but I can't figure out why/how. Anyway, moving on the actual problem :

Now I followed the example in the doc to make a conversion-interface for my Toto class:

class TotoConverter(units.ConversionInterface):

    @staticmethod
    def convert(value, unit, axis):
        'Convert a toto object value to a scalar or array'
        old_toto_unit = axis.get_unit()
        # stupid computation to determine new_unit (simpler for a MWE)
        new_unit = old_toto_unit
        new_toto = Toto(value, new_unit)
        return new_toto.value_like

    @staticmethod
    def axisinfo(unit, axis):
        return units.AxisInfo(label=str(unit))

    @staticmethod
    def default_units(x, axis):
        'Return the default unit for x or None'
        return getattr(x, 'unit_like', None)



In the end, I add the conversion interface of my class to matplotlib's registry of conversion interfaces:

units.registry[Toto] = TotoConverter()


Then the problem :
At this point, I should get the unit on the label when plotting Toto instances, but I get the same result as before defining and registering my unit conversion-interface. Why is that ? 

I suspect the conversion object is never called since my Toto instances are converted into ndarray but I'm not sure

Cheers
",0,410,"I suspect you mean something like this, where you have a list/array of Totos, and not a Toto of values.

import matplotlib.pyplot as plt
import matplotlib.units as units

class Toto:
    def __init__(self, value_like, unit_like):
        self.value_like = value_like # typically a scalar or array
        self.unit_like = unit_like # a string describing the unit

class TotoConverter(units.ConversionInterface):

    @staticmethod
    def convert(value, unit, axis):
        if isinstance(value, Toto):
            return value.value_like
        else:
            return [toto.value_like for toto in value]

    @staticmethod
    def axisinfo(unit, axis):
        return units.AxisInfo(label=str(unit))

    @staticmethod
    def default_units(x, axis):
        'Return the default unit for x or None'
        if isinstance(x, Toto):
            return getattr(x, 'unit_like', None)
        else:
            return getattr(x[0], 'unit_like', None)


Then register and use it,

units.registry[Toto] = TotoConverter()


arr_x = [Toto(i, ""meter"") for i in range(5)]
arr_y = [Toto(i, ""second"") for i in range(5)]

plt.plot(arr_x, arr_y)            #use lists of Totos
plt.axhline(Toto(2, ""second""))    # use Toto scalars
plt.xlim(Toto(-1, ""meter""), None) # use Toto scalars

plt.show()

",,
matplotlib unexpected result,https://stackoverflow.com/questions/54386130,How to set x-axis tick mark labels using matplotlib in python?,"I only want x-labels displayed for the bars in the bar chart using matplotlib in python i.e. 1992, 1993, 1994, 1995 instead of 1990, 1990.5, 1991, 1991.5, etc on the x-axis which is what it does by default

I've tried using the code suggested here: Manipulating x axis tick labels in matplotlib
which suggests using this: ind = range(2,6); plt.xticks(ind,x)

import pandas as pd
import numpy as np

np.random.seed(12346)

df = pd.DataFrame([np.random.normal(32000,200000,3650), 
                   np.random.normal(43000,100000,3650), 
                   np.random.normal(43500,140000,3650), 
                   np.random.normal(48000,70000,3650)], 
                  index=[1992,1993,1994,1995])
Y = 40000

import math

%matplotlib notebook

plt.figure()

std = df.std(axis = 1)
stderror = std/math.sqrt(df.shape[1])
marginoferror = stderror*1.96
print(df.mean(axis=1).iloc[0] - marginoferror.iloc[0])

def color_def(y,dataframe,mofe):
    color = []
    for i in range(0,dataframe.shape[0]):
        if (dataframe.mean(axis=1).iloc[i] - mofe.iloc[i] &gt; y):
            color.append('darkred')
        elif (dataframe.mean(axis=1).iloc[i] + mofe.iloc[i] &lt; y):
            color.append('darkblue')
        else:
            color.append('white')
    return(color)
x = df.index.values
plt.bar(df.index.values,df.mean(axis=1),yerr=marginoferror,align='center', color = color_def(Y,df,marginoferror),alpha=0.5, edgecolor = 'black',ecolor='black', capsize=10)

plt.axhline(y=Y)

ind = range(2, 6)    # the x locations for the groups
plt.xticks( ind, x)


When I use these 2 lines of code  ind = range(2,6); plt.xticks(ind,x), the unexpected result can be seen at this link:
https://www.dropbox.com/s/0ciuix13nhdj5jz/HW3%20v0.1.jpg?dl=0

which is not a nicely formatted bar graph and does not have the x-axis labeled with 1992, 1993, 1994 and 1995.

When I take these 2 lines of code out: 
ind = range(2,6); plt.xticks(ind,x),
I do get a nicely formatted bar chart just with the clunky x-axis labeling
",0,584,"Not sure how the numbers 2 to 5 would relate to your data. Your data is 1992 ... 1995, which is the index of the dataframe, hence

plt.xticks(df.index.values)


would tick exactly those values.
",,
matplotlib unexpected result,https://stackoverflow.com/questions/46613117,Why does conda install tk not work in my docker container even though it says its installed?,"I was having issues with tk in my python 3 docker container.

I tried:

conda install tk


but it says it did install it

root@36602e2cd649:/home_simulation_research/overparametrized_experiments/pytorch_experiments# conda install tk
Fetching package metadata ...........
Solving package specifications: .

# All requested packages already installed.
# packages in environment at /opt/conda:
#
tk                        8.6.7                h5979e9b_1


but when I go to python and try to import it it does not work:

&gt;&gt;&gt; import Tkinter
Traceback (most recent call last):
  File ""&lt;stdin&gt;"", line 1, in &lt;module&gt;
ImportError: No module named 'Tkinter'


and other errors:

&gt;&gt;&gt; import tkinter
Traceback (most recent call last):
  File ""&lt;stdin&gt;"", line 1, in &lt;module&gt;
  File ""/opt/conda/envs/pytorch-py35/lib/python3.5/tkinter/__init__.py"", line 35, in &lt;module&gt;
    import _tkinter # If this fails your Python may not be configured for Tk
ImportError: libX11.so.6: cannot open shared object file: No such file or directory


when I run a script that needs it:

Traceback (most recent call last):
  File ""bulk_experiment_dispatcher.py"", line 18, in &lt;module&gt;
    from data_file import *
  File ""/home_simulation_research/overparametrized_experiments/pytorch_experiments/data_file.py"", line 16, in &lt;module&gt;
    import matplotlib.pyplot as plt
  File ""/opt/conda/envs/pytorch-py35/lib/python3.5/site-packages/matplotlib/pyplot.py"", line 115, in &lt;module&gt;
    _backend_mod, new_figure_manager, draw_if_interactive, _show = pylab_setup()
  File ""/opt/conda/envs/pytorch-py35/lib/python3.5/site-packages/matplotlib/backends/__init__.py"", line 32, in pylab_setup
    globals(),locals(),[backend_name],0)
  File ""/opt/conda/envs/pytorch-py35/lib/python3.5/site-packages/matplotlib/backends/backend_tkagg.py"", line 6, in &lt;module&gt;
    from six.moves import tkinter as Tk
  File ""/opt/conda/envs/pytorch-py35/lib/python3.5/site-packages/six.py"", line 92, in __get__
    result = self._resolve()
  File ""/opt/conda/envs/pytorch-py35/lib/python3.5/site-packages/six.py"", line 115, in _resolve
    return _import_module(self.mod)
  File ""/opt/conda/envs/pytorch-py35/lib/python3.5/site-packages/six.py"", line 82, in _import_module
    __import__(name)
  File ""/opt/conda/envs/pytorch-py35/lib/python3.5/tkinter/__init__.py"", line 35, in &lt;module&gt;
    import _tkinter # If this fails your Python may not be configured for Tk
ImportError: libX11.so.6: cannot open shared object file: No such file or directory


I tried apt-get install python-tk (from Install tkinter for Python) but it did not work:

root@36602e2cd649:/home_simulation_research/overparametrized_experiments/pytorch_experiments# apt-get install python-tk
Reading package lists... Done
Building dependency tree
Reading state information... Done
E: Unable to locate package python-tk




I tried running ENTERYPOINT as one of the answers suggested but it threw me some more errors:

/path/fake_gui.sh: 8: /home_simulation_research/overparametrized_experiments/docker_files/runtime/fake_gui.sh: source: not found
/path/fake_gui.sh: 12: /home_simulation_research/overparametrized_experiments/docker_files/runtime/fake_gui.sh: function: not found
/path/fake_gui.sh: 13: kill: invalid signal number or name: SIGTERM
/path/fake_gui.sh: 15: /home_simulation_research/overparametrized_experiments/docker_files/runtime/fake_gui.sh: Syntax error: ""}"" unexpected


but not sure what to do...



Helpful questions:

How to install python-tk in my docker image
",0,5461,"Ok so once I put a dummy screen in the image it stopped crashing:

RUN apt-get update
RUN apt-get install -y xvfb
#RUN Xvfb :1 -screen 0 1024x768x16 &amp;&gt; xvfb.log  &amp;


when I ran my docker image.
","You need to use a Docker container that has a virtual framebuffer installed and running.

This blog post explains the ""Why and How"" of putting X11 into a docker container.

You can see how they're doing it in the Selenium Docker container via their entry_point.sh:

#!/bin/bash
#
# IMPORTANT: Change this file only in directory Standalone!

source /opt/bin/functions.sh

export GEOMETRY=""$SCREEN_WIDTH""""x""""$SCREEN_HEIGHT""""x""""$SCREEN_DEPTH""

function shutdown {
  kill -s SIGTERM $NODE_PID
  wait $NODE_PID
}

if [ ! -z ""$SE_OPTS"" ]; then
  echo ""appending selenium options: ${SE_OPTS}""
fi

SERVERNUM=$(get_server_num)

rm -f /tmp/.X*lock

xvfb-run -n $SERVERNUM --server-args=""-screen 0 $GEOMETRY -ac +extension RANDR"" \
  java ${JAVA_OPTS} -jar /opt/selenium/selenium-server-standalone.jar \
  ${SE_OPTS} &amp;
NODE_PID=$!

trap shutdown SIGTERM SIGINT
wait $NODE_PID


Source for the above code is from this repository on GitHub, SeleniumHQ/docker-selenium.
","With python 3, you must import as follows:

import tkinter   # with a small caps 't'

"
matplotlib unexpected result,https://stackoverflow.com/questions/43132792,matplotlib unexpected results polar plot,"I am trying to plot simple function r = 3*sin(2*theta) using matplotlib:

import numpy as np
import matplotlib.pyplot as plt
theta = np.arange(0,2*np.pi,0.01)
r = 3.0*np.sin(2.0*theta)
ax = plt.subplot(111, projection='polar')
ax.plot(theta, r)
plt.show()


This is the result I get (it is not correct):



This is what I expect to see (wolfram alpha): 

Am I missing something?
Thanks!
",0,537,,,
matplotlib unexpected result,https://stackoverflow.com/questions/38512870,Can AxesGrid be used to plot two imshows (overlay) with two separate colorbars?,"I am using AxesGrid in matplotlib to create a plot that overlays two imshow plots, with a separate colourbar side by side for each image colourmap. While I can see in this question/answer that using pyplot.colorbar() automatically plots the second colour bar next to the first, this doesn't seem to work with AxesGrid.

    import matplotlib.pyplot as plt
    from mpl_toolkits.axes_grid1 import AxesGrid

    fig = plt.figure(1, facecolor='white',figsize=(10,7.5))

    grid = AxesGrid(fig, 111, 
                    nrows_ncols=(1, 1),
                    axes_pad=(0.45, 0.15),
                    label_mode=""1"",
                    share_all=True,
                    cbar_location=""right"",
                    cbar_mode=""each"",
                    cbar_size=""7%"",
                    cbar_pad=""2%"",
                    )

    im = grid[0].imshow(my_image, my_cmap)
    cbar = grid.cbar_axes[0].colorbar(im)

    im2 = grid[0].imshow(my_image_overlay, my_cmap2, alpha=0.5)
    cbar2 = grid.cbar_axes[0].colorbar(im2)

    plt.show()


However, this just shows the second colourbar. (Presumably overlaying the first one). I tried overriding the padding in cbar2 with:

    cbar2 = grid.cbar_axes[0].colorbar(im2, pad=0.5)


but this results in an error with ""got an unexpected keyword argument 'pad'""

Is there a way to offset the second colourbar?
",0,349,,,
matplotlib unexpected result,https://stackoverflow.com/questions/30292291,matplotlib line with markers and line width - unexpected results,"Seems like a super simple thing, hopefully I'm missing something obvious or there is a bug in matplotlib. I'm using matplotlib 1.4.3 if that helps.

from pylab import *
t = arange(0.0, 2.0, 0.01)
s = sin(2 * pi * t)
plot(t, s, c='m', lw=10)




Everything looks good. Awesome.

from pylab import *
t = arange(0.0, 2.0, 0.01)
s = sin(2 * pi * t)
plot(t, s, c='m', lw=10, marker='o', mfc='m')




Say what?? My line is now blue, even though I have c='m'??

Even more confusing ... don't set the line width...

from pylab import *
t = arange(0.0, 2.0, 0.01)
s = sin(2 * pi * t)
plot(t, s, c='m', marker='o', mfc='m')




Again, works as expected, no problems, line color is magenta c='m'. What am I missing here? Why is the line color blue if I set a line width and have markers?

Thanks,
Bob
",0,677,,,
matplotlib unexpected result,https://stackoverflow.com/questions/23406677,matplotlib GridSpec indexing yields unexpected results,,-1,304,,,
matplotlib unexpected issue,https://stackoverflow.com/questions/14034529,How to deploy matplotlib on heroku - fulfilling numpy requirement,"I'm trying to deploy a web app on Heroku that uses matplotlib. This question is related but doesn't seem to address my specific problem. More precisely, I'm deploying a duplicate app ""staging"" for testing purposes. When I run the command:

git push staging master


to push my app to Heroku I get an unexpected Heroku push rejection:  

           ============================================================================
           BUILDING MATPLOTLIB
                       matplotlib: 1.1.1
                           python: 2.7.2 (default, Oct 31 2011, 16:22:04)  [GCC 4.4.3]
                         platform: linux2

           REQUIRED DEPENDENCIES
                            numpy: no
                                   * You must install numpy 1.4 or later to build
                                   * matplotlib.
           Complete output from command python setup.py egg_info:
           basedirlist is: ['/usr/local', '/usr']

       ============================================================================

       BUILDING MATPLOTLIB

                   matplotlib: 1.1.1

                       python: 2.7.2 (default, Oct 31 2011, 16:22:04)  [GCC 4.4.3]

                     platform: linux2



       REQUIRED DEPENDENCIES

                        numpy: no

                               * You must install numpy 1.4 or later to build

                               * matplotlib.

       ----------------------------------------
       Command python setup.py egg_info failed with error code 1 in /tmp/build_sj82km4g47z3/.heroku/venv/build/matplotlib
       Storing complete log in /app/.pip/pip.log
 !     Heroku push rejected, failed to compile Python app

To git@heroku.com:warm-atoll-3630.git
 ! [remote rejected] master -&gt; master (pre-receive hook declined)
error: failed to push some refs to 'git@heroku.com:warm-atoll-3630.git'


Unexpected because I thought I had resolved this problem. Indeed my production app is working just fine. I resolved it by having a two layered requirements file.

requirements.txt:

numpy==1.6.2
-r ./prod-requirements.txt


prod-requirements.txt:

matplotlib==1.1.1
other requirements... 


Clearly I have forgotten how I actually resolved this issue. I remember that because of the way matplotlib depends on numpy and how Heroku installs requirements via pip that this is tricky.  Here is the issue I'm referring to. What can be done? Thanks in advance.
",3,1387,"I resolved the issue by removing matplotlib from my prod-requirements.txt file (see orginal question). Then deploying, then adding matplotlib to my prod-requirements.txt file then deploying again. I had assumed that's what was achieving by having  a requirements.txt: 

numpy==1.6.2
-r ./prod-requirements.txt


And then putting matplotlib=1.1.1 in the prod-requirements file .
But apparently not. It seems likely that i could have achieved this with just one requirements file.
",,
matplotlib unexpected issue,https://stackoverflow.com/questions/69569617,Unexpected negative values in x-axis matplotlib,"I have the following x and y values:
x = [0.  , 0.02, 0.04, 0.06, 0.08, 0.1 , 0.12, 0.14, 0.16, 0.18, 0.2 ,
       0.22, 0.24, 0.26, 0.28, 0.3 , 0.32, 0.34, 0.36, 0.38, 0.4 , 0.42,
       0.44, 0.46, 0.48, 0.5 , 0.52, 0.54, 0.56, 0.58, 0.6 , 0.62, 0.64,
       0.66, 0.68, 0.7 , 0.72, 0.74, 0.76, 0.78, 0.8 , 0.82, 0.84, 0.86,
       0.88, 0.9 , 0.92, 0.94, 0.96, 0.98]

y = [4179,  628,   41,  142,  117,    6,   11,    2,    1,    0,    6,
          2,   12,    5,    7,    3,    2,    3,    2,    3,   30,   27,
         31,   29,   14,   31,   22,   28,   18,   19,   19,   22,   20,
         19,   21,   23,   25,   24,   20,   29,   23,   25,   31,   25,
         24,   28,   23,   26,   32,   21]

I'm plotting a bar chart using the code
fig, cx = plt.subplots(figsize =(15, 7))
cx.bar(x,y)
plt.show()

Which is giving me the following plot

This plot is unexpected because as shown in the diagram, the values of the x-axis lie between 0 and 1. There are no negative values and no values greater than 1 in the data. I wasn't sure about the reason for the presence of bars beyond the 0-1 region.
I tried out the solutions given in the following two question links:
Why is Python Matplotlib bar-chart's X axis ticks showing strange and wrong negative values? and
Matplotlib yaxis range display using absolute values rather than offset values?
I tried using the following solutions:

cx = plt.gca() cx.ticklabel_format(useOffset=False)
plt.bar(x, y, tick_label=x)
cx.get_xaxis().get_major_formatter().set_useOffset(False)
cx = plt.gca() cx.set_xticklabels(ax.get_xticks())

Solutions #1, #3, and #4 didn't lead to any changes. Solution #2 added ticks and the values were overlapped as shown in the figure below. But the bars didn't change.

Since few of the bars in the middle regions of the graphs have smaller bar widths it doesn't seem like bigger bar width is the issue here.
Any idea to solve this is appreciated. Thanks in advance :)
",0,741,"The documentation for plt.bar() should clear things up.
The function creates bars centered at x, each with width width (0.8 by default). So your first bar is centered at x = 0 and spans from -0.4 to 0.4. Likewise for the last one. You can change the alignment or width (smaller would make them thinner), but that's about it.
",,
matplotlib strange behavior,https://stackoverflow.com/questions/20655246,Intersecting matplotlib graph with unsorted data,,15,8925,,,
matplotlib strange behavior,https://stackoverflow.com/questions/15538099,Conversion of unicode minus sign ( from matplotlib ticklabels ),"I'm having a problem with the Text object that matplotlib use to represent the ticklabels.

For testing reason I need to check the value of the ticks labels that are created in a plot. If the label is a string or a positive number, there is no problem: a unicode string is returned, I test it (or convert it to a number, given the circumstances) and everything is fine.

But if the label is a negative number what I get back is a mangled unicode string for a reason I cannot understand.

Let's take this example code:

import pylab as plt
fig, ax = plt.subplots(1)
ax.plot([-1, 0, 1, 2], range(4))
labels = ax.get_xticklabels()


now, if I ask the text content of the second label (the 0) I obtain a normal unicode string:

labels[1].get_text()
# u'0.0'


but the unicode of the first one (the -1) is a strange thing

labels[1].get_text()
# u'\u22121'


This is printed correctly in the terminal, but in this case I need to confront it with a numerical value, and every conversion fail, both with int and float.

I tried to convert it to an UTF-8 string with

text = labels[1].get_text()
text.encode('utf8')
# '\xe2\x88\x921'


but again it is something that is correctly printed and raise an error when converted. I also looked to the unicodedata module, but looks like it can only convert single character, so in this case is useless. I've tried also to normalize the string with unicodedata.normalize and any possible format, but again no success.

I moved to the pipy module unidecode (as suggested in Python and character normalization), again without any success

from unidecode import unidecode
unidecode(text)
# '[?]1'


I have tried also to avoid font issues using the solution in Non-ASCII characters in Matplotlib, but with the same result (I'm not sure if it should even have something to do, being that a problem of visualization...). the question Accented characters in Matplotlib has a similar problem, as it is concerned about the visualization and not the value in itself

I'm starting to feel a little lost...I know that python 2.7 has some unicode ""difficulty"", but normally I can avoid them in a way or the other.

I know that the issue is the minus sign, as I can avoid the problem using a brute replacement of the culprit:

text.replace(u'\u2212', '-')
# u'-1'


But this is more and hack than a solution, and I'm almost certain that it's not stable across different systems, so I would like something closer to a solution.

I'm working with 


python 2.7.3 
matplotlib 1.2.0
pylab 1.7.0
IPython 0.13.1 


on Kubuntu 12.10.

Thank you very much for your help!

EDIT:

Corrected the order of the plot, as I got the x and y inverted, sorry

EDIT2:

a similar info is present at this link:http://www.coniferproductions.com/2012/12/17/unicode-character-dump-in-python/

in the end it shows how in some books the minus sign used is a more estetically pleasant one but not recognized by the python interpreter as a valid character.

EDIT3:

Riddle solved. the character that matplotlib return is the ""MINUS SIGN"", i.e. the correct typografical sign for the minus. The one the keybord create is in fact ""HYPHEN-MINUS"", that is commonly used but not typografically correct. see on wikipedia for an explanation http://en.wikipedia.org/wiki/Hyphen-minus.

So, the simple replace I used is in fact the correct practical thing to do, but ""ethically"" is a bug in python (2.7 and 3.x alike) that do not recognize the correct symbol for the minus sign.

see the bug tracking in http://bugs.python.org/issue6632

EDIT4:

to disable this behavior there is a simple solution on matplotlib, just modify the rcparams, either in the .matplotlibrc or programmatically.

import matplotlib as mpl
mpl.rcParams['axes.unicode_minus']=False

",14,5722,,,
matplotlib strange behavior,https://stackoverflow.com/questions/11893414,how to make square subplots in matplotlib with heatmaps?,,13,18471,,,
matplotlib strange behavior,https://stackoverflow.com/questions/15165065,matplotlib datetime xlabel issue,"I'm seeing some strange behavior in the x-axis auto-labeling for dates in matplotlib. When I issue the command:

from datetime import datetime as dt
plot( [ dt(2013, 1, 1), dt(2013, 5, 17)], [ 1 , 1 ], linestyle='None', marker='.')


I get the very reasonably labeled chart:



But if I increase the end date by 1 day:

plot( [ dt(2013, 1, 1), dt(2013, 5, 18)], [ 1 , 1 ], linestyle='None', marker='.')


I get this:



I've reproduced this at several different calendar date ranges(in 2012), and each time the magic number of days required to trip the bug is around 140 (in this case 136/137). Anyone know what's going on here? Is this a known bug, and if so, is there a workaround?

A couple notes: In the above commands, I'm using IPython in --pylab mode to create the plots, but I first encountered this issue using matplotlib directly, and it is reproducible in script form (i.e. I don't think this is an IPython issue). Also, I've observed this in both matplotlib 1.1.0 and 1.2.X.

UPDATE:

It looks like there is a window where, if you push far enough ahead, the labels start behaving normally again. For the example above, the labels remain garbled from May 18 through May 31, but on June 1, the labels start plotting normally again. So,

(labels are garbled)
plot( [ dt(2013, 1, 1), dt(2013, 5, 31)], [ 1 , 1 ], linestyle='None', marker='.')

(labels are fine)
plot( [ dt(2013, 1, 1), dt(2013, 6, 1)], [ 1 , 1 ], linestyle='None', marker='.')

",11,8073,,,
matplotlib strange behavior,https://stackoverflow.com/questions/24961674,IPython Notebook widgets for Matplotlib interactivity,"I would like to use the ipython notebook widgets to add some degree of interactivity to inline matplotlib plots. 

In general the plot can be quite heavy and I want to only update a specific element of the plot. I understand that widgets have a throttling feature built-in that helps to don't flood the kernel, but when the plot takes let say 30s I don't want to wait so long just to update a line.

By reading the example notebooks I was able to create a basic example in which I add a cross cursor (driven by 2 sliders) to a mpl axis.

The problem is that the figure is displayed twice. Here is the code (cell 1):

fig, ax = plt.subplots() 
ax.plot([3,1,2,4,0,5,3,2,0,2,4])


... figure displayed ..., cell 2 (edit: thanks Thomas K for the improvement):

vline = ax.axvline(1)
hline = ax.axhline(0.5)

def set_cursor(x, y):
    vline.set_xdata((x, x))
    hline.set_ydata((y, y))
    display(fig)


and finally (cell 3):

interact(set_cursor, x=(1, 9, 0.01), y=(0, 5, 0.01))


shows again the figure with the widgets.

So the question is:


how can I inhibit the first figure display?
is that the right way to do it or is there a better approach?


EDIT

I found an ipython config knob that, according to this notebook, allows inhibiting the figure display 

%config InlineBackend.close_figures = False


While the example notebook works, I can't figure out how to use this option by itself (without the context manager class provided in the linked example) to hide a figure display.

EDIT 2

I found some documentation of the InlineBackend.close_figures configurable.

EDIT 3

Triggered by @shadanan answer, I want to clarify that my purpose is to add a cursor to an existing figure without redrawing the plot from scratch at each cursor movement. Merging the 3 cells in a single  cell:

fig, ax = plt.subplots()
ax.plot([3,1,2,4,0,5,3,2,0,2,4])

vline = ax.axvline(1)
hline = ax.axhline(0.5)

def set_cursor(x, y):
    vline.set_xdata((x, x))
    hline.set_ydata((y, y))
    display(fig)

interact(set_cursor, x=(1, 9, 0.01), y=(0, 5, 0.01))


it ""should"" work but it doesn't. The first time the cell is executed it shows 2 figures. After widget interaction only 1 figure is displayed. This is the ""strange behavior"" that requires a workaround like the one shown in @shadanan answer. Can an ipython dev comment on this? Is it a bug? 
",10,8844,"The solution turns out to be really simple. To avoid showing the first figure we just need to add a close() call before the interact call.

Recalling the example of the question, a cell like this will correctly show a single interactive figure (instead of two):

fig, ax = plt.subplots()
ax.plot([3,1,2,4,0,5,3,2,0,2,4])
plt.close(fig)

vline = ax.axvline(1)
hline = ax.axhline(0.5)

def set_cursor(x, y):
    vline.set_xdata((x, x))
    hline.set_ydata((y, y))
    display(fig)

interact(set_cursor, x=(1, 9, 0.01), y=(0, 5, 0.01))


A cleaner approach is defining the function add_cursor (in a separate cell or script):

def add_cursor(fig, ax):
    plt.close(fig)

    vline = ax.axvline(1, color='k')
    hline = ax.axhline(0.5, color='k')

    def set_cursor(x, y):
        vline.set_xdata((x, x))
        hline.set_ydata((y, y))
        display(fig)

    interact(set_cursor, x=ax.get_xlim(), y=ax.get_ylim())


and then call it whenever we want to add an interactive cursor:

fig, ax = plt.subplots()
ax.plot([3,1,2,4,0,5,3,2,0,2,4])
add_cursor(fig, ax)

","You can do this in a very strait forward way using the new(ish) notebook backend

%matplotlib notebook
import matplotlib.pyplot as plt
from IPython.html.widgets import interactive

fig, ax = plt.subplots()
ax.plot(range(5))


vline = ax.axvline(1, color='k')
hline = ax.axhline(0.5, color='k')

def set_cursor(x, y):
    vline.set_xdata((x, x))
    hline.set_ydata((y, y))
    ax.figure.canvas.draw_idle()


and in a separate cell:

interactive(set_cursor, x=ax.get_xlim(), y=ax.get_ylim())


This will still re-draw the entire figure every time you move the cursor because notebook does not currently support blitting (which is being worked on https://github.com/matplotlib/matplotlib/pull/4290 )
",
matplotlib strange behavior,https://stackoverflow.com/questions/16770049,Strange matplotlib zorder behavior with legend and errorbar,"I encountered a rather strange behavior of legend and the errorbar plot commands. I am using Python xy 2.7.3.1 with matplotlib 1.1.1 
The code below exemplifies the observed behavior:

import pylab as P
import numpy as N

x1=N.linspace(0,6,10)
y1=N.sin(x1)
x2=N.linspace(0,6,5000)
y2=N.sin(x2)
xerr = N.repeat(0.01,10)
yerr = N.repeat(0.01,10)

#error bar caps visible in scatter dots
P.figure()
P.subplot(121)
P.title(""strange error bar caps"")
P.scatter(x1,y1,s=100,c=""k"",zorder=1)
P.errorbar(x1,y1,yerr=yerr,xerr=xerr,color=""0.7"", 
    ecolor=""0.7"",fmt=None, zorder=0)
P.plot(x2,y2,label=""a label"")
P.legend(loc=""center"")

P.subplot(122)
P.title(""strange legend behaviour"")
P.scatter(x1,y1,s=100,c=""k"",zorder=100)
P.errorbar(x1,y1,yerr=yerr,xerr=xerr,color=""0.7"", 
    ecolor=""0.7"",fmt=None, zorder=99)
P.plot(x2,y2,label=""a label"", zorder=101)
P.legend(loc=""center"")
P.show()


which yields this plot:



As you can see, the errorbar caps are overwriting the scatter plot. If I increase zorder enough this does not happen any more, but the plot line overwrites the legend. I have the suspicion that the problem is related to this zorder problem with matplotlib.

Quick, dirty, hacky solutions also appreciated.

Edit (thanks @nordev): the desired outcome is the following: 


errorbars, as well as the ending caps shall be below the scatter plot point.
the line plot shall be above the scatter and the error bars
the legend shall be above all others


Adjusting the zorder according to your answer:


P.legend(zorder=100)  --&gt; self.legend_ = mlegend.Legend(self, handles, labels, **kwargs)
TypeError: __init__() got an unexpected keyword argument 'zorder'
P.errorbar(zorder=0), P.scatter(zorder=1), ... as correctly suggested by you, still yields the same plot, the error bar caps are still above the scatter dots. I corrected the example above accordingly.

",8,16394,"i use
plt.legend(loc='center').set_zorder(100)

",,
matplotlib strange behavior,https://stackoverflow.com/questions/6485000,colorbar setting tick formator/locator changes tick labels,"users, 
I want to customize the ticks on a colorbar. However, I found the following strange behavior. I try to change the tick formator to the default formator (I thought this should change nothing at all) but I end up with different labels. Does anybody know what I am doing wrong? Or is this a bug?

I use matplotlib from git (v1.0.1-961-gb516ae0 , git describe).
The following code produces the two plots:

#http://matplotlib.sourceforge.net/examples/pylab_examples/griddata_demo.html
from numpy.random import uniform, seed
from matplotlib.mlab import griddata
import matplotlib.pyplot as plt
import matplotlib.ticker
import numpy as np
# make up data.
seed(0)
npts = 200
x = uniform(-2,2,npts)
y = uniform(-2,2,npts)
z = x*np.exp(-x**2-y**2)
# define grid
xi = np.linspace(-2.1,2.1,100)
yi = np.linspace(-2.1,2.1,200)
# grid the data.
zi = griddata(x,y,z,xi,yi,interp='linear')

##### FIRST PLOT
plt.figure()
CS  = plt.contour(xi,yi,zi,25,cmap=plt.cm.jet)
bar = plt.colorbar() # draw colorbar
# plot data points.
#plt.scatter(x,y,marker='o',c='b',s=5,zorder=10)
plt.xlim(-2,2)
plt.ylim(-2,2)
plt.title('griddata test (%d points)' % npts)
plt.show()

##### SECOND PLOT
plt.figure()
CS  = plt.contour(xi,yi,zi,25,cmap=plt.cm.jet)
bar = plt.colorbar() # draw colorbar
bar.ax.yaxis.set_minor_locator(matplotlib.ticker.AutoLocator())
bar.ax.yaxis.set_major_locator(matplotlib.ticker.AutoLocator())
# plot data points.
#plt.scatter(x,y,marker='o',c='b',s=5,zorder=10)
plt.xlim(-2,2)
plt.ylim(-2,2)
plt.title('griddata test (%d points)' % npts)
plt.show()

",7,14457,,,
matplotlib strange behavior,https://stackoverflow.com/questions/21189954,Can pandas plot a time-series without trying to convert the index to Periods?,"When plotting a time-series, I observe an unusual behavior, which eventually results in not being able to format the xticks of the plot.
It seems that pandas internally tries to convert the index into a PeriodIndex, but obviously only succeeds if the timestamp values are equally spaced. If they are unevenly spaced (or - strangely - if they are evenly spaced but timezone-aware) the index remains a DatetimeIndex.
The latter case works as expected. I can set DateFormatter and Locators. If however the index is interally converted to a PeriodIndex before plotting, the x-axis of the resulting plott seems to be messed up.

Here is an Example to reproduce the problem.

from pandas import Series, DataFrame
import pandas as pd
from datetime import datetime
import pytz
import matplotlib.pyplot as plt
import matplotlib as mpl
import numpy as np

idx1 = np.array([datetime(2014, 1, 16, 0),
                 datetime(2014, 1, 16, 5),
                 datetime(2014, 1, 16, 10),
                 datetime(2014, 1, 16, 15), 
                 datetime(2014, 1, 16, 20), 
                 datetime(2014, 1, 17, 1)])
idx2 = np.array([datetime(2014, 1, 16, 0),
                 datetime(2014, 1, 16, 5),
                 datetime(2014, 1, 16, 10),
                 datetime(2014, 1, 16, 15),
                 datetime(2014, 1, 16, 20),
                 datetime(2014, 1, 16, 23)])
y = [0, 2, np.nan, 5, 2, 1]
tz = pytz.timezone('Europe/Berlin')

fig, (ax1, ax2, ax3) = plt.subplots(1,3, figsize=(15,4))

# index convertible to period index
s1 = Series(y, index=idx1)
s1.plot(ax=ax1)
print ax1.get_xticks()
print ax1.xaxis.get_major_locator()
print ax1.xaxis.get_major_formatter()
#ax1.xaxis.set_major_formatter(mpl.dates.DateFormatter('%H'))
#ax1.xaxis.set_major_locator(mpl.ticker.MultipleLocator(0.25))

# index not convertible to period index
s2 = Series(y, index=idx2)
s2.plot(ax=ax2)
print ax2.get_xticks()
#ax2.xaxis.set_major_formatter(mpl.dates.DateFormatter('%H'))
#ax2.xaxis.set_major_locator(mpl.ticker.MultipleLocator(0.25))

# index convertible to period index but tz-aware
s3 = Series(y, index=idx1)
s3 = s3.tz_localize(tz)
s3.plot(ax=ax3)
print ax3.get_xticks()
#ax2.xaxis.set_major_formatter(mpl.dates.DateFormatter('%H'))
#ax2.xaxis.set_major_locator(mpl.ticker.MultipleLocator(0.25))

fig.autofmt_xdate()  # just temporarily

plt.tight_layout()
plt.show(block=False)


Is there a way to tell pandas to keep the index in its original format and not to convert it to Periods? Any ideas how to deal with this are greatly appreciated!

I use pandas 0.13 and matplotlib 1.3.1

As a sidenote: 
It would of course be great if the timezones were not converted all to UTC. However I realize this problem may still persist for a while. But if anyone has a hint for a workaround I'd be glad to hear (I tried passing a tz directly to the DateFormatter. That works, but the Locators don't seem to like it much).
",7,3578,"Dealing with the conversion from UTC to local time
import time
import pytz
import matplotlib.dates
…
# Get the time zone.
tz = pytz.timezone(time.tzname[0])
…
ax1.xaxis.set_major_locator(matplotlib.dates.HourLocator(interval=1, tz=tz))
ax1.xaxis.set_major_formatter(matplotlib.dates.DateFormatter('%H', tz=tz))

",,
matplotlib strange behavior,https://stackoverflow.com/questions/25272701,Matplotlib pick event order for overlapping artists,"I'm hitting a very strange issue with matplotlib pick events.  I have two artists that are both pickable and are non-overlapping to begin with (""holes"" and ""pegs"").  When I pick one of them, during the event handling I move the other one to where I just clicked (moving a ""peg"" into the ""hole"").  Then, without doing anything else, a pick event from the moved artist (the peg) is generated even though it wasn't there when the first event was generated.  My only explanation for it is that somehow the event manager is still moving through artist layers when the event is processed, and therefore hits the second artist after it is moved under the cursor.

So then my question is - how do pick events (or any events for that matter) iterate through overlapping artists on the canvas, and is there a way to control it?  I think I would get my desired behavior if it moved from the top down always (rather than bottom up or randomly).  I haven't been able to find sufficient enough documentation, and a lengthy search on SO has not revealed this exact issue.  Below is a working example that illustrates the problem, with PathCollections from scatter as pegs and holes:

import matplotlib.pyplot as plt
import sys

class peg_tester():
    def __init__(self):
        self.fig = plt.figure(figsize=(3,1))
        self.ax = self.fig.add_axes([0,0,1,1])
        self.ax.set_xlim([-0.5,2.5])
        self.ax.set_ylim([-0.25,0.25])
        self.ax.text(-0.4, 0.15, 'One click on the hole, and I get 2 events not 1',
                     fontsize=8)

        self.holes = self.ax.scatter([1], [0], color='black', picker=0)
        self.pegs = self.ax.scatter([0], [0], s=100, facecolor='#dd8800',
                                    edgecolor='black', picker=0)

        self.fig.canvas.mpl_connect('pick_event', self.handler)
        plt.show()

    def handler(self, event):
        if event.artist is self.holes:
            # If I get a hole event, then move a peg (to that hole) ...
            # but then I get a peg event also with no extra clicks!
            offs = self.pegs.get_offsets()
            offs[0,:] = [1,0] # Moves left peg to the middle
            self.pegs.set_offsets(offs)
            self.fig.canvas.draw()
            print 'picked a hole, moving left peg to center'
        elif event.artist is self.pegs:
            print 'picked a peg'
        sys.stdout.flush() # Necessary when in ipython qtconsole

if __name__ == ""__main__"":
    pt = peg_tester()


I have tried setting the zorder to make the pegs always above the holes, but that doesn't change how the pick events are generated, and particularly this funny phantom event.

EDIT:
The context is an implementation of peg solitaire, so I want to be able to pick up a peg then click on an empty hole to drop it there.  Currently the same peg is immediately picked up again as soon as it is dropped.
",4,2571,,,
matplotlib strange behavior,https://stackoverflow.com/questions/43334273,Matplotlib multiple grids zorder not working with errorbars,"I am currently dealing with some strange behavior of matplotlib when plotting multiple errorbars with different y axes into one subplot. 

When doing so, the grid corresponding to the second errorbar plot always overlaps the first errorbar plot. With the zorder options I was only able to move the other grid below the errorbars. 

I would like both grids to be below the two errorbars. 
Does anyone know a solution to this, or is this a bug in matplotlib? 

I am using Python 2.7.12 with matplotlib 1.5.1. 

Minimal working example:

import numpy as np
import matplotlib
matplotlib.use('Agg')
import matplotlib.pyplot as plt
import matplotlib.colors as colors

# numbers of epochs
ttimes_means_list = [251.4, 153.3, 22.0, 202.1, 46.6]
ttimes_stddevs_list = [32.1, 35.1, 12.0, 84.9, 14.7]
# numbers of times
times_means_list = [5231, 3167, 860, 3932, 1244]
times_stddevs_list = [1381, 572, 253, 1445, 215]

labels = ['x1', 'x2', 'x3', 'x4', 'x5']
linewidth=3

xvalues = np.arange(0, len(times_means_list), 1)
xvalues_s = xvalues + 0.1

fig = plt.figure()
ax1 = fig.add_subplot(111)
ax2 = ax1.twinx()
ax1.set_ylabel('Number')
ax1.xaxis.grid(False)
ax1.yaxis.grid(True, color='navy', linewidth=linewidth/2.0)
ax2.xaxis.grid(False)
ax1.set_zorder(0)
ax2.set_zorder(0)
ax2.yaxis.grid(True, color='darkorange', linewidth=linewidth/2.0)
ax2.set_ylabel('Time')
ax1.set_xticks(xvalues)
ax2.set_xticks(xvalues)
ax1.set_xticklabels(labels)
ax2.set_xticklabels(labels)
ax1.set_xlabel('x label')
errplot1 = ax1.errorbar(xvalues, ttimes_means_list, yerr=ttimes_stddevs_list,
        linestyle=""None"", elinewidth=linewidth, color='navy', label='Number',
        capthick=linewidth, zorder=10)
errplot2 = ax2.errorbar(xvalues_s, times_means_list, yerr=times_stddevs_list, 
        linestyle=""None"", elinewidth=linewidth, color='darkorange',
        label='Time', capthick=linewidth, zorder=10)
ax1.set_axisbelow(True)
ax2.set_axisbelow(True)
fig.legend((errplot1, errplot2), ('number', 'time'), 
        loc='upper right', numpoints=1)
plt.xlim(-0.5, len(times_means_list)-0.5)
plt.title('Some plot title')
plt.savefig('mwe_plot.pdf')
plt.clf()


Output plot (the orange grid dots are overlapping the blue bars): 

",4,1041,"Maybe you can draw the 'navy' line twice.

import numpy as np
import matplotlib
matplotlib.use('Agg')
import matplotlib.pyplot as plt
import matplotlib.colors as colors

# numbers of epochs
ttimes_means_list = [251.4, 153.3, 22.0, 202.1, 46.6]
ttimes_stddevs_list = [32.1, 35.1, 12.0, 84.9, 14.7]
# numbers of times
times_means_list = [5231, 3167, 860, 3932, 1244]
times_stddevs_list = [1381, 572, 253, 1445, 215]

labels = ['x1', 'x2', 'x3', 'x4', 'x5']
linewidth=3

xvalues = np.arange(0, len(times_means_list), 1)
xvalues_s = xvalues + 0.1

fig = plt.figure()
ax1 = fig.add_subplot(111)
ax2 = ax1.twinx()
ax1.set_ylabel('Number')
ax1.xaxis.grid(False)
g1 = ax1.yaxis.grid(True, color='navy', linewidth=linewidth/2.0)
ax2.xaxis.grid(False)
ax1.set_zorder(0)
ax2.set_zorder(0)
g2 = ax2.yaxis.grid(True, color='darkorange', linewidth=linewidth/2.0)
ax2.set_ylabel('Time')
ax1.set_xticks(xvalues)
ax2.set_xticks(xvalues)
ax1.set_xticklabels(labels)
ax2.set_xticklabels(labels)
ax1.set_xlabel('x label')
ax1.errorbar(xvalues, ttimes_means_list, yerr=ttimes_stddevs_list,
        linestyle=""None"", elinewidth=1, color='navy', label='Number',
        capthick=1, zorder=10,)
errplot2 = ax2.errorbar(xvalues_s, times_means_list, yerr=times_stddevs_list, 
        linestyle=""None"", elinewidth=linewidth, color='darkorange',
        label='Time', capthick=linewidth, zorder=10)
ax1.set_axisbelow(True)
ax2.set_axisbelow(True)

# draw the 'navy' line again
ax3 = ax1.twinx()
ax3.yaxis.grid(False)
errplot1 = ax3.errorbar(xvalues, ttimes_means_list, yerr=ttimes_stddevs_list,
        linestyle=""None"", elinewidth=linewidth, color='navy', label='Number',
        capthick=linewidth, zorder=10)

fig.legend((errplot1, errplot2), ('number', 'time'), 
        loc='upper right', numpoints=1)
plt.xlim(-0.5, len(times_means_list)-0.5)
plt.title('Some plot title')
# plt.savefig('mwe_plot.pdf')
# plt.clf()

plt.show()


And you will get:

",,
matplotlib strange behavior,https://stackoverflow.com/questions/46880493,matplotlib.pyplot.Figure.show freezes instantly,"Following strange problem: This code

import matplotlib.pyplot as plt
fig = plt.figure()
ax = fig.add_subplot(111)
ax.plot([1, 2, 3], [3, 0, 3])
fig.show()


causes the graph window (backend: tkagg) to freeze as soon as it opens, however this code

import matplotlib.pyplot as plt
plt.plot([1, 2, 3], [3, 0, 3])
plt.show()


opens the graph window (also tkagg) as expected. I tried debugging, but attaching Visual Studio to python.exe and hitting pause causes VS to crash and python_d.exe always complains cannot import name 'multiarray'. I did a fresh reinstall of python (purging all files, installing python 3.6.3 x86_64, pip install matplotlib) and the behavior continues. What is causing this behavior? Is there a way to fix it?

More information about my system: I'm running Windows 8.1 x86_64 with Python v3.6.3:2c5fed8 x86_64 and matplotlib 2.1.0 (rev-id b392d46466e98cd6a437e16b52b3ed8de23b0b52).

Solution:

import matplotlib.pyplot as plt
fig = plt.figure()
ax = fig.add_subplot(111)
ax.plot([1, 2, 3], [3, 0, 3])

root = fig.canvas._tkcanvas.winfo_toplevel() # Get tkinter root
fig.show()
root.mainloop() # Enter mainloop

",4,4902,"I don't see quite the same behaviour, but I'm testing on Python 3.5, matplotlib 2.1, and Ubuntu 16.04. When I run your first version, I see the plot window open very briefly, and then close itself.

However, if you look at the documentation, it's not too surprising that the behaviour of the two examples is different. You're calling two different show() methods.

In the first version, you're calling Figure.show():


  If using a GUI backend with pyplot, display the figure window.


In the second version, you're calling pyplot.show():


  Display a figure... In non-interactive mode, display all figures and block until the figures have been closed...


I stepped through the second method, and it's basically equivalent to this:

fig.show()
tkinter.mainloop()


So I'm not sure why it's freezing on you, but it probably wasn't what you wanted in the first place. Create the subplots if you want, but call plt.show() at the end.
",,
matplotlib strange behavior,https://stackoverflow.com/questions/16481224,Strange Behavior of Python&#39;s Matplotlib Module - Plotting a Circle,,3,977,,,
matplotlib strange behavior,https://stackoverflow.com/questions/33648920,matplotlib.legend.set_zorder() doesn&#39;t work in ipython notebook,"here's a quick thing which is quite annoying me. I've followed the indications found in  strange matplotlib zorder behavior with legend and errorbar but it doesn't work. Here's a very brief working example

x = np.linspace(0,10) 
y = (x-5)**2 
plt.plot(x,y,label='test curve',zorder=100)
plt.legend(loc='center right').set_zorder(102)


If you try this you will see that the legend is still underneath the curve, despite the zordering. Does anyone know why?



I'm using matplotlib 1.3.1 on ipython 3.1.0
",3,1886,"Following on from @cel's comment, I agree you need to add an opaque background to the legend. Try adding:

leg=plt.legend(loc='center right')
leg.set_zorder(102)
leg.get_frame().set_facecolor('w')


Its also possible to set legend.facecolor in your matplotlibrc or via plt.rcParams



Alternatively make sure the fill of the legend frame is set to True:

print leg.get_frame().get_fill()


If that prints False, try

leg.get_frame().set_fill(True)

",,
matplotlib strange behavior,https://stackoverflow.com/questions/38107493,Memory order behavior issue when converting numpy array to QImage,"I have a simple viewer in PyQt4, and I'm having some strange behavior when converting from a numpy array (8-bit grayscale) to a QImage to display.  It seems to me like something is going haywire when I try to construct a QImage from a transposed array, even though the memory order should have been updated (i.e. it's not just a view with different strides).

Below is a heavily cut-down version of the viewer:

import numpy as np
from PyQt4 import QtGui

class SimpleViewer(QtGui.QMainWindow):

    def __init__(self, data, parent=None):
        super(SimpleViewer, self).__init__(parent=parent)

        hgt, wid = data.shape

        dmin = data.min()
        dmax = data.max()
        bdata = ((data - dmin)/(dmax - dmin)*255).astype(np.uint8)

        img = QtGui.QImage(bdata, wid, hgt, QtGui.QImage.Format_Indexed8)

        self.scene = QtGui.QGraphicsScene(0, 0, wid, hgt)
        self.px = self.scene.addPixmap(QtGui.QPixmap.fromImage(img))

        self.view = QtGui.QGraphicsView(self.scene)
        self.setCentralWidget(self.view)


if __name__ == ""__main__"":

    app = QtGui.QApplication.instance()
    if app is None:
        app = QtGui.QApplication(['python'])

    xx, yy = np.meshgrid(np.arange(-100,100), np.arange(350))
    zz = xx * np.cos(yy/350.*6*np.pi)

    viewer = SimpleViewer(zz)
    viewer.show()
    app.exec_()


Now, as-is this produces something like this, which is fine:



However, I need to view my real images with the first dimension as ""x"" and the second dimension as ""y"" so it needs to be transposed (plus a y-flip so the origin is lower-left, but that's outside the scope here).

If I change hgt, wid = data.shape to wid, hgt = data.shape and do nothing else (which is wrong), I get the following.  It makes sense, because the QImage is just reading the memory.



However, if I also pass in bdata.T.copy() instead of bdata, I'm expecting that the array will be transposed and the memory re-ordered by the copy operation.  But what I get is this:



It's so close but off by just a hair for some reason.  Also, I noticed that if the dimensions just happen to be integer multiples of each other (e.g. 200x100), then it comes out okay.  What is going on?  Have I just missed something really really stupid?

I can't tell if this is a PyQt issue or a numpy issue ... the transposed array plots fine with matplotlib, but matplotlib handles the striding so that may not mean anything.

I get the same behavior in Python 2.7 and Python 3.5.  The PyQt4 version is 4.11.4, using Qt version 4.8.7.  Numpy version is 1.10.1.
",3,412,"The issue stems from the creation of the QImage.

img = QtGui.QImage(bdata, wid, hgt, QtGui.QImage.Format_Indexed8)


The documentation for the signature of the constructor you are using says


  ... data must be 32-bit aligned, and each scanline of data in the image must also be 32-bit aligned.


If you change the dimensions of your array so that it is 32-bit aligned (for example using xx, yy = np.meshgrid(np.arange(-128,128), np.arange(384))) then your code works correctly.

To correct this issue without needing a multiple of 32 bytes per line, you want to use a slightly different constructor for the QImage:

QImage(data, width, height, bytesPerLine, format)


In your case, this is simply:

img = QtGui.QImage(bdata, wid, hgt, wid, QtGui.QImage.Format_Indexed8)


since each pixel contains 1 byte of information.
",,
matplotlib strange behavior,https://stackoverflow.com/questions/53758472,Why is plt.pause not described in any tutorials if it is so essential? (Or am I doing this wrong?),"I am writing a python script with interspersed plots and computations.
I am confused about the behavior of matplotlib and in particular the necessity of plt.pause. Consider the following snippets:

import matplotlib.pyplot as plt
import time
fig,ax=plt.subplots()
ax.plot([1,2])
fig.show()
time.sleep(5) #This is a substitute for real computations


-&gt; Nothing happens for five seconds

import matplotlib.pyplot as plt
import time
fig,ax=plt.subplots()
ax.plot([1,2])
plt.pause(0.1)
fig.show()
time.sleep(5) #This is a substitute for real computations


-&gt; Window displays the desired plot for five seconds

It seems that plt.pause is required to see anything. Why then does the documentation say ""This function is experimental; its behavior may be changed or extended in a future release."" and why did I not see plt.pause in any tutorials?

Also, why would such an essential function be designed so strangely that the user has to input a small enough time, yet not zero? I get that some people actually want to pause exeuction, but I don't, I just want to see the plots. Is this so unusual? 



By the way, I noticed that I can also do plt.show(), which, for reasons unknown to me, behaves different than plt.gcf().show()[=fig.show] and blocks the execution until the user closes the window. While this does show the plot when I want it, I do not want the execution to be stopped and I want the user to keep seeing the plot during the subsequent computations. Using plt.show(block=False) DOES seem to behave like plt.gcf().show()[=fig.show()], so it is also useless. 

Furthermore, I read somewhere that plt.ion should help, but it doesn't. Adding plt.ion() before fig,ax=plt.subplots() in the snippets above doesn't change anything.

Finally, I heard that different backends might behave differently. I am using python 3.6 (anaconda) on Ubuntu 18 with matplotlib 2.2.2. If I add import matplotlib; matplotlib.use('Qt5Agg') at the beginning of the snippets, not much changes, but instead of showing nothing for five seconds the first snippets shows a garbage window for five seconds (the window shows whatever was shown on the screen at the location where it popped up).
",3,4916,"From the documentation of plt.pause()  


  Pause for interval seconds.
  If there is an active figure, it will be updated and displayed before the pause, and the GUI event loop (if any) will run during the pause.
  This can be used for crude animation. For more complex animation, see matplotlib.animation.


Hence the pause will actually draw the figure.

The key why plt.pause is ""required"" is that it starts the mock-up event-loop such that it has time to run at least once and produce the figure in completeness.  After that your code can continue. While the code is running, no further events are processed. Therefore the figure may appear unresponsive. This means you should call pause repeatedly afterwards to not let the window freeze. In this way you emulate an event-loop while still being able to run other code in between.

plt.pause() is mentionned in this example, which does what is described above.

fig.show() does not run the event-loop. So without a running event loop, and without pause it will only show the figure window; but if you don't give any time for any events to be processed, it will freeze instantly. Here you may experience differences between operating systems and backends. Maybe you only see the window border and nothing painted in between, or you may see the toolbars and a white surface. 

In general, the key of all of this is to understand that python (like other programming languages) processes code linearly. The desire to have an event loop running, meaning a responsive GUI window, and some other code being executed in parallel contradicts this principle. The usual way to circumvent this is to run any other code in a different thread. However, matplotlib cannot know what code that is and how to synchronize it to the main thread. Hence such solution would require the user to implement it. 
",,
matplotlib strange behavior,https://stackoverflow.com/questions/56299971,"When I use basemap in python, longitude is sometimes reversed, flipping the map","In python, I use basemap (https://matplotlib.org/basemap/) for plotting spatial data, and I've used it for several years without any large problems.  I recently had to reinstall python3 (through conda, along with a number of modules) and basemap now has a strange issue: under certain conditions, the map will be displayed with flipped longitudes, switching east and west.  As an example, I use this code: https://matplotlib.org/basemap/users/robin.html.  If I use that code as-is, the map displays fine, but when I set lon_0=180, the map gets flipped, as shown in the image below.

Image of map problem

Setting lon_0 to any positive number results in a flipped map, while 0 or negative numbers result in a correct map.  lon_0 should simply set the central longitude of the plotted map, and should not have this behavior, so I'm unsure what's going on.  Has anyone seen this behavior before, or have suggestions for how to fix it?  I could alter my code to work around it, but I'd rather have things work properly.

I am using python3.7.3.  I've tried updating basemap with the command ""conda install -c anaconda basemap"", but it tells me that basemap is up to date already.

Here is the code.  It is identical to the code linked above, but with lon_0 set to 180.

from mpl_toolkits.basemap import Basemap
import numpy as np
import matplotlib.pyplot as plt
# lon_0 is central longitude of projection.
# resolution = 'c' means use crude resolution coastlines.
m = Basemap(projection='robin',lon_0=180,resolution='c')
m.drawcoastlines()
m.fillcontinents(color='coral',lake_color='aqua')
# draw parallels and meridians.
m.drawparallels(np.arange(-90.,120.,30.))
m.drawmeridians(np.arange(0.,360.,60.))
m.drawmapboundary(fill_color='aqua')
plt.title(""Robinson Projection"")
plt.show()


When I run the code, the only output is this, which seems unrelated:

map_test.py:36: MatplotlibDeprecationWarning:
The dedent function was deprecated in Matplotlib 3.1 and will be removed in 3.3. Use inspect.cleandoc instead.
  m = Basemap(projection='robin',lon_0=180,resolution='c')

Any ideas?
",3,1637,"This should solve the problem. Just add this after creating the axes:

# Matplotlib &gt;= 3.1.0 (introduced ""set_inverted"")
ax.xaxis.set_inverted(False)

# Matplotlib &lt; 3.1.0 (this is just the source code for ""set_inverted"")
interval = ax.xaxis.get_view_interval()
ax.set_xlim(sorted(interval), auto=None)


After some trial and error it also seems that selecting a negative central longitude instead of positive longitude works:

m = Basemap('robin', lon_0=-180)

",,
matplotlib strange behavior,https://stackoverflow.com/questions/62667696,matplotlib twinx strange behavior after set_position(),"I'm working on a function that would make it easy for me to add parasite axes wherever I want on a plot, but I have encountered a weird situation. Take a look at the following code:
import matplotlib.pyplot as plt

plt.plot()
plt.gca().set_position([1/3, 1/3, 1/3, 1/3])
plt.gca().twinx()

plt.show()

The above code produces the following figure:

As you can see, the twinx plot is generated using the old positions of the host plot. I would have expected the twinx plot to be generated on the new positions of the host plot.
How can I draw a twinx plot on the new positions of the host plot? (I know drawing the twinx plot beforehand ""solves"" this but it's not a practical solution)
Thanks for your help!
",3,413,"I figured out a way to go around the problem by trial and error. However, it should be noted that I have no idea how twinx works and I am very beginner in terms of matplotlib, so I would definitely not be able to understand any of what is happening.
What I did to solve this is I used plt.gcf().subplots_adjust() before creating the twinx. However, doing this effectively trashes the new positions, so I saved them beforehand to be able to reapply them after creating the twinx. The code becomes:
import matplotlib.pyplot as plt

plt.plot()
plt.gca().set_position([1/3, 1/3, 1/3, 1/3])
pos = plt.gca().get_position()     #added line
plt.gcf().subplots_adjust()        #added line
plt.gca().twinx()
plt.gca().set_position(pos)        #added line

plt.show()

I'm still open to reading some better answers than mine, however, so feel free to give your propositions!
NOTE: The code above seems pretty dumb (I'm setting the position just to reset it and then reapply it later), but in my actual application, the first set_position would have been executed by an earlier function call, and a later function call needs to add the twinx, so this does have a use case.
","You can set the position of the axes after calling twinx, using the position data from the initial setting prior to twinx.
There are several ways to do it.
import matplotlib.pyplot as plt

# Create original plot
plt.plot()
ax = plt.gca()
ax.set_position([1/3,1/3,1/3,1/3])
# Make twin axes.
ax2 = ax.twinx()
ax2.set_position(ax.get_position())

plt.show()

Alternatively, you can do ax.set_position(ax.get_position()). But a key point is recording the original Axes to a variable because gca points to the new Axes (which have the wrong position) once you call twinx.
If you don't want to store the Axes in a variable, you can store the axes position in a variable.
import matplotlib.pyplot as plt

# Create original plot
plt.plot()
plt.gca().set_position([1/3, 1/3, 1/3, 1/3])
# Make twin axes
pos = plt.gca().get_position()
plt.gca().twinx()
plt.gca().set_position(pos)

plt.show()

It is not necessary to throw out the positions set previously.
",
matplotlib strange behavior,https://stackoverflow.com/questions/65693394,Why am I getting the ImportError ../lib/libgobject-2.0.so.0: undefined symbol: g_uri_ref based on order of import?,"On one of my experiments, I found this strange behavior.
If I do
import matplotlib.pyplot as plt
import cv2
Then I get ImportError

ImportError: /home/deep/anaconda3/bin/../lib/libgobject-2.0.so.0: undefined symbol: g_uri_ref


But, If I change the order of imports as
import cv2
import matplotlib.pyplot as plt
Everything runs fine. Matplotlib version is 3.2.2 and cv2 version is 4.3.0
I tried to see if I could find some similar problem or explanation, but have not yet found an answer. I'm not sure why this problem occurs and how to fix it.
",3,5622,"You can try install the mentioned package, libgobject-2.0.so.0 from here:   https://pkgs.org/search/?q=libgobject-2.0.so.0
For Ubuntu:
Step 1: Update the package index:
sudo apt-get update

Step 2: Install libglib2.0-0 deb package:
sudo apt-get install libglib2.0-0

","set up a virtual enviroment (best with anaconda) to be save and try:
pip3 install opencv-python==4.5.1.48

and
pip3 install pyqt5

So far it works, but I get other errors. owever, this could be due to my own script. I will update if I have more info.
Good luck
",
matplotlib strange behavior,https://stackoverflow.com/questions/16409081,Strange behavior of matplotlib&#39;s griddata,,2,415,,,
matplotlib strange behavior,https://stackoverflow.com/questions/45536365,Error when using mathtext.fontset = &#39;custom&#39; in stylelib,"I encounter a strange behavior using matplotlib when I try define a custom math font (with usetex=false):

mpl.__version__ = '2.0.2' with python 2.7

I define a stylesheet (i.e. *.mplstyle file in stylelib folder) in which I write

mathtext.fontset = 'custom'
mathtext.rm = 'Avenir Next'
mathtext.it = 'Avenir Next:italic'
mathtext.bf = 'Avenir Next:bold'
mathtext.fallback_to_cm : True


If I try to make a plot with that style I get the error:

Bad key ""mathtext.rm = 'Avenir Next"" on line 49 in
/Users/gp/.matplotlib/stylelib/simple.mplstyle.
You probably need to get an updated matplotlibrc file from
http://github.com/matplotlib/matplotlib/blob/master/matplotlibrc.template
or from the matplotlib source distribution
 UserWarning: In /Users/gp/.matplotlib/stylelib/simple.mplstyle: Illegal line #48
""mathtext.fontset = 'custom'
""
in file ""/Users/gp/.matplotlib/stylelib/simple.mplstyle""


And also similar errors about mathtext.it and mathtext.bf.

If I instead comment these lines out and just write into the py document that I use to generate the plot (after loading the style with the now commented lines about mathtext) the following lines than everything works flawless (w/o error and with the expected behavior).

mpl.rcParams['mathtext.fontset'] = 'custom'
mpl.rcParams['mathtext.rm'] = 'Avenir Next'
mpl.rcParams['mathtext.it'] = 'Avenir Next:italic'
mpl.rcParams['mathtext.bf'] = 'Avenir Next:bold'


I am puzzled because the lines of my stylesheet are as described in the matplotlib documentary. Can somebody make anything of it?

Georg

edit: typos
",2,484,"After sitting over it for some days I posted the question just to realize directly afterwards that the syntax in the style file is wrong at several places.
The single quotation marks don't belong there. Also the =must be replaced by colons :.

Then it works!
",,
matplotlib strange behavior,https://stackoverflow.com/questions/65415390,Geopandas consistent user defined color scheme for subplots,"I am new to geopandas and I am having trouble creating choropleth subplots with consistent bins. I need to create a consistent user defined color scheme across all subplots.
I have followed the examples below:
matplotlib geopandas plot chloropleth with set bins for colorscheme
https://github.com/geopandas/geopandas/issues/1019
While I am able to reproduce both examples, I get very strange behavior with my own data. Below is a toy example that replicates my problem.
import geopandas as gpd
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from mapclassify import Quantiles, UserDefined
import os

# Note you can read directly from the URL
gdf = gpd.read_file('https://opendata.arcgis.com/datasets/8d3a9e6e7bd445e2bdcc26cdf007eac7_4.geojson')
#gdf.plot()
gdf.shape
gdf.columns
gdf['rgn15nm'].head(9)


d = {
'rgn15nm': ['North East', 'North West', 'Yorkshire and The Humber', 'East Midlands', 'West Midlands', 'East of England', 'London', 'South East', 'South West'],
'1980' : pd.Series([0, 1, 0, 0, 0, 0, 0, 0, 0]),
'2000' : pd.Series([1, 1, 1, 0, 0, 0, 0, 0, 0]),
'2020' : pd.Series([1, 1, 10, 3, 1, 0, 0, 0, 1])
}

df = pd.DataFrame(d) 

The data looks like this:

gdf = gdf.merge(df, on='rgn15nm')


# Define bins
gdf['2020'].describe()
bins= UserDefined(gdf['2020'], bins=[0,1,2,3,4,5,6,7,8,9,10]).bins
bins

# create a new column with the discretized values and plot that col
# repeat for each view
fig,(ax1,ax2,ax3) = plt.subplots(1,3,figsize=(15,6))
gdf.assign(cl=UserDefined(gdf['1980'].dropna(), bins).yb).plot(column='cl', ax=ax1, cmap='OrRd', legend = True )
gdf.assign(cl=UserDefined(gdf['2000'].dropna(), bins).yb).plot(column='cl', ax=ax2, cmap='OrRd', legend = True) 
gdf.assign(cl=UserDefined(gdf['2020'].dropna(), list(bins)).yb).plot(column='cl', ax=ax3, cmap='OrRd', legend = True)
for ax in (ax1,ax2,ax3,):
    ax.axis('off')


Clearly, the color scheme is not the same across subplots. What I mean by this is that 'Northwest' (the only region highlighted in the 1980 subplot) had the same value of 1 in all years 1980, 2000 and 2020. Yet, this region shows in different colors across the 3 subplots, despite the value being constant. I want ""Northwest"" to show in the same color (that of the subplot for 2020) across all 3 subplots.
I also tried this:
fig,(ax1,ax2,ax3) = plt.subplots(1,3,figsize=(15,6))
ax1.set_title('1980')
ax2.set_title('2000')
ax3.set_title('2020')
gdf.plot(column='1980', ax=ax1, cmap='OrRd', scheme='userdefined', classification_kwds={'bins':[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10]})
gdf.plot(column='2000', ax=ax2, cmap='OrRd', scheme='userdefined', classification_kwds={'bins':[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10]})
gdf.plot(column='2020', ax=ax3, cmap='OrRd', scheme='userdefined', classification_kwds={'bins':[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10]})
for ax in (ax1,ax2,ax3):
    ax.axis('off')

But got exactly the same figure as immediately above (see below)

Does any one have any insight? I want a consistent color scheme across all 3 subplots.
",2,1306,"So ultimately the solution was using the ""norm"" option. Following this example: Geopandas userdefined color scheme drops colors. See below:
from matplotlib.colors import Normalize
bins= UserDefined(gdf['2020'], bins=[0,1,2,3,4,5,6,7,8,9,10]).bins
bins

fig,(ax1,ax2,ax3) = plt.subplots(1,3,figsize=(15,6))
ax1.set_title('1980')
ax2.set_title('2000')
ax3.set_title('2020')
gdf.plot(column='1980', ax=ax1, cmap='OrRd', scheme='userdefined', classification_kwds={'bins':bins}, norm=Normalize(0, len(bins)))
gdf.plot(column='2000', ax=ax2, cmap='OrRd', scheme='userdefined', classification_kwds={'bins':bins}, norm=Normalize(0, len(bins)))
gdf.plot(column='2020', ax=ax3, cmap='OrRd', scheme='userdefined', classification_kwds={'bins':bins}, norm=Normalize(0, len(bins)))
for ax in (ax1,ax2,ax3):
    ax.axis('off')

The result is what I wanted:
Expected Graph
or as suggested by Paul H:
fig,(ax1,ax2,ax3) = plt.subplots(1,3,figsize=(15,6))
ax1.set_title('1980')
ax2.set_title('2000')
ax3.set_title('2020')
gdf.plot(column='1980', ax=ax1, cmap='OrRd', scheme='userdefined', classification_kwds={'bins':bins}, vmin = 0, vmax = 10)
gdf.plot(column='2000', ax=ax2, cmap='OrRd', scheme='userdefined', classification_kwds={'bins':bins}, vmin = 0, vmax = 10)
gdf.plot(column='2020', ax=ax3, cmap='OrRd', scheme='userdefined', classification_kwds={'bins':bins}, vmin = 0, vmax = 10)
for ax in (ax1,ax2,ax3):
    ax.axis('off')

",,
matplotlib strange behavior,https://stackoverflow.com/questions/17189313,strange behavior of numpy.asmatrix or matplotlib.pyplot.scatter,,1,507,,,
matplotlib strange behavior,https://stackoverflow.com/questions/64966611,Why does pandas dataframe return 2 columns when only 1 is selected,"While creating some plots with matplotlib I found a strange behavior of pandas, when I select only 1 column it returns 2.
import pandas as pd
import io

data = io.StringIO(""""""time_0,1,time_1,2,time_2,0,time_3,3
-0.002,-0.1225,-0.002,-0.0904,-0.002,0.0331,-0.002,0.,
0.0,-0.1225,0.,-0.0904,0.,0.0331,0.,0.,
0.002,-0.1224,0.002,-0.0904,0.002,0.0331,0.002,0.,
0.004,-0.1225,0.004,-0.0904,0.004,0.0331,0.004,0.,"""""")

df = pd.read_csv(data)
print(df[""time_0""])

Output:

-0.002   -0.1225
0.000   -0.1225
0.002   -0.1224
0.004   -0.1225
Name: time_0, dtype: float64

It shows values from both column ""time_0"" and ""1"", but only ""time_0"" was selected. Is this a bug or a feature?
",1,660,"your dataframe returns only one line, but it also prionting the index which is same as the column ""1""
df
Out[3]: 
        time_0      1  time_1      2  time_2      0  time_3   3
-0.002 -0.1225 -0.002 -0.0904 -0.002  0.0331 -0.002     0.0 NaN
 0.000 -0.1225  0.000 -0.0904  0.000  0.0331  0.000     0.0 NaN
 0.002 -0.1224  0.002 -0.0904  0.002  0.0331  0.002     0.0 NaN
 0.004 -0.1225  0.004 -0.0904  0.004  0.0331  0.004     0.0 NaN

it seems like it takes unintentionally the first column as index...
it  takes the last column as a nan value because of the extra , in each line....
try removing the ,:
 import pandas as pd
 import io
 
 data = io.StringIO(""""""time_0,1,time_1,2,time_2,0,time_3,3
 -0.002,-0.1225,-0.002,-0.0904,-0.002,0.0331,-0.002,0.
 0.0,-0.1225,0.,-0.0904,0.,0.0331,0.,0.
 0.002,-0.1224,0.002,-0.0904,0.002,0.0331,0.002,0.
 0.004,-0.1225,0.004,-0.0904,0.004,0.0331,0.004,0."""""")
 
 df = pd.read_csv(data)
 print(df[""time_0""])

this code will print
0   -0.002
1    0.000
2    0.002
3    0.004
Name: time_0, dtype: float64

",,
matplotlib strange behavior,https://stackoverflow.com/questions/66737752,Python: How to save EXACT numpy array data to image using matplotlib.image.imsave(),"I observed some strange behavior when saving and loading data using matplotlibs imsave() and imread() functions. The RGB values I save are different to the ones I get when loading the picture back in again.
import numpy as np
from matplotlib import image

s_pic = np.zeros((1, 1, 3))

s_pic[0,0,0] = 0.123
s_pic[0,0,1] = 0.456
s_pic[0,0,2] = 0.789

image.imsave('pic.png', s_pic)

l_pic = image.imread('pic.png')

print(l_pic[0,0,0])
print(l_pic[0,0,1])
print(l_pic[0,0,2])

The output I get is:
0.12156863
0.45490196
0.7882353

Can somebody explain why the RGB values change in this process? I have checked the matplotlib documentation but couldn't find an answer to this question.
",1,611,"
Can somebody explain why the RGB values change in this process?

RGB values are integers in the range 0-255. Your float is interpreted as:
&gt;&gt;&gt; .123 * 255
31.365
&gt;&gt;&gt; int(.123 * 255)
31

Thirty-one is being written to that pixel. Then the reverse..
&gt;&gt;&gt;
&gt;&gt;&gt; 31 / 255
0.12156862745098039
&gt;&gt;&gt;


Delving into the source for imsave() the array passed to imsave() is converted to RGBA values using matplotlib.cm.ScalarMappable().to_rgba(bytes=True)
&gt;&gt;&gt; from matplotlib import cm
&gt;&gt;&gt; sm = cm.ScalarMappable()
&gt;&gt;&gt; rgba = sm.to_rgba(s_pic, bytes=True)
&gt;&gt;&gt; rgba
array([[[ 31, 116, 201, 255]]], dtype=uint8)
&gt;&gt;&gt;

",,
matplotlib strange behavior,https://stackoverflow.com/questions/23897118,pylab.plot &quot;cannot convert float NaN to integer&quot; after calling scipy.stats.multivariate_normal,"While testing a regression algorithm I found this strange behavior: for some covariance matrices, the multivariate_normal function gives correct samples but then an exception is raised (only) the first time pylab.plot() is called:

ValueError: cannot convert float NaN to integer

The following code reproduces the error:
import numpy as np
from scipy.stats import multivariate_normal as mnorm
from matplotlib import pyplot as plt

B = np.array([ 0, 0, 0])

# works fine
v1 = np.array([[1, 0, 0],
              [0, 1, 0],
              [0, 0, 1]])


# OK. non positive semidefinite, well raised exception
v2 = np.array([[ 0.2 , -0.2, -0.3],
              [-0.2,  0.4, -0.9],
              [-0.3, -0.9,  0.7]])

# KO. exception (?)
v3 = np.array([[ 0.2 , -0.02, -0.026],
              [-0.02,  0.014, -0.009],
              [-0.026, -0.009,  0.017]])



w = mnorm(mean=B, cov=v3).rvs()
print w

plt.plot(w)
plt.show()

And if plt.plot(w) is called a second time, then it works. Any ideas?
Versions:

python 2.7.5 Anaconda 1.9.1 (64-bit)
scipy 0.14.0
matplotlib 1.3.1
numpy 1.8.1

",0,1459,,,
matplotlib strange behavior,https://stackoverflow.com/questions/28048647,Python : strange behavior with matplotlib barchart,"I have a strange behavior using matplotlib in an ipython notebook: when I change the attribute ""color"" of the barchart, the result is ugly, I have some bars in red and some others in black. This is happening only when I have a high amount of bars to display (&gt;100).

You can execute the code below to reproduce the problem, playing with the dataPoints parameter to see the effect of the number of bars:

import random
import matplotlib.pyplot as plt

dataPoints = 400
data = random.sample(range(300, 1000), dataPoints)
xCoords = range(dataPoints)

fig = plt.figure(figsize=[13,9])
plt.bar(xCoords,data,color='red')
plt.show()


Here is an example of the result :


",0,361,,,
matplotlib strange behavior,https://stackoverflow.com/questions/41837171,Matplotlib 2.0 subscript outside of baseline when super and subscript are both used,"With matplotlib 2.0 I have been having strange behavior when I use both subscript and superscript on the same character. When they are combined, the subscript goes down entirely below the baseline. This didn't happen with MPL 1.5. Here is a complete example:

import matplotlib as mpl
import matplotlib.pyplot as plt
mpl.rc(""font"", family=""Times New Roman"",weight='normal')
plt.rcParams.update({'mathtext.default':  'regular' })
plt.plot(1,1, label='$A_x^{b}$')
plt.plot(2,2,label='$A_x$')
plt.plot(3,3,label='$A^b$')
plt.plot(4,4,label='$A_x^{*}$')
plt.plot(5,5,label='$A^*$')
plt.legend(fontsize='xx-large')
plt.show()


I've taken this plot and zoomed in on the legend and drawn some horizontal lines to show the relative super and subscript positions.



I found in the mathtext.py file these parameters under the class FontConstantBase:

# Percentage of x-height of additional horiz. space after sub/superscripts
script_space = 0.05

# Percentage of x-height that sub/superscripts drop below the baseline
subdrop = 0.4

# Percentage of x-height that superscripts are raised from the baseline
sup1 = 0.7

# Percentage of x-height that subscripts drop below the baseline
sub1 = 0.3

# Percentage of x-height that subscripts drop below the baseline when a
# superscript is present
sub2 = 0.5

# Percentage of x-height that sub/supercripts are offset relative to the
# nucleus edge for non-slanted nuclei
delta = 0.025

# Additional percentage of last character height above 2/3 of the
# x-height that supercripts are offset relative to the subscript
# for slanted nuclei
delta_slanted = 0.2

# Percentage of x-height that supercripts and subscripts are offset for
# integrals
delta_integral = 0.1


Did sub2 exist in previous versions? Could going from 0.3 to 0.5 really drop it completely below the baseline like I'm seeing? I'd like to have simultaneous superscripts and subscripts that aren't completely outside the baseline and I don't see any other way besides modifying mathtext.py itself. Also, it appears that when including an asterisk in the superscript, it also goes higher than anticipated with mpl 2.0. Is there a way to lower it down just a bit without changing mathtext? Thanks. 
",0,1348,"There seems to be no API to change this but you can monkey-patch the appropriate class instead of editing mathtext.py.
Using the default mathtext font the position of the subscript changes if there is a superscript (not totally under the baseline but you can see the effect):
def test_plot():
    plt.figure()
    plt.plot(1, 1, label=""$A_x^b$"")
    plt.plot(2, 2, label=""$A^b_x$"")
    plt.plot(3, 3, label=""$A_x$"")
    plt.plot(4, 4, label=""$A_x^*$"")
    plt.plot(4, 4, label=""$A^*_x$"")
    plt.plot(5, 5, label=""$A^*$"")
    plt.legend(fontsize=""xx-large"")


# default mathtext font in matplotlib 2.0.0 is 'dejavusans'
# set explicitly for reproducibility
plt.rcParams[""mathtext.fontset""] = ""dejavusans""
test_plot()


Monkey-patching mathtext.DejaVuSansFontConstants you can make the effect disappear:
import matplotlib.mathtext as mathtext
mathtext.DejaVuSansFontConstants.sub2 = 0.3  # default 0.5
test_plot()


(For more recent versions of matplotlib, like 3.4.2, this class appears to have been moved to a _mathtext submodule. You may need to do something like the following:)
# Later versions of matplotlib (e.g., 3.4.2)
from matplotlib.mathtext import _mathtext as mathtext

mathtext.FontConstantsBase.sup1 = 0.5

I can't see any issue with the asterisk.
I don't have Times New Roman installed so I can't test your exact use case but probably you need to patch FontConstantsBase instead of DejaVuSansFontConstants. It worked for me using Liberation Serif.
",,
matplotlib strange behavior,https://stackoverflow.com/questions/43792081,"Destroying tkinter mainloop (with canvas and buttons), strange behaviour","I have root=Tk.Tk() which consist from labels (Tk.Label), entry forms (Tk.Entry), button (Tk.Button) and embedded matplotlib FigureCanvasTkAgg. This is simple script, which plots quadtratic functions with coefficients (a, b, c) specified by user. But I have some problems and misunderstanding with destroying the mainloop. 
I was expecting, that simple closing of the main window (by clicking ""x"" at the right corner), should also lead to the exit from mainloop. This is the case for my script, if I would comment all the code, related to the embeded canvas. But with embedded FigureCanvasTkAgg, it doesn't happen (so there is a need for interrupting kernel for running script again). As was discussed in other questions, there are 2 methods for exiting mainloop: root.quit() and root.destroy(). So I also tried to use these methods for exiting mainloop:

def close_all():
    root.quit()
    root.destroy()


by new button :

button_quit=Tk.Button(root, text=""Quit"", command=close_all)
button_quit.grid(row=5, column=1)


by overwriting ""x"" button :

root.protocol('WM_DELETE_WINDOW', close_all)


My questions:


Using both methods worked. But why if I use only root.destroy(), root.mainloop() would still execute? Why mainloop is not being destroyed just after closing window? And why this problem causes FigureCanvasTkAgg?
Embedding matplotlib figure was done by the following (if it helps to explain):

fig, ax = plt.subplots()
canvas = FigureCanvasTkAgg(fig, master=root)
plot_widget = canvas.get_tk_widget()
#(plotting with plt.plot(..))
canvas.show()
plot_widget.grid(row=6, column = 0, columnspan=3)

close_all() function worked in that sense, that there was performed exit from mainloop, and script execution stopped. But there is still some strange behavior. There variables such as var_a = Tk.StringVar(), used for receiving data from entry fields in a way:

L_a = Tk.Label(master=root, width=10, height=2, bd =2, text=""a"")
E_a = Tk.Entry(master=root, width=10, bd =2, textvariable=var_a) 


with

def get_value():
    if not check_if_float(var_a.get()) or not check_if_float(var_b.get()) or not check_if_float(var_c.get()):
        showerror(title = ""Error"", message = ""Invalid entry. Enter integer or decimal numbers, please"")
        return None
    a=float(var_a.get())
    b=float(var_b.get())
    c=float(var_c.get())


So the problem and strange behavior is that after executing code once, and closing main window (either by clicking ""x"" at the right top corner, or by clicking ""Quit"" button), in the next execution code receives empty var_a, var_b, var_c. If kernel interrupted, and code runs again, it works correctly (running it in jupyter notebook). I have no idea why is it like that.

",0,924,"This is because you did not provide the variables with a master, so they default to the first Tk() instance created, even if it's been destroyed. The fix is simply to provide the correct root:

var_a = Tk.StringVar(root)


This usually isn't an issue since there are very few situations where you would want to call Tk() more than once. 
",,
matplotlib strange behavior,https://stackoverflow.com/questions/59656632,"Using qt5agg backend with Matplotlib 3.1.2, get_backend() changes behavior","I have been using a project which embeds Matplotlib figures in a Qt window and found that it failed when I updated Matplotlib to a version greater than 3.1.0rc1. I have created a minimum working example (testImports.py):
import matplotlib

from matplotlib.backends.backend_qt5agg import FigureCanvasQTAgg as FigureCanvas
from matplotlib.backends.backend_qt5agg import NavigationToolbar2QT as NavigationToolbar

import matplotlib.pyplot as plt
import numpy as np

f = plt.figure()
x = np.arange(0, 10, 0.001)
ysin = np.sin(x)
plt.plot(x, ysin, '--')
plt.show()

With Matplotlib 3.1.0rc1 or earlier, this runs properly, displaying a graph. With Matplotlib 3.1.0rc2 or newer, I get the following error (note that I am working in a virtual environment and I have changed the root of the paths to test_env):
Traceback (most recent call last):
  File ""testImports.py"", line 14, in &lt;module&gt;
    f = plt.figure()
  File ""test_env/lib/python3.6/site-packages/matplotlib/pyplot.py"", line 534, in figure
    **kwargs)
  File ""test_env/lib/python3.6/site-packages/matplotlib/backend_bases.py"", line 3249, in new_figure_manager
    return cls.new_figure_manager_given_figure(num, fig)
  File ""test_env/lib/python3.6/site-packages/matplotlib/backend_bases.py"", line 3255, in new_figure_manager_given_figure
    canvas = cls.FigureCanvas(figure)
TypeError: 'NoneType' object is not callable

I have found that if I include the following lines at the top of testImports.py, it works properly for all versions of Matplotlib 3+.
import matplotlib
matplotlib.use('qt5agg')

So, it seems that something has changed in the way Matplotlib registers backends.
In my testing I also found that having the following at the top of testImports.py solves the problem (without also calling use('qt5agg'):
import matplotlib
print(matplotlib.get_backend())

That seems very strange to me, since the matplotlib.get_backend() function is simply returning a value from a dictionary (from the source):
def get_backend():
    """"""
    Return the name of the current backend.

    See Also
    --------
    matplotlib.use
    """"""
    return rcParams['backend']

This brings me to my questions:

What changed between Matplotlib 3.1.0rc1 and 3.1.0rc2 that has made it necessary to use the matplotlib.use() function prior to importing specific backend functions?
Why is calling matplotlib.get_backend() drastically changing the behavior of my program, making it unnecessary to call matplotlib.use()?

If it helps I am using Python 3.6.9 and Numpy 1.18.0.
Edit:
I installed Matplotlib 3.2.0rc2 and am still experiencing the same behavior.
",0,1903,"I don't know if what I'm about to describe is the intended behavior of matplotlib or a bug. Perhaps the maintainers can weight in.
In essence, matplotlib.get_backend() is NOT is simply returning a value from a dictionary. In fact, it is triggering a lazy initialization of the ""backend"" via importing pyplot under the hook, and I can imagine that creating some variations in behavior, since merely calling import matplotlib is not sufficient to trigger the initialization (at least in all cases).
When you import matplotlib or any of it's modules, matplotlib.__init__.py is implicitly run.
It appears that rcParams.__getitem__() is intended to do a lazy initialization of the ""backend"" if the key value is set to rcsetup._auto_backend_sentinel.
When an rcParam object is first constructed, there is no 'backend' key in the dict, even if the desired default value (could depend on matplotlibrc?) is rcsetup._auto_backend_sentinel.
These lines of the __setitem__() method can involve a situation where key == backend and val == rcsetup._auto_backend_sentinel, but backend is not yet a valid key in the rcParams(MutableMapping, dict) object. As a consequence, when the 3rd line if 'backend' in self:, which implicitly calls __getitem__ with a key of 'backend' to check for the presence of the key, gets processed, the lazy initialization of the backend doesn't occur, since the line of __getitem__ that I've marked ""THIS MAY THROW"" does in fact throw, and prevent the subsequent lazy initialization.
# Excerpt of rcParams(MutableMapping, dict).__setitem__ from around lines 672
elif key == 'backend':
    if val is rcsetup._auto_backend_sentinel:
        if 'backend' in self:
            return

# Excerpt of rcParams(MutableMapping, dict).__getitem__ from around lines 699
elif key == ""backend"":
    val = dict.__getitem__(self, key) # THIS MAY THROW
    if val is rcsetup._auto_backend_sentinel:
        from matplotlib import pyplot as plt
        plt.switch_backend(rcsetup._auto_backend_sentinel)

However, in __setitem__ the code does succeed in getting rcParams['backend'] set to rcsetup._auto_backend_sentinel (it is normal for x in X calls to ignore KeyErrors). So the rcParams is populated, but the lazy initialization is yet to be done.
Now when you call matplotlib.get_backend() (or other related functions), part of the code path will do rcParams['backend'], which this time will actually trigger the lazy initialization in __getitem__.
",,
matplotlib strange behavior,https://stackoverflow.com/questions/59810707,MatPlotLib text position strange behavior,"I have a function to display scatter plots:

def critics_and_users_score_corr_with_total_sales(_df, platform):
    critic_score_not_na = df['critic_score'].notna() &amp; (df['platform'] == platform)
    total_sales_for_critic_score = df[critic_score_not_na]['total_sales']
    critic_score = df[critic_score_not_na]['critic_score']

    user_score_not_na = df['user_score'].notna() &amp; (df['platform'] == platform)
    total_sales_for_user_score = df[user_score_not_na]['total_sales']
    user_score = df[user_score_not_na]['user_score']

    info_bbox_props = dict(boxstyle='round', facecolor='white', alpha=0.2)

    fig = plt.figure(figsize=(16, 7))
    ax1 = fig.add_subplot(121)
    ax2 = fig.add_subplot(122)

    ax1.grid(True)
    ax1.set_xlim(0,25)
    ax1.set_title('{} Critics score &amp; Total sales correlation'.format(platform))
    ax1.set_xlabel('Total sales')
    ax1.set_ylabel('Critics score')
    ax1.scatter(total_sales_for_critic_score, critic_score, s = 10)

    ax1.text(0.345, 0.88, 'Corelation: {:.2f}'.format(total_sales_for_critic_score.corr(critic_score)), 
             transform=ax.transAxes, verticalalignment='top', horizontalalignment='right', bbox=info_bbox_props)

    ax2.grid(True)
    ax2.set_xlim(0,25)
    ax2.set_title('{} Users score &amp; Total sales correlation'.format(platform))
    ax2.set_xlabel('Total sales')
    ax2.set_ylabel('Users score')
    ax2.scatter(total_sales_for_user_score, user_score, s = 10)
    ax2.text(0.892, 0.88, 'Corelation: {:.2f}'.format(total_sales_for_user_score.corr(user_score)), 
             transform=ax.transAxes, verticalalignment='top', horizontalalignment='right', bbox=info_bbox_props)

    plt.show()


I call this function further in code:

critics_and_users_score_corr_with_total_sales(df, top_platforms.index[0])


And here is the result:

As we can see, text fields are placed outside the plots.

But if I run this cell by Shift+Enter the second and subsequent time - text the fields are placed in other location:


If I restart the Kernel and run all cells, the text fields are placed outside the plot again.

What kind of magic is this?
",0,284,"Problem was in following lines:

ax1.text(0.345, 0.88, 'Corelation: {:.2f}'.format(total_sales_for_critic_score.corr(critic_score)), 
         transform=ax.transAxes, verticalalignment='top', horizontalalignment='right', bbox=info_bbox_props)


My ax has a ax1 name, but I send to parameters transform=ax.transAxes. Obviously, ax was declared in other place.

My bad.
",,
matplotlib strange behavior,https://stackoverflow.com/questions/62011120,Matplotlib histogram misplaced and missing bars,"I have large data files and thus am using numpy histogram (same as used in matplotlib) to manually generate histograms and update them.  However, at plotting, I feel that the graph is shifted.

This is the code I use to manually create and update histograms in batches.  Note that all histograms share the same bins.

temp = np.histogram(batch, bins=np.linspace(0, 40, 41))
hist += temp[0]


The code above is repeated as I parse the data files.  For example, a small data set would have the following as the final histogram data:

[8190, 666, 278, 145, 113, 83, 52, 48, 45, 44, 45, 29, 28, 45, 29, 15, 16, 10, 17, 7, 15, 6, 10, 7, 3, 5, 7, 4, 2, 3, 0, 1, 0, 0, 0, 0, 0, 0, 0, 29]

Below is the plotting code.

import matplotlib
matplotlib.use('agg')
import matplotlib.pyplot as plt
import numpy as np
plt.xticks(np.linspace(0, 1, 11))
plt.hist([i/40 for i in range(40)], bins=np.linspace(0, 1, 41), weights=scores, rwidth=0.7)
plt.yscale('log', nonposy='clip')


The resulting figure is quite strange.  It shows no bar at [0.475, 0.5) and I expect the 0.975 bin which is range [0.975, 1.0] to include the last 29 values.  However instead, I see that bar at the [0.950, 0.975) position.  I thought this might have to do with using bins and linspace, but the size of the decoy array and weights are the same.  



I'm never seen this kind of behavior.  I also thought it would be the way the ranges are [ x, x+width), but I haven't had issues with this.

A note on using linspace.  It specifies edges, so 40 bins is specified by 41 edges.

In [2]: np.linspace(0,1,41)                                                     
Out[2]: 
array([0.   , 0.025, 0.05 , 0.075, 0.1  , 0.125, 0.15 , 0.175, 0.2  ,
       0.225, 0.25 , 0.275, 0.3  , 0.325, 0.35 , 0.375, 0.4  , 0.425,
       0.45 , 0.475, 0.5  , 0.525, 0.55 , 0.575, 0.6  , 0.625, 0.65 ,
       0.675, 0.7  , 0.725, 0.75 , 0.775, 0.8  , 0.825, 0.85 , 0.875,
       0.9  , 0.925, 0.95 , 0.975, 1.   ])

In [3]: len(np.linspace(0,1,41))                                                
Out[3]: 41

",0,2428,"It seems you're using plt.hist with the idea to put one value into each bin, so simulating a bar plot. As the x-values fall exactly on the bin bounds, due to rounding they might end up in the neighbor bin. That could be mitigated by moving the x-values half a bin width. The simplest is drawing the bars directly.

The following code creates a bar plot with the given data, with each bar at the center of the region it represents. As a check, the bars are measured again at the end and their height displayed.

from  matplotlib.ticker import MultipleLocator
import matplotlib.pyplot as plt
import numpy as np

scores =[8190,666,278,145,113,83,52,48,45,44,45,29,28,45,29,15,16,10,17,7,15,6,10,7,3,5,7,4,2,3,0,1,0,0,0,0,0,0,0,29]
binbounds = np.linspace(0, 1, 41)
rwidth = 0.7
width = binbounds[1] - binbounds[0]
bars = plt.bar(binbounds[:-1] + width / 2, height=scores, width=width * rwidth, align='center')
plt.gca().xaxis.set_major_locator(MultipleLocator(0.1))
plt.gca().xaxis.set_minor_locator(MultipleLocator(0.05))
plt.yscale('log', nonposy='clip')
for rect in bars:
    x, y = rect.get_xy()
    w = rect.get_width()
    h = rect.get_height()
    plt.text(x + w / 2, h, f'{h}\n', ha='center', va='center')
plt.show()




PS: To see what's happening with the original histogram, just do a test plot without the weights:

plt.hist([i/40 for i in range(40)], bins=np.linspace(0, 1, 41), rwidth=1, ec='k')
plt.plot([i/40 for i in range(40)], [0.5] * 40, 'ro')
plt.xticks(np.linspace(0, 1, 11))


A red dot shows where the x-values are. Some fall into the correct bin, some into the neighbor which suddenly gets 2 values.


To create a histogram with the x-values at the center of each bin:

plt.hist([i/40 + 1/80 for i in range(40)], bins=np.linspace(0, 1, 41), rwidth=1, ec='k')
plt.plot([i/40 + 1/80 for i in range(40)], [0.5] * 40, 'ro')
plt.xticks(np.linspace(0, 1, 11))
plt.yticks([0, 1])



","The problem is due to the rounding error of  np.linspace(0, 1, 11).
bins = []
for abin in np.linspace(0, 1, 41):
    bins.append(abin)

The code above will get
bins = [0.0, 0.025, 0.05, 0.07500000000000001, 0.1, 0.125, 0.15000000000000002, ...] 

,which causes the problem.
However, when you do np.round(np.linspace(0, 1, 41), 4), the problem is fixed.
Example:
plt.hist([i/40 for i in range(40)], bins=np.round(np.linspace(0, 1, 41), 4), rwidth=1, ec='k')
plt.plot([i/40 for i in range(40)], [0.5] * 40, 'ro')
plt.xticks(np.linspace(0, 1, 11))


",
matplotlib strange behavior,https://stackoverflow.com/questions/62893128,Python Matplotlib: Animating a rotating PatchCollection,"I am trying to make an animation in which a PatchCollection rotates around the origin. The easiest way seems to be through using a FuncAnimation.
As per the answer on matplotlib change a Patch in PatchCollection I am using an extension of the PatchCollection class which allows me to update the positions of the individual patches.
The animation function simply calls one of the class methods and returns the collection which is updated:
fig, ax = plt.subplots(1)

grid = HexagonalGrid()

ax.add_collection(grid)

def animate(frame):
    grid.rotateGrid(0.1)
    return (grid,)
    
anim = anim.FuncAnimation(fig, animate, blit=True)


When using RegularPolygon patches all works fine:
class HexagonalGrid(UpdateablePatchCollection):    
    def __init__(self, latticeParameter = 5):
        self.patches = self._initiatePatches(hexRadius)
        UpdateablePatchCollection.__init__(self, self.patches, match_original=True, animated=True)

    def _initiatePatches(self, hexRadius):
        patches = []
        for :
            #calculates the correct starting position for each patch
            patches.append(self._createHex(x,y,hexRadius))
        
    def _createHex(self, x,y, hexRadius):
        return matplotlib.patches.RegularPolygon(
            (x, y),
            numVertices = 6,
            radius = hexRadius,
            orientation = 0,
            fc = (0.1, 0.1, 0.1, 0.1), # fill colour RGBA (black and transparent)
            ec = 'black' # edge colour
        )

        def rotateGrid(self, angle):
        for patch in self.patches:
            x, y = patch.xy
            xNew = x * math.cos(angle) - y * math.sin(angle)
            yNew = x * math.sin(angle) + y * math.cos(angle)
            patch.xy = (xNew, yNew)
            patch.orientation = patch.orientation + angle

But when trying to do this with Rectangle patches which is the same class except for the following functions:
    def _createRectangle(self, x, y, width, height):
        return matplotlib.patches.Rectangle(
            (x, y),
            width,
            height,
            fc=(0.1, 0.1, 0.1, 0.1),
            ec='black',
            animated=True
        )

    def rotateGrid(self, angle):
    for patch in self.patches:
        x, y = patch.xy
        xNew = x * math.cos(angle) - y * math.sin(angle)
        yNew = x * math.sin(angle) + y * math.cos(angle)
        patch.xy = (xNew, yNew)
        patch.angle = patch.angle + angle

The rectangles are not rotated with the grid
Example
(For clarity: I expected the rectangles to be rotated 45 degrees as well)
I have already tried replacing the rotation by
patch.set_transform(patch.get_transform() + matplotlib.transforms.Affine2D().rotate(angle))  but this had some very strange behavior by moving and enlarging the patches.
Does anyone know a way to make the rectangles rotate?
",0,352,"After some research into the matplotlib source-code I found out what the problem was. In case anyone else encounters the same or a similar problem please read the solution below and see if it will help you!
In the last part of my post I already mentioned the possible use of the Rectangle.set_transform() function. The problem lies in the subsequent calls of this setter function.
Because I wanted to modify my existing transform I used the getter Rectangle.get_transform() to which I added another transform.
For some reason the getter of the transform of Rectangle (but it might be the case for all Patches) does not return the set value. That is:
t = some_transformation
rect = Rectangle(x,y,width,height)
rect.set_transform(t)
print(rect.get_transform() == t) 

prints False
The correct getter to use in this case would be rect.get_data_transform() after which subsequent transforms can be added.
",,
matplotlib strange behavior,https://stackoverflow.com/questions/41655430,Strange behavior in matplotlib (multiple) histograms,,-1,1068,,,
matplotlib strange behavior,https://stackoverflow.com/questions/67692855,Approximating sin using the Taylor series,,-2,458,,,
matplotlib strange output,https://stackoverflow.com/questions/25328818,python 2.7: cannot pip on windows &quot;bash: pip: command not found&quot;,"I am trying to install the SciPy stack located at https://scipy.org/stackspec.html [I am only allowed 2 links; trying to use them wisely].  I realize that there are much easier ways to do this, but I think there is a lot to be learned by doing it manually.  I am relatively new to a lot of this stuff, so I apologize if I sound ignorant at any point.
I am running  Windows 7 Enterprise - 64 bit.  Here is what I have done so far:

Installed python-2.7.8.msi (32-bit) from https://www.python.org/download/releases/2.7.8/

Installed numpy-1.8.1-win32-superpack-python2.7 from
http://sourceforge.net/projects/numpy/files/
Test: import numpy as np ---&gt; no errors

Installed scipy library,
scipy-0.14.0-win32-superpack-python2.7.exe from
(SCIPY DOT ORG LINK REMOVED)
Test: import scipy as sp ---&gt; no errors

Installed matplotlib: matplotlib-1.3.1.win32-py2.7.exe from
(MATPLOTLIB DOT ORG LINK REMOVED)

Installed PIP by running script here:
https://raw.githubusercontent.com/pypa/pip/master/contrib/get-pip.py
I just copied-pasted script to a new file in IDLE,
saved as C:\Python27\Scripts\pip_install.py and clicked Run&gt;module. No errors reported.


Does the path on which I saved
pip_install.py matter?




HERE IS WHERE I FAIL
Attempted to install matlibplot dependency dateutil: Opened a
Cygwin Shell, and typed
        cd C:\Python27          ! is it necessary to cd to python directtory?
        pip install python-dateutil

This results in the error:
    bash: pip: command not found

I get the same error attempting from cmd.
Any help  is appreciated; the closest I found was bash: pip: command not found.  But the OSX nature of it is just enough to confise me further.

UPDATE:
I added the pip-path per Paul H's suggestion below.  It made the error go away, but strangely, nothing I pip actually installs. For example, in Cygwin, I type:
cbennett2&gt; pip install python-dateutil
cbennett2&gt;                            

You can see that there is no output or feedback from the shell (which I think there should be).  Then when I go to a new python shell:
&gt;&gt;&gt; from dateutil.parser import parse
Traceback (most recent call last):
  File ""&lt;pyshell#12&gt;"", line 1, in &lt;module&gt;
    from dateutil.parser import parse
ImportError: No module named dateutil.parser
&gt;&gt;&gt;&gt;

This happens with all of the modules that I thought I had pip'd ... pandas, tornado, etc.
",45,166576,"The problem is that your Python version and the library you want to use are not same versionally (Python). Even if you install Python's latest version, your PATH might not change properly and automatically. Thus, you should change it manually.After matching their version, it will work.

Ex: When I tried to install Django3, I got same error. I noticed that my PATH still seems C:\python27\Scripts though I already install Python3.8, so that I manually edited my PATH C:\python38\Scripts and reinstalled pip install Django and everything worked well. 
",,
matplotlib strange output,https://stackoverflow.com/questions/60832177,Matplotlib generated pdf cannot be opened by Acrobat Reader,"I am trying to use serif fonts with latex with matplotlib, unfortunately it does not display in Acrobat Reader, but it is OK on Internet Explorer or Chrome. My minimal code produces an error StandardSymL_Slant_167 not found and varepsilon is not visible, what is missing?
import matplotlib as mpl
import matplotlib.pyplot as plt

mpl.rcParams.update(mpl.rcParamsDefault)
mpl.rcParams['text.usetex'] = True
mpl.rcParams['font.family'] = ['serif']
mpl.rcParams['font.serif'] = ['times']

fig, ax = plt.subplots()
ax.set_xlabel(r'$x$ $\alpha \varepsilon$')
fig.savefig('test.pdf')


The output of my enviroment is:
import matplotlib
print(matplotlib.__version__)
3.2.0

edit
Found this as well strange:
pdffonts test.pdf
name                                 type              encoding         emb sub uni object ID
------------------------------------ ----------------- ---------------- --- --- --- ---------
NimbusRomNo9L-Regu                   Type 1            Custom           yes no  no      21  0
CMMI10                               Type 1            Builtin          yes no  no      13  0
NimbusRomNo9L-ReguItal               Type 1            Custom           yes no  no      25  0
StandardSymL_Slant_167               Type 1            Builtin          yes no  no      17  0

",5,957,"This is not really a nice solution, but I'm not sure a work-around is possible short of manually editing the resulting pdf in a text editor. It seems that this is an error caused by the Adobe Acrobat parser, because matplotlib outputs double precision numbers, whereas Type 1 fonts only support single precision.
Relevant issue which seems it will be fixed in the upcoming version of matplotlib 3.4.0:
https://github.com/matplotlib/matplotlib/issues/16087
The fix has since been merged, so updating to the latest master, e.g. with pip install git+https://github.com/matplotlib/matplotlib.git@master should fix it.
","As I understand, the problem with Greek fonts.
Interesting, that problems usually with win os...
Try win adapted version, please:
import matplotlib as mpl
import matplotlib.pyplot as plt
mpl.rcParams.update(mpl.rcParamsDefault)

mpl.rcParams['text.usetex'] = True
# mpl.rcParams['font.family'] = ['serif']
# mpl.rcParams['font.serif'] = ['times']
mpl.rcParams['font.family'] = ['times new roman']

fig, ax = plt.subplots()
ax.set_xlabel(r'$x$ $\alpha \varepsilon$')
fig.savefig('test.pdf')

","I had a similar issue, I didn't get any informative warnings from pdf readers, but Chrome could display the pdfs.
For me the solution was by changing a linewidth that was set to zero. More specifically in my case for fill_between:
plt.fill_between(steps, lower_line, top_line,
                                 linewidth=0.01, # CANNOT BE ZERO, GIVES ERRORS IN PDF READERS
                                 linestyle='dashdot',
                                 antialiased=True,
                                 )

"
matplotlib strange output,https://stackoverflow.com/questions/40309448,Piecewise Affine Transform+warp output looks strange,"I have an image I am trying to warp using skimage.PiecewiseAffineTransform and skimage.warp.  I have a set of control points (true) mapped to a new set of control points (ideal) but the warp is not returning what I expect.

In this example, I have a simple gradient of wavelengths I am trying to 'straighten out' into columns. (You might ask why I am finding contours and interpolating but that is because I am actually applying this code to a more complex use case. I just wanted to reproduce all the code for this simple example which results in the same strange output.)

Any reason why my output image just has the input image warped into a square and inset? I'm using Python 2.7.12 and matplotlib 1.5.1. Here is the code. 

import matplotlib.pyplot as plt
import numpy as np
from skimage import measure, transform

true = np.array([range(i,i+10) for i in range(20)])
ideal = np.array([range(10)]*20)

# Find contours of ideal and true images and create list of control points for warp
true_control_pts = []
ideal_control_pts = []

for lam in ideal[0]:
    try:
        # Get the isowavelength contour in the true and ideal images
        tc = measure.find_contours(true, lam)[0]
        ic = measure.find_contours(ideal, lam)[0]
        nc = np.ones(ic.shape)

        # Use the y coordinates of the ideal contour
        nc[:, 0] = ic[:, 0]

        # Interpolate true contour onto ideal contour y axis so there are the same number of points
        nc[:, 1] = np.interp(ic[:, 0], tc[:, 0], tc[:, 1])

        # Add the control points to the appropriate list
        true_control_pts.append(nc.tolist())
        ideal_control_pts.append(ic.tolist())

    except (IndexError,AttributeError):
        pass

true_control_pts = np.array(true_control_pts)
ideal_control_pts = np.array(ideal_control_pts)

length = len(true_control_pts.flatten())/2
true_control_pts = true_control_pts.reshape(length,2)
ideal_control_pts = ideal_control_pts.reshape(length,2)

# Plot the original image
image = np.array([range(i,i+10) for i in range(20)]).astype(np.int32)
plt.figure()
plt.imshow(image, origin='lower', interpolation='none')
plt.title('Input image')

# Warp the actual image given the transformation between the true and ideal wavelength maps
tform = transform.PiecewiseAffineTransform()
tform.estimate(true_control_pts, ideal_control_pts)
out = transform.warp(image, tform)

# Plot the warped image!
fig, ax = plt.subplots()
ax.imshow(out, origin='lower', interpolation='none')
plt.title('Should be parallel lines')


The output for this looks like:



Any assistance would be greatly appreciated!
",5,2916,,,
matplotlib strange output,https://stackoverflow.com/questions/48434919,Why are bar chart x-axis ticks showing strange and wrong negative values?,"Why is my X-axis ticks showing Negative values when the Xaxis values range from 43990 - 44003. 

import matplotlib.pyplot as plt
x=[44000, 44001, 44002, 44003, 43990, 43991, 43992, 43993, 43994, 43995, 43996, 43997, 43998, 43999]
y=[8, 5, 3, 1, 1, 3, 4, 10, 4, 11, 4, 10, 17, 19]
plt.bar(x,y)
plt.show()


I am seeing the following output. I was expecting x-axis to range from 43990 - 44003


I have tried this on a couple of machines, all showing similar strange behaviour on recent versions of python and matplotlib (tried a few different versions)


  Python 3.6.4 |Anaconda, Inc.| (default, Jan 16 2018, 18:10:19)
  [GCC 7.2.0] on linux
  
  Python 3.4.3 (default, Nov 17 2016, 01:08:31)


Strangely enough many trivial toy example x and y arrays are giving me expected figures.

For e.g the follwowing snippet shows the expected graph with correct x-axis tick labels

x=[20,30,90,70, 50, 60, 80, 70]
y=[3,2,5,10, 3, 9, 7, 6]
plt.bar(x,y)
plt.show()




What obvious thing am i missing here ?
",4,3297,,,
matplotlib strange output,https://stackoverflow.com/questions/34071637,NetworkX: How to draw label nodes/edges with tex symbols?,,3,2326,,,
matplotlib strange output,https://stackoverflow.com/questions/17379351,Six subplots with the same number of xticklabels in matplotlib,,3,3264,,,
matplotlib strange output,https://stackoverflow.com/questions/72468387,Creating a conda environment with python 3.8 with conflicting requirement(s)?,"Context
After setting up a conda environment.yml and trying to install it with a python 3.8 version, I am experiencing some difficulties.
Attempts
I tried explicitly specifying the python version at the environment creation command:
conda env create --file environment.yml python=3.8

I tried explicitly including the python version in the environment.yml file:
...
dependencies:
- anaconda
- python=3.8
- conda:
# Run python tests.
  - pytest-cov
...

And I tried explicitly installing python 3.8 inside the environment with:
conda activate env_name
conda install python==3.8

Which yields a conflict:

Found conflicts! Looking for incompatible packages.

So I tried determining what the conflict is, by evaluating the 2000 output lines that describe the conflicts. I think the first conflict is most relevant:
UnsatisfiableError: The following specifications were found to be incompatible with a past
explicit spec that is not an explicit spec in this operation (pip):

  - python=3.8 -&gt; pip

Environment
The conda environment consists of the following four files:

environment.yml
pyproject.toml
requirements.txt
.pre-commit-config.yaml

Which have contents:
environment.yml:
# This file is to automatically configure your environment. It allows you to
# run the code with a single command without having to install anything
# (extra).

# First run:: conda env create --file environment.yml
# If you change this file, run: conda env update --file environment.yml

# Instructions for this networkx-to-lava-nc repository only. First time usage
# On Ubuntu (this is needed for lava-nc):
# sudo apt upgrade
# sudo apt full-upgrade
# yes | sudo apt install gcc

# Conda configuration settings. (Specify which modules/packages are installed.)
name: networkx-to-lava
channels:
  - conda-forge
dependencies:
- python=3.8
- conda:
# Run python tests.
  - pytest-cov
# Generate plots.
  - matplotlib
# Run graph software quickly.
  - networkx
- pip
- pip:
# Run pip install on .tar.gz file in GitHub repository (For lava-nc only).
  - https://github.com/lava-nc/lava/releases/download/v0.3.0/lava-nc-0.3.0.tar.gz
# Turns relative import paths into absolute import paths.
  - absolufy-imports
# Auto format Python code to make it flake8 compliant.
  - autoflake
# Scan Python code for security issues.
  - bandit
# Code formatting compliance.
  - black
# Correct code misspellings.
  - codespell
# Verify percentage of code that has at least 1 test.
  - coverage
# Auto formats the Python documentation written in the code.
  - docformatter
# Auto generate docstrings.
  - flake8
# Auto sort the import statements.
  - isort
# Auto format Markdown files.
  - mdformat
# Auto check static typing.
  - mypy
# Auto generate documentation.
  - pdoc3
# Auto check programming style aspects.
  - pylint
# Auto generate docstrings.
  - pyment
# Identify and remove dead code.
  - vulture
# Include GitHub pre-commit hook.
  - pre-commit
# TODO: identify exact function(and usage).
# Seems to be an autoformatter like black, but installed using npm instead of pip.
  - prettier
# Automatically upgrades Python syntax to the new Python version syntax.
  - pyupgrade
# Another static type checker for python like mypy.
  - pyright


pyproject.toml:
# This is used to configure the black, isort and mypy such that the packages don't conflict.
# This file is read by the pre-commit program.
[tool.black]
line-length = 79
include = '\.pyi?$'
exclude = '''
/(
    \.git
  | \.mypy_cache
  | build
  | dist
)/
'''


[tool.coverage.run]
# Due to a strange bug with xml output of coverage.py not writing the full-path
# of the sources, the full root directory is presented as a source alongside
# the main package. As a result any importable Python file/package needs to be
# included in the omit
source = [
    ""foo"",
    ""."",
]
# Excludes the following directories from the coverage report
omit = [
    ""tests/*"",
    ""setup.py"",
]


[tool.isort]
profile = ""black""


[tool.mypy]
ignore_missing_imports = true


[tool.pylint.basic]
bad-names=[]
[tool.pylint.messages_control]
# Example: Disable error on needing a module-level docstring
disable=[
    ""import-error"",
    ""invalid-name"",
    ""fixme"",
]


[tool.pytest.ini_options]
# Runs coverage.py through use of the pytest-cov plugin
# An xml report is generated and results are output to the terminal
addopts = ""--cov --cov-report xml:cov.xml --cov-report term""
# Sets the minimum allowed pytest version
minversion = 5.0
# Sets the path where test files are located (Speeds up Test Discovery)
testpaths = [""tests""]

requirements.txt
# This file ensures that the pre-commit service is ran every time you commit.
# Basically it ensures people only push files to GIT that are up to standard.
pre-commit

.pre-commit-config.yaml
# This file specifies which checks are performed by the pre-commit service.
# The pre-commit service prevents people from pushing code to git that is not
# up to standards. # The reason mirrors are used instead of the actual
# repositories for e.g. black and flake8, is because those repositories also
# need to contain a pre-commit hook file, which they often don't by default.
# So to resolve that, a mirror is created that includes such a file.

default_language_version:
    python: python3.10  # or python3


repos:
# Test if the python code is formatted according to the Black standard.
 - repo: https://github.com/Quantco/pre-commit-mirrors-black
   rev: 22.3.0
   hooks:
     - id: black-conda
       args:
         - --safe
         - --target-version=py36

# Test if the python code is formatted according to the flake8 standard.
 - repo: https://github.com/Quantco/pre-commit-mirrors-flake8
   rev: 4.0.1
   hooks:
    - id: flake8-conda

# Test if the import statements are sorted correctly.
 - repo: https://github.com/PyCQA/isort
   rev: 5.10.1
   hooks:
    - id: isort
      args: [""--profile"", ""black"", --line-length=79]

# Test if the variable typing is correct. (Variable typing is when you say:
# def is_larger(nr: int) -&gt; bool: instead of def is_larger(nr). It makes
# it explicit what type of input and output a function has.
# - repo: https://github.com/python/mypy
 - repo: https://github.com/pre-commit/mirrors-mypy
# - repo: https://github.com/a-t-0/mypy
   rev: v0.950
   hooks:
    - id: mypy

# Tests if there are spelling errors in the code.
 - repo: https://github.com/codespell-project/codespell
   rev: v2.1.0
   hooks:
    - id: codespell

# Performs static code analysis to check for programming errors.
 - repo: local
   hooks:
     - id: pylint
       name: pylint
       entry: pylint
       language: system
       types: [python]
       args:
         [
           ""-rn"", # Only display messages
           ""-sn"", # Don't display the score
         ]

# Runs additional tests that are created by the pre-commit software itself.
 - repo: https://github.com/pre-commit/pre-commit-hooks
   rev: v4.2.0
   hooks:
    # Check user did not add large files.
    - id: check-added-large-files
    # Check if `.py` files are written in valid Python syntax.
    - id: check-ast
    # Require literal syntax when initializing empty or zero Python builtin types.
    - id: check-builtin-literals
    # Checks if there are filenames that would conflict if case is changed.
    - id: check-case-conflict
    # Checks if the Python functions have docstrings.
    - id: check-docstring-first
    # Checks if any `.sh` files have a shebang like #!/bin/bash
    - id: check-executables-have-shebangs
    # Verifies json format of any `.json` files in repo.
    - id: check-json
    # Checks if there are any existing merge conflicts caused by the commit.
    - id: check-merge-conflict
    # Checks for symlinks which do not point to anything.
    - id: check-symlinks
    # Checks if xml files are formatted correctly.
    - id: check-xml
    # Checks if .yml files are valid.
    - id: check-yaml
    # Checks if debugger imports are performed.
    - id: debug-statements
    # Detects symlinks changed to regular files with content path symlink was pointing to.
    - id: destroyed-symlinks
    # Checks if you don't accidentally push a private key.
    - id: detect-private-key
    # Replaces double quoted strings with single quoted strings.
    # This is not compatible with Python Black.
    #- id: double-quote-string-fixer
    # Makes sure files end in a newline and only a newline.
    - id: end-of-file-fixer
    # Removes UTF-8 byte order marker.
    - id: fix-byte-order-marker
    # Add &lt;# -*- coding: utf-8 -*-&gt; to the top of python files.
    - id: fix-encoding-pragma
    # Checks if there are different line endings, like \n and crlf.
    - id: mixed-line-ending
    # Asserts `.py` files in folder `/test/` (by default:) end in `_test.py`.
    - id: name-tests-test
      # Override default to check if `.py` files in `/test/` START with `test_`.
      args: ['--django']
    # Ensures JSON files are properly formatted.
    - id: pretty-format-json
    # Sorts entries in requirements.txt and removes incorrect pkg-resources entries.
    - id: requirements-txt-fixer
    # Sorts simple YAML files which consist only of top-level keys.
    - id: sort-simple-yaml
    # Removes trailing whitespaces at end of lines of .. files.
    - id: trailing-whitespace





 - repo: https://github.com/PyCQA/autoflake
   rev: v1.4
   hooks:
    - id: autoflake
      args: [""--in-place"", ""--remove-unused-variables"", ""--remove-all-unused-imports"", ""--recursive""]
      name: AutoFlake
      description: ""Format with AutoFlake""
      stages: [commit]

 - repo: https://github.com/PyCQA/bandit
   rev: 1.7.4
   hooks:
   - id: bandit
     name: Bandit
     stages: [commit]

# Enforces formatting style in Markdown (.md) files.
 - repo: https://github.com/executablebooks/mdformat
   rev: 0.7.14
   hooks:
   - id: mdformat
     additional_dependencies:
     - mdformat-toc
     - mdformat-gfm
     - mdformat-black

 - repo: https://github.com/MarcoGorelli/absolufy-imports
   rev: v0.3.1
   hooks:
   - id: absolufy-imports
     files: '^src/.+\.py$'
     args: ['--never', '--application-directories', 'src']

 - repo: https://github.com/myint/docformatter
   rev: v1.4
   hooks:
   - id: docformatter

 - repo: https://github.com/pre-commit/pygrep-hooks
   rev: v1.9.0
   hooks:
   - id: python-use-type-annotations
   - id: python-check-blanket-noqa
   - id: python-check-blanket-type-ignore

# Updates the syntax of `.py` files to the specified python version.
# It is not compatible with: pre-commit hook: fix-encoding-pragma
# - repo: https://github.com/asottile/pyupgrade
#   rev: v2.32.1
#   hooks:
#     - id: pyupgrade
#       args: [--py310-plus]


 - repo: https://github.com/markdownlint/markdownlint
   rev: v0.11.0
   hooks:
     - id: markdownlint


Package Conflict Output
conda install python=3.8
Collecting package metadata (current_repodata.json): done
Solving environment: failed with initial frozen solve. Retrying with flexible solve.
Solving environment: failed with repodata from current_repodata.json, will retry with next repodata source.
Collecting package metadata (repodata.json): done
Solving environment: failed with initial frozen solve. Retrying with flexible solve.
Solving environment: | 
Found conflicts! Looking for incompatible packages.
This can take several minutes.  Press CTRL-C to abort.
failed                                                                                                                                     \  

UnsatisfiableError: The following specifications were found to be incompatible with a past
explicit spec that is not an explicit spec in this operation (pip):

  - python=3.8 -&gt; pip

The following specifications were found to be incompatible with each other:

Output in format: Requested package -&gt; Available versions

Package openjpeg conflicts for:
openjpeg
pillow -&gt; openjpeg[version='&gt;=2.3.0,&lt;3.0a0|&gt;=2.4.0,&lt;2.5.0a0']
matplotlib-base -&gt; pillow[version='&gt;=6.2.0'] -&gt; openjpeg[version='&gt;=2.3.0,&lt;3.0a0|&gt;=2.4.0,&lt;2.5.0a0']

Package ncurses conflicts for:
wheel -&gt; python -&gt; ncurses[version='6.0.*|&gt;=6.0,&lt;7.0a0|&gt;=6.1,&lt;7.0a0|&gt;=6.2,&lt;7.0a0|&gt;=6.3,&lt;7.0a0']
krb5 -&gt; libedit[version='&gt;=3.1.20210216,&lt;3.2.0a0'] -&gt; ncurses[version='6.0.*|&gt;=6.1,&lt;7.0a0|&gt;=6.2,&lt;7.0.0a0|&gt;=6.2,&lt;7.0a0|&gt;=6.3,&lt;7.0a0']
pluggy -&gt; python[version='&gt;=3.7,&lt;3.8.0a0'] -&gt; ncurses[version='6.0.*|&gt;=6.0,&lt;7.0a0|&gt;=6.1,&lt;7.0a0|&gt;=6.2,&lt;7.0a0|&gt;=6.3,&lt;7.0a0']
...
...

  - tornado -&gt; libgcc-ng[version='&gt;=7.5.0'] -&gt; __glibc[version='&gt;=2.17']
  - unicodedata2 -&gt; libgcc-ng[version='&gt;=7.5.0'] -&gt; __glibc[version='&gt;=2.17']
  - xorg-libxau -&gt; libgcc-ng[version='&gt;=9.3.0'] -&gt; __glibc[version='&gt;=2.17']
  - xorg-libxdmcp -&gt; libgcc-ng[version='&gt;=9.3.0'] -&gt; __glibc[version='&gt;=2.17']
  - xz -&gt; libgcc-ng[version='&gt;=7.5.0'] -&gt; __glibc[version='&gt;=2.17']
  - zlib -&gt; libgcc-ng[version='&gt;=7.5.0'] -&gt; __glibc[version='&gt;=2.17']
  - zstd -&gt; libgcc-ng[version='&gt;=7.5.0'] -&gt; __glibc[version='&gt;=2.17']

Your installed version is: 2.33


Question
How can I determine the conflicting package in this conda environment and/or how can I create the environment using python 3.8?
",2,5722,"After specifying the python version as the first dependency, and removing the unneeded elements as suggested by merv, I found a working yaml. I removed anaconda, and the conda channel. Furthermore, I ensured the default_version in the .pre-commit-config.yaml file was set to:
default_language_version:
    python: python3.8.  # or python3

I also deleted the .mypy_cache folder in the .git repository (even though I think this was not required). And I deleted the directory /home/&lt;username&gt;/.cache/pre-commit before running pre-commit run --all-files. (I thought it was worth mentioning as it is inherent to this environment.yml)
I did not have to specify the python version in the environment creation command. Instead, I ran:
conda env create --file environment.yml

The working environment.yml content is:
# This file is to automatically configure your environment. It allows you to
# run the code with a single command without having to install anything
# (extra).

# First run: conda env create --file environment.yml
# If you change this file, run: conda env update --file environment.yml

# Instructions for this networkx-to-lava-nc repository only. First time usage
# On Ubuntu (this is needed for lava-nc):
# sudo apt upgrade
# sudo apt full-upgrade
# yes | sudo apt install gcc

# Conda configuration settings. (Specify which modules/packages are installed.)
name: nx2lava
channels:
  - conda-forge
dependencies:
# Specify specific python version.
  - python=3.8
# Run python tests.
  - pytest-cov
# Generate plots.
  - matplotlib
# Run graph software quickly.
  - networkx
  - pip
  - pip:
# Run pip install on .tar.gz file in GitHub repository (For lava-nc only).
    - https://github.com/lava-nc/lava/releases/download/v0.3.0/lava-nc-0.3.0.tar.gz
# Turns relative import paths into absolute import paths.
    - absolufy-imports
# Auto format Python code to make it flake8 compliant.
    - autoflake
# Scan Python code for security issues.
    - bandit
# Code formatting compliance.
    - black
# Correct code misspellings.
    - codespell
# Verify percentage of code that has at least 1 test.
    - coverage
# Auto formats the Python documentation written in the code.
    - docformatter
# Auto generate docstrings.
    - flake8
# Auto sort the import statements.
    - isort
# Auto format Markdown files.
    - mdformat
# Auto check static typing.
    - mypy
# Auto generate documentation.
    - pdoc3
# Auto check programming style aspects.
    - pylint
# Auto generate docstrings.
    - pyment
# Identify and remove dead code.
    - vulture
# Include GitHub pre-commit hook.
    - pre-commit
# TODO: identify exact function(and usage).
# Seems to be an autoformatter like black, but installed using npm instead of pip.
    - prettier
# Automatically upgrades Python syntax to the new Python version syntax.
    - pyupgrade
# Another static type checker for python like mypy.
    - pyright

Which returns the following to the python --version command:

Python 3.8.13

",,
matplotlib strange output,https://stackoverflow.com/questions/55169294,Convert AsciiMath/MathML to SVG/PNG,"I'm looking for a way to convert my AsciiMath (or MathML) sources to SVG and/or PNG. I've found a nodeJS library for SVG conversion but calling that from Python is not very convenient and the output is not entirely satisfying.

Taking the fact I'd like to render mathematical formulas to svg/png it seems logical to look for a solution in math libraries (NumPy, SciPy, Pandas, Matplotlib, Sympy, etc...) but to no avail. All my google results combining all possible permutations of asciimath+mathml+svg+png lead to nothing which is strange.

Please recommend me either search patterns to find a solution or share your experiences/ideas to get this seemingly simple job done in Python.

All help would be highly appreciated!
",2,2513,"I just created ziamath for exactly this purpose. It comes bundled with the STIX math font, so no setup is required beyond the pip install, but it should also work with other math-enabled fonts. Pure-Python, so it does not need a Latex installation or anything else to work. This first version doesn't quite cover the full MathML specification, but the most useful parts are in there.
To bundle it into an app with something like PyInstaller, you'll need to make sure the STIX font gets included, but that should just be one line in the PyInstaller config.
","I have used this for years without issues. It is written in python.

https://sourceforge.net/projects/svgmath/files/svgmath/0.3.3/
",
matplotlib strange output,https://stackoverflow.com/questions/51447184,Why does matplotlib ax.transData.transform gives different values in different ipython cells? How to fix this?,"I was suffering a really bizarre issue with matplotlib's ax.transData.transform in ipython. Basically, in the cell where fig, ax = plt.subplots() is declared, running ax.transData.transform for either the x or y axis is incorrect. However, if you run ax.transData.transform in a later cell, it returns the correct values. 

To demonstrate, here is code running the transformation in the cell where fig, ax = plt.subplots() is declared:

fig, ax = plt.subplots()
ax.set_aspect('equal')
ax.set_ylim(-2,2)
ax.set_xlim(-5,5)
nsize=[x*100 for x in d]
yradius = (ax.transData.transform([(0,2)]) - ax.transData.transform([(0,1)]))[0,1]
print(""yRadius: {}"".format(yradius))
xradius = (ax.transData.transform([(2,0)]) - ax.transData.transform([(1,0)]))[0,0]
print(""xRadius: {}"".format(xradius))


The print output is 

yRadius: 54.360000000000014
xRadius: 33.48000000000002


In a cell below the above cell, if I run 

yradius = (ax.transData.transform([(0,2)]) - ax.transData.transform([(0,1)]))[0,1]
print(""yRadius: {}"".format(yradius))
xradius = (ax.transData.transform([(2,0)]) - ax.transData.transform([(1,0)]))[0,0]
print(""xRadius: {}"".format(xradius))


The output is 

yRadius: 33.48000000000002
xRadius: 33.48000000000002


Why are the values of the two cells not the same? Although the xRadius of the first cell matches the values of the bottom cell, the yRadius is off. I think the bottom cell's values are correct, because since the ax aspect is set to be equal, the two radii should be the same. Why is the yRadius of the first cell wrong here? After trying out a bunch of different values for xlim and ylim I found the pattern that the minimum of the two radii in the first cell is correct, while the other one is off. This seems to me a very strange error. Is there any way of making it return the correct values in the initial cell?
",2,4719,"Your observation is pretty accurate. What happens is simply that the equal aspect is only applied once the figure is drawn. This is the case for the second cell (because the figure is shown as output of the first). However in the first cell the axes still has its original extent. 

Solution: Draw the figure before trying to obtain any coordinates from it,

fig.canvas.draw()

# Now obtain coordinates:
yradius = (ax.transData.transform([(0,2)]) - ax.transData.transform([(0,1)]))[0,1]
print(""yRadius: {}"".format(yradius))
xradius = (ax.transData.transform([(2,0)]) - ax.transData.transform([(1,0)]))[0,0]
print(""xRadius: {}"".format(xradius))

",,
matplotlib strange output,https://stackoverflow.com/questions/50855834,Matplotlib 2.2.2 fails to xkcdify plots,"I use matplotlib 2.2.2 with Python 3.5.2

When I try to use matplotlib's xkcd function, I get a normal graph:

import matplotlib
print(matplotlib.get_cachedir())
print(matplotlib.__version__)

import matplotlib.font_manager
x = [f.name for f in matplotlib.font_manager.fontManager.ttflist if f.name.startswith(""Humor"")]
print(set(x))

from matplotlib import pyplot as plt
import numpy as np

plt.xkcd(scale=10, length=100, randomness=20)
plt.plot(np.sin(np.linspace(0, 10)))
plt.title('Whoo Hoo!!!')
plt.show()


Output:


  /home/thatsme/.cache/matplotlib
  2.2.2
  {'Humor Sans'}


So, Humor Sans is installed, I deleted the font cache, just in case, as suggested by other threads, I increased the jitter parameters, but nonetheless no sign of xkcdification:



Strange thing is that I have Eclipse installed as a dual-boot on Ubuntu 16.04 and Windows 10, and the xkcd function fails on both systems. I would have expected that the Python and Eclipse installation significantly differs between those systems, but here we are.
Any ideas, what might be the problem?

Edit: As Hans pointed out this might be a problem specific for matplotlib 2.2.2
Even more bizarre - if I run the same script using with : without any other changes like

with plt.xkcd(scale=10, length=100, randomness=20):
    plt.plot(np.sin(np.linspace(0, 10)))
    plt.title('Whoo Hoo!!!')
plt.show()


the output is the expected xkcd figure:


Removing the with : statement brings back the first figure. The same behaviour again on both Linux and Windows, so seemingly a matplotlib version problem. 

Edit: Now that we know it is a temporary, limited bug, back to the main question - How to get rid of the white edgecolor in xkcd
",2,255,"This is a bug purely present in matplotlib 2.2. The problem is that the garbage collector is too quick. See this comment and below.

This means that currently you need to use the xkcd style inside a context to make it work. The problem should be fixed in the next release. The code from the question hence works as expected in the current development version.
",,
matplotlib strange output,https://stackoverflow.com/questions/14715679,Strange output from matplotlib mathtext,"I tried to use mathtext to get a specially rendered figure title, but it failed. Instead of my test title it just printed cryptic characters. What am I doing wrong?


Operating System: Fedora release 18 (Spherical Cow)
Python and matplotlib are installed from official repositories via yum


Here's the (full) code:

import sys
print sys.version             # prints:
                              # 2.7.3 (default, Aug  9 2012, 17:23:57)
                              # [GCC 4.7.1 20120720 (Red Hat 4.7.1-5)]
import matplotlib
print matplotlib.__version__  # prints:
                              # 1.2.0

import matplotlib.pyplot as plt
plt.plot([1,5])
plt.title(r""$1.2345$"")
plt.show()


Here's the output image:

",2,1676,,,
matplotlib strange output,https://stackoverflow.com/questions/67734899,How to avoid the box around a matplotlib image with jupyter?,"On a jupyterhub notebook (python 3.8.5) and using matplotlib (version 3.3.2) I am trying to create a simple plot. The cell content is as follows:
import matplotlib.pyplot as plt

def plot_something(time_array, dependent_var, label=""label""):
    plt.plot(time_array, dependent_var, label=label)
    plt.xlabel('Time (ms)')
    plt.ylabel(label)
plot_something([1,2,3], [4,5,6])

However, there is a strange scroll box drawn around the created image with a scrollbar at the right (i.e. you cannot see the whole image, you have to scroll!):

which does not happen for other matplotlib plots in the same notebook. The image shows the complete output cell (and you can see the upper part of the plot is missing. You need yo scroll up to see it).
How can I programatically avoid this strange box around the matplotlib plot?
P.S. I am not sure how to reproduce this problem. If I execute the code on top of the notebook it work fine, I get just the plot without that strange scroll box:

I also checked for differences in the actual displayed html code between these two cases. I did not any differences!
",1,655,"That is because of scrolling. To change that, you can go to Cell &gt; All Outputs &gt; Toggle Scrolling or one of the other options until you get the desired result.
","As an actual better solution which neither requires additional code nor user-interaction in the notebook is to check the metadata of the notebook itself. The metadata of the cell that shows the scroll box looks like
{
   ""cell_type"": ""code"",
   ""execution_count"": null,
   ""metadata"": {
    ""scrolled"": true
   },
   ""outputs"": [],
   ""source"": [
    ""plot_timecourse(time, v_soma)""
   ]
  },

Just remove the scroll part, i.e. change it to
{
   ""cell_type"": ""code"",
   ""execution_count"": null,
   ""metadata"": {}
   ""outputs"": [],
   ""source"": [
    ""plot_timecourse(time, v_soma)""
   ]
  },

",
matplotlib strange output,https://stackoverflow.com/questions/61025167,Lambda slack file upload is getting triggered multiple times,"My app architecture is slack events -&gt; API Gateway -&gt; Lambda -&gt; does someoperation and returns an .png file which is generated using numpy and matplotlib.

When i deal with just text output in lambda, it works fine, but when i deal with file uploads, it works strange,

It uploads files to slack using[files.upload] method and then after a minute again my lambda gets triggered and ends up in uploading another file.

Is it because slack return HTTP response for file.upload method and somehow my app catches that and it runs agian?

It would be of great help as even in the slack events, events are same without any difference but i am really not sure why my lambda gets invoked again and i verified the request ID's and it is different and even at API getway there are two different request ID's but i have requested only one time...it drives me crazy...
",1,666,"I found out the way to do this. With the help of this article https://aws.amazon.com/premiumsupport/knowledge-center/custom-headers-api-gateway-lambda/ i added HTTP Header [client Header information] in API Gateway and i pass it to lambda. So, in Lambda i catch the retry events from slack with the help of header which contains X-Slack-Retry-Num for retry events and return it immediately as return 200.

if 'X-Slack-Retry-Num' in output['headers']:
    slk_retry = output['headers']['X-Slack-Retry-Num']
    return 200
else:
    ""Consider this as first event and provide your actual code and logic""

",,
matplotlib strange output,https://stackoverflow.com/questions/57303228,How to effectively redraw multiple matplotlib plots with blit,"I'm using matplotlib with pyqt5 to draw data into 3 axes, and than user can make selection in one plot that will be shown in other two plots too. Since I'm working with big data (up to 10 millions of points), drawing selection could be slow, especially when I need to draw to scatterplot. 

I am trying to use matplotlib blit function, but have some issues with result. Here is minimum simple example.

import matplotlib
matplotlib.use('Qt5Agg')
import numpy as np
import sys

from matplotlib.backends.qt_compat import QtCore, QtWidgets
from matplotlib.backends.backend_qt5agg import (FigureCanvas, NavigationToolbar2QT as NavigationToolbar)
from matplotlib.figure import Figure


class ApplicationWindow(QtWidgets.QMainWindow):
    def __init__(self):
        super().__init__()
        self._main = QtWidgets.QWidget()
        self.setCentralWidget(self._main)
        layout = QtWidgets.QVBoxLayout(self._main)

        self.static_canvas = FigureCanvas(Figure(figsize=(10, 10)))
        layout.addWidget(self.static_canvas)
        layout.addWidget(NavigationToolbar(self.static_canvas, self))
        axes = self.static_canvas.figure.subplots(2, 1)
        self.ax1 = axes[0]
        self.ax2 = axes[1]
        self.ax1.cla()
        self.ax2.cla()

        button = QtWidgets.QPushButton('Click me!')
        button.clicked.connect(self.update_canvas_blit)
        layout.addWidget(button)
        # Fixing random state for reproducibility
        np.random.seed(19680801)

        # Create random data
        N = 50000
        x = np.random.rand(N)
        y = np.random.rand(N)

        self.ax1.scatter(x, y)
        self.points = self.ax1.scatter([],[], s=5, color='red')

        x = np.linspace(0, 1000, 100000)
        self.ax2.plot(x, np.sin(x))
        self.lines, = self.ax2.plot([],[], color='red')
        self.static_canvas.draw()

        self.background1 = self.static_canvas.copy_from_bbox(self.ax1.bbox)
        self.background2 = self.static_canvas.copy_from_bbox(self.ax2.bbox)

    def update_canvas_blit(self):
        N = 50
        x = np.random.rand(N)
        y = np.random.rand(N)

        self.static_canvas.restore_region(self.background1)
        self.points.set_offsets(np.c_[x,y])
        self.ax1.draw_artist(self.points)
        self.ax1.figure.canvas.blit(self.ax1.bbox)

        self.static_canvas.restore_region(self.background2)
        x = np.linspace(0, np.random.randint(500,1000), 1000)
        self.lines.set_data(x, np.sin(x))
        self.ax2.draw_artist(self.lines)
        self.ax2.figure.canvas.blit(self.ax2.bbox)

if __name__ == ""__main__"":
    qapp = QtWidgets.QApplication(sys.argv)
    app = ApplicationWindow()
    app.show()
    qapp.exec_()


When clicking button, expected output should be still same background with random points/lines redrawing. In a way it is happening but there are some strange artifacts that looks like somehow axes are drawn to each other. But when I try to save it to .png, it will restore to good state.


",1,685,"The problem is that the snapshot of the background is taken at a moment in time where the figure has not yet been shown on screen. At that point the figure is 10 by 10 inches large. Later, it is shown inside the QMainWindow and resized to fit into the widget.
Only once that has happened, it makes sense to take the background snapshot. 

One option is to use a timer of 1 second and only then copy the background. This would look as follows.

import numpy as np
import sys

from matplotlib.backends.qt_compat import QtCore, QtWidgets
from matplotlib.backends.backend_qt5agg import (FigureCanvas, NavigationToolbar2QT as NavigationToolbar)
from matplotlib.figure import Figure


class ApplicationWindow(QtWidgets.QMainWindow):
    def __init__(self):
        super().__init__()
        self._main = QtWidgets.QWidget()
        self.setCentralWidget(self._main)
        layout = QtWidgets.QVBoxLayout(self._main)

        self.static_canvas = FigureCanvas(Figure(figsize=(10, 10)))
        layout.addWidget(self.static_canvas)
        layout.addWidget(NavigationToolbar(self.static_canvas, self))
        axes = self.static_canvas.figure.subplots(2, 1)
        self.ax1 = axes[0]
        self.ax2 = axes[1]
        self.ax1.cla()
        self.ax2.cla()

        button = QtWidgets.QPushButton('Click me!')
        button.clicked.connect(self.update_canvas_blit)
        layout.addWidget(button)
        # Fixing random state for reproducibility
        np.random.seed(19680801)

        # Create random data
        N = 50000
        x = np.random.rand(N)
        y = np.random.rand(N)

        self.ax1.scatter(x, y)
        self.points = self.ax1.scatter([],[], s=5, color='red')

        x = np.linspace(0, 1000, 100000)
        self.ax2.plot(x, np.sin(x))
        self.lines, = self.ax2.plot([],[], color='red')
        self.static_canvas.draw()

        self._later()


    def _later(self, evt=None):
        self.timer = self.static_canvas.new_timer(interval=1000)
        self.timer.single_shot = True
        self.timer.add_callback(self.update_background)
        self.timer.start()


    def update_background(self, evt=None):
        self.background1 = self.static_canvas.copy_from_bbox(self.ax1.bbox)
        self.background2 = self.static_canvas.copy_from_bbox(self.ax2.bbox)

    def update_canvas_blit(self):
        N = 50
        x = np.random.rand(N)
        y = np.random.rand(N)

        self.static_canvas.restore_region(self.background1)
        self.points.set_offsets(np.c_[x,y])
        self.ax1.draw_artist(self.points)
        self.ax1.figure.canvas.blit(self.ax1.bbox)

        self.static_canvas.restore_region(self.background2)
        x = np.linspace(0, np.random.randint(500,1000), 1000)
        self.lines.set_data(x, np.sin(x))
        self.ax2.draw_artist(self.lines)
        self.ax2.figure.canvas.blit(self.ax2.bbox)

if __name__ == ""__main__"":
    qapp = QtWidgets.QApplication(sys.argv)
    app = ApplicationWindow()
    app.show()
    qapp.exec_()

",,
matplotlib strange output,https://stackoverflow.com/questions/44525758,matplotlib rgb color not working after upgrading matplotlib,"I just upgraded matplotlib to matplotlib2.0. As far I could see nothing was supposed to change in the basic use, yet I now have this strange bu with rgb coding : 

myDF_DoMS.mean().plot(color =(0.2,0.2,0.7),xticks=np.arange(1,31,1))
plt.plot([1,32],[zeMeanS,zeMeanS],color=(0.2,0.7,0.9))
plt.xlabel('xlabel')
plt.ylabel('Some Score')
plt.title(Study+""\n A name"")
plt.show()


Complaints from the kernel : 

ValueError: Invalid RGBA argument: 0.2

(full error stack below) 

suddently 0.2 is not a float anymore  ! 
What's worst if I put 'b' it works 
while on line 2 the other 'rgb' list on line 2 works fine (color=(0.2,0.7,0.9)) ...

I'm a bit lost. 

Setting : 
Copy of a cell in a jupyter notebook that use to work perfectly.
Python 2.7 in an anaconda environment. Windows 10 as OS.  

Here are the outputs : 

 ValueError                                Traceback (most recent call
 last) C:\Program
 Files\Anaconda2\envs\moonshade\lib\site-packages\IPython\core\formatters.pyc
 in __call__(self, obj)
     305                 pass
     306             else:
 --&gt; 307                 return printer(obj)
     308             # Finally look for special method names
     309             method = get_real_method(obj, self.print_method)

 C:\Program
 Files\Anaconda2\envs\moonshade\lib\site-packages\IPython\core\pylabtools.pyc
 in &lt;lambda&gt;(fig)
     238 
     239     if 'png' in formats:
 --&gt; 240         png_formatter.for_type(Figure, lambda fig: print_figure(fig, 'png', **kwargs))
     241     if 'retina' in formats or 'png2x' in formats:
     242         png_formatter.for_type(Figure, lambda fig: retina_figure(fig, **kwargs))

 C:\Program
 Files\Anaconda2\envs\moonshade\lib\site-packages\IPython\core\pylabtools.pyc
 in print_figure(fig, fmt, bbox_inches, **kwargs)
     122 
     123     bytes_io = BytesIO()
 --&gt; 124     fig.canvas.print_figure(bytes_io, **kw)
     125     data = bytes_io.getvalue()
     126     if fmt == 'svg':

 C:\Program
 Files\Anaconda2\envs\moonshade\lib\site-packages\matplotlib\backend_bases.pyc
 in print_figure(self, filename, dpi, facecolor, edgecolor,
 orientation, format, **kwargs)    2198                    
 orientation=orientation,    2199                     dryrun=True,
 -&gt; 2200                     **kwargs)    2201                 renderer = self.figure._cachedRenderer    2202                 bbox_inches = self.figure.get_tightbbox(renderer)

 C:\Program
 Files\Anaconda2\envs\moonshade\lib\site-packages\matplotlib\backends\backend_agg.pyc
 in print_png(self, filename_or_obj, *args, **kwargs)
     543 
     544     def print_png(self, filename_or_obj, *args, **kwargs):
 --&gt; 545         FigureCanvasAgg.draw(self)
     546         renderer = self.get_renderer()
     547         original_dpi = renderer.dpi

 C:\Program
 Files\Anaconda2\envs\moonshade\lib\site-packages\matplotlib\backends\backend_agg.pyc
 in draw(self)
     462 
     463         try:
 --&gt; 464             self.figure.draw(self.renderer)
     465         finally:
     466             RendererAgg.lock.release()

 C:\Program
 Files\Anaconda2\envs\moonshade\lib\site-packages\matplotlib\artist.pyc
 in draw_wrapper(artist, renderer, *args, **kwargs)
      61     def draw_wrapper(artist, renderer, *args, **kwargs):
      62         before(artist, renderer)
 ---&gt; 63         draw(artist, renderer, *args, **kwargs)
      64         after(artist, renderer)
      65 

 C:\Program
 Files\Anaconda2\envs\moonshade\lib\site-packages\matplotlib\figure.pyc
 in draw(self, renderer)    1142     1143            
 mimage._draw_list_compositing_images(
 -&gt; 1144                 renderer, self, dsu, self.suppressComposite)    1145     1146             renderer.close_group('figure')

 C:\Program
 Files\Anaconda2\envs\moonshade\lib\site-packages\matplotlib\image.pyc
 in _draw_list_compositing_images(renderer, parent, dsu,
 suppress_composite)
     137     if not_composite or not has_images:
     138         for zorder, a in dsu:
 --&gt; 139             a.draw(renderer)
     140     else:
     141         # Composite any adjacent images together

 C:\Program
 Files\Anaconda2\envs\moonshade\lib\site-packages\matplotlib\artist.pyc
 in draw_wrapper(artist, renderer, *args, **kwargs)
      61     def draw_wrapper(artist, renderer, *args, **kwargs):
      62         before(artist, renderer)
 ---&gt; 63         draw(artist, renderer, *args, **kwargs)
      64         after(artist, renderer)
      65 

 C:\Program
 Files\Anaconda2\envs\moonshade\lib\site-packages\matplotlib\axes\_base.pyc
 in draw(self, renderer, inframe)    2424            
 renderer.stop_rasterizing()    2425 
 -&gt; 2426         mimage._draw_list_compositing_images(renderer, self, dsu)    2427     2428         renderer.close_group('axes')

 C:\Program
 Files\Anaconda2\envs\moonshade\lib\site-packages\matplotlib\image.pyc
 in _draw_list_compositing_images(renderer, parent, dsu,
 suppress_composite)
     137     if not_composite or not has_images:
     138         for zorder, a in dsu:
 --&gt; 139             a.draw(renderer)
     140     else:
     141         # Composite any adjacent images together

 C:\Program
 Files\Anaconda2\envs\moonshade\lib\site-packages\matplotlib\artist.pyc
 in draw_wrapper(artist, renderer, *args, **kwargs)
      61     def draw_wrapper(artist, renderer, *args, **kwargs):
      62         before(artist, renderer)
 ---&gt; 63         draw(artist, renderer, *args, **kwargs)
      64         after(artist, renderer)
      65 

 C:\Program
 Files\Anaconda2\envs\moonshade\lib\site-packages\matplotlib\lines.pyc
 in draw(self, renderer)
     801                 self._set_gc_clip(gc)
     802 
 --&gt; 803                 ln_color_rgba = self._get_rgba_ln_color()
     804                 gc.set_foreground(ln_color_rgba, isRGBA=True)
     805                 gc.set_alpha(ln_color_rgba[3])

 C:\Program
 Files\Anaconda2\envs\moonshade\lib\site-packages\matplotlib\lines.pyc
 in _get_rgba_ln_color(self, alt)    1342     1343     def
 _get_rgba_ln_color(self, alt=False):
 -&gt; 1344         return mcolors.to_rgba(self._color, self._alpha)    1345     1346     # some aliases....

 C:\Program
 Files\Anaconda2\envs\moonshade\lib\site-packages\matplotlib\colors.pyc
 in to_rgba(c, alpha)
     141         rgba = _colors_full_map.cache[c, alpha]
     142     except (KeyError, TypeError):  # Not in cache, or unhashable.
 --&gt; 143         rgba = _to_rgba_no_colorcycle(c, alpha)
     144         try:
     145             _colors_full_map.cache[c, alpha] = rgba

 C:\Program
 Files\Anaconda2\envs\moonshade\lib\site-packages\matplotlib\colors.pyc
 in _to_rgba_no_colorcycle(c, alpha)
     192         # float)` and `np.array(...).astype(float)` all convert ""0.5"" to 0.5.
     193         # Test dimensionality to reject single floats.
 --&gt; 194         raise ValueError(""Invalid RGBA argument: {!r}"".format(orig_c))
     195     # Return a tuple to prevent the cached value from being modified.
     196     c = tuple(c.astype(float))

 ValueError: Invalid RGBA argument: 0.2

 &lt;matplotlib.figure.Figure at 0x14e54ac8&gt;

",1,3374,,,
matplotlib strange output,https://stackoverflow.com/questions/39681620,Gantt Chart python machine scheduling,"I'm having some bad time trying to plot a Gantt chart from a data set with python. I have a set of machines that work on different tasks during a time period. I want to make a Gantt chart that shows by the y axis the machines and x axis the time spent on each task.
Each machine should appear just once on the y axis and to make easier to see the tasks I want the same color for each task.
The idea is to check strange things like having the same task being processed by two or more machines together.
Let me show what data set I have with a small example:
machine   equipment   start   finish
  m1         e2       date1    date2
  m2         e2       date3    date4
  m1         e1       date5    date6
  m3         e3       date7    date8
  m3         e4       date9    date10

I tried to use the broken_barh from matplotlib, but I can't figure out a way to add the data for the plot efficiently. Since I have some thing like 100 machines and 400 tasks.
Here is a picture to show how the output should look like.

Current code below:
import datetime as dt

machines = set(list(mydata[""machine""]))
tasks = set(list(mydata[""task""]))

fig, ax = plt.subplots(figsize=(20, 10))

yrange = 5 # y width of gantt bar
ymin = 0
orign = min(list(mydata[""start""])) # time origin

for i in machines:
    
    stdur = [] # list of tuples (start, duration)
    ymin = index*6 # start y of gantt bar
    
    for index, row in mydata.iterrows():
        
        if row[""machine""] == i:
            start = (row[""start""] -  orign).total_seconds()/3600
            duration = (row[""finish""] -  row[""start""]).total_seconds()/3600
        
            stdur.append((start,duration))
        
    ax.broken_barh(stdur,(ymin,yrange))
        
ax.set_xlabel('Time')
ax.set_yticklabels(machines)

plt.show()

",1,2056,"If you are fine passing the entire data to the client and let a JS Gantt do its thing, then the RadiantQ jQuery Gantt Package will be a better option for you. Here is an online demo of something similar:
http://demos.radiantq.com/jQueryGanttDemo/Samples/ServerStatusWithHugeData.htm
This is how it looks:

More on the product here:
http://radiantq.com/products/jquery-gantt-package/jquery-gantt-package-features/
",,
matplotlib strange output,https://stackoverflow.com/questions/68140087,Export matplotlib as emf and avoid black bacground fill,"The previous discussion [1] helps us to export a matplotlib plot as an .emf file. However, the strange error is that the output file has nodes filled with black background as shown below.
How is it possible to obtain the .emf exactly as the .svg?
import matplotlib.pyplot as plt
import subprocess, os

# Graph with 4 nodes - its coordinates
x = [0, 5, 5, 0]
y = [0, 0, 5, 5]

fig, ax = plt.subplots()

# Plot the vertices
for i in range(len(x)):
        ax.plot(x[i], y[i], 'ro',
                 markersize = 12,
                 fillstyle = 'none',
                 label = 'A')

# Plot the edges
plt.plot(x,y)

# Legend
handles, labels = plt.gca().get_legend_handles_labels()
by_label = dict(zip(labels, handles))
plt.legend(by_label.values(), by_label.keys())

# File export
inkscape_path = ""C://Program Files//Inkscape//bin//inkscape.exe""
out_file = ""C:/Users/banbar/Desktop/out1.emf""

path, filename = os.path.split(out_file)
filename, extension = os.path.splitext(filename)

svg_filepath = os.path.join(path, filename+'.svg')
emf_filepath = os.path.join(path, filename+'.emf')

fig.savefig(svg_filepath, format='svg')

subprocess.call([inkscape_path, svg_filepath, '--export-filename', emf_filepath])   


",0,791,"This is only one half of the answer, which is the SVG half:
When you select one of those squares in Inkscape, you will probably notice that it says 'Fill unset' in the bottom left where the fill indicator is.
That would be because matplotlib leaves the fill attribute out entirely. A valid value in SVG would be 'none' or a fill color with opacity = 0.
I've found a link about how to determine the fill color here:
https://matplotlib.org/2.1.1/api/_as_gen/matplotlib.pyplot.plot.html
You should be able to figure out the rest from there, hopefully.
","I had the same issue, simple workaround:

let matplotlib generate a pdf
convert to svg (pdftocairo or pdf2svg)
convert to emf finally using inkscape

Regards Stefan
",
matplotlib strange output,https://stackoverflow.com/questions/67878607,Strange problem with multiple polar plots in matplotlib,"I've run into a strange issue plotting two ellipses on a single polar plot with matplotlib. Here's my Python code:
import numpy as np
import matplotlib.pyplot as plt

def EllipseRadius(a, b, angle):
    return a * b / np.sqrt((b * np.cos(angle)) ** 2 + (a * np.sin(angle)) ** 2)

ell_a = 9   # Outer ellipse semi-major axis
ell_b = 1   # Outer ellipse semi-minor axis
ell_k = 0.1 # Ratio of inner ellipse to outer ellipse

fig = plt.figure(figsize=(15,15))
ax = fig.add_subplot(projection='polar')

# First ellipse
angles1 = np.linspace(0, 2*np.pi, 1000)
rs1 = EllipseRadius(ell_a, ell_b, angles1)
ax.plot(angles1, rs1)

# Second ellipse
angles2 = np.linspace(0, 2*np.pi, 1000)
rs2 = EllipseRadius(ell_k*ell_a, ell_k*ell_b, angles2)
ax.plot(angles2, rs2)

##for i in range(len(angles1)):
##    print(angles1[i], rs1[i], rs2[i])

plt.show()

If I run this code, here's what I see:

Note the central ""bulge"" in both ellipses. Strangely, if I comment out the larger ellipse (labeled ""first ellipse"" in the code above), here's the output:

That looks a lot more like what I expected. If I comment out the other ellipse instead, I get another great looking ellipse. So, for some reason, if I plot just either ellipse alone, it looks great, but if I plot both, they develop this strange shape.
I am using Python 3.7.3 and matplotlib 3.1.0.
What could be causing this?
Thanks!
Michael
",0,776,"I have matplotlib version 3.4.1 and this is my output. Try to update matplotlib
pip install --upgrade matplotlib
or
conda update matplotlib


",,
matplotlib strange output,https://stackoverflow.com/questions/66054173,How to run Octave code without the Octave IDE (similarly to Python)?,"Context: When I use Python + matplotlib, I can compose the code in any text editor (like Sublime Text), do CTRL+B, and then the text output appears in the ""Build results"" panel of the text editor, and, optionally, graph/plots are rendered in a new GUI window.
Under the hood, the text editor calls python myscript.py when we do ""Build"", and that's it.
It's simple and working flawlessly, easily.

Now I'm trying to do the same with GNU Octave: write a test.m code (such as this one). Then run it from my favorite text editor (and not the Octave IDE), or from command-line. I tried:

octave test.m: the plot is displayed during 100 ms and then disappears! not OK

octave --persist test.m: the plot stays displayed, this is ok ... but this part is not good: it opens an IDE (which I don't want since I want to continue using my usual text editor!), see the background window:



How to have GNU Octave behave like expected: the text output in stdout (either in terminal or in the ""Build Results"" panel of your text editor) and the plot output in a new window? Important: without spawning an IDE window.
I find strange that this behaviour is not the default behaviour. Shouldn't it be?

Edit: solved with:
octave-cli test.m

and
k = plot(...)
waitfor(k)

",0,484,"to have a similar effect I run the command line interface octave-cli or octave --no-gui in a terminal and vim in a different one.
Not exactly what you are looking for, but matplotlib is a python module while octave is a separate program from your editor.
See example of the two terminals and the graphs, all on separate windows.

",,
matplotlib strange output,https://stackoverflow.com/questions/59667803,Python only uses one core,"i have a python script which handles files with sensor datas from a md4 file and plots them in an image. 
Therefore i am using the libs asammdf and matplotlib - they have a dependency to numpy.

On my windows main computer where i developed the script everything works fine - all threads are used and the speed of the script is fine.

But on other devices where the script should also run to handle more files the script only uses one core with multiple threads while running on Ubuntu 16.04 with own compiled python 3.7.0:



I searched a lot and tried everything what is sugessted - some of them are a bit older for ubuntu 12.. :
https://shahhj.wordpress.com/2013/10/27/numpy-and-blas-no-problemo/
Importing scipy breaks multiprocessing support in Python
Why does multiprocessing use only a single core after I import numpy?

The linux machines are new installed - only python3.7, pip3 and the python libs are installed. I even downloaded the newest ubuntu 19.10 image where python3.7.5 is preinstalled.
What i have done:


tried os.sched_setaffinity
tried os.system(""taskset -p 0xff %d"" % os.getpid())



  pid 20534's current affinity mask: ff
  pid 20534's new affinity mask: ff



installed the ATLAS and set the alternatives but those does not seems
to get taken..


on my windows machine i have:
python 3.7.1
and the output for &gt;&gt;&gt; import numpy; numpy.show_config() 

blas_mkl_info:   NOT AVAILABLE blis_info:   NOT AVAILABLE openblas_info:
    library_dirs = ['C:\\projects\\numpy-wheels\\numpy\\build\\openblas']
    libraries = ['openblas']
    language = f77
    define_macros = [('HAVE_CBLAS', None)] blas_opt_info:
    library_dirs = ['C:\\projects\\numpy-wheels\\numpy\\build\\openblas']
    libraries = ['openblas']
    language = f77
    define_macros = [('HAVE_CBLAS', None)] lapack_mkl_info:   NOT AVAILABLE openblas_lapack_info:
    library_dirs = ['C:\\projects\\numpy-wheels\\numpy\\build\\openblas']
    libraries = ['openblas']
    language = f77
    define_macros = [('HAVE_CBLAS', None)] lapack_opt_info:
    library_dirs = ['C:\\projects\\numpy-wheels\\numpy\\build\\openblas']
    libraries = ['openblas']
    language = f77
    define_macros = [('HAVE_CBLAS', None)]


on my linux machines:
python 3.7.0
and the numpy config:

blas_mkl_info:
  NOT AVAILABLE
blis_info:
  NOT AVAILABLE
openblas_info:
    libraries = ['openblas', 'openblas']
    library_dirs = ['/usr/local/lib']
    language = c
    define_macros = [('HAVE_CBLAS', None)]
blas_opt_info:
    libraries = ['openblas', 'openblas']
    library_dirs = ['/usr/local/lib']
    language = c
    define_macros = [('HAVE_CBLAS', None)]
lapack_mkl_info:
  NOT AVAILABLE
openblas_lapack_info:
    libraries = ['openblas', 'openblas']
    library_dirs = ['/usr/local/lib']
    language = c
    define_macros = [('HAVE_CBLAS', None)]
lapack_opt_info:
    libraries = ['openblas', 'openblas']
    library_dirs = ['/usr/local/lib']
    language = c
    define_macros = [('HAVE_CBLAS', None)]


I do not have any multi threading logic in my code or something similar - all the multi threading logic is handled inside the libs i use. I hope someone can help. I think its very strange i still have this kind of problems so many years later of the questions above i linked.

EDIT: cause question got deleted: i checked in bios that all cores and hyperthreading is activated
",0,830,"asammdf is completely single threaded. On the windows machine you might be using numpy with the Intel mkl libraries so this might be why the numpy operations are vectorized
",,
matplotlib strange output,https://stackoverflow.com/questions/51755124,Why doesn&#39;t an array made from a PIL draw.text() image show properly in Matplotlib?,"I'd like to understand why, when I convert the PIL image imageRGB to a float array arrayRGB_f and use matplotlib's imshow() without a cmap it looks either black, or strange and unreadable, even though PIL's imageRGB.show() looks fine, and each of the individual r, g, b channels shown with cmap='gray' look okay as well.

I have workarounds, but I just don't understand why this happens.

matplotlib.__version__ returns '2.0.2' and I'm using MacOS with an Anaconda installation.

See this answer for more on the conversion of a ttf rendering to a 1bit.

fyi the output of the print statements are:

float64 (41, 101, 3)
int64 (41, 101, 3)
int64 (41, 101)
int64 (41, 101)




fontname   = 'default' 


imageRGB.show()

 

plt.imshow()



fontname   = 'Arial Unicode.ttf' 


imageRGB.show()



plt.imshow()

 

font   = ImageFont.truetype(fontname, 20)


imageRGB.show()



plt.imshow()



from PIL import Image, ImageDraw, ImageFont
import numpy as np
import matplotlib.pyplot as plt

# fontname   = 'Arial Unicode.ttf' 
fontname   = 'default' 

if fontname == 'default':
    font   = ImageFont.load_default()
else:
    font   = ImageFont.truetype(fontname, 12)

string     = ""Hello "" + fontname[:6]
ww, hh     = 101, 41
threshold  = 80   # https://stackoverflow.com/a/47546095/3904031

imageRGB   = Image.new('RGB', (ww, hh))
draw       = ImageDraw.Draw(imageRGB)
image8bit  = draw.text((10, 12), string, font=font,
                       fill=(255, 255, 255, 255))  # R, G, B alpha

image8bit  = imageRGB.convert(""L"")
image1bit  = image8bit.point(lambda x: 0 if x &lt; threshold else 1, mode='1')  # https://stackoverflow.com/a/47546095/3904031

arrayRGB   = np.array(list(imageRGB.getdata())).reshape(hh, ww, 3)
arrayRGB_f = arrayRGB.astype(float)

array8bit  = np.array(list(image8bit.getdata())).reshape(hh, ww)
array1bit  = np.array(list(image1bit.getdata())).reshape(hh, ww)

for a in (arrayRGB_f, arrayRGB, array8bit, array1bit):
    print a.dtype, a.shape

imageRGB.show()

if True:
    plt.figure()

    a = arrayRGB_f
    plt.subplot(2, 2, 1)
    plt.imshow(a)  # , interpolation='nearest',  cmap='gray',

    for i in range(3):
        plt.subplot(2, 2, 2+i)
        plt.imshow(a[:, :, i], cmap='gray') 

    plt.suptitle('arrayRGB_f, fontname = ' + fontname)
    plt.show()

",0,485,"I can't find an ideal duplicate so I'll post an answer.

As @ImportanceOfBeingErnest mentions when .imshow() is given an n x m x 3 or n x m x 4 array, it is expecting a normalized array between 0.0 and 1.0.

Best way to do this is:

arrayRGB_f = arrayRGB.astype(float)/255.


though this seems to work as well:

arrayRGB_f = arrayRGB.astype(float)
arrayRGB_f = arrayRGB_f / arrayRGB_f.max()


For longer discussions, see this and this.
",,
matplotlib strange output,https://stackoverflow.com/questions/48498766,Problems with unpacking Matplotlib hist2d outputs,"I'm using Matplotlib's function hist2d() and I want to unpack the output in order to further use it. Here's what I do: I simply load with numpy a 2-column file containing my data and use the following code 

import matplotlib.pyplot as plt
from matplotlib.colors import LogNorm
import numpy as np

traj = np.loadtxt('trajectory.txt')
x = traj[:,0]
y = traj[:,1]
M, xe, ye, img = plt.hist2d(x, y, bins = 80, norm = LogNorm())
plt.imshow(M)
plt.show()


The result I get is the following:



Instead, if I try to directly plot the hist2d results without unpacking them:

import matplotlib.pyplot as plt
from matplotlib.colors import LogNorm
import numpy as np

traj = np.loadtxt('trajectory.txt')
x = traj[:,0]
y = traj[:,1]
plt.hist2d(x, y, bins = 80, norm = LogNorm())
plt.show()


I get the whole plot without the strange blue box. What am I doing wrong?
",0,1142,"You can create a histogram plot directly with plt.hist2d. This calculates the histogram and plots it to the current axes. There is no need to show it yet another time using imshow.

import matplotlib.pyplot as plt
from matplotlib.colors import LogNorm
import numpy as np; np.random.seed(9)

x = np.random.rayleigh(size=9900)
y = np.random.rayleigh(size=9900)

M, xe, ye, img = plt.hist2d(x, y, bins = 80, norm = LogNorm())

plt.show()




Or, you may first calculate the histogram and afterwards plot the result as an image to the current axes. Note that the histogram produced by numpy is transposed, see Matplotlib 2D histogram seems transposed, making it necessary to call imshow(M.T). Also note that in order to obtain the correct axes labeling, you need to set the imshow's extent to the extremal values of the xe and ye edge arrays. 

import matplotlib.pyplot as plt
from matplotlib.colors import LogNorm
import numpy as np; np.random.seed(9)

x = np.random.rayleigh(size=9900)
y = np.random.rayleigh(size=9900)

M, xe, ye = np.histogram2d(x, y, bins = 80)

extent = [xe[0], xe[-1], ye[0], ye[-1]]
plt.imshow(M.T, extent=extent, norm = LogNorm(), origin=""lower"")

plt.show()



",,
matplotlib strange output,https://stackoverflow.com/questions/41538059,"Create a dictionary from values of a function, and using that for a graph","what I am trying to do is make a math function that takes input n and outputs a. I input a larger number and this goes through a while loop, subtracts one from that value, prints a, and repeats until a certain value of n. What I want to do is to put this into a dictionary, with n as a key and a as a value, and use that to graph via matplotlib. Looking around a bit there, it doesn't seem like it takes dictionaries, only lists or arrays, so it might be best to make separate lists of keys and values, then input that, and change markers and such. Here is the code I have so far:

def intan(n=3):
    a = 180 -(360/n)
    while n &gt;= 3:
        print(a)
        n -= 1
        intan(n)

intan(4)
    '''Returns strange output of 90.0, 60.0, 90.0,
    instead of just the first two'''


As you can see, there is still a slightly odd error with the code where it cycles through the outputs more than once, but after some tinkering I can't quite figure out why that is. Thank you guys!

Update: The odd error is fixed by StephenRauch's helpful suggestion.
",0,225,,,
matplotlib strange output,https://stackoverflow.com/questions/41432010,Strange Error: QObject::~QObject: Timers cannot be stopped from another thread,"This code is running fine as far producing the required output is concerned but is displaying a strange error message:


  QObject::~QObject: Timers cannot be stopped from another thread.


from multiprocessing import Pool,Manager
import matplotlib.pyplot as plt
import numpy as np
import time

def scatter_join(args):
    num = args[2]
    print(num)
    plt.scatter(args[0],args[1],s=1)
    plt.savefig('test_p'+str(num)+'.png')
    if args[3].empty() is False:
        args[4].acquire()
        (ax,fig) = args[3].get()
        ax.scatter(args[0],args[1],s=1)
        args[3].put((ax,fig))
        args[4].release()
    else:
        args[4].acquire()
        fig = plt.figure()
        ax = fig.add_subplot(111)
        ax.scatter(args[0],args[1],s=1)
        args[3].put((ax,fig))
        args[4].release()

    return None



if __name__ == '__main__':
    p = Pool()
    m = Manager()
    q = m.Queue()
    l = m.Lock()
    snap = []
    for looper in range(0,50):
        np.random.seed(int(time.time()))
        snap.append( np.random.normal(0+np.random.randint(-5,5),20+np.random.randint(-5,5),(10000,2)) )

    task = [(snap[x][:,0],snap[x][:,1],x,q,l) for x in range(0,50)]
    t = time.time()
    results = p.map(scatter_join,task,chunksize=18)

    p.close()
    p.join()
    print('Time Elapsed(Parallel): ', abs((time.time()-t)))

    (ax,fig) = q.get()
    fig.savefig('superimposedimg.png')


    t = time.time()
    for looper in range(0,50):
        print(looper)
        np.random.seed(int(time.time()))
        plt.scatter(snap[looper][:,0],snap[looper][:,1],s=1)
        plt.savefig('test_s'+str(looper)+'.png')
        plt.clf()
    print('Time Elapsed(Linear): ',abs(time.time()-t))


I basically want to use multiprocessing to plot plot graphs in a simgle matplotlib object.
",0,1953,"When plotting like that, you need to make sure the backend
is set to the agg backend.  You can do that in a matplotlibrc file or
directly in the plotting script, immediately after importing matplotlib
and before importing pyplot (if you use pyplot at all).

# do this before importing pylab or pyplot
import matplotlib
matplotlib.use('Agg')
import matplotlib.pyplot as plt


http://matplotlib.org/faq/howto_faq.html#howto-webapp

Also you may plot in isolation. For eg:

fig = plt.figure()
fig, ax = plt.subplots()
 &gt; plot stuff, scatterplot, etc..., set title, lables, etc...
fig.savefig(filename, format='png', dpi=100, facecolor='w',
edgecolor='k') 
plt.close(fig)


NB: Since we close the plot after you don't have to use fig.clf() to clear.
",,
matplotlib strange output,https://stackoverflow.com/questions/40803498,"Error when plotting from a pandas dataframe using matplotlib, with different IPython versions","I am facing a strange problem, when trying to simply plot a dummy histogram from fake data (copy/paste from pandas documentation).
What causes this crash?

#!/usr/bin/python3
# -*- coding: utf-8 -*-


import pandas as pd
import numpy as np

import matplotlib.pyplot as plt

df = pd.DataFrame({'A' : [1,2,3,4],
                   'C' : pd.Series(1,index=list(range(4)),dtype='float32'),
                   'D' : np.array([3] * 4,dtype='int32'),
                   'E' : pd.Categorical([""test"",""train"",""test"",""train""]),
                   'F' : 'foo' })
c=['A']

plt.figure()
plt.hist(df[c])
plt.show()


When running with ipython2 ttplot.py -i, I obtain the following output:

Python 2.7.12 |Anaconda 4.1.1 (64-bit)| (default, Jul  2 2016, 17:42:40)
Type ""copyright"", ""credits"" or ""license"" for more information.

IPython 4.2.0 -- An enhanced Interactive Python.
?         -&gt; Introduction and overview of IPython's features.
%quickref -&gt; Quick reference.
help      -&gt; Python's own help system.
object?   -&gt; Details about 'object', use 'object??' for extra details.
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
/srv/logs/tiaki/sentryo/exps/tools/ttplot.py in &lt;module&gt;()
     28
     29 plt.figure()
---&gt; 30 plt.hist(df[c])
     31 plt.show()
     32

/home/inendi/anaconda2/lib/python2.7/site-packages/matplotlib/pyplot.pyc in hist(x, bins, range, normed, weights, cumulative, bottom, histtype, align, orientation, rwidth, log, color, label, stacked, hold, data, **kwargs)
   2956                       histtype=histtype, align=align, orientation=orientation,
   2957                       rwidth=rwidth, log=log, color=color, label=label,
-&gt; 2958                       stacked=stacked, data=data, **kwargs)
   2959     finally:
   2960         ax.hold(washold)

/home/inendi/anaconda2/lib/python2.7/site-packages/matplotlib/__init__.pyc in inner(ax, *args, **kwargs)
   1810                     warnings.warn(msg % (label_namer, func.__name__),
   1811                                   RuntimeWarning, stacklevel=2)
-&gt; 1812             return func(ax, *args, **kwargs)
   1813         pre_doc = inner.__doc__
   1814         if pre_doc is None:

/home/inendi/anaconda2/lib/python2.7/site-packages/matplotlib/axes/_axes.pyc in hist(self, x, bins, range, normed, weights, cumulative, bottom, histtype, align, orientation, rwidth, log, color, label, stacked, **kwargs)
   5993             xmax = -np.inf
   5994             for xi in x:
-&gt; 5995                 if len(xi) &gt; 0:
   5996                     xmin = min(xmin, xi.min())
   5997                     xmax = max(xmax, xi.max())

TypeError: len() of unsized object

In [1]:


With ipython3 ttplot.py -i, I obtain the following output:

Python 3.4.2 (default, Oct  8 2014, 10:45:20)
Type ""copyright"", ""credits"" or ""license"" for more information.

IPython 5.1.0 -- An enhanced Interactive Python.
?         -&gt; Introduction and overview of IPython's features.
%quickref -&gt; Quick reference.
help      -&gt; Python's own help system.
object?   -&gt; Details about 'object', use 'object??' for extra details.
---------------------------------------------------------------------------
KeyError                                  Traceback (most recent call last)
/usr/local/lib/python3.4/dist-packages/pandas/indexes/base.py in get_loc(self, key, method, tolerance)
   2133             try:
-&gt; 2134                 return self._engine.get_loc(key)
   2135             except KeyError:

pandas/index.pyx in pandas.index.IndexEngine.get_loc (pandas/index.c:4164)()

pandas/index.pyx in pandas.index.IndexEngine.get_loc (pandas/index.c:4028)()

pandas/src/hashtable_class_helper.pxi in pandas.hashtable.PyObjectHashTable.get_item (pandas/hashtable.c:13166)()

pandas/src/hashtable_class_helper.pxi in pandas.hashtable.PyObjectHashTable.get_item (pandas/hashtable.c:13120)()

KeyError: 0

During handling of the above exception, another exception occurred:

KeyError                                  Traceback (most recent call last)
/srv/logs/tiaki/sentryo/exps/tools/ttplot.py in &lt;module&gt;()
     28
     29 plt.figure()
---&gt; 30 plt.hist(df[c])
     31 plt.show()
     32

/usr/lib/python3/dist-packages/matplotlib/pyplot.py in hist(x, bins, range, normed, weights, cumulative, bottom, histtype, align, orientation, rwidth, log, color, label, stacked, hold, **kwargs)
   2894                       histtype=histtype, align=align, orientation=orientation,
   2895                       rwidth=rwidth, log=log, color=color, label=label,
-&gt; 2896                       stacked=stacked, **kwargs)
   2897         draw_if_interactive()
   2898     finally:

/usr/lib/python3/dist-packages/matplotlib/axes/_axes.py in hist(self, x, bins, range, normed, weights, cumulative, bottom, histtype, align, orientation, rwidth, log, color, label, stacked, **kwargs)
   5576         # Massage 'x' for processing.
   5577         # NOTE: Be sure any changes here is also done below to 'weights'
-&gt; 5578         if isinstance(x, np.ndarray) or not iterable(x[0]):
   5579             # TODO: support masked arrays;
   5580             x = np.asarray(x)

/usr/local/lib/python3.4/dist-packages/pandas/core/frame.py in __getitem__(self, key)
   2057             return self._getitem_multilevel(key)
   2058         else:
-&gt; 2059             return self._getitem_column(key)
   2060
   2061     def _getitem_column(self, key):

/usr/local/lib/python3.4/dist-packages/pandas/core/frame.py in _getitem_column(self, key)
   2064         # get column
   2065         if self.columns.is_unique:
-&gt; 2066             return self._get_item_cache(key)
   2067
   2068         # duplicate columns &amp; possible reduce dimensionality

/usr/local/lib/python3.4/dist-packages/pandas/core/generic.py in _get_item_cache(self, item)
   1384         res = cache.get(item)
   1385         if res is None:
-&gt; 1386             values = self._data.get(item)
   1387             res = self._box_item_values(item, values)
   1388             cache[item] = res

/usr/local/lib/python3.4/dist-packages/pandas/core/internals.py in get(self, item, fastpath)
   3539
   3540             if not isnull(item):
-&gt; 3541                 loc = self.items.get_loc(item)
   3542             else:
   3543                 indexer = np.arange(len(self.items))[isnull(self.items)]

/usr/local/lib/python3.4/dist-packages/pandas/indexes/base.py in get_loc(self, key, method, tolerance)
   2134                 return self._engine.get_loc(key)
   2135             except KeyError:
-&gt; 2136                 return self._engine.get_loc(self._maybe_cast_indexer(key))
   2137
   2138         indexer = self.get_indexer([key], method=method, tolerance=tolerance)

pandas/index.pyx in pandas.index.IndexEngine.get_loc (pandas/index.c:4164)()

pandas/index.pyx in pandas.index.IndexEngine.get_loc (pandas/index.c:4028)()

pandas/src/hashtable_class_helper.pxi in pandas.hashtable.PyObjectHashTable.get_item (pandas/hashtable.c:13166)()

pandas/src/hashtable_class_helper.pxi in pandas.hashtable.PyObjectHashTable.get_item (pandas/hashtable.c:13120)()

KeyError: 0

In [1]:


Somehow, python2 and 3 point to different versions of matplotlib and pandas

Python2: matplotlib : '1.5.1', pandas : '0.18.1'

Python3: matplotlib : '1.4.2', pandas : '0.19.1'
",0,547,,,
matplotlib strange output,https://stackoverflow.com/questions/27489534,Matplotlib floating point format issue,"I experience strange behaviour loading a FANN2 network from file in Python 2 while using Matplotlib or PyQtGraph simultaneously. I need to plot FANN network's MSE in real time. I will use matplotlib with interactivity and while True loop or Thread to update plot.
Create neural network and store to file:
#!/usr/bin/python2

from fann2 import libfann
import matplotlib.pyplot as plt

PLOT = False

if PLOT:
    plt.show()
    axes = plt.subplot(111)

connection_rate = 1
layers = (10, 5)
activation_f = libfann.SIGMOID_SYMMETRIC_STEPWISE
ann = libfann.neural_net()
ann.create_sparse_array(connection_rate, layers)
ann.randomize_weights(-0.1, 0.1)
ann.save('my_network.ann')

PLOT flag disables plot. Output file, last line:
connections (connected_to_neuron, weight)=(0, -9.04591381549835205078e-02) ...

... means omitted characters. After I change PLOT to True corresponding line looks like:
connections (connected_to_neuron, weight)=(0, -7,55163878202438354492e-02) ...

This leads to errors loading saved network:
from fann2 import libfann
ann = libfann.neural_net()
ann.create_from_file('my_network.ann')


FANN Error 4: Error reading ""connection_rate"" from configuration file ""my_network.ann"".

How to fix this? Should I change a floating point number format in Matplotlib? PyQtGraph gives same results:
#!/usr/bin/python2

from fann2 import libfann
import pyqtgraph as pg

PLOT = False

if PLOT:
pg.plot([1, 2], [3, 4], pen=None, symbol='o')

connection_rate = 1
layers = (10, 5)
activation_f = libfann.SIGMOID_SYMMETRIC_STEPWISE
ann = libfann.neural_net()
ann.create_sparse_array(connection_rate, layers)
ann.randomize_weights(-0.1, 0.1)
ann.save('my_network.ann')

",0,822,"Add connection_rate in ""my_network.ann"" (and settings like cascade_min_out_epochs that FANN2 complains about):
connection_rate = 0.7000 {or whichever you prefer}

When I updated libfann to FANN2 on loading configuration file I got:

FANN Error 3: Wrong version of configuration file, aborting read of configuration file “myfile.net”. segmentation fault

Fixed it by adding settings FANN2 complained about.
",,
matplotlib strange output,https://stackoverflow.com/questions/25227598,matplotlib candlestick chart bar output error - seems to be plotting more than one timeframe on single bar,,0,342,,,
matplotlib strange output,https://stackoverflow.com/questions/70252697,Matplotlib animation.FuncAnimation() animation miss first frame?,,-1,1351,,,
matplotlib strange result,https://stackoverflow.com/questions/2066279,The memory usage reported by guppy differ from ps command,"I am profiling my twisted server. It uses much more memory than I expected. Its memory usage grows over time.

 ps -o pid,rss,vsz,sz,size,command
  PID   RSS    VSZ    SZ    SZ COMMAND
 7697 70856 102176 25544 88320 twistd -y broadcast.tac


As you can see it costs 102176 KBs, namely, 99.78125 MBs. And I use guppy from a twisted manhole to watch the memory usage profile.

&gt;&gt;&gt; hp.heap()
Partition of a set of 120537 objects. Total size = 10096636 bytes.
 Index  Count   %     Size   % Cumulative  % Kind (class / dict of class)
     0  61145  51  5309736  53   5309736  53 str
     1  27139  23  1031596  10   6341332  63 tuple
     2   2138   2   541328   5   6882660  68 dict (no owner)
     3   7190   6   488920   5   7371580  73 types.CodeType
     4    325   0   436264   4   7807844  77 dict of module
     5   7272   6   407232   4   8215076  81 function
     6    574   0   305776   3   8520852  84 dict of class
     7    605   1   263432   3   8784284  87 type
     8    602   0   237200   2   9021484  89 dict of type
     9    303   0   157560   2   9179044  91 dict of zope.interface.interface.Method
&lt;384 more rows. Type e.g. '_.more' to view.&gt;


Hum... It seems there is something wrong. Guppy shows that the total usage of memory is 10096636 bytes, namely 9859.996 KBs or 9.628 MBs.

That's a huge difference. What's wrong this strange result? What am I doing wrong?

Update:
I wrote a monitor script last night. It records the memory usage and number of on-line users. It is a radio server, so you can see there is radios and total listeners. Here is the figure I generated by matplotlib.


Something is strange. Sometimes the memory usage printed by ps is very low, like this

2010-01-15 00:46:05,139 INFO 4 4 17904 36732 9183 25944
2010-01-15 00:47:03,967 INFO 4 4 17916 36732 9183 25944
2010-01-15 00:48:04,373 INFO 4 4 17916 36732 9183 25944
2010-01-15 00:49:04,379 INFO 4 4 17916 36732 9183 25944
2010-01-15 00:50:02,989 INFO 4 4 3700 5256 1314 2260


What is the reason of the super low value of memory usage? And what's more, even there is no on-line radios, no listeners, the memory usage is still high.
",9,4726,,,
matplotlib strange result,https://stackoverflow.com/questions/58050200,How to draw a planar graph with networkx?,"I'm trying to draw a planar portayal of a digraph with the python packages ""matplotlib"" and ""networkx"".
I've tried using the ""networkx.planar_layout"" for the node positions in the plot, but don't like the outcome.
In the following example, ""graph"" is a (planar) directed graph. The keys of the dictionary ""graph"" are the nodes. The value of a key is a list, which contains all neighbors of this node:
import networkx as nx
import matplotlib.pyplot as plt

graph = {'s1': ['v', 't1','w'],
     's2': ['t1','s1'],
     's3': ['v','w'],
     's4': ['x','y'],
     'x': ['v','w'],
     'v': ['t1', 'w'],
     'w': ['y','t1','t2'],
     'y': ['v','t1','t2'],
     't1': [],
     't2': []
     }

def main(G):
    fig = plt.figure()
    fig.show()

    graph = nx.DiGraph()

    for v in G.keys():
        graph.add_node(v)

    for delta in G.items():
        for w in delta[1]:
            graph.add_edge(delta[0],w)

    posit = nx.planar_layout(G)

    nx.draw(graph, posit , with_labels = True)
    fig.canvas.draw()


main(graph)

The image i get: 
What I don't like about it is that the nodes are lined up in a way which results in a ""stack"" of edges. For example, it is not possible to tell from the plot where the edge (s2,t1) is really ending, since the edges are all overlapping in this part of the Image (I don't even think this fits the definition of a planar portrayal of my graph, which is strange since the layout I used is called ""planar_layout"" and the graph is in fact planar).
Is there a better way to plot this?
",7,5977,"I've found a function that could help:
I'm using ""nx.draw_planar"" instead of ""nx.draw"" like this:
def main(G):
    fig = plt.figure()
    fig.show()

    graph = nx.DiGraph()

    for v in G.keys():
        graph.add_node(v)

    for delta in G.items():
        for w in delta[1]:
            graph.add_edge(delta[0],w)

    #posit = nx.shell_layout(G) #ISN'T NEEDED ANYMORE

    nx.draw_planar(graph,with_labels = True, alpha=0.8) #NEW FUNCTION
    fig.canvas.draw()


    main(graph)

I get the following result:

The problem with this solution is that I don't get to save the node positions as in the previous version with ""posit"". I would like to use those later in the program, though. Does anyone know how I can get them without using a layout from networkx?
","The important thing is to ensure that the graph is actually planar.
One way of doing this is by using a PlanarEmbedding rather than a DiGraph - while the PlanarEmbedding is a subclass of DiGraph, it ensures that the graph is planar.
import json
from random import random

import networkx as nx
import matplotlib.pyplot as plt


def draw(n : dict):
    g = nx.PlanarEmbedding()
    g.set_data(n)
    pos = nx.planar_layout(g)  # here are your positions.
    # pos = nx.spring_layout(g, pos=pos, seed=int(2**32 - 1 * random()))
    nx.draw_networkx(g, pos, with_labels=True)
    plt.show()

if __name__ == '__main__':
    j_obj = {}
    with open('planar.json', 'r') as infile:
        nodes = json.load(infile)
        infile.close()
    draw(nodes)

planar.json
{
   ""s1"": [""s2"",""t1"",""w"", ""v""],
   ""s2"": [""t1"",""s1""],
   ""s3"": [""w"",""v""],
   ""s4"": [""x"",""y""],
   ""x"": [""v"",""w"",""s4""],
   ""v"": [""s1"",""s3"",""w"",""x"",""y"",""t1""],
   ""w"": [""t1"",""t2"",""y"",""x"",""v"",""s3"",""s1""],
   ""y"": [""v"",""s4"",""w"",""t2"",""t1""],
   ""t1"": [""s2"",""v"",""y"",""w"",""s1""],
   ""t2"": [""y"",""w""]
}



",
matplotlib strange result,https://stackoverflow.com/questions/23455226,Parametric equation with numpy,,6,10124,,,
matplotlib strange result,https://stackoverflow.com/questions/18982418,Discontinuous timeseries plot with dates on x-axis,,6,2881,,,
matplotlib strange result,https://stackoverflow.com/questions/4787821,bundle_files = 1 fails with py2exe using matplotlib,,5,3512,,,
matplotlib strange result,https://stackoverflow.com/questions/37899364,autoscaling not working on drawing arrow,,4,1277,,,
matplotlib strange result,https://stackoverflow.com/questions/31589300,Matplotlib shows different fonts when saving jpg with respect to png. Why?,,4,414,,,
matplotlib strange result,https://stackoverflow.com/questions/23405728,Matplotlib figure window disappears when in interactive mode,"I need to use interactive mode when plotting with matplotlib (it should be a script, not python or ipython console). But setting plt.ion() causes a strange bug (?). When I try to plot my figure (I don't think it really matters, what I do exactly, because in non-interactive mode it works perfectly fine) I don't see it - I get a blank grey window for split-second, which momentarily disappears and the programme exits.
If I explicitly add plt.draw() (and plt.pause(1) to see the result), I see that the figure appears as expected. If I do the same after modifications I want to do to the figure when it is visible, the figure changes. But the window still disappears after the pause is over.

I run it in Spyder with Qt4Agg as a backend under Ubuntu. Tried running the script from terminal as python my_script.py, the result is identical.

What could be the problem? How do I stop the figure from disappearing when in interactive mode?

UPDATE

Working example:

import matplotlib.pyplot as plt
import numpy as np

plt.ion()

x = np.linspace(1, 10)
y = np.sin(x)
plt.plot(x, y)
plt.draw()
plt.pause(1)


If I run this code I see the sine plot for 1 second, then the window disappears.

UPDATE 2

I found a solution here: https://stackoverflow.com/a/10724654/1304161
If I set the run options in Spyder correctly, it works perfectly fine. Although running it in gnome-terminal doesn't work, I don't really need it. Hopefully, there won't be a problem with this when it becomes a part of a GUI app...
",4,10585,"You can make it work by adding these two lines at the end:

plt.ioff()
plt.show()


So this program works fine:

import matplotlib.pyplot as plt
import numpy as np

plt.ion()

x = np.linspace(1, 10)
y = np.sin(x)
plt.plot(x, y)
plt.draw()
plt.ioff()
plt.show()

","To display multiple figures at different times, you can use code.interact(local=locals()), after plt.show() to pause the Python interpreter until a Ctrl-Z is pressed in the Python Shell:
import code
import matplotlib.pyplot as plt
import numpy as np

# Start pyplot's ""interactive mode"" which lets you run plt.show multiple times
plt.ion()

x = np.linspace(1, 10)
y = np.sin(x)

# PLOT #1 - displayed
plt.plot(x, y)
plt.draw()
plt.show()

# Wait for figure to be closed AND Ctrl-Z pressed in Python Shell
code.interact(local=locals())

print(""Some more code can go here..."")

# PLOT #2 - displayed
plt.plot(x, y)
plt.show()

# Wait for figure to be closed AND Ctrl-Z pressed in Python Shell
code.interact(local=locals())

",
matplotlib strange result,https://stackoverflow.com/questions/33890596,using matplotlib from main and spawned process,,3,707,,,
matplotlib strange result,https://stackoverflow.com/questions/30699403,py2exe exe closes immediately after launch,,3,2618,,,
matplotlib strange result,https://stackoverflow.com/questions/24824192,Redisplaying plots in Matplotlib in conjunction with PyQt,,3,313,,,
matplotlib strange result,https://stackoverflow.com/questions/64120358,matplotlib plot_surface colormap does not scale with the z-axis,"I try to make simple 3D plot with plot_surface of matplotlib, below is the minimum example:
import numpy as np
import matplotlib.pyplot as plt
from matplotlib import cm
x_test = np.arange(0.001, 0.01, 0.0005)
y_test = np.arange(0.1, 100, 0.05)

fig = plt.figure()
ax = fig.add_subplot(111, projection='3d')

Xtest, Ytest = np.meshgrid(x_test, y_test)

Ztest = Xtest**-1 + Ytest

surf = ax.plot_surface(Xtest, Ytest, Ztest,
                       cmap=cm.plasma, alpha=1,
                       antialiased=False)

fig.colorbar(surf, shrink=0.5, aspect=5)
ax.set_ylabel(r'$ Y $', fontsize=16)
ax.set_xlabel(r'$ X $', fontsize=16)
ax.set_zlabel(r'$ Z $', fontsize=16)


The result gives strange colormap, that does not represent the magnitude of the z scale, as you can see from here 3D plot result.
I mean if you take a straight line of constant Z, you don't see the same color.
I've tried to change the ccount and rcount inside plot_surface function or changing the interval of the Xtest or Ytest data, nothing helps.
I've tried some suggestion from here and here. But it seems not related.
Could you help me how to solve this? It seems a simple problem, but I couldn't solve it.
Thanks!
Edit
I add example but using the original equation that I don't write it here (it's complicated), please take a look: Comparison.
While the left figure was done in Matlab (by my advisor),
the right figure by using matplotlib.
You can see clearly the plot on the left really make sense,
the brightest color always on the maximum z-axis. Unfortunately, i don't know matlab. I hope i can do it by using python,
Hopefully this edit makes it more clear of the problem.
Edit 2
I'm sure this is not the best solution. That's why I put it here, not as an answer. As suggested by @swatchai to use the contour3D.
Since surface is set of lines, I can generate the correct result by plotting a lot of contour lines, by using:
surf = ax.contour3D(Xtest, Ytest, Ztest, 500, cmap=cm.plasma,
                 alpha=0.5, antialiased=False)

The colormap is correct as you can see from herealternative1
But the plot is very heavy. When you zoom-in, it doesn't look good, unless you increase again the number of the contour.
Any suggestion are welcome :).
",2,1858,"I guess, the formal answer to plot this kind of surface is by using Axes3D.contour and Axes3D.contourf. Based on documentation
, for example:
surf2 = ax.contourf(Xtest, Ytest, Ztest, 250, cmap=cm.plasma,
                    alpha=0.6, antialiased=False)
surf = ax.contour(Xtest, Ytest, Ztest, 250, cmap=cm.plasma,
                  alpha=0.6, antialiased=False)

The result is here. The colormap shows correct z-scale.
It's not as perfect as smooth surface, as it depends on how much we zoom it or how much we put the contour. I don't know if there's a way to create this by plot_surface. thanks @swatchai.
","I don't know how this can be achieved but maybe some words on why this is happening.
plot_surface generates a mesh where the vertices are defined by x and y and z.
Each patch has 4 corners and gets a color corresponding to its z value. Looking at the plot it
could be the maximal z value of the 4 corners (just a guess).
And if you look closely the colors of the patches actually do get lighter as you move in +y direction.
But what is far more obvious are the color changes in x direction, producing the slopes you mentioned.
But this can not be avoided if each patch has just a single color.
You can see this maybe more clearly if you change the formula to Z = (X**-1 + 10 * Y)

","The behavior of the surface plot is not what you expect. Only contour3D or contourf3D can display such behavior. Here is relevant code that you can try to get the plot that follows:
surf = ax.plot_surface(Xtest, Ytest, Ztest, cmap=cm.plasma, alpha=0.55)
ax.contourf3D(Xtest, Ytest, Ztest, cmap=cm.plasma)

The plot that show both surface and contourf3D:

"
matplotlib strange result,https://stackoverflow.com/questions/63954726,Put one curve exactly underneath the other in Matplotlib,"I have two curves in Matplotlib and I would like to have them exactly beneath each other. My naive approach was to just decrease the values of the one curve by a constand factor. Strangely in Matplotlib (opposed to Excel) the created curves have some deviations and are not always beneath each other.
Here is my code:
from matplotlib import pyplot as plt
from scipy.interpolate import interp1d
import numpy as np
load = [0.0, 0.1, 0.5, 0.7, 0.4, 0.5, 0.4, 0.3, 0.4, 0.5, 0.65, 0.75, 0.8, 0.65, 0.15, 0.55, 0.1, 0.4, 0.0, 0.25, 0.25, 0.35, 0.6, 0.25, 0.05]
#demand = [0.35, 0.25, 0.15, 0.1, 0.1, 0.2, 0.40, 0.50, 0.50, 0.40, 0.40, 0.47, 0.47, 0.40, 0.35, 0.35, 0.4, 0.5, 0.65, 0.7, 0.65, 0.5, 0.4, 0.35, 0.25]
help = 0.025;
demand = [0.0-help, 0.1-help, 0.5-help, 0.7-help, 0.4-help, 0.5-help, 0.4-help, 0.3-help, 0.4-help, 0.5-help, 0.65-help, 0.75-help, 0.8-help, 0.65-help, 0.15-help, 0.55-help, 0.1-help, 0.4-help, 0.0-help, 0.25-help, 0.25-help, 0.35-help, 0.6-help, 0.25-help, 0.05-help]
hours = list(range(25)) # [0, 1, 2, ... 22, 23, 24]
labels = [f'{h:02d}:00' for h in hours] # [""00:00"", ""01:00"", ... ""23:00"", ""24:00""]


f = interp1d(hours, load, kind='cubic')
f2 = interp1d(hours, demand, kind='cubic')


xnew = np.linspace(0, 24, num=500, endpoint=True)

plt.xticks(np.arange(0, 25, step=1))  # Set label locations.
plt.xticks(np.arange(25), labels)  # Set text labels.
plt.xticks(np.arange(25), labels, rotation=90)

plt.plot(hours, load, 'o', xnew, f(xnew), '-', xnew, f2(xnew), '--')
#plt.legend(['data', 'linear', 'cubic'], loc='best')
plt.ylabel(""Electrical power in W"", fontsize=14, labelpad=8)
plt.xlabel(""Time of day"", fontsize=14, labelpad=8)
plt.show()


plt.xticks(np.arange(0, 25, step=1))  # Set label locations.
plt.xticks(np.arange(25), labels)  # Set text labels.
plt.xticks(np.arange(25), labels, rotation=90)
plt.xlim(xmin=0.0)

xnew = np.linspace(0, 24, num=500, endpoint=True)
plt.xticks(np.arange(0, 25, step=1))  # Set label locations.
plt.xticks(np.arange(25), labels)  # Set text labels.
plt.xticks(np.arange(25), labels, rotation=90)


plt.plot(xnew, f(xnew), color=""gold"", linewidth=3)
plt.plot(xnew, f2(xnew), color=""green"", linewidth=3)
#plt.fill_between(xnew, f(xnew), color=""gold"", alpha=0.30, edgecolor=None)
#plt.fill_between(xnew, f2(xnew), color=""red"", alp_ha=0.30, edgecolor=None)

plt.legend( ['Electricity generation', 'Electricity demand'], bbox_to_anchor=(0.5, 1.2), loc='upper center' )
plt.ylabel(""Electrical power"", fontsize=14, labelpad=8)
plt.xlabel(""Time of day"", fontsize=14, labelpad=8)
plt.tick_params(labelleft=False)
plt.savefig('Generation_Demand_DSM.png', edgecolor='black', dpi=400, bbox_inches='tight')
plt.show()

And here you see the results. I illustratively marked some areas where the curves are either almost identical (and thus the one is not far away from the other) and areas where the deviations are too high.
Any ideas how I can do that in a 'clean' way? I'd appreciate every comment.
Update: As this seems to be a difficult task I'd like to know, if it is somehow possible to just get the curve with a transparent background in Matplotlib such that I can just create the curve and then manually place it under the original curve (by using e.g. Paint)?
So there is no option in Matplotlib to just create the curve with a transparent background?
",2,334,"As many people have already suggested in the comments when you subtract the y values only y-axis is shifted while x-axis remains the same, thus not creating the effect you want. This is mainly because of the varying slope in your curve. However, we can use the slope information to artificially create a shift.
The effect will need further refining and fine-tuning in order to get the exact displacement. I am sharing the code here as a starting point.
from matplotlib import pyplot as plt
from scipy.interpolate import interp1d
import numpy as np
load = [0.0, 0.1, 0.5, 0.7, 0.4, 0.5, 0.4, 0.3, 0.4, 0.5, 0.65, 0.75, 0.8, 0.65, 0.15, 0.55, 0.1, 0.4, 0.0, 0.25, 0.25, 0.35, 0.6, 0.25, 0.05]
shift = 0.02;
demand = [l-shift for l in load]
hours = list(range(25)) # [0, 1, 2, ... 22, 23, 24]
labels = [f'{h:02d}:00' for h in hours] # [""00:00"", ""01:00"", ... ""23:00"", ""24:00""]

fig = plt.figure(figsize=(20,10))
f = interp1d(hours, load, kind='quadratic')
f2 = interp1d(hours, demand, kind='quadratic')


xnew = np.linspace(0, 24, num=500, endpoint=True)    

plt.xticks(np.arange(0, 25, step=1))  # Set label locations.
plt.xticks(np.arange(25), labels)  # Set text labels.
plt.xticks(np.arange(25), labels, rotation=90)
plt.xlim(xmin=0.0)

xnew = np.linspace(0, 24, num=500, endpoint=True)
plt.xticks(np.arange(0, 25, step=1))  # Set label locations.
plt.xticks(np.arange(25), labels)  # Set text labels.
plt.xticks(np.arange(25), labels, rotation=90)

xshift = xnew+np.concatenate(([0],np.diff(f(xnew))*7))

plt.plot(xnew, f(xnew), color=""gold"", linewidth=5)
plt.plot(xshift, f2(xnew), color=""green"", linewidth=5)

# plt.fill_between(xnew+np.concatenate(([0],np.diff(f(xnew))*7)), 
#                  f(xnew)-0.02, f(xnew), color=""gold"", alpha=0.30, edgecolor=None)
#plt.fill_between(xnew, f2(xnew), color=""red"", alp_ha=0.30, edgecolor=None)

plt.legend( ['Electricity generation', 'Electricity demand'], bbox_to_anchor=(0.5, 1.2), loc='upper center' )
plt.ylabel(""Electrical power"", fontsize=14, labelpad=8)
plt.xlabel(""Time of day"", fontsize=14, labelpad=8)
plt.tick_params(labelleft=False)
plt.savefig('Generation_Demand_DSM.png', edgecolor='black', dpi=400, bbox_inches='tight')
plt.show()

COMMENTS:

I have also simplified your demand values assignment using list comprehension.
The slope is obtained using np.diff The slope also takes care of +ve and -ve directions and hence can be directly used to shift the plot.
A multiplier is used to adjust the spacing.
To fine tune the plot you may use different multipliers for different directions and slope thresholds.

Here's the output obtained using the above code. HTH.

UPDATE 1:
Saving plots using transparent background with matplotlib
As requested by the OP if you simply want to generate the plot without other plot elements you can do this with the following code change:
# plt.plot(xnew, f(xnew), color=""gold"", linewidth=5)
plt.plot(xshift, f2(xnew), color=""green"", linewidth=5)

# plt.fill_between(xnew+np.concatenate(([0],np.diff(f(xnew))*7)), 
#                  f(xnew)-0.02, f(xnew), color=""gold"", alpha=0.30, edgecolor=None)
#plt.fill_between(xnew, f2(xnew), color=""red"", alp_ha=0.30, edgecolor=None)

# plt.legend( ['Electricity generation', 'Electricity demand'], bbox_to_anchor=(0.5, 1.2), loc='upper center' )
# plt.ylabel(""Electrical power"", fontsize=14, labelpad=8)
# plt.xlabel(""Time of day"", fontsize=14, labelpad=8)
# plt.tick_params(labelleft=False)

plt.xticks([])
plt.yticks([])

plt.gca().spines['top'].set_visible(False)
plt.gca().spines['right'].set_visible(False)
plt.gca().spines['left'].set_visible(False)
plt.gca().spines['bottom'].set_visible(False)

plt.savefig('Generation_Demand_DSM_second.png', edgecolor='none', dpi=400, bbox_inches='tight', transparent=True)
plt.show()

Which generates an output like this (note it is rendered with white background here but the original png will retain transparency):

Now you can create the first curve using original settings and second curve as shown above. You can then superimpose the second curve on the first using paint (or similar tools) and manipulate as needed. HTH
UPDATE 2:
Editing with Inkscape
If you are using inkscape the plot becomes significantly editable when you save figure as pdf. You can retain all your original settings and only change the savefig line as follows:
plt.savefig('Generation_Demand_DSM_second.pdf', bbox_inches='tight')

The pdf generated is fully editable in inkscape. You just need to open it like any other file (mostly default settings should work or you can choose poppler import in the dialog that opens). Each element can be selected individually as shown in the screenshot here:

You can manipulate the output as required.
",,
matplotlib strange result,https://stackoverflow.com/questions/61688891,How best to create a filled volume of Lorenz Attractor in python for volume rendering,"I am trying to create a 3D array to then perform volume rendering on (in other software or volume rendering packages) of strange attractor like Lorenz Attractor. It is easy enough to plot the attractor from data points and provide a value to assign a color and visualize in matplotlib for example. 

However I would like a filled volume array. I have tried interpolation methods like griddata but it doesn't give the desired result. What I am envisioning is something like: 

Which is from the wikipedia page. 

Here is what I have tried but if you open the result in a simple viewer it doesn't look great. I am thinking instead maybe doing a interpolation only between the points that make up the x,y,z array... I am a little lost after playing with this for several hours. What I think I need is to take the points and do some sort of interpolation or filling into an array, here I am calling interp_im. This can then be viewed in volume rendering. Any help is greatly appreciated on this!

import numpy as np
import matplotlib.pyplot as plt
from scipy.integrate import odeint
from scipy.interpolate import griddata
from scipy.interpolate import LinearNDInterpolator
from skimage.external import tifffile

rho = 28.0
sigma = 10.0
beta = 8.0 / 3.0

def f(state, t):
    x, y, z = state  # Unpack the state vector
    return sigma * (y - x), x * (rho - z) - y, x * y - beta * z  # Derivatives

state0 = [1.0, 1.0, 1.0]
t = np.arange(0.0, 40.0, 0.01) #t = np.arange(0.0, 40.0, 0.01)

states = odeint(f, state0, t)

# shift x,y,z positions to int for regular image volume

x = states[:, 0]
y = states[:, 1]
z = states[:, 2]
x_min = x.min()
y_min = y.min()
z_min = z.min()
states_int = states + [abs(x_min),abs(y_min),abs(z_min)] + 1
states_int = states_int * 10
states_int = states_int.astype(int)

# values will be in order of tracing for color
values = []
for i,j in enumerate(states_int):
    values.append(i*10)

values = np.asarray(values)

fig = plt.figure()
ax = fig.gca(projection='3d')
sc = ax.scatter(states_int[:, 0], states_int[:, 1], states_int[:, 2],c=values)
plt.colorbar(sc)
plt.draw()
plt.show()

#print(x.shape, y.shape, z.shape, values.shape)

#Interpolate for volume rendering
x_ = np.linspace(0,999,500)
y_ = np.linspace(0,999,500)
z_ = np.linspace(0,999,500)
xx,yy,zz = np.meshgrid(x_,y_,z_, sparse = True)
#
# X = states_int.tolist()
#
interp_im = griddata(states_int, values, (xx,yy,zz), method='linear')
interp_im = interp_im.astype(np.uint16)

np.save('interp_im.npy', interp_im)

tifffile.imsave('LorenzAttractor.tif', interp_im)


",2,421,"Your data is in the volume, it is just pixelated. If you blur the volume, for example with a gaussian, you get something much more usable. For example:

from scipy import ndimage

vol = np.zeros((512, 512, 512), dtype=states_int.dtype)
# add data to vol
vol[tuple(np.split(states_int, vol.ndim, axis=1))] = values[:, np.newaxis]
# apply gaussian filter, sigma=5 in this case
vol = ndimage.gaussian_filter(vol, 5)


I would then use something like napari to view the data in 3D:

import napari

with napari.gui_qt():
    napari.view_image(v)




To make the volume smoother you may want to reduce your integration step size.
",,
matplotlib strange result,https://stackoverflow.com/questions/42055415,Undesired colorbar and axis labels/plot titles interaction,,2,561,,,
matplotlib strange result,https://stackoverflow.com/questions/11898073,How can I draw a cross which follows my mouse cursor?,,2,5909,,,
matplotlib strange result,https://stackoverflow.com/questions/67408807,y-axis in scatter plot not monotonic,"I am plotting the data from a .csv file using matplotlib. The data in the file is well behaved - meaning the x labels are equally spaced and monotonic increasing from line 1 to the end.
The y-axis of the plot however, starts at the minimum y-value and increases vertically to the maximum value and THEN jumps back down to a lesser value and DECREASES from there.  Very strange.
Opening the csv file in excel and plotting the same columns results in a normal plot.
import pandas as pd 
import matplotlib.pyplot as plt
weather = pd.read_csv('Weather 210221.csv', names=['Timestamp', 'Wind Speed', 'Wind Direction', 'Outdoor Temp', 'Rain Total', 'Barometer', 'Indoor Temp', 'Outdoor Humidity', 'Indoor Humidity', 'Rain Today', '1 min. Ave Wind Speed', 'Heat Index', 'Dew Point', 'Wind Chill'])
weather.plot.scatter(x='Timestamp', y='Outdoor Temp', title='Temps')
plt.show()

Any ideas what could be happening?  I would attach the data file if I knew how
",1,400,"found the answer. Apparently python is interpreting my data as strings.  I need to convert to float somehow.  I don't know how to do that yet but I'll figure that out next.
Solution
",,
matplotlib strange result,https://stackoverflow.com/questions/65263282,Display a pdf file using colab,"I have saved some images of my work in .pdf format using matplotlib, I know this is my fault from the beginning and I should save it directly as image but I did not know that I can not display pdf files on colab. To get these results I need another 10 days which is not good choice for me.
Actually I have found this which express my problem precisely but there was not answer.
It just seems strange to me that using matplotlib I can save pdf files but I can not load them using it again.
I just need to display the pdf file in colab cell ,I have tried:
import subprocess
subprocess.Popen(['myfile.pdf'],shell=True)


and this was the result:
&lt;subprocess.Popen at 0x7f4d6a395978&gt;
another methods as in this page do not work for me
",1,6550,"In a Jupyter notebook / Colab you can simply
from pdf2image import convert_from_path

images = convert_from_path(""myfile.pdf"")
images[0]  # first page

The image will be able to render as the cell output. No need for IPython.display
","Ok this works for me, maybe there are a simpler solution but for now this works
from pdf2image import convert_from_path
from IPython.display import display, Image

images = convert_from_path(""myfile.pdf"")
for i, image in enumerate(images):    
    fname = ""image"" + str(i) + "".png""
    image.save(fname, ""PNG"")
Image(fname, width=600, height=300)



",
matplotlib strange result,https://stackoverflow.com/questions/63760061,Color cycler example in matplotlib documentation is cryptic (python),"Would someone be so kind as to explain to me how this particular example works, taken from the matplotlib documentation under the section Persistent Cycles?
Essentially they create color cyclers, which I am not sure if they are factory functions or iterators or normal functions or objects. Somehow you can make a generator out of them with iter(my_cycler) that is finite and an itertools.cycle generator with my_cycler(). This is just over my head how this works.
So my question is: why can you do iter(my_cycler) and it gives something finite and why can you do my_cycler() and it gives the same thing but infinite (it loops).
&gt;&gt;&gt; from cycler import cycler
&gt;&gt;&gt; cyl = cycler(color=['r', 'g', 'b'])

&gt;&gt;&gt; type(cyl)
cycler.Cycler

&gt;&gt;&gt; import cycler
&gt;&gt;&gt; cycler.Cycler.__mro__
(cycler.Cycler, object)

&gt;&gt;&gt; type(cyl())
itertools.cycle

&gt;&gt; type(iter(cyl))
generator

When I type:
&gt;&gt;&gt; next(cyl)
TypeError: 'Cycler' object is not an iterator

but somehow this here works:
&gt;&gt;&gt; for s in cyl:
...     print(s)
 
{'color': 'r'}
{'color': 'g'}
{'color': 'b'}

The other question is the strange application of defaultdict. styles = defaultdict(lambda : next(finite_cy_iter)). I think I roughly understand it but I want to be sure: Every time a key is not in the dict, (lambda : next(finite_cy_iter))() is called or something. According to the documentation on defaultdicts, you have to give it a function and in this case function = (lambda : next(finite_cy_iter)). So I guess every time a key is not in the dict function gets called, which is actually a disguised iterator which gives you the next dict out of the cycler, which then gets stored as the value for that key.
Here is the section in the matplotlib documentation:
Persistent Cycles
It can be useful to associate a given label with a style via dictionary lookup and to dynamically generate that mapping. This can easily be accomplished using a defaultdict
In [40]: from cycler import cycler as cy

In [41]: from collections import defaultdict

In [42]: cyl = cy('c', 'rgb') + cy('lw', range(1, 4))

To get a finite set of styles
In [43]: finite_cy_iter = iter(cyl)

In [44]: dd_finite = defaultdict(lambda : next(finite_cy_iter))
or repeating

In [45]: loop_cy_iter = cyl()

In [46]: dd_loop = defaultdict(lambda : next(loop_cy_iter))

This can be helpful when plotting complex data which has both a classification and a label
finite_cy_iter = iter(cyl)
styles = defaultdict(lambda : next(finite_cy_iter))
for group, label, data in DataSet:
    ax.plot(data, label=label, **styles[group])

which will result in every data with the same group being plotted with the same style.
",1,980,"Re the first part of the question,

So my question is: why can you do iter(my_cycler) and it gives something finite and why can you do my_cycler() and it gives the same thing but infinite (it loops).

the cycler class behaves like that because it's defined like that, look at this simpler example to see what I mean…
In [13]: class callable_list(list): 
    ...:     def __call__(self): 
    ...:         from itertools import cycle 
    ...:         return cycle(self)                                                       

In [14]: l = callable_list((1,2))                                                         

In [15]: l                                                                                
Out[15]: [1, 2]

In [16]: it_l = iter(l) ; cy_l = l()                                                      

In [17]: for _ in range(3): print(next(it_l))                                             
1
2
---------------------------------------------------------------------------
StopIteration                             Traceback (most recent call last)
&lt;ipython-input-17-e4324d8268a6&gt; in &lt;module&gt;
----&gt; 1 for _ in range(3): print(next(it_l))

StopIteration: 

In [18]: for _ in range(3): print(next(cy_l))                                             
1
2
1

and what is the ""why"" of this behavior?
We want to place an arbitrary number of Artists in an Axes and we want to be able to uniquely identify them, using different properties (not just color) to differentiate them.
The default is to call the Cycler object, and use the infinite cycle returned, allowing for some level of repetition but without the risk of a StopIteration exception being raised.
The alternate approach, showed in the example cited by the OP, is the use of iter on the Cycler object, this way repetitions are avoided but the coder has to anticipate the possibility of a StopIteration exception.

Addendum addressing an OP's comment


When I do cyl[1] with my Cycler object. I get: ValueError: Can only use slices with Cycler.__getitem__ What does that mean?

It means that the coders of the cycler module decided that a Cycler object can be addressed only by slices…

cyl[1:2] works, but strangely gives two elements back instead of one, cyl[1:1] gives back one element

This is not what I see… define two Cycler and their outer product
In [1]: from cycler import cycler 
   ...: c, w = cycler(c=['r','g','b']), cycler(lw=range(1,5)) 
   ...: cw = c*w 
   ...: print(c, w, cw, sep='\n')                                                         
cycler('c', ['r', 'g', 'b'])
cycler('lw', [1, 2, 3, 4])
(cycler('c', ['r', 'g', 'b']) * cycler('lw', [1, 2, 3, 4]))

and start with the single Cycler, first [1:2], for me it returns a single value
In [2]: c[1:2]                                                                            
Out[2]: cycler('c', ['g'])

and then [1:1], again a single value, that is an empty Cycler
In [3]: c[1:1] # empty cycler                                                             
Out[3]: cycler('c', [])

note that this is similar to slicing a list, in any case the value of the expression is a single object, a list, possibly an empty list… nothing new.
Finally, let's see what happens when slicing the outer product but first let's see what is inside it
In [5]: for i, d in enumerate(cw): print(i, d)                                            
0 {'c': 'r', 'lw': 1}
1 {'c': 'r', 'lw': 2}
2 {'c': 'r', 'lw': 3}
3 {'c': 'r', 'lw': 4}
4 {'c': 'g', 'lw': 1}
5 {'c': 'g', 'lw': 2}
6 {'c': 'g', 'lw': 3}
7 {'c': 'g', 'lw': 4}
8 {'c': 'b', 'lw': 1}
9 {'c': 'b', 'lw': 2}
10 {'c': 'b', 'lw': 3}
11 {'c': 'b', 'lw': 4}

Now we address it by slicing
In [6]: cw[3:9]                                                                           
Out[6]: (cycler('c', ['r', 'g', 'g', 'g', 'g', 'b']) + cycler('lw', [4, 1, 2, 3, 4, 1]))

The result is a single Cycler, the inner product of two Cycler, corresponding when unfolded to the positions 3÷8 of the enumerated printout above.
Finally, let's address cw using [3:3], i.e., an empty slice (spoiler, nothing good is going to happen!)
In [7]: cw[3:3]                                                                           
---------------------------------------------------------------------------
StopIteration                             Traceback (most recent call last)
&lt;ipython-input-7-1bf3cb52c570&gt; in &lt;module&gt;
----&gt; 1 cw[3:3]

~/lib/miniconda3/lib/python3.7/site-packages/cycler.py in __getitem__(self, key)
    219             trans = self.by_key()
    220             return reduce(add, (_cycler(k, v[key])
--&gt; 221                                 for k, v in six.iteritems(trans)))
    222         else:
    223             raise ValueError(""Can only use slices with Cycler.__getitem__"")

~/lib/miniconda3/lib/python3.7/site-packages/cycler.py in __add__(self, other)
    241             raise ValueError(""Can only add equal length cycles, ""
    242                              ""not {0} and {1}"".format(len(self), len(other)))
--&gt; 243         return Cycler(self, other, zip)
    244 
    245     def __mul__(self, other):

~/lib/miniconda3/lib/python3.7/site-packages/cycler.py in __init__(self, left, right, op)
    116         """"""
    117         if isinstance(left, Cycler):
--&gt; 118             self._left = Cycler(left._left, left._right, left._op)
    119         elif left is not None:
    120             # Need to copy the dictionary or else that will be a residual

~/lib/miniconda3/lib/python3.7/site-packages/cycler.py in __init__(self, left, right, op)
    133             self._right = None
    134 
--&gt; 135         self._keys = _process_keys(self._left, self._right)
    136         self._op = op
    137 

~/lib/miniconda3/lib/python3.7/site-packages/cycler.py in _process_keys(left, right)
     66         The keys in the composition of the two cyclers
     67     """"""
---&gt; 68     l_peek = next(iter(left)) if left is not None else {}
     69     r_peek = next(iter(right)) if right is not None else {}
     70     l_key = set(l_peek.keys())

StopIteration: 

In [8]:                                                                                   

In my humble opinion, this last feature very much resembles a bug…
",,
matplotlib strange result,https://stackoverflow.com/questions/62822121,Contour line error with plt.contour in python 3,"I am plotting a contour plot in python 3 with matplotlib, and I am getting a strange result. At first, I was using plt.contourf, and notices there was a strange north-south linear artifact in the data that I knew shouldn't be there (I used simulated data). So I changed plt.contourf to plt.contour, and the problem seems to be that some of the edge contours are deformed for some reason (see picture).

Unfortunately, it is hard for me to past a simple version of my code because this is part of a large GUI based app. Here is what I am doing though.
#grid the x,y,z data so it can be used in the contouring
self.beta_zi = 
#This is matplot griddata, not the scipy.interpolate.griddata
griddata(self.output_df['x'].values,self.output_df['y'].values,
                              self.output_df['Beta'].values,
                              self.cont_grid_x,
                              self.cont_grid_y,
                              interp='linear')

    #call to the contour itself
self.beta_contour=self.beta_cont_ax.contour(self.cont_grid_x,self.cont_grid_y,
                                  self.beta_zi,
                                  levels=np.linspace(start=0,stop=1, num=11, endpoint=True),
                                  cmap=cm.get_cmap(self.user_beta_cmap.get()))        

This seems like a simple problem based on the edges. Has anyone seen this before that can help. I am use a TK backend, which works better with the tkinter based GUI I wrote.
UPDATE: I also tried changing to scipy.interpolate.griddata because matplot's griddata is deprecated, but the problem is the same and persists, so it must be with the actual contour plotting function.
",1,571,"I found that the problem had to do with how I was interpreting the inputs of contour and grid data.
plt.contour and matplot.griddata takes 
x = x location of sample data
y = y location of sample data
z = height or z value of sample data
xi = locations of x tick marks on grid
zi = locations of y ticks marks on grid

Typically xi and yi are all the locatoins of each grid node, which is what I was supplying, but in this case you only need the unqiue tick marks on each axis.
Thanks to this post I figured it out.
Matplotlib contour from xyz data: griddata invalid index
",,
matplotlib strange result,https://stackoverflow.com/questions/62597540,How to disable &#39;zoom to rectangle&#39; programatically in matplotlib,"I'm trying to modify a pan function for matplotlib such that when pressing ctrl+leftbutton I would be able to pan the axes in the figure.
It works well.
The problem is that if the zoom button is pressed, then trying the above would also 'zoom to rectangle' would happen with strange result.
I want to automatically disable the 'zoom to rectangle' button when pressing the 'control' button.
Any ides how this can be accomplished?
Thanks!
Omri
",1,1315,"If the window comes from matplotlib.pyplot (rather than your own embedding) it will have a tool bar associated with it which you can access via
import matplotlib.pyplot as plt
fig, ax = plt.subplots()
tool_bar = fig.canvas.manager.toolbar

You can then set the mode attribute to '' (empty string)
However I suggest caution when doing this because if you change the mode during a pan or zoom things may not go well.
Another issue is that ctrl does not show up as a normal key, but as a modifier on another mouse or keyboard event (so we can not register a key press event to turn off the zoom).
Instead I suggest you do
if tool_bar.mode != '':
    print(""axes panning does not work with zoom enabled!"")
    return

in your callback.  It leaves it on the user to turn the zoom off, but it will work without having to reach into the (private) internals of the toolbar.
The relevant implementation is at https://github.com/matplotlib/matplotlib/blob/e73d4e056588abc201335d8a491fd9cb37d4c296/lib/matplotlib/backend_bases.py#L2781-L3275
",,
matplotlib strange result,https://stackoverflow.com/questions/60405991,Python matplotlib stacked bar chart -- strange results,"I'm trying to plot four stacked bar charts on the same plot using Python's matplotlib library.

For each observation (obs1, obs2, obs3, obs4), I want to view the quantity of each component (c1, c2, c3, c4, c5, c6) using a stacked bar chart. This is the code I have written:

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

data = np.array([[-904., 97., 59., 5., 252., 138.], [-603., 65., 0., 29., 0., 0.], [-571., -27., 0., -28., 0., 0.], [-80., 40., 0., -9., 0., 0.]])

data2 = pd.DataFrame(data=data)
data2.index = ['obs1', 'obs2', 'obs3', 'obs4']
data2.columns = ['c1', 'c2', 'c3', 'c4', 'c5', 'c6']

ind = np.arange(4)
width = 0.4

p1 = plt.bar(ind, data2['c1'], width)
p2 = plt.bar(ind, data2['c2'], width)
p3 = plt.bar(ind, data2['c3'], width)
p4 = plt.bar(ind, data2['c4'], width)
p5 = plt.bar(ind, data2['c5'], width)
p6 = plt.bar(ind, data2['c6'], width)

plt.legend((p1[0], p2[0], p3[0], p4[0], p5[0], p6[0]), tuple(data2.columns), bbox_to_anchor = (1.05, 1), loc = 'upper left', borderaxespad = 0.)


For clarity, this is the DataFrame (used to produce the plot):

print(data2)
         c1    c2    c3    c4     c5     c6
obs1 -904.0  97.0  59.0   5.0  252.0  138.0
obs2 -603.0  65.0   0.0  29.0    0.0    0.0
obs3 -571.0 -27.0   0.0 -28.0    0.0    0.0
obs4  -80.0  40.0   0.0  -9.0    0.0    0.0


This is the plot:



Note on the plot, the bar for obs1 is at x=0, the bar for obs2 is at x=1, and so on. 

However, there are two problems:


obs1 has a value of 252 for component 5, but the height of component 5 (in purple) is substantially below 252. How can I fix this?
obs3 has a value of -27 for component 2, but this is not shown on the plot at all. How can I solve this?


Thanks.
",1,257,,,
matplotlib strange result,https://stackoverflow.com/questions/56045406,How to properly set projection and transformation in cartopy geoaxes in matplotlib plotting,"I am facing serious difficulties in using Python geopandas, cartopy and matplotlib to work together in a proper plot of my shapefile data.

The issue comes from the difficulty in properly setting the Transform and the Projection objects of my shapefile data.

The example I am describing here is relative to a SHP in SIRGAS 2000 projection, whose WKT format is:

GEOGCS[""SIRGAS 2000"",DATUM[""D_SIRGAS_2000"",SPHEROID[""GRS_1980"",6378137,298.257222101]],PRIMEM[""Greenwich"",0],UNIT[""Degree"",0.017453292519943295]]

From the WKT above, one may see that my coordinates are in degrees. The globe is not the WGS84, but GRS_1980, which is similar (but not equal) to WGS84. 

This crs is not yet implemented in cartopy. So trying to use functions as cartopy.crs.EPSG(4674) doesn't work, especially because this is not a projection crs, but a geographic crs.

Following to the plotting Issue, I logically assumed that I should use one of the four cartopy.crs options to plot my data:

transform_option_1 = cartopy.crs.PlateCarree() # works well

transform_option_2 = cartopy.crs.PlateCarree(globe=ccrs.Globe(ellipse='GRS80')) # works well

transform_option_3 = cartopy.crs.Geodetic(globe=ccrs.Globe(ellipse='GRS80')) # doesn't work

# using wkt:

WKT = """"""GEOGCS[""SIRGAS 2000"",DATUM[""D_SIRGAS_2000"",SPHEROID[""GRS_1980"",6378137,298.257222101]],PRIMEM[""Greenwich"",0],UNIT[""Degree"",0.017453292519943295]]""""""

transform_option_4 = ccrs.PlateCarree(WKT) # results in an error message (see message in annex)



However, when I try the four transform options to plot my data, an error appears for my transform_option_3 and transform_option_4 options. 

The first two options (transform_option_1, transform_option_2), everything works well. Also when I compare the first two Transform options in the plot, they result in the same figure, though they should result in slightly different plots (due to the globe object). So here is my first question:

Question 1: why the ccrs.PlateCarree() object result in the same plot for my data, despite different Globe objects?

Question 2: since cartopy accepts WKT string format for transform object instanciation, why an error appear? See error message annexed below this message (error message number 1).

Question 3: what is the difference between geodetic and platecarree projections in cartopy? I can't figure out when I should use or the other. Since the description of both projections imply coordinates in degrees, their difference is strange to me. Also, when I try both options for plotting my SHP using cartopy, only the PlateCarree works. The other results in a black geoaxes.

Here is a code snippet for better clarification of my Question 3:


# importing libraries:
import geopandas as gpd
import matplotlib.pyplot as plt
import cartopy.crs as ccrs


# importing data:

SHP_path = 'Data_SHP.shp'

SHP = gpd.read_file(SHP_path)


# setting projection to PlateCarre:

projection = ccrs.PlateCarree() # projection.proj4_init


# setting SHP data transform

Transform2 = ccrs.Geodetic(globe=ccrs.Globe(ellipse='GRS80'))


# creating figure:

fig, ax = plt.subplots(1, subplot_kw={'projection':projection})


SHP.plot(ax=ax, transform=Transform2, 
         color='k', alpha=0.5)

# defining a small function to add grilines to the plot:

def custom_plot(geoaxes, projection):



    gl = geoaxes.gridlines(crs=projection)

    tick_axis_positions={'xlabels_top':False,
                     'ylabels_left':True,
                     'ylabels_right':False,
                     'xlabels_bottom':True}

    gl.xlabels_top = tick_axis_positions['xlabels_top']
    gl.ylabels_left = tick_axis_positions['ylabels_left']
    gl.ylabels_right= tick_axis_positions['ylabels_right']
    gl.xlabels_bottom = tick_axis_positions['xlabels_bottom']


    return geoaxes


# adding gridlines to the geoaxes:

ax = custom_plot(ax, projection)

fig.show()

# end of code




Question 4: Why can't I instanciate a cartopy Projection object using my crs (in WKT format)? When I try that, an error message appears (see error message 2 in annex).

Projection = ccrs.Projection(proj4_params=WKT, globe=ccrs.Globe(ellipse='GRS80'))



References:

1) discussion relating how to create cartopy.crs transform object from WKT or proj4 coordinate system projection formats

2) Mapping proj4 to cartopy CRS

3) Plotting a straight line in Cartopy, Robinson projection

4) example of plotting SHP with cartopy

5) cartopy transform and projection theory



Annex:

Error message 1)

TypeError: unsupported operand type(s) for -: 'str' and 'float'


Traceback (most recent call last):
  File ""C:\Users\lealp\AppData\Local\conda\conda\envs\Python_3.7\lib\site-packages\matplotlib\backends\backend_qt5.py"", line 519, in _draw_idle
    self.draw()
  File ""C:\Users\lealp\AppData\Local\conda\conda\envs\Python_3.7\lib\site-packages\matplotlib\backends\backend_agg.py"", line 402, in draw
    self.figure.draw(self.renderer)
  File ""C:\Users\lealp\AppData\Local\conda\conda\envs\Python_3.7\lib\site-packages\matplotlib\artist.py"", line 50, in draw_wrapper
    return draw(artist, renderer, *args, **kwargs)
  File ""C:\Users\lealp\AppData\Local\conda\conda\envs\Python_3.7\lib\site-packages\matplotlib\figure.py"", line 1649, in draw
    renderer, self, artists, self.suppressComposite)
  File ""C:\Users\lealp\AppData\Local\conda\conda\envs\Python_3.7\lib\site-packages\matplotlib\image.py"", line 138, in _draw_list_compositing_images
    a.draw(renderer)
  File ""C:\Users\lealp\AppData\Local\conda\conda\envs\Python_3.7\lib\site-packages\matplotlib\artist.py"", line 50, in draw_wrapper
    return draw(artist, renderer, *args, **kwargs)
  File ""C:\Users\lealp\AppData\Local\conda\conda\envs\Python_3.7\lib\site-packages\cartopy\mpl\geoaxes.py"", line 385, in draw
    inframe=inframe)
  File ""C:\Users\lealp\AppData\Local\conda\conda\envs\Python_3.7\lib\site-packages\matplotlib\artist.py"", line 50, in draw_wrapper
    return draw(artist, renderer, *args, **kwargs)
  File ""C:\Users\lealp\AppData\Local\conda\conda\envs\Python_3.7\lib\site-packages\matplotlib\axes\_base.py"", line 2628, in draw
    mimage._draw_list_compositing_images(renderer, self, artists)
  File ""C:\Users\lealp\AppData\Local\conda\conda\envs\Python_3.7\lib\site-packages\matplotlib\image.py"", line 138, in _draw_list_compositing_images
    a.draw(renderer)
  File ""C:\Users\lealp\AppData\Local\conda\conda\envs\Python_3.7\lib\site-packages\matplotlib\artist.py"", line 50, in draw_wrapper
    return draw(artist, renderer, *args, **kwargs)
  File ""C:\Users\lealp\AppData\Local\conda\conda\envs\Python_3.7\lib\site-packages\matplotlib\collections.py"", line 262, in draw
    transform, transOffset, offsets, paths = self._prepare_points()
  File ""C:\Users\lealp\AppData\Local\conda\conda\envs\Python_3.7\lib\site-packages\matplotlib\collections.py"", line 240, in _prepare_points
    for path in paths]
  File ""C:\Users\lealp\AppData\Local\conda\conda\envs\Python_3.7\lib\site-packages\matplotlib\collections.py"", line 240, in &lt;listcomp&gt;
    for path in paths]
  File ""C:\Users\lealp\AppData\Local\conda\conda\envs\Python_3.7\lib\site-packages\matplotlib\transforms.py"", line 2451, in transform_path_non_affine
    return self._a.transform_path_non_affine(path)
  File ""C:\Users\lealp\AppData\Local\conda\conda\envs\Python_3.7\lib\site-packages\cartopy\mpl\geoaxes.py"", line 173, in transform_path_non_affine
    src_path.vertices, self.source_projection)
  File ""C:\Users\lealp\AppData\Local\conda\conda\envs\Python_3.7\lib\site-packages\cartopy\crs.py"", line 746, in quick_vertices_transform
    bboxes, proj_offset = self._bbox_and_offset(src_crs)
  File ""C:\Users\lealp\AppData\Local\conda\conda\envs\Python_3.7\lib\site-packages\cartopy\crs.py"", line 709, in _bbox_and_offset
    lon_0_offset = other_lon_0 - self_lon_0
TypeError: unsupported operand type(s) for -: 'str' and 'float'
Traceback (most recent call last):
  File ""C:\Users\lealp\AppData\Local\conda\conda\envs\Python_3.7\lib\site-packages\matplotlib\backends\backend_qt5.py"", line 519, in _draw_idle
    self.draw()
  File ""C:\Users\lealp\AppData\Local\conda\conda\envs\Python_3.7\lib\site-packages\matplotlib\backends\backend_agg.py"", line 402, in draw
    self.figure.draw(self.renderer)
  File ""C:\Users\lealp\AppData\Local\conda\conda\envs\Python_3.7\lib\site-packages\matplotlib\artist.py"", line 50, in draw_wrapper
    return draw(artist, renderer, *args, **kwargs)
  File ""C:\Users\lealp\AppData\Local\conda\conda\envs\Python_3.7\lib\site-packages\matplotlib\figure.py"", line 1649, in draw
    renderer, self, artists, self.suppressComposite)
  File ""C:\Users\lealp\AppData\Local\conda\conda\envs\Python_3.7\lib\site-packages\matplotlib\image.py"", line 138, in _draw_list_compositing_images
    a.draw(renderer)
  File ""C:\Users\lealp\AppData\Local\conda\conda\envs\Python_3.7\lib\site-packages\matplotlib\artist.py"", line 50, in draw_wrapper
    return draw(artist, renderer, *args, **kwargs)
  File ""C:\Users\lealp\AppData\Local\conda\conda\envs\Python_3.7\lib\site-packages\cartopy\mpl\geoaxes.py"", line 385, in draw
    inframe=inframe)
  File ""C:\Users\lealp\AppData\Local\conda\conda\envs\Python_3.7\lib\site-packages\matplotlib\artist.py"", line 50, in draw_wrapper
    return draw(artist, renderer, *args, **kwargs)
  File ""C:\Users\lealp\AppData\Local\conda\conda\envs\Python_3.7\lib\site-packages\matplotlib\axes\_base.py"", line 2628, in draw
    mimage._draw_list_compositing_images(renderer, self, artists)
  File ""C:\Users\lealp\AppData\Local\conda\conda\envs\Python_3.7\lib\site-packages\matplotlib\image.py"", line 138, in _draw_list_compositing_images
    a.draw(renderer)
  File ""C:\Users\lealp\AppData\Local\conda\conda\envs\Python_3.7\lib\site-packages\matplotlib\artist.py"", line 50, in draw_wrapper
    return draw(artist, renderer, *args, **kwargs)
  File ""C:\Users\lealp\AppData\Local\conda\conda\envs\Python_3.7\lib\site-packages\matplotlib\collections.py"", line 262, in draw
    transform, transOffset, offsets, paths = self._prepare_points()
  File ""C:\Users\lealp\AppData\Local\conda\conda\envs\Python_3.7\lib\site-packages\matplotlib\collections.py"", line 240, in _prepare_points
    for path in paths]
  File ""C:\Users\lealp\AppData\Local\conda\conda\envs\Python_3.7\lib\site-packages\matplotlib\collections.py"", line 240, in &lt;listcomp&gt;
    for path in paths]
  File ""C:\Users\lealp\AppData\Local\conda\conda\envs\Python_3.7\lib\site-packages\matplotlib\transforms.py"", line 2451, in transform_path_non_affine
    return self._a.transform_path_non_affine(path)
  File ""C:\Users\lealp\AppData\Local\conda\conda\envs\Python_3.7\lib\site-packages\cartopy\mpl\geoaxes.py"", line 173, in transform_path_non_affine
    src_path.vertices, self.source_projection)
  File ""C:\Users\lealp\AppData\Local\conda\conda\envs\Python_3.7\lib\site-packages\cartopy\crs.py"", line 746, in quick_vertices_transform
    bboxes, proj_offset = self._bbox_and_offset(src_crs)
  File ""C:\Users\lealp\AppData\Local\conda\conda\envs\Python_3.7\lib\site-packages\cartopy\crs.py"", line 709, in _bbox_and_offset
    lon_0_offset = other_lon_0 - self_lon_0
TypeError: unsupported operand type(s) for -: 'str' and 'float'



Error message 2)

---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
&lt;ipython-input-165-9916b2ddcc67&gt; in &lt;module&gt;
----&gt; 1 ccrs.Projection(proj4_params=Transform_from_wkt, globe=ccrs.Globe(ellipse='GRS80'))

TypeError: Can't instantiate abstract class Projection with abstract methods boundary, threshold, x_limits, y_limits


",1,4596,,,
matplotlib strange result,https://stackoverflow.com/questions/55010236,`set_major_locator` removes x-ticks (and labels) from previous subplots?,"I ran into this strange problem with using set_major_locator(), when using subplots which have different x-axis limits. A minimal example:

import matplotlib.pyplot as pl
import matplotlib.dates as mdates
from datetime import datetime

h24 = mdates.HourLocator(interval=24)
fmt = mdates.DateFormatter('%d-%m %H:%M')

start1 = datetime(year=2016, month=7, day=7, hour=0)
end1   = datetime(year=2016, month=7, day=9, hour=0)

start2 = datetime(year=2016, month=9, day=30, hour=0)
end2   = datetime(year=2016, month=10, day=2, hour=0)

start3 = datetime(year=2016, month=5, day=8,  hour=0)
end3   = datetime(year=2016, month=5, day=10, hour=0)

pl.figure(figsize=(9,3))

ax=pl.subplot(131)
ax.set_xlim(start1, end1)
ax.xaxis.set_major_locator(h24)
ax.xaxis.set_major_formatter(fmt)

ax=pl.subplot(132)
ax.set_xlim(start2, end2)
ax.xaxis.set_major_locator(h24)
ax.xaxis.set_major_formatter(fmt)

ax=pl.subplot(133)
ax.set_xlim(start3, end3)
ax.xaxis.set_major_locator(h24)
ax.xaxis.set_major_formatter(fmt)

pl.tight_layout()


Which results in:



If I set the x-limit of all sub-plots the same (using in this case ax.set_xlim(start1, end1) for all sub-plots) it works as expected:



Also, leaving the different set_xlim()'s and removing the set_major_locator() and set_major_formatter() lines works (although I get unreadable x-labels in this case..):



Am I making a silly mistake somewhere, or are the missing x-ticks and labels in my first example a bug in Matplotlib?

p.s. Matplotlib 3.0.2, Python 3.7.2
",1,2576,,,
matplotlib strange result,https://stackoverflow.com/questions/53177039,Scatterplot of pandas DataFrame ends in KeyError: 0,"After I updated pandas (0.23.4) and matplotlib (3.01) I get a strange error trying to do something like the following:

import pandas as pd
import matplotlib.pyplot as plt


clrdict = {1: ""#a6cee3"", 2: ""#1f78b4"", 3: ""#b2df8a"", 4: ""#33a02c""}

df_full = pd.DataFrame({'x':[20,30,30,40],
                        'y':[25,20,30,25],
                        's':[100,200,300,400],
                        'l':[1,2,3,4]})

df_full['c'] = df_full['l'].replace(clrdict)

df_part = df_full[(df_full.x == 30)]

fig = plt.figure()
plt.scatter(x=df_full['x'],
            y=df_full['y'],
            s=df_full['s'],
            c=df_full['c'])
plt.show()

fig = plt.figure()
plt.scatter(x=df_part['x'],
            y=df_part['y'],
            s=df_part['s'],
            c=df_part['c'])
plt.show()


The scatterplot of the original DataFrame (df_full) is shown without problems. But the plot of the partially DataFrame raises the following error:

Traceback (most recent call last):
  File ""G:\data\project\test.py"", line 27, in &lt;module&gt;
    c=df_part['c'])
  File ""C:\Program Files\Python37\lib\site-packages\matplotlib\pyplot.py"", line 2864, in scatter
    is not None else {}), **kwargs)
  File ""C:\Program Files\Python37\lib\site-packages\matplotlib\__init__.py"", line 1805, in inner
    return func(ax, *args, **kwargs)
  File ""C:\Program Files\Python37\lib\site-packages\matplotlib\axes\_axes.py"", line 4195, in scatter
    isinstance(c[0], str))):
  File ""C:\Program Files\Python37\lib\site-packages\pandas\core\series.py"", line 767, in __getitem__
    result = self.index.get_value(self, key)
  File ""C:\Program Files\Python37\lib\site-packages\pandas\core\indexes\base.py"", line 3118, in get_value
    tz=getattr(series.dtype, 'tz', None))
  File ""pandas\_libs\index.pyx"", line 106, in pandas._libs.index.IndexEngine.get_value
  File ""pandas\_libs\index.pyx"", line 114, in pandas._libs.index.IndexEngine.get_value
  File ""pandas\_libs\index.pyx"", line 162, in pandas._libs.index.IndexEngine.get_loc
  File ""pandas\_libs\hashtable_class_helper.pxi"", line 958, in pandas._libs.hashtable.Int64HashTable.get_item
  File ""pandas\_libs\hashtable_class_helper.pxi"", line 964, in pandas._libs.hashtable.Int64HashTable.get_item
KeyError: 0


This is due to the color-option c=df_part['c']. When you leave it out – the problem doesn't occur. This hasn't happend before the updates, so maybe you're not able to reproduce this with lower versions of matplotlib or pandas (I have no idea which one causes it).

In my project the df_part = df_full[(df_full.x == i)] line is used within the update-function of a matplotlib.animation.FuncAnimation. The result is an animation over the values of x (which are timestamps in my project). So I need a way to part the DataFrame.
",1,1019,,,
matplotlib strange result,https://stackoverflow.com/questions/48355655,OpenCV error while converting image,"I am trying to import a picture of size (540,960) using matplotlib.
This step is successfully executed. The result is stored in 'image' object (type ndarray). 

# Do relevant imports
import matplotlib.pyplot as plt
import matplotlib.image as mpimg
import numpy as np
import cv2

# Read in and grayscale the image
image = mpimg.imread(r'C:\Temp\pic24_bw.jpg')
gray = cv2.cvtColor(image,cv2.COLOR_RGB2GRAY)


But when I try to convert the image into another color space (Gray), using cv2.cvtColor(). I face a error:

error: C:\projects\opencv-python\opencv\modules\imgproc\src\color.cpp:11111:        error: (-215) scn == 3 || scn == 4 in function cv::cvtColor


Please help. Strange thing is this code is running successfully in another citrix environment.
",1,1330,,,
matplotlib strange result,https://stackoverflow.com/questions/45356848,scipy curve_fit not producing smooth graph when fitting fourier function,,1,611,,,
matplotlib strange result,https://stackoverflow.com/questions/20050413,Python issue with matplotlib Pie Chart Function (erraneous labels),,1,1497,,,
matplotlib strange result,https://stackoverflow.com/questions/19556888,bounding box problems saving to pdf,,1,4361,,,
matplotlib strange result,https://stackoverflow.com/questions/69741863,Issue plotting a simple tensor with Torch,"Just beginning to use Pyorch, and I am trying to plot a very simple, 1-D array Tensor onto a histogram with Matplotlib.
 torch.manual_seed(8436) 
 a = torch.Tensor(1000) 

 a.normal_(0, 2.) #This will fill our array with a normal distribution
 plt.hist(a);

However, the result is strange..., and just consists of a bunch of vertical, multicolored lines.
The result I am supposed to get, which I do when entering:
plt.hist(a.numpy())

is the normal histogram.
Thanks in advance for any help!
",0,264,"ok, it seems to me that here is the reason:
cbook._reshape_2D is used to preprocess the data coming into plt.hist . in 3.4.3  it returns a list of arrays with only one element each, which obviously produces the wrong image above.
in 3.2.2 , however, it returns a list with one 1D array, basically a NumPy version of the tensor we provided. and this one is plotted as expected.
I downgraded the package and it worked. Would be interested to hear other solutions.
",,
matplotlib strange result,https://stackoverflow.com/questions/62190948,Triangulation in Matplotlib with Provided Traingles,"I have data as follow: x, y, triangles

On MATLAB and using the trisurf function, I provide (triangles, x, y) to get the result, very straight forward.

On Matplotlib however, if I use the same data I get:


Triangles number out of bound (if I use existing triangles)
Strange line outside my geometry (red line) if I let tri.triangulation de decide on the triangles. (Delaunay triangulation).


maybe my problem requires different approach, I would be happy if someone can direct me to a better solution.

import matplotlib.pyplot as plt
import matplotlib.tri as tri
import pandas as pd
import numpy as np

dlel = pd.read_fwf('HSH_Calculation.LEL')
dlnd = pd.read_fwf('HSH_Calculation.LND')
dt = pd.read_fwf('HSH_Calculation.HTD')

xy = np.asarray(dlnd.iloc[:, 3:5])

x = xy[:, 0]
y = xy[:, 1]
triangles = np.asarray(dlel.iloc[:, 1:4])
# triang = tri.triangulation(x,y)
triang = tri.Triangulation(x, y)


plt.figure()
plt.gca().set_aspect('equal')
# plt.triplot(x, y, triangles, 'go-', lw=1.0)
plt.triplot(triang, 'go-', lw=1.0)
plt.title('triplot of user-specified triangulation')
plt.xlabel('Longitude (degrees)')
plt.ylabel('Latitude (degrees)')

plt.show()


[Calculation files][1]



[1]: https://drive.google.com/file/d/1HPlSu6HYzVpIgtT7y_maeNESUa5y1WW9/view?usp=sharing, https://drive.google.com/file/d/1YKFxfkU1iIkEfXPs9nZd6f-STkJNqT2Q/view?usp=sharing, https://drive.google.com/file/d/1njxGiYqucUv4YyhY6H0U35lfuN2_zPfw/view?usp=sharing
",0,425,"Solved!
Finally discovered that this is not the right way to approach the problem.
I had to use patches.Polygon to create separately each triangle.
I later added a color map as per the function output (z).

super happy that it worked!
",,
matplotlib strange result,https://stackoverflow.com/questions/60067958,Photutils DAOPhot Not Fitting stars well?,"I recently ran across the PhotUtils package and am trying to use it to perform PSF Photometry on some images I have. However, when I try to run the code, I get very strange results. When I plot the image generated by get_residual_image(), the stars are not removed well. Some sample images are shown below. 

The first image has sigma set to 2.05, as it is in one of the sample programs in the PhotUtils documentation: 

However, the stars only appear to be removed in their center.

The second image has sigma set to 5.0. This one is especially strange. Some stars are way over-removed, some are under removed, some black squares are added to the image, etc.


Here is my code:

import photutils
from photutils.psf import DAOPhotPSFPhotometry as DAOP
from photutils.psf import IntegratedGaussianPRF as PRF
from photutils.background import MMMBackground

bkg = MMMBackground()
background = 2.5*bkg(img)
gaussian_prf = PRF(sigma=5.0)
gaussian_prf.sigma.fixed = False
photTester = DAOP(8,background,5,gaussian_prf,31)
photResults = photTester(imgStars)

finalImg = photTester.get_residual_image()


After this, I simply plot the original and final image in MatPlotLib. I use a greyscale colormap.  The reason that the left images appear slightly darker is that they use a different color scaling.

Perhaps I have set one of the parameters incorrectly?

Could someone help me out with this? Thank you!
",0,521,"Looking at the residual image instantly told me that the background subtraction might be wrong. I could reproduce the result and wondered, if MMMBackground did not do the job correctly.
After taking a closer look at the documentation, Getting startet with Photutils finally gave the essential hint:

image -= np.median(image)  

",,
matplotlib strange result,https://stackoverflow.com/questions/54575834,Plot with Matplotlib using List - Datetime - Different Behaviour on Format,"Why Matplotlib has this strange behaviour with Date Data Type? 


  Matplotlib allows you to natively plots python datetime instances, and for the most part does a good job picking tick locations and string formats. 
  From the documentation  ""Fixing common date annoyances""


I also read this question that gave me some clues related to Matplotlib Date Format.
I also read the most voted questions about matplotlib and Datetime but I still don't understand the following behaviour. 

#timestamp is a &lt;class 'list'&gt;
timestamp=['2019-02-04', '2019-01-15', '2018-10-08', '2018-07-09',
           '2018-04-09', '2018-02-08', '2017-09-08', '2017-09-08',
           '2017-07-07', '2017-04-07', '2017-01-09', '2016-10-07',
           '2016-07-01', '2016-03-25', '2015-12-27', '2015-09-25',
           '2015-06-26', '2015-03-27', '2014-12-24', '2014-10-06',
           '2014-07-02', '2014-03-28', '2013-12-20', '2013-09-27',
           '2013-06-11', '2013-03-27', '2012-12-27', '2012-09-26',
           '2012-06-13', '2012-03-28', '2011-12-14', '2011-09-28',
           '2011-06-14', '2011-03-30', '2010-12-15', '2010-09-29',
           '2010-06-19', '2010-03-31', '2009-12-29', '2009-09-30',
           '2009-06-17', '2009-04-01', '2008-12-20', '2008-08-25',
           '2008-08-25', '2008-06-19', '2008-03-19', '2008-03-19',
           '2006-04-11', '2005-12-27', '2005-09-28', '2005-07-02',
           '2005-04-20', '2004-12-21', '2004-10-20', '2004-07-21',
           '2003-09-22', '2003-08-20', '2002-12-31']

#time_python is a &lt;class 'datetime.datetime'&gt;
time_python=[datetime.strptime(d, ""%Y-%m-%d"") for d in timestamp]
#time_series is a &lt;class 'pandas.core.indexes.datetimes.DatetimeIndex'&gt;
time_series=pd.to_datetime(timestamp)

array=np.arange(1,len(timestamp)+1) 

time_2_num=mdates.date2num(time_series.to_pydatetime())

#First plot using the List Format as x axes
plt.subplot(411)
plt.bar(timestamp,array)
plt.xticks(rotation='vertical')

#Second plot using the padas Datatime Format as x axes
plt.subplot(412)
plt.bar(time_series,array)
plt.xticks(rotation='vertical')
plt.subplots_adjust(hspace = 1.2)

#Third plot using the DateTime Format as x axes 
plt.subplot(413)
plt.bar(time_python,array)
plt.xticks(rotation='vertical')
plt.subplots_adjust(hspace = 1.2)

#Fourth plot using the Matplot Date Format as x axes 
plt.subplot(414)
plt.bar(time_2_num,array)
plt.xticks(rotation='vertical')
plt.subplots_adjust(hspace = 1.2)

plt.gcf().autofmt_xdate()  

plt.show()


The desired result is obviously the first plot.



I want to understand better why the bars of the II,III,IV plot has this representation, different from the I. The y input is the same for the 4 plots.
",0,229,,,
matplotlib strange result,https://stackoverflow.com/questions/47957314,matplotlib / scipy produces weird results when calculating and plotting multiple data sets (phase portraits of ODEs),,0,326,,,
matplotlib strange result,https://stackoverflow.com/questions/20494505,language and toolkit for fast plotting of live data from serial port,,0,3061,,,
matplotlib strange result,https://stackoverflow.com/questions/14345908,strange looking plots in Matplotlib and django,,0,313,,,
matplotlib strange issue,https://stackoverflow.com/questions/64833558,Apps not popping up on macOS Big Sur 11.0.1,"It is always risky to upgrade your operation system. It is likely you will encounter some compatibility issue. I took the risk to upgrade my macOS from Catalina to the newest Big Sur. After that, the display in the new OS looks pretty, but all my PyQt5 apps could not be launched in this new OS. The GUI window does not pop up as usual, and there is no error message showing in the terminal. I spent the whole day trying to figure out what makes this problem. I found the solution but in a weird way which I feel confused.
It turns out that the apps comes back to normal after I add the following three lines in the main script.
import matplotlib
import matplotlib.pyplot as plt

matplotlib.use('TkAgg')

It seems to me the new OS has some compatibility issue with Qt5Agg back-end. But the strange thing is that this solution also works for one of the Pyqt5 app, where I don't use matplotlib at all.
The Python version I used is 3.8.4, and the PyQt5 version I have is 5.15.1.
I hope somebody could explain to me what happen under the hood that makes this solution work. Also I hope this temporary solution can help somebody with the same problem.
",12,3866,"A reply to the PyQt mailing list pointed out that setting this env var works:
QT_MAC_WANTS_LAYER=1

Found via Is there any solution regarding to PyQt library doesn't work in Mac OS Big Sur? and https://forums.macrumors.com/threads/pyqt5-and-big-sur.2260773/?post=29243620#post-29243620
","I can confirm that matplotlib.use('TkAgg') followed by matplotlib.use('Qt5Agg') makes things work for me, too. I whittled it down to this as also working:
# from matplotlib.backends import _tkagg
import _tkinter
import matplotlib.pyplot as plt
plt.figure()

So it's something about the compiled _tkinter module. Maybe the inputhook?
","As @Eric said, just add the following on the very start of your code, before the PySide2 import:
import os
os.environ[""QT_MAC_WANTS_LAYER""] = ""1""

Then import PyQt5/PySide2.
"
matplotlib strange issue,https://stackoverflow.com/questions/16905028,Why is matplotlib plot produced from ipython notebook slightly different from terminal version?,"I have a strange issue. Using IPython Notebook, I created a quite extensive script using pandas and matplotlib to create a number of charts.
When my tinkering was finished, I copied (and cleaned) the code into a standalone python script (so that I can push it into the svn and my paper co-authors can create the charts as well).

For convenience, I import the standalone python script into the notebook again and create a number of charts: 

import create_charts as cc
df = cc.read_csv_files(""./data"")
cc.chart_1(df, 'fig_chart1.pdf')
...


Strange enough, the .pdf file I get using the above method is slightly different from the .pdf file I get when I run my standalone python script from my Windows 7 terminal. The most notable difference is that in a particular chart the legend is located in the upper corner instead of the lower corner. But there are other small diferences as well (bounding box size, font seems slightly different)

What could be the cause of this. And how can I troubleshoot it?
(I already shut down my notebook and restarted it, to reimport my create_charts script and rule out any unsaved changes) 
My terminal reports I am using Python 2.7.2, and pip freeze | grep ipython reports ipython 0.13.1
",9,5153,,,
matplotlib strange issue,https://stackoverflow.com/questions/11075436,"How do I connect discontinous curves in matplotlab, scipy, or etc",,6,464,,,
matplotlib strange issue,https://stackoverflow.com/questions/7450881,python segmentation fault when closing / quitting,,6,7563,,,
matplotlib strange issue,https://stackoverflow.com/questions/48289129,Removing space around wedge polar plots in Matplotlib,"I am starting to play around with creating polar plots in Matplotlib that do NOT encompass an entire circle - i.e. a ""wedge"" plot - by setting the thetamin and thetamax properties. This is something I was waiting for for a long time, and I am glad they have it done :)

However, I have noticed that the figure location inside the axes seem to change in a strange manner when using this feature; depending on the wedge angular aperture, it can be difficult to fine tune the figure so it looks nice.

Here's an example:

import numpy as np
import matplotlib.pyplot as plt

# get 4 polar axes in a row
fig, axes = plt.subplots(2, 2, subplot_kw={'projection': 'polar'},
                         figsize=(8, 8))

# set facecolor to better display the boundaries
# (as suggested by ImportanceOfBeingErnest)
fig.set_facecolor('paleturquoise')

for i, theta_max in enumerate([2*np.pi, np.pi, 2*np.pi/3, np.pi/3]):

    # define theta vector with varying end point and some data to plot
    theta = np.linspace(0, theta_max, 181)
    data = (1/6)*np.abs(np.sin(3*theta)/np.sin(theta/2))

    # set 'thetamin' and 'thetamax' according to data
    axes[i//2, i%2].set_thetamin(0)
    axes[i//2, i%2].set_thetamax(theta_max*180/np.pi)

    # actually plot the data, fine tune radius limits and add labels
    axes[i//2, i%2].plot(theta, data)
    axes[i//2, i%2].set_ylim([0, 1])
    axes[i//2, i%2].set_xlabel('Magnitude', fontsize=15)
    axes[i//2, i%2].set_ylabel('Angles', fontsize=15)

fig.set_tight_layout(True)
#fig.savefig('fig.png', facecolor='skyblue')




The labels are in awkward locations and over the tick labels, but can be moved closer or further away from the axes by adding an extra labelpad parameter to set_xlabel, set_ylabel commands, so it's not a big issue.

Unfortunately, I have the impression that the plot is adjusted to fit inside the existing axes dimensions, which in turn lead to a very awkward white space above and below the half circle plot (which of course is the one I need to use).

It sounds like something that should be reasonably easy to get rid of - I mean, the wedge plots are doing it automatically - but I can't seem to figure it out how to do it for the half circle. Can anyone shed a light on this?



EDIT: Apologies, my question was not very clear; I want to create a half circle polar plot, but it seems that using set_thetamin() you end up with large amounts of white space around the image (especially above and below) which I would rather have removed, if possible.

It's the kind of stuff that normally tight_layout() takes care of, but it doesn't seem to be doing the trick here. I tried manually changing the figure window size after plotting, but the white space simply scales with the changes. Below is a minimum working example; I can get the xlabel closer to the image if I want to, but saved image file still contains tons of white space around it.

Does anyone knows how to remove this white space?

import numpy as np
import matplotlib.pyplot as plt

# get a half circle polar plot
fig1, ax1 = plt.subplots(1, 1, subplot_kw={'projection': 'polar'})

# set facecolor to better display the boundaries
# (as suggested by ImportanceOfBeingErnest)
fig1.set_facecolor('skyblue')

theta_min = 0
theta_max = np.pi

theta = np.linspace(theta_min, theta_max, 181)
data = (1/6)*np.abs(np.sin(3*theta)/np.sin(theta/2))

# set 'thetamin' and 'thetamax' according to data
ax1.set_thetamin(0)
ax1.set_thetamax(theta_max*180/np.pi)

# actually plot the data, fine tune radius limits and add labels
ax1.plot(theta, data)
ax1.set_ylim([0, 1])
ax1.set_xlabel('Magnitude', fontsize=15)
ax1.set_ylabel('Angles', fontsize=15)

fig1.set_tight_layout(True)
#fig1.savefig('fig1.png', facecolor='skyblue')






EDIT 2: Added background color to figures to better show the boundaries, as suggested in ImportanteOfBeingErnest's answer.
",5,2367,,,
matplotlib strange issue,https://stackoverflow.com/questions/26444318,How to point pyinstaller to the right versions of MSVC?90.dll?,"I have a python application that I am trying to build as a pyinstaller distributable. A similar script builds successfully on Linux.

I am building it on Windows 7 x64, but want to build 32-bit binary for better compatibility, so I am using 32-bit python-2.7. Among my dependencies are matplotlib and pyside which require MSVC. I install a package called VCForPython27 from Microsoft.

I run into an error when I run my pyinstaller script. I get the following message:


1250 INFO: Adding Microsoft.VC90.CRT to dependent assemblies of final executable
7428 INFO: Searching for assembly x86_Microsoft.VC90.CRT_1fc8b3b9a1e18e3b_9.0.21022.8_none ...
7428 WARNING: Assembly not found
7428 ERROR: Assembly x86_Microsoft.VC90.CRT_1fc8b3b9a1e18e3b_9.0.21022.8_none not found
7475 WARNING: lib not found: MSVCR90.dll dependency of C:\Python27\python.exe
7553 INFO: Searching for assembly x86_Microsoft.VC90.CRT_1fc8b3b9a1e18e3b_9.0.21022.8_none ...
7553 WARNING: Assembly not found
7553 ERROR: Assembly x86_Microsoft.VC90.CRT_1fc8b3b9a1e18e3b_9.0.21022.8_none not found
7662 WARNING: lib not found: MSVCR90.dll dependency of C:\Windows\system32\python27.dll
7662 INFO: Analyzing C:\Python27\lib\site-packages\PyInstaller\loader\_pyi_boots



There are multiple messages like that about both the files MSVCP90.dll and MSVCR90.dll

I can see that I have a folder C:\Windows\winsxs\x86_microsoft.vc90.crt_1fc8b3b9a1e18e3b_9.0.30729.4148_none_5090ab56bcba71c2 that contains versions of both files.

This mismatch occurs both when I install my python packages from Christoph Gohlke's page and with pip (except for matplotlib, which I can't install with pip because of missing dependencies).

Strangely enough pyinstaller makes a binary. Yet, when I try to run it I get a popup saying:


WARNING: file already exists but should not:
C:\Users\Martin\AppData\Local\Temp\_MEI34922\Include\pyconfig.h



Does anyone know how I can do any of the following:


Install the precious x86_Microsoft.VC90.CRT_1fc8b3b9a1e18e3b_9.0.21022.8_none assembly? Where can I take this specific version from?
Tell python to look for the other version (x86_microsoft.vc90.crt_1fc8b3b9a1e18e3b_9.0.30729.4148_none_5090ab56bcba71c2)?
Solve the pyconfig.h unwanted presence issue? Doesn't seem to lead anywhere, but I thought I should try it too.
Find another way to build my code to a binary? It's a complicated code, running external binaries, but if I have to I will try py2exe, not sure that it would be any better though.

",5,4610,,,
matplotlib strange issue,https://stackoverflow.com/questions/11972096,matplotlib log scales causes missing points,"I'm having a really strange issue with matplotlib. Plotting some points looks like this:



When I switch to a log scale on the y-axis, some of the points are not connected:



Is this a bug? Am I missing something? Code is below. Comment out the log scale line to see the first graph.

import matplotlib.pyplot as plt

fig = plt.figure()
ax1 = fig.add_subplot(111)

x = [1.0, 2.0, 3.01, 4.01, 5.01, 6.01, 7.04, 8.04, 9.04, 10.05,
     11.05, 12.09, 13.17, 14.18, 15.73, 16.74, 17.74, 18.9, 19.91,
     20.94, 22.05, 23.15, 24.33, 25.48, 26.51, 27.58, 28.86, 29.93,
     30.93, 32.23, 33.25, 34.26, 35.27, 36.29, 37.33, 38.35, 39.36,
     40.37, 41.37]
y = [552427, 464338, 446687, 201960, 227238, 265140, 148903, 134851,
     172234, 120263, 115385, 100671, 164542, 171176, 28, 356, 0, 0,
     195, 313, 9, 0, 132, 0, 249, 242, 81, 217, 159, 140, 203, 215,
     171, 141, 154, 114, 99, 97, 97]

ax1.plot(x, y, c='b', marker='o')

ax1.set_yscale('log')
plt.ylim((-50000, 600000))
plt.show()

",4,2974,"You can try to use ax1.set_yscale('symlog')

",,
matplotlib strange issue,https://stackoverflow.com/questions/25767469,tkinter and matplotlib: windows not showing until program closes under Linux,,3,2400,,,
matplotlib strange issue,https://stackoverflow.com/questions/63028779,Error using tight layout with nested Matplotlib Gridspec,"I'm having a strange issue with Matplotlib Gridspec. I am making a plot with a nested grid, and I would like a tight/constrained layout to avoid tick label overlap:

My idea was to use tight_layout, which worked fine before I had the nested Gridspec. Now, however I call it, I get an error. Here's a MWE that replicates the problem:
import matplotlib.pyplot as plt
import matplotlib.gridspec as gs
import numpy as np

fig = plt.figure(figsize=(15,10), constrained_layout=True)
grid = gs.GridSpec(8,5, width_ratios=[1,2,1.5,1,1], figure=fig)

a = plt.subplot(grid[0:3,0])
b = plt.subplot(grid[0:4,1])
c = plt.subplot(grid[4:,0:2])
d = plt.subplot(grid[0:2,2:])
e = plt.subplot(grid[2:4,2:])
f = plt.subplot(grid[4:6,2:])
g = plt.subplot(grid[6:,2:])

# create nested grid plot
nested_grid = gs.GridSpecFromSubplotSpec(5,5, subplot_spec=c, wspace=0, hspace=0)
c.get_xaxis().set_ticks([])
c.get_yaxis().set_ticks([])
for k in range(25):
    x, y = np.unravel_index(k, (5, 5))
    gax = fig.add_subplot(nested_grid[x, y])
    gax.set_xticklabels('')
    gax.set_yticklabels('')
    gax.set_xticks([])
    gax.set_yticks([])

grid.tight_layout(fig)

The final line here is what's suggested in the Gridspec docs; previously, I wasn't setting a figure, and I just used plt.tight_layout(), which threw an error and led me to explore the Gridspec-specific tight_layout() method. But I still get the same error:
---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
&lt;ipython-input-14-1002aa79c5cf&gt; in &lt;module&gt;
     25     gax.set_yticks([])
     26 
---&gt; 27 grid.tight_layout(fig)

~/miniconda3/lib/python3.8/site-packages/matplotlib/gridspec.py in tight_layout(self, figure, renderer, pad, h_pad, w_pad, rect)
    412         """"""
    413 
--&gt; 414         subplotspec_list = tight_layout.get_subplotspec_list(
    415             figure.axes, grid_spec=self)
    416         if None in subplotspec_list:

~/miniconda3/lib/python3.8/site-packages/matplotlib/tight_layout.py in get_subplotspec_list(axes_list, grid_spec)
    247         if hasattr(axes_or_locator, ""get_subplotspec""):
    248             subplotspec = axes_or_locator.get_subplotspec()
--&gt; 249             subplotspec = subplotspec.get_topmost_subplotspec()
    250             gs = subplotspec.get_gridspec()
    251             if grid_spec is not None:

~/miniconda3/lib/python3.8/site-packages/matplotlib/gridspec.py in get_topmost_subplotspec(self)
    611         gridspec = self.get_gridspec()
    612         if hasattr(gridspec, ""get_topmost_subplotspec""):
--&gt; 613             return gridspec.get_topmost_subplotspec()
    614         else:
    615             return self

~/miniconda3/lib/python3.8/site-packages/matplotlib/gridspec.py in get_topmost_subplotspec(self)
    483         Return the topmost `.SubplotSpec` instance associated with the subplot.
    484         """"""
--&gt; 485         return self._subplot_spec.get_topmost_subplotspec()
    486 
    487 

AttributeError: 'AxesSubplot' object has no attribute 'get_topmost_subplotspec'

Note: I have tried using constrained_layout() with the plt.figure() call, which makes no difference (and does not work when I specify a figure in the first Gridspec call.)
EDIT: See answer below for the solution! Here's what it looks like now:

",2,1406,"The problem why you got this error is that the subplot_spec parameter type in gs.GridSpecFromSubplotSpec(5,5, subplot_spec=c) needs to be matplotlib.gridspec.SubplotSpec. But the type of c is matplotlib.axes._subplots.AxesSubplot.
Change c to grid[4:,0:2] could solve the problem.
gs.GridSpecFromSubplotSpec(5, 5, subplot_spec=grid[4:,0:2])

",,
matplotlib strange issue,https://stackoverflow.com/questions/62853745,Matplotlib interference figure strange pattern,"I am trying to make an interference figure animation like this one:

The difference is that the above image shows the interference figure over time, so the constructive and destructive interference points remain fixed. On the contrary, I am trying to make an animation where I change the frequency of the two sources, keeping them fixed in the space.
Here is my code:
import numpy as np
import matplotlib.pyplot as plt
from matplotlib.animation import FuncAnimation

source = 0.5
sources = [-source, source]
axlim = max(sources)*2 + 1
N = 1000

x = np.linspace(-axlim, axlim, N)
y = np.linspace(-axlim, axlim, N)
X, Y = np.meshgrid(x, y)

fig = plt.figure()

def update(f):
    plt.gca().cla()
    C1 = np.sin(2*np.pi*f*((X - sources[0])**2 + Y**2))
    C2 = np.sin(2*np.pi*f*((X - sources[1])**2 + Y**2))
    Z = C1 + C2

    plt.contour(X, Y, Z)

    plt.plot(sources, [0, 0], 'ro')
    plt.gca().set_aspect('equal')
    plt.axis('off')

ani = FuncAnimation(fig = fig, func = update, frames = 11, interval = 100)

plt.show()


The issue is that strange patterns appear like in the last frames:

those patterns are not physical (they are not consistent with physics' laws), so there must be an error in my code. I cannot find out where, but I think the issue is in the matplotlib contour function: I suspect it introduce a sort of aliasing...
",2,2713,"The issue is in your definition of C1 and C2: you apply the sinusoidal function to the sum of the squared x and y distances, without applying a squared root. You should use:
C1 = np.sin(2*np.pi*f*np.sqrt((X - sources[0])**2 + Y**2))
C2 = np.sin(2*np.pi*f*np.sqrt((X - sources[1])**2 + Y**2))


There is no issue with the contour plot method, however, I suggest you to replace it with contourf or, even better, imshow. The reason is that contour plot lines where your field has the same value, keeping the rest of the plot empty. On the contrary, contourf or imshow fill the empty space with the colormap you choose, so you can display your field in a better way and avoid ambiguous white spaces.
See this code for reference:
import numpy as np
import matplotlib.pyplot as plt
from matplotlib.colors import LinearSegmentedColormap
from matplotlib.animation import FuncAnimation

source = 0.5
sources = [-source, source]
axlim = max(sources)*2 + 1
N = 1000

x = np.linspace(-axlim, axlim, N)
y = np.linspace(-axlim, axlim, N)
X, Y = np.meshgrid(x, y)

norm = plt.Normalize(-2, 2)
cmap = LinearSegmentedColormap.from_list('', ['black', 'white', 'black'])
fig, ax = plt.subplots()

def update(f):
    ax.cla()
    C1 = np.sin(2*np.pi*f*np.sqrt((X - sources[0])**2 + Y**2))
    C2 = np.sin(2*np.pi*f*np.sqrt((X - sources[1])**2 + Y**2))
    Z = C1 + C2

    ax.imshow(Z,
              cmap = cmap,
              norm = norm)
    ax.plot(N/2*(1 + source/axlim), N/2, 'ro')
    ax.plot(N/2*(1 - source/axlim), N/2, 'ro')

    ax.set_title(f'f = {f} Hz')
    ax.set_aspect('equal')
    ax.axis('off')

ani = FuncAnimation(fig = fig, func = update, frames = 11, interval = 100)

plt.show()


Since peaks and valleys of your field are both constructive interference points, while in the destructive points the field is null, I chose a black - white - black colormap, you cannot distinguish peaks from valleys but it is easier to distinguish constructive from destructive interference points.
",,
matplotlib strange issue,https://stackoverflow.com/questions/53350917,matplotlib storing and removing artist,"I have encountered a strange issue with matplotlib artists.

Disclaimer: Unfortunately I only ever used matplotlib in jupyter notebooks and in a tkinter GUI (the latter is where I found this) so I do not know how to write simple code that will replicate the issue. However I do not think sample is code is absolutely needed in this case.

Now the issue:

In the interest of speeding up plotting in a GUI I do not plot everything anew whenever elements of the plot change but rather make use of methods like set_ydata and canvas.draw. Sometimes it is also necessary to remove lines altogether which can be accomplished with artist.remove. Here is the problem: When I have one or several artist(s) stored in a list I can successfully remove them from the plot by iterating over the list and calling remove. If however I store the reference directly (as an attribute of the class that is managing plots), calling remove does not do anything.

As sketch of the code suppose we have

import matplotlib.pyplot as plt
fig = plt.figure()
ax = fig.add_subplot()


the first case is generated by something like

artist_list = list()
for x in range(5):
    line = ax.axhline(x)
    artist_list.append(line)


and can be removed by 

for line in artist_list:
    line.remove()
artist_list = list()


(the last is needed for this to work).

whereas the second would be 

line = ax.axhline(1)
line.remove()


which does not remove the line from the plot (even if del line or line = None are added).

It seems that storing the artist in a list and then assigning that variable to a new empty list is somehow a more complete removal than reassigning the variable that stores the artist directly or even deleting it. Does somebody know what is going here? How could a line be removed if it is simply stored as line rather than in a list?
",2,4218,"As can be seen from the below piece of code, removing a line is pretty easy. Indeed, you just call .remove() on the object in question and redraw the canvas.

import matplotlib.pyplot as plt

fig, ax = plt.subplots()
ax.set(title=""Click to remove line"", xlim=(0,2))

line=ax.axvline(1)

def remove_line(event):
    line.remove()
    fig.canvas.draw()

fig.canvas.mpl_connect(""button_press_event"", remove_line)

plt.show()

",,
matplotlib strange issue,https://stackoverflow.com/questions/44056833,matplotlib.scatter color argument not accepting numpy array,"I have written the following python plotting script using matplotlib:

import pynbody as pyn
import numpy as np
import matplotlib.pyplot as plt
import glob

s = pyn.load('./ballsV2.00001')
sl = s.g[np.where((s.g['z'] &lt; 0.005) &amp; (s.g['z']&gt;-0.005))]

sx = s.s['x'][0]
sy = s.s['y'][0]
sz = s.s['z'][0]
r2 = ((s.g['x']-sx)**2+(s.g['y']-sy)**2+(s.g['z']-sz)**2)
Flux = np.array(1./(4*np.pi*r2)*np.exp(-1*7.00114988051*np.sqrt(r2)))

print(type(np.log10(sl['radFlux'])))
print(type(np.log10(Flux)))

plt.figure(figsize = (15,12))
#plt.scatter(sl['x'],sl['y'],c=np.log10(sl['radFlux']),s=75,edgecolors='none', marker = '.',vmin=-6,vmax=1)
plt.scatter(sl['x'],sl['y'],c=np.log10(Flux),s=75,edgecolors='none', marker = '.',vmin=-8,vmax=4)
plt.xlim([-0.5,0.5])
plt.ylim([-0.5,0.5])
plt.xlabel(""x"")
plt.ylabel(""y"")
plt.colorbar(label=""log(Code Flux)"")
plt.savefig('./ballsV2_0.1.pdf')
plt.savefig('./ballsV2_0.1.png')
plt.show()
plt.close()


When I run the script I get the following error:

foo@bar ~/Data/RadTransfer/Scaling_Tests/ballsV2 $ py 
balls.py 
balls.py:15: RuntimeWarning: divide by zero encountered in log10
    print(type(np.log10(sl['radFlux'])))
&lt;class 'numpy.ndarray'&gt;
&lt;class 'numpy.ndarray'&gt;

Traceback (most recent call last):
  File ""/home/grondjj/anaconda3/lib/python3.6/site-packages/matplotlib/colors.py"", line 141, in to_rgba
    rgba = _colors_full_map.cache[c, alpha]
KeyError: (-4.1574455411341349, None)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/home/grondjj/anaconda3/lib/python3.6/site-packages/matplotlib/colors.py"", line 192, in _to_rgba_no_colorcycle
    c = tuple(map(float, c))
TypeError: 'numpy.float64' object is not iterable

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""balls.py"", line 17, in &lt;module&gt;
    plt.scatter(sl['x'],sl['y'],c=np.log10(Flux),s=75,edgecolors='none', marker = '.',vmin=-8,vmax=4)
  File ""/home/grondjj/anaconda3/lib/python3.6/site-packages/matplotlib/pyplot.py"", line 3435, in scatter
    edgecolors=edgecolors, data=data, **kwargs)
  File ""/home/grondjj/anaconda3/lib/python3.6/site-packages/matplotlib/__init__.py"", line 1892, in inner
    return func(ax, *args, **kwargs)
  File ""/home/grondjj/anaconda3/lib/python3.6/site-packages/matplotlib/axes/_axes.py"", line 4028, in scatter
    alpha=alpha
  File ""/home/grondjj/anaconda3/lib/python3.6/site-packages/matplotlib/collections.py"", line 890, in __init__
    Collection.__init__(self, **kwargs)
  File ""/home/grondjj/anaconda3/lib/python3.6/site-packages/matplotlib/collections.py"", line 139, in __init__
    self.set_facecolor(facecolors)
  File ""/home/grondjj/anaconda3/lib/python3.6/site-packages/matplotlib/collections.py"", line 674, in set_facecolor
    self._set_facecolor(c)
  File ""/home/grondjj/anaconda3/lib/python3.6/site-packages/matplotlib/collections.py"", line 659, in _set_facecolor
    self._facecolors = mcolors.to_rgba_array(c, self._alpha)
  File ""/home/grondjj/anaconda3/lib/python3.6/site-packages/matplotlib/colors.py"", line 237, in to_rgba_array
    result[i] = to_rgba(cc, alpha)
  File ""/home/grondjj/anaconda3/lib/python3.6/site-packages/matplotlib/colors.py"", line 143, in to_rgba
    rgba = _to_rgba_no_colorcycle(c, alpha)
  File ""/home/grondjj/anaconda3/lib/python3.6/site-packages/matplotlib/colors.py"", line 194, in _to_rgba_no_colorcycle
    raise ValueError(""Invalid RGBA argument: {!r}"".format(orig_c))
ValueError: Invalid RGBA argument: -4.1574455411341349


Ignore the divide by zero stuff,the issue is the scatter plot function isn't taking my array of values to map colour to. What is strange is that the commented out scatter plot command above it runs fine. The only difference is the array of values I am passing it. I made sure to cast them to the same type (they are both &lt;class 'numpy.ndarray'&gt;). Also, the values themselves are more sane ranging between ~4000 and 1E-7 in the Flux array, it is only the np.log10(sl['radFlux'] that has the divide by zero errors and that one works. Any suggestions?
",2,3020,,,
matplotlib strange issue,https://stackoverflow.com/questions/43891136,Timeserie datetick problems when using pandas.DataFrame.plot method,"I just discovered something really strange when using plot method of pandas.DataFrame. I am using pandas 0.19.1. Here is my MWE:

import numpy as np
import matplotlib.pyplot as plt
import matplotlib.dates as mdates
import pandas as pd

t = pd.date_range('1990-01-01', '1990-01-08', freq='1H')
x = pd.DataFrame(np.random.rand(len(t)), index=t)

fig, axe = plt.subplots()
x.plot(ax=axe)
plt.show(axe)

xt = axe.get_xticks()


When I try to format my xticklabels I get strange beahviours, then I insepcted objects to understand and I have found the following:


t[-1] - t[0] = Timedelta('7 days 00:00:00'), confirming the DateTimeIndex is what I expect;
xt = [175320, 175488], xticks are integers but they are not equals to a number of days since epoch (I do not have any idea about what it is);
xt[-1] - xt[0] = 168 there are more like index, there is the same amount that len(x) = 169.


This explains why I cannot succed to format my axe using:

axe.xaxis.set_major_locator(mdates.HourLocator(byhour=(0,6,12,18)))
axe.xaxis.set_major_formatter(mdates.DateFormatter(""%a %H:%M""))


The first raise an error that there is to many ticks to generate
The second show that my first tick is  Fri 00:00 but it should be Mon 00:00 (in fact matplotlib assumes the first tick to be 0481-01-03 00:00, oops this is where my bug is).



It looks like there is some incompatibility between pandas and matplotlib integer to date conversion but I cannot find out how to fix this issue.

If I run instead:

fig, axe = plt.subplots()
axe.plot(x)
axe.xaxis.set_major_formatter(mdates.DateFormatter(""%a %H:%M""))
plt.show(axe)

xt = axe.get_xticks()


Everything works as expected but I miss all cool features from pandas.DataFrame.plot method such as curve labeling, etc. And here xt = [726468.  726475.].

How can I properly format my ticks using pandas.DataFrame.plot method instead of axe.plot and avoiding this issue?

Update

The problem seems to be about origin and scale (units) of underlying numbers for date representation. Anyway I cannot control it, even by forcing it to the correct type:

t = pd.date_range('1990-01-01', '1990-01-08', freq='1H', origin='unix', units='D')


There is a discrepancy between matplotlib and pandas representation. And I could not find any documentation of this problem.
",2,383,,,
matplotlib strange issue,https://stackoverflow.com/questions/44682789,Python Bar Chart not Displaying Correctly (Matplotlib),"I have some data I'd like to display in 2 stacked bar charts (side by side).  Using matplotlib.
keys=['BF', 'VL', 'GM', 'VM']
data=[[0.10992407597195027, 0.084754817342900857],
 [0.20119173770112833, 0.24308696457787043],
 [0.49912704691619575, 0.53468456580435009],
 [0.18975713941072578, 0.13747365227487865]]
x = range(2)

f, ax1 = plt.subplots(1, figsize=(10,5))

plt.bar(x,data[0], label=keys[0])
plt.bar(x,data[1], bottom=data[0], label=keys[1])
plt.bar(x,data[2], bottom=data[1], label=keys[2])
plt.bar(x,data[3], bottom=data[2],label=keys[3])

plt.legend(loc='upper left')
plt.show()

The above code displays the following graph:

It seems to me that the red bar (VM) has not been stacked on top of the rest of the bars, leading to the strange look of the graph (despite me specifying the correct bottom attribute.  The data in each bar should sum to 1 exactly.  How can I fix this?
EDIT:
I was able to fix the issue by setting the bottom attribute to the sum of the lower bars, rather than just the lower one explicitly, using the following rather messy code:
plt.bar(x,data[0], width = bar_width, label=keys[0])
plt.bar(x,data[1], width = bar_width, bottom=data[0], label=keys[1])
plt.bar(x,data[2], width = bar_width, bottom=[data[0][i]+data[1][i] for i in x], label=keys[2])
plt.bar(x,data[3], width = bar_width, bottom=[data[0][i]+data[1][i]+data[2][i] for i in x],label=keys[3]

Is there a better way to do this?  It doesn't scale well with more bars.
",1,1261,,,
matplotlib strange issue,https://stackoverflow.com/questions/29300510,matplotlib UserWarning When log axis is used in some cases,"Basically I am running some optimisation algorithms that I have created using Numpy and I want to plot the log of the error against the number of iterations. Having done this with linear regression and having had no issues, it is very strange that I seem to get issues when doing the exact same thing with logistic regression. I get the following ""warning"":  


  /usr/lib64/python2.6/site-packages/matplotlib/axis.py:1004:
  UserWarning: Unable to find pixel distance along axis for interval
  padding; assuming no interval padding needed.   warnings.warn(""Unable
  to find pixel distance along axis for interval padding; assuming no
  interval padding needed."")


However, when I don't use a log axis for the y axis, I don't get the error either. All the elements of the array that I am using are also positive, so it shouldn't have anything to do with taking the log of a non-positive number.

Has anyone ever encountered this before? Does anyone know what it may be referring to?

Thanks
",1,1329,"not sure if this issue is still open, but I had a similar problem and updating seaborn fixed the issue for me. For you it might be matplotlib, depending which package you used for creating the graph.
",,
matplotlib strange issue,https://stackoverflow.com/questions/71410497,How to plot data from a .kml file using matplotlib on python 3.7 and Windows 10&quot;?,"I will first give a little bit of context to my problem.
I have obtained a .kml file of the territorial seas around the world on the site : https://www.marineregions.org/downloads.php, and I would like to display it not on Google Earth but on a matplotlib.pyplot plot (with a cartopy map if possible too). The .kml file looks like this :
&lt;?xml version=""1.0"" encoding=""UTF-8""?&gt;
&lt;kml xmlns=""http://www.opengis.net/kml/2.2"" xmlns:gx=""http://www.google.com/kml/ext/2.2"" xmlns:kml=""http://www.opengis.net/kml/2.2"" xmlns:atom=""http://www.w3.org/2005/Atom""&gt;
&lt;NetworkLink&gt;
    &lt;name&gt;Territorial Seas (12NM) v3&lt;/name&gt;
    &lt;description&gt;&lt;![CDATA[Flanders Marine Institute (2019). Maritime Boundaries Geodatabase: Territorial Seas (12NM), version 3. Available online at &lt;a href=""http://www.marineregions.org""&gt;http://www.marineregions.org&lt;/a&gt; https://doi.org/10.14284/387. Consulted on YYYY-MM-DD.]]&gt;&lt;/description&gt;
    &lt;Link&gt;
        &lt;href&gt;http://geo.vliz.be/geoserver/gwc/service/kml/MarineRegions:eez_12nm.png.kml&lt;/href&gt;
    &lt;/Link&gt;
&lt;/NetworkLink&gt;
&lt;/kml&gt;

For that I saws on this other stackoverflow question (Reading KML Files Using Fastkml) that using fastkml to read the file was possible.
So this is the test.py code I am trying to run (it comes from the usage guide https://fastkml.readthedocs.io/en/latest/usage_guide.html#read-a-kml-file-string):
from fastkml import  kml

filename = ""C:\\Users\\dumasal\\Documents\\GOOGLE_EARTH\\MarineRegions-eez_12nm.kml""
with open(filename, 'rt', encoding=""utf-8"") as myfile:
    doc=myfile.read()
    print(doc)
    
    # Create the KML object to store the parsed result
    k = kml.KML()
    
    # Read in the KML string
    k.from_string(doc)
    print('k = ', k)
    
    ### Next we perform some simple sanity checks ###
    
    # Check that the number of features is correct
    # This corresponds to the single ``Document``
    features = list(k.features())
    print(len(features))
    
    # Check that we can access the features as a generator
    # (The two Placemarks of the Document)
    print(features[0].features())
    f2 = list(features[0].features())
    print(len(f2))
    
    # Check specifics of the first Placemark in the Document
    print(f2[0])
    print(f2[0].description)
    print(f2[0].name)
    
    # Check specifics of the second Placemark in the Document
    print(f2[1].name)
    f2[1].name = ""ANOTHER NAME""
    
    # Verify that we can print back out the KML object as a string
    print(k.to_string(prettyprint=True))

When I ran it I got the error: ""ValueError: Unicode strings with encoding declaration are not supported. Please use bytes input or XML fragments without declaration."".
I looked the error up on google and found this git-hub page (https://github.com/cleder/fastkml/issues/57) where they were saying that the ""from_string()"" function only takes bytes so the beginning of my code could be changed to :
from fastkml import  kml

filename = ""C:\\Users\\dumasal\\Documents\\GOOGLE_EARTH\\MarineRegions-eez_12nm.kml""
with open(filename, 'r') as myfile:
    doc=myfile.read().encode('UTF-8')
    print(doc)
    
    # Create the KML object to store the parsed result
    k = kml.KML()
    
    # Read in the KML string
    k.from_string(doc)
    print('k = ', k)

    ### Next we perform some simple sanity checks ###
    
    # Check that the number of features is correct
    # This corresponds to the single ``Document``
    features = list(k.features())
    print(len(features))

And strangely enough the ValueError stopped appearing. However now I get the error :
IndexError: list index out of range

this is because my variables features = [], and I don't know why.
So could you either explain to me why the features variable is empty, or a more direct method to plot a .kml file with python and matplotlib?
Thank you very much !
",0,2848,"One issue is the KML file you have is a super-overlay, which is auto-generated as multiple KML ""files"" referenced as NetworkLinks in sub regions and few KML python packages support recursive NetworkLinks directly. The fastkml module you're using does not implement NetworkLinks so the content is skipped.
The pyKML package can parse the KML file and iterate over the KML layers referenced in the Network Links.
You can do something like this to iterate over the NetworkLinks and the KML content.
import requests
import re
from pykml import parser

count = 0

def walk(elt):
    global count
    for elt in elt.getchildren():
        tag = re.sub(r'^.*\}', '', elt.tag)
        if tag in ['Folder', 'Document']:
            walk(elt)
        elif tag == 'NetworkLink':
            print(tag)
            print(elt.Link.href)
            if count == 10:
                # for debugging stop after 10 links
                raise Exception(""too many links"")
            count += 1
            response = requests.get(elt.Link.href)
            walk(parser.fromstring(response.content))
        elif tag == 'GroundOverlay':
            print(tag)
            # do something with the ground overlay
        else:
            # other tag; e.g. Region, comment, etc
            print(""&gt;&gt;"", tag)

with open(""MarineRegions-eez_12nm.kml"", 'rb') as myfile:
    root = parser.parse(myfile).getroot()
walk(root)

The MarineRegions KML is a super-overlay that recursively sub-divides into smaller regions so trying to plot the GroundOverlay at all levels will be overwrite the larger low-res overlays with smaller hi-res overlays. You need to decide if you only want the hi-res ground overlays or the low-res overlays. For example, if the KML content at a given level doesn't have any NetworkLinks then it's at the lowest level.
Note GeoServer has two different types of super-overlays: raster and vector. The KML you're using is a raster super-overlay. You may want to check if there is a vector overlay available in the case dealing with vectors might be easier than dealing with ground overlay images.
",,
matplotlib strange issue,https://stackoverflow.com/questions/63495493,Plotting global contour data that crosses both the meridian and the dateline,"So I have two different datasets.  I have one that is a polar orbiting satellite that starts near the north pole goes down towards Africa then goes over the south pole and back up the Atlantic ocean.  I also have a Global composite of a number of Geostationary satellites.  That grid is a full global coverage from -90 to 90 latitude, and 0 to -0.2 (around the dateline) longitudes.
So when I plot my polar orbiting data.  I get a funny striping across the northern hemisphere where the satellite crosses the dateline.  Like this...
polar orbiting data
I did a search and found that if I do this to my longitude values
lons[lons &lt; 0] += 360

that it then removes the strange striping.  I think it actually just moves the stripes to the southern hemisphere (where is crosses the meridian), but since that area is all ""missing"" values it doesn't show on the plot, so I don't care.
polar orbiting data - longitudes all positive
So now there is the problem of the full Global dataset.  When I plot it without converting all the longitudes to positive values, it has quite a few stripes going on.  I believe it is every place there is rain that crosses the dateline.
Global dataset
When I try to apply the ""fix"" that worked for the polar data I get a really nasty very uninformative error.
IllegalArgumentException: Invalid number of points in LinearRing found 3 - must be 0 or &gt;= 4

Process finished with exit code 139 (interrupted by signal 11: SIGSEGV)

There is no traceback or anything, so I'm not even sure if it is matplotlib or cartopy or what that is throwing the error.
I've tried cropping the image by using set_extent() but it still has the stripes.
Any ideas how to fix this, or how to trick matplotlib/cartopy into plotting the data correctly?

Adding code, but not sure how much it will actually be of help.  It is pretty basic.  The issue is more with the fact that the rain data covers the full globe.
normal_proj = ccrs.PlateCarree(central_longitude=0)
pos_lons = lon_data.copy()
pos_lons[pos_lons &lt; 0] += 360
cs = plt.contourf(pos_lons, lat_data, rr_data, clevs, cmap=cmap,
                      transform=normal_proj, norm=norm)

",0,434,"I think you're running into an annoying, but difficult to track down bug in CartoPy. Basically, in transforming the contours and clipping them to the map extent it generates some Shapely geometries incorrectly. Some things that have worked to get around the problem (not solve it) include slightly adjusting the map boundaries and adjusting the contour levels.
",,
matplotlib strange issue,https://stackoverflow.com/questions/52344307,Matplotlib: fill_between drawing rounded edges in SVG when color is set,"I've encountered a strange issue when using matplotlibs fill_between with data that has a tight gap. For example, when I simply want to plot the following data and save it as SVG, it works:

import matplotlib.pyplot as plt

plt.fill_between([0,1e4-1000,1e4,1e4+1000,1e6], [1000,1000,1,1000,1000])
plt.savefig(""test.svg"")




But when I set the color to any value, the gap disappears:

import matplotlib.pyplot as plt

plt.fill_between([0,1e4-1000,1e4,1e4+1000,1e6], [1000,1000,1,1000,1000], color='tab:red')
plt.savefig(""test_colored.svg"")




After zooming in I noticed the edge line is drawn with rounded borders:


How can I change the plot color without this gap disappearing?
I've tried to set different kwargs like capstyle, joinstyle, antialiased, rasterized,... to all possible values without success.
",0,311,"You are setting the color of the patch. This would include the edgecolor and the facecolor. However, it seems you only want to have the face colored. Hence use facecolor='tab:red'

import matplotlib.pyplot as plt

plt.fill_between([0,1e4-1000,1e4,1e4+1000,1e6], [1000,1000,1,1000,1000], facecolor='tab:red')
plt.savefig(""test_colored.svg"")
plt.show()



",,
matplotlib strange issue,https://stackoverflow.com/questions/45042722,Cartopy + Matplotlib (contourf) - Map Overriding data,"I'm trying to do a Contour Plot having the Global Map in background. 
Having in mind that my data have LON and LAT values, I decided to use Cartopy with MatplotLib.

The problem is that I can plot my data and the map perfectly when separated, but when I try to integrate the data with the map the Cartopy map override my data plot.

This is my code: 

ax = plt.axes(projection=cartopy.crs.PlateCarree())

v = np.linspace(0, 80, 25, endpoint=True)
cp = plt.contourf(matrixLon, matrixLat, matrixTec, v, transform=cartopy.crs.PlateCarree())
plt.colorbar(cp)

ax.add_feature(cartopy.feature.LAND)
ax.add_feature(cartopy.feature.OCEAN)
ax.add_feature(cartopy.feature.COASTLINE)
ax.add_feature(cartopy.feature.BORDERS, linestyle=':')
ax.set_extent([-85, -30, -60, 15])

plt.title('TEC Map')
plt.show()


Plots:

Plotting only Data

Plotting only the map

It is strange because I think that the logical is the data override the map (and maybe I have to try a transparent color scale) but not the other way around. 

Can someone help me with this issue?
",0,3708,,,
matplotlib strange issue,https://stackoverflow.com/questions/43844361,Plot legend goes out of window while zooming in Matplotlib plot window,"I am using Python 2.7 and plotting using matplotlib package. I have a plot with legend at top. When i try to zoom the plot window to see more information in the plot, sometimes the legend moves out of the window. I am not able to figure out why this happens. This does not happen all the time though. I have attached the code. The lists x11,x22...., temp11,...temp25 and humi11,...humi25 are data from database. The plot is displayed correctly. However i am facing issues with strange situation of legend moving out of plot window. I have also attached the portion of the image for  your reference. Kindly provide your insights. 

This is the normal view of the plot.
. 

This is the view of the plot after zooming. See the part of the legend is visible on the corner of window.



Regards

Sriniketh

import matplotlib
import matplotlib.pyplot as plt,mpld3
import matplotlib.dates as mdates
import numpy as np
from matplotlib.backends.backend_tkagg importFigureCanvasTkAgg,NavigationToolbar2TkAgg
from matplotlib.backend_bases import key_press_handler
from matplotlib.figure import Figure
import tkMessageBox   
from drawnow import *
from Tkinter import *
import  Tkinter as tk
from tkinter import ttk
from dateutil.parser import parse
import decimal
plt.subplot(211)
plt.title('Temperatur')
plt.plot((x11), (temp11), 'ro-', label='M1-Node 1')
plt.plot((x12), (temp12), 'go-', label='M1-Node 2')
plt.plot((x13), (temp13), 'bo-', label='M1-Node 3')
plt.plot((x14), (temp14), 'co-', label='M1-Node 4')
plt.plot((x15), (temp15), 'mo-', label='M1-Node 5')
plt.plot((x21), (temp21), 'ro--', label='M2-Node 1')
plt.plot((x22), (temp22), 'go--', label='M2-Node 2')
plt.plot((x23), (temp23), 'bo--', label='M2-Node 3')
plt.plot((x24), (temp24), 'co--', label='M2-Node 4')
plt.plot((x25), (temp25), 'mo--', label='M2-Node 5')

plt.ylabel('Temperatur Grad C')
# plt.legend(loc='upper left')
plt.legend(bbox_to_anchor=(0.985, 0.89),
           bbox_transform=plt.gcf().transFigure)
plt.grid(True)

plt.subplot(212)
plt.title('Relativ Luftfeuchtigkeit')
plt.plot((x11), (humi11), 'ro-', label='M1-Node 1')
plt.plot((x12), (humi12), 'go-', label='M1-Node 2')
plt.plot((x13), (humi13), 'bo-', label='M1-Node 3')
plt.plot((x14), (humi14), 'co-', label='M1-Node 4')
plt.plot((x15), (humi15), 'mo-', label='M1-Node 5')
plt.plot((x21), (humi21), 'rd-', label='M2-Node 1')
plt.plot((x22), (humi22), 'gd-', label='M2-Node 2')
plt.plot((x23), (humi23), 'bd-', label='M2-Node 3')
plt.plot((x24), (humi24), 'cd-', label='M2-Node 4')
plt.plot((x25), (humi25), 'md-', label='M2-Node 5')
plt.ylabel('Relativ Luftfeuchtigkeit')
plt.grid(True)
master = var1.get()
plt.suptitle(""Temperatur und Luftfeuchtigkeit - %s "" % (master))

plt.show()

",0,845,,,
matplotlib strange issue,https://stackoverflow.com/questions/20503889,"OpenCV 1.1, IplImage can not show image correctly",,0,452,,,
matplotlib strange issue,https://stackoverflow.com/questions/14419193,"hg-git can pull from forked repo, but not original repo",,0,296,,,
matplotlib strange issue,https://stackoverflow.com/questions/46077477,Python only printing last element of the list,,-2,817,,,
matplotlib strange issue,https://stackoverflow.com/questions/41050643,Bug in matplotlib scatter plot script,,-2,755,,,
matplotlib inconsistent behavior,https://stackoverflow.com/questions/25240972,IPython keyboard interrupt CTRL + C inconsistent,"I'm getting inconsistent responses from the Keyboard interrupt Ctrl+C in interactive python (run in xterm) after plotting from matplotlib.

As expected, when executing Ctrl+C inside ipython2 I get the KeyboardInterrupt message.

However, once I plot anything using matplotlib (specifically matplotlib.pyplot) Ctrl+C will exit the interactive python session, as opposed to exiting the running script in the interactive python session (if there is one).

A primitive example.

import numpy as n
import matplotlib.pyplot as m
x = n.linspace(0,4*n.pi,500)
y = x**2*n.sin(x)

m.plot(x,y)
m.show()


Preferrable behavior would be for Ctrl+C to always only interrupt a running script (if any is running), instead of the interactive python session itself.
",12,4423,"I did have the same problem for a long time. Try to run ipython with qt:

ipython --matplotlib=qt

",,
matplotlib inconsistent output,https://stackoverflow.com/questions/42249945,Matplotlib 2 inconsistent font,"I updated Anaconda Python to the latest version (4.3), where they upgraded Matplotlib to version 2.

The upgrade has made some major changes to the default style (see here).
And, while I really like some of those changes, I am not in agreement with a few of them.

Hence I did some modifications, as suggested in the link above:

#%matplotlib inline
#%config InlineBackend.figure_format = 'svg'
import scipy as sc
import matplotlib.pyplot as plt
import matplotlib

# http://matplotlib.org/users/dflt_style_changes.html
params = {'legend.fontsize': 18,
          'axes.labelsize': 18,
          'axes.titlesize': 18,
          'xtick.labelsize' :12,
          'ytick.labelsize': 12,
          'mathtext.fontset': 'cm',
          'mathtext.rm': 'serif',
          'grid.color': 'k',
          'grid.linestyle': ':',
          'grid.linewidth': 0.5,
         }
matplotlib.rcParams.update(params)

x = sc.linspace(0,100)
y = x**2
fig = plt.figure('Fig')
ax = fig.add_subplot(1, 1, 1)
lines = ax.semilogy(x, y)
ax.set_yticks([300], minor=True)
ax.yaxis.grid(True, which='minor')
ax.yaxis.set_minor_formatter(matplotlib.ticker.ScalarFormatter())
ax.tick_params(axis='y', pad=10)
ax.set_xlabel(r'$\mathrm{R_L}$')
ax.set_ylabel(r'$\sigma \int_l \; dx$')
#fig.savefig('./PNG/test.png', dpi=300, bbox_inches='tight')


Using Latex as the axes labels, as in the code above, results in a figure with inconsistent text on axes (see the following image).



How to get back to the previous behaviour (see the image below) or to a consistent font scheme?



EDIT:
Using the Latex back-end I am able to get a good result, but it is extremely slow. 
Anyway, I think the internal back-end should be able to get a consistent output and switching to a different back-end is not a real solution, but more a workaround.

To use the latex back-end:

#%matplotlib inline
#%matplotlib notebook
#%config InlineBackend.figure_format = 'svg'
import scipy as sc
import matplotlib.pyplot as plt
import matplotlib

# http://matplotlib.org/users/dflt_style_changes.html
params = {'legend.fontsize': 18,
          'axes.labelsize': 18,
          'axes.titlesize': 18,
          'xtick.labelsize' :12,
          'ytick.labelsize': 12,
          'mathtext.fontset': 'cm',
          'mathtext.rm': 'serif',
          'grid.color': 'k',
          'grid.linestyle': ':',
          'grid.linewidth': 0.5,
         }
matplotlib.rcParams.update(params)
matplotlib.rcParams.update({'text.usetex':True, 'text.latex.preamble':[r'\usepackage{amsmath, newtxmath}']})

x = sc.linspace(0,100)
y = x**2
fig = plt.figure('Fig')
ax = fig.add_subplot(1, 1, 1)
lines = ax.semilogy(x, y)
ax.set_yticks([300], minor=True)
ax.yaxis.grid(True, which='minor')
ax.yaxis.set_minor_formatter(matplotlib.ticker.ScalarFormatter())
ax.tick_params(axis='y', pad=10)
ax.set_xlabel(r'$\mathrm{R_L}$')
ax.set_ylabel(r'$\sigma \int_l \; dx$')
#fig.savefig('./PNG/test.png', dpi=300, bbox_inches='tight')


The result with matplotlib 2 is:



The resulting plot with the older version is (still a bit different, maybe due to some latex differences):



But again, the desired result is what obtained from an older version of matplotlib and in displayed in figure 2.
",6,2405,"If consistency is the only issue, you can use a ""Roman"" style using the ""Times"" font. It is not necessary to use Latex via usetex. Instead simply use the STIX fontset, the Times font and serif mathtext.

import scipy as sc
import matplotlib.style
import matplotlib.pyplot as plt

params = {'legend.fontsize': 18,
          'axes.labelsize': 18,
          'axes.titlesize': 18,
          'xtick.labelsize' :12,
          'ytick.labelsize': 12,
          'grid.color': 'k',
          'grid.linestyle': ':',
          'grid.linewidth': 0.5,
          'mathtext.fontset' : 'stix',
          'mathtext.rm'      : 'serif',
          'font.family'      : 'serif',
          'font.serif'       : ""Times New Roman"", # or ""Times""          
         }
matplotlib.rcParams.update(params)

x = sc.linspace(0,100)
y = x**2
fig = plt.figure('Fig')
ax = fig.add_subplot(1, 1, 1)
lines = ax.semilogy(x, y)

ax.yaxis.set_minor_formatter(matplotlib.ticker.ScalarFormatter())
ax.tick_params(axis='y', pad=10)
ax.set_yticks([300], minor=True)
ax.yaxis.grid(True, which='minor')
ax.set_xlabel(r'$\mathrm{R_L}$')
ax.set_ylabel(r'$\sigma \int_l \; dx$')
plt.tight_layout()
plt.show()



","From the link you did provide:


  A ‘classic’ style sheet is provided so reverting to the 1.x default values is a single line of python
  
  mpl.style.use('classic')    


Adding this line

matplotlib.style.use('classic')


to your script should solve your problem.

I tested it on my python2.7/matplotlib 2, and it worked fine (i.e. I get back the matplotlib 1.x fonts).
","While trying to find a solution to my question, I tried comparing the dictionaries of the old and new rcParams and setting the elements which were different and related to mathtext font: the result is quite good.

The code is:

#%matplotlib inline
#%matplotlib notebook
#%config InlineBackend.figure_format = 'svg'
import scipy as sc
import matplotlib.pyplot as plt
import matplotlib

# http://matplotlib.org/users/dflt_style_changes.html
params = {'legend.fontsize': 18,
          'axes.labelsize': 18,
          'axes.titlesize': 18,
          'xtick.labelsize' :12,
          'ytick.labelsize': 12,
          'mathtext.fontset': 'cm',
          'mathtext.rm': 'serif',
          'mathtext.bf': 'serif:bold',
          'mathtext.it': 'serif:italic',
          'mathtext.sf': 'sans\\-serif',
          'grid.color': 'k',
          'grid.linestyle': ':',
          'grid.linewidth': 0.5,
         }
matplotlib.rcParams.update(params)
#matplotlib.rcParams.update({'text.usetex':True, 'text.latex.preamble':[r'\usepackage{amsmath, newtxmath}']})
#matplotlib.rcParams.update({'text.usetex':True, 'text.latex.preamble':[r'\usepackage{amsmath, mathptmx}']})
#matplotlib.rcParams.update({'text.usetex':True, 'text.latex.preamble':[r'\usepackage{amsmath}']})

x = sc.linspace(0,100)
y = x**2
fig = plt.figure('Fig')
ax = fig.add_subplot(1, 1, 1)
lines = ax.semilogy(x, y)
ax.set_yticks([300], minor=True)
ax.yaxis.grid(True, which='minor')
ax.yaxis.set_minor_formatter(matplotlib.ticker.ScalarFormatter())
ax.tick_params(axis='y', pad=10)
ax.set_xlabel(r'$\mathrm{R_L}$')
ax.set_ylabel(r'$\sigma \int_l \; dx$')
fig.savefig('./PNG/test.png', dpi=300, bbox_inches='tight')


hence adding also:

          'mathtext.rm': 'serif',
          'mathtext.bf': 'serif:bold',
          'mathtext.it': 'serif:italic',
          'mathtext.sf': 'sans\\-serif',


which results in:



that I consider quite good and consistent in a Latex document.

The other answer in this thread from @ImportanceOfBeingErnest is also neat and nice. 
"
matplotlib inconsistent output,https://stackoverflow.com/questions/47143754,Matplotlib path.contains_points returns false for points on some edges but not others,"I'm attempting to use Matplotlib to find all points contained within a polygonal path, but it seems to be missing a few. More specifically, my path is a rectangle, and the points are on an underlying uniform grid. In the following test script it will not consider the points laying on the top line of the polygon as being part of the polygon, but will consider the points on the rest of the edges.

Code:

import matplotlib.path as mpltPath

polygon = [(5,5),(10,5),(10,10),(5,10)]
width =11
height =11

points = [[0,0],[1,0],[2,0],[3,0],[4,0],[5,0],[6,0],[7,0],[8,0],[9,0],[10,0],[11,0], \
          [0,1],[1,1],[2,1],[3,1],[4,1],[5,1],[6,1],[7,1],[8,1],[9,1],[10,1],[11,1],\
          [0,2],[1,2],[2,2],[3,2],[4,2],[5,2],[6,2],[7,2],[8,2],[9,2],[10,2],[11,2],\
          [0,3],[1,3],[2,3],[3,3],[4,3],[5,3],[6,3],[7,3],[8,3],[9,3],[10,3],[11,3],\
          [0,4],[1,4],[2,4],[3,4],[4,4],[5,4],[6,4],[7,4],[8,4],[9,4],[10,4],[11,4],\
          [0,5],[1,5],[2,5],[3,5],[4,5],[5,5],[6,5],[7,5],[8,5],[9,5],[10,5],[11,5],\
          [0,6],[1,6],[2,6],[3,6],[4,6],[5,6],[6,6],[7,6],[8,6],[9,6],[10,6],[11,6],\
          [0,7],[1,7],[2,7],[3,7],[4,7],[5,7],[6,7],[7,7],[8,7],[9,7],[10,7],[11,7],\
          [0,8],[1,8],[2,8],[3,8],[4,8],[5,8],[6,8],[7,8],[8,8],[9,8],[10,8],[11,8],\
          [0,9],[1,9],[2,9],[3,9],[4,9],[5,9],[6,9],[7,9],[8,9],[9,9],[10,9],[11,9],\
          [0,10],[1,10],[2,10],[3,10],[4,10],[5,10],[6,10],[7,10],[8,10],[9,10],[10,10],[11,10],\
          [0,11],[1,11],[2,11],[3,11],[4,11],[5,11],[6,11],[7,11],[8,11],[9,11],[10,11],[11,11]]


path = mpltPath.Path(polygon)
inside = path.contains_points(points)
print(inside)


As is, the code above will return

[False False False False False False False False False False False False
 False False False False False False False False False False False False
 False False False False False False False False False False False False
 False False False False False False False False False False False False
 False False False False False False False False False False False False
 False False False False False False False False False False False False
 False False False False False  True  True  True  True  True  True False
 False False False False False  True  True  True  True  True  True False
 False False False False False  True  True  True  True  True  True False
 False False False False False  True  True  True  True  True  True False
 False False False False False  True  True  True  True  True  True False
 False False False False False False False False False False False False]


I would expect row 5 of the result to contain True values like the ones following it. If I change the coordinates in the polygon from 5 to 4.9, then I do get the result I expect.

I'm assuming this has something to do with misusing or misunderstanding the function, but I'm not quite sure what or how that might be.

EDIT: It was brought up that contains_points should be returning False for points that fall on the edges of the polygon. In my example, we see this behaviour for the top edge [5,5]-[10,5], but not for the other edges (i.e. [5,5]-[5,10], [5,10]-[10,10], and [10,10]-[10,5]). These three other edges correspond to the first and last columns with True values and the last row containing True values in the example output above. It is this apparent inconsistency that's the problem.
",6,3566,"update: Is now an open issue in matplotlib.



Excluding lines on the border might be the expected behavior for a function like contains_points. However, in this case, the points on the border of a polygon are not treated in a consistent manner:

In the example you give line 5 indicates the exclusion of points on the boarder, but rows 5 10 and line 10 indicate an inclusion of the boarder points.

Drawing the path with polygon[::-1], so inversed orientation, leads to the expected behavior for all borders except for line 10 where again inclusion is applied.

To me there is no logical pattern apparent here. But even if there is one, this behavior certainly is confusing and should be fixed.



Now you can still get the desired behavior by avoiding the points to lay on the boarders of your polygon. You can do so using the radius attribute of the contains_points function: 


  radius allows the path to be made slightly larger or smaller.


So if you provide some small, positive or negative value for the radius attribute like so:

# ...
path = mpltPath.Path(polygon)
inside = path.contains_points(points,radius=0.1)

print(inside)


you get:

[False False False False False False False False False False False False
 False False False False False False False False False False False False
 False False False False False False False False False False False False
 False False False False False False False False False False False False
 False False False False False False False False False False False False
 False False False False False  True  True  True  True  True  True False
 False False False False False  True  True  True  True  True  True False
 False False False False False  True  True  True  True  True  True False
 False False False False False  True  True  True  True  True  True False
 False False False False False  True  True  True  True  True  True False
 False False False False False  True  True  True  True  True  True False
 False False False False False False False False False False False False]


Note, whether the radius should be positive or negative depends on the orientation. You find more information about this here. As a rule of thumb: A positive radius expands the path when the path goes counterclockwise and shrinks the path when the path goes clockwise.
",,
matplotlib inconsistent output,https://stackoverflow.com/questions/15147287,(numpy) Wrong amplitude(?) of FFT&#39;d array?,"I'm using numpy and matplotlib to analyze data output form my simulations. There is one (apparent) inconsistency that I can't find the roots of. It's the following:

I have a signal that has a given energy a^2~1. When I use rfft to take the FFT and compute the energy in the Fourier space, it comes out to be significantly larger. To void giving the details of my data etc., here is an example with a simple sin wave:

from pylab import *
xx=np.linspace(0.,2*pi,128)
a=np.zeros(128)
for i in range(0,128):
    a[i]=sin(xx[i])
aft=rfft(a)
print mean(abs(aft)**2),mean(a**2) 


In principle both the numbers should be the same (at least in the numerical sense) but this is what I get out of this code:

62.523081632 0.49609375


I tried to go through numpy.fft documentation but could not find anything. A search here gave the following but I was not able to understand the explanations there:

Big FFT amplitude difference between the existing (synthesized) signal and the filtered signal

What am I missing/ misunderstanding? Any help/ pointer in this regard would be greatly appreciated.

Thanks!
",4,10879,"Henry is right on the non-normalization part, but there is a little more to it, because you are using rfft, not fft. The following is consistent with his answer:

&gt;&gt;&gt; x = np.linspace(0, 2 * np.pi, 128)
&gt;&gt;&gt; y = 1 - np.sin(x)
&gt;&gt;&gt; fft = np.fft.fft(y)
&gt;&gt;&gt; np.mean((fft * fft.conj()).real)
191.49999999999991
&gt;&gt;&gt; np.mean(y**2)
1.4960937500000004
&gt;&gt;&gt; fft = fft / np.sqrt(len(fft))
&gt;&gt;&gt; np.mean((fft * fft.conj()).real)
1.4960937499999991


But if you now try the same with rfft, things don't quite work out:

&gt;&gt;&gt; rfft = np.fft.rfft(y)
&gt;&gt;&gt; np.mean((rfft * rfft.conj()).real)
314.58462009358772
&gt;&gt;&gt; rfft /= np.sqrt(len(rfft))
&gt;&gt;&gt; np.mean((rfft * rfft.conj()).real)
4.8397633860551954
65
&gt;&gt;&gt; np.mean((rfft * rfft.conj()).real) / len(rfft)
4.8397633860551954


The following does work properly, though:

&gt;&gt;&gt; (rfft[0] * rfft[0].conj() +
...  2 * np.sum(rfft[1:] * rfft[1:].conj())).real / len(y)
1.4960937873636722


When you use rfft what you are getting is not properly the DFT of your data, but only the positive half of it, since the negative would be symmetric to it. To compute the mean, you need to consider every value other than the DC component twice, which is what the last line of code does.
","In most FFT libraries, the various DFT flavours are not orthogonal. The numpy.fft library applies the necessary normalizations only during the inverse transform.

Consider the Wikipedia description of the DFT; the inverse DFT has the 1/N term that the DFT does not have (in which N is the length of the transform). To make an orthogonal version of the DFT, you need to scale the result of the un-normalised DFT by 1/sqrt(N). In this case, the transform is orthogonal (that is, if we define the orthogonal DFT as F, then the inverse DFT is the conjugate, or hermitian, transpose of F).

In your case, you can get the correct answer by simply scaling aft by 1.0/sqrt(len(a)) (note that N is found from the length of the transform; the real FFT just throws about half the values away, so it's the length of a that is important).

I suspect that the reason for leaving the normalization until the end is that in most situations, it doesn't matter and you therefore save the computational cost of doing the normalization twice. Indeed, the very quick FFTW library doesn't do any normalization in either direction, and leaves it entirely up to the user to deal with.

Edit: Just to be clear, the explanation above is not quite correct. The correct answer will not be arrived at with that simple scaling, as in your case the DC component will be added in twice, although 1.0/sqrt(len(a)) is still the correct scaling to produce the unitary transform.
",
matplotlib inconsistent output,https://stackoverflow.com/questions/56067942,"Trouble installing python library, cyvcf2, using both pip and anaconda","I am trying to install cyvcf2. 

The github instruction recommends pip install cyvcf2 or bioconda. I am on a windows 10 PC with Anaconda 1.9.7 and PyCharm. I first tried conda install -c bioconda cyvcf2 but it could not find the package even though it is the suggested location on Anaconda. Then I tried pip and that's when it said I needed Visual Studio 14. I installed it through the link it provided, but it still can't build it.

Error from conda

(base) C:\Users\Nelson\PycharmProjects\source_profiles&gt;conda install cyvcf2
Collecting package metadata: done
Solving environment: -
The environment is inconsistent, please check the package plan carefully
The following packages are causing the inconsistency:

  - defaults/win-64::dask==0.17.2=py36_0
  - defaults/win-64::h5py==2.7.1=py36he54a1c3_0
  - defaults/win-64::matplotlib==2.2.2=py36h153e9ff_0
  - defaults/win-64::numba==0.37.0=np114py36hea3a760_0
  - defaults/win-64::numexpr==2.6.4=py36h30784b8_0
  - defaults/win-64::odo==0.5.1=py36h7560279_0
  - defaults/win-64::patsy==0.5.0=py36_0
  - defaults/win-64::pytables==3.4.2=py36h71138e3_2
  - defaults/win-64::scikit-image==0.13.1=py36hfa6e2cd_1
  - defaults/win-64::scikit-learn==0.19.1=py36h53aea1b_0
  - defaults/win-64::scipy==1.0.1=py36hce232c7_0
  - defaults/win-64::statsmodels==0.8.0=py36h6189b4c_0
failed

PackagesNotFoundError: The following packages are not available from current channels:

  - cyvcf2

Current channels:

  - https://conda.anaconda.org/r/win-64
  - https://conda.anaconda.org/r/noarch
  - https://conda.anaconda.org/bioconda/win-64
  - https://conda.anaconda.org/bioconda/noarch
  - https://conda.anaconda.org/conda-forge/win-64
  - https://conda.anaconda.org/conda-forge/noarch
  - https://repo.anaconda.com/pkgs/main/win-64
  - https://repo.anaconda.com/pkgs/main/noarch
  - https://repo.anaconda.com/pkgs/free/win-64
  - https://repo.anaconda.com/pkgs/free/noarch
  - https://repo.anaconda.com/pkgs/r/win-64
  - https://repo.anaconda.com/pkgs/r/noarch
  - https://repo.anaconda.com/pkgs/msys2/win-64
  - https://repo.anaconda.com/pkgs/msys2/noarch

To search for alternate channels that may provide the conda package you're
looking for, navigate to

    https://anaconda.org

and use the search bar at the top of the page.


Error from pip

Collecting cyvcf2
  Using cached https://files.pythonhosted.org/packages/10/69/888f23e07de1364533ac666941b0e1229c7e60c1a5080b1234b849bd8b97/cyvcf2-0.10.10.tar.gz
Requirement already satisfied: numpy in c:\anaconda3\lib\site-packages (from cyvcf2) (1.15.4)
Building wheels for collected packages: cyvcf2
  Building wheel for cyvcf2 (setup.py) ... error
  ERROR: Complete output from command 'c:\anaconda3\python.exe' -u -c 'import setuptools, tokenize;__file__='""'""'C:\\Users\\Nelson\\AppData\\Local\\Temp\\pip-install-d8uogmra\\cyvcf2\\setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d 'C:\Users\Nelson\AppData\Local\Temp\pip-wheel-s8tt0cdb' --python-tag cp36:
  ERROR: running bdist_wheel
  running build
  running build_py
  creating build
  creating build\lib.win-amd64-3.6
  creating build\lib.win-amd64-3.6\cyvcf2
  copying cyvcf2\cli.py -&gt; build\lib.win-amd64-3.6\cyvcf2
  copying cyvcf2\__init__.py -&gt; build\lib.win-amd64-3.6\cyvcf2
  copying cyvcf2\__main__.py -&gt; build\lib.win-amd64-3.6\cyvcf2
  creating build\lib.win-amd64-3.6\cyvcf2\tests
  copying cyvcf2\tests\test_cli.py -&gt; build\lib.win-amd64-3.6\cyvcf2\tests
  copying cyvcf2\tests\test_hemi.py -&gt; build\lib.win-amd64-3.6\cyvcf2\tests
  copying cyvcf2\tests\test_reader.py -&gt; build\lib.win-amd64-3.6\cyvcf2\tests
  copying cyvcf2\tests\__init__.py -&gt; build\lib.win-amd64-3.6\cyvcf2\tests
  running egg_info
  writing cyvcf2.egg-info\PKG-INFO
  writing dependency_links to cyvcf2.egg-info\dependency_links.txt
  writing entry points to cyvcf2.egg-info\entry_points.txt
  writing requirements to cyvcf2.egg-info\requires.txt
  writing top-level names to cyvcf2.egg-info\top_level.txt
  reading manifest file 'cyvcf2.egg-info\SOURCES.txt'
  reading manifest template 'MANIFEST.in'
  writing manifest file 'cyvcf2.egg-info\SOURCES.txt'
  copying cyvcf2\cyvcf2.c -&gt; build\lib.win-amd64-3.6\cyvcf2
  copying cyvcf2\cyvcf2.pxd -&gt; build\lib.win-amd64-3.6\cyvcf2
  copying cyvcf2\cyvcf2.pyx -&gt; build\lib.win-amd64-3.6\cyvcf2
  copying cyvcf2\helpers.c -&gt; build\lib.win-amd64-3.6\cyvcf2
  copying cyvcf2\helpers.h -&gt; build\lib.win-amd64-3.6\cyvcf2
  copying cyvcf2\relatedness.h -&gt; build\lib.win-amd64-3.6\cyvcf2
  copying cyvcf2\tests\__init__.pyc -&gt; build\lib.win-amd64-3.6\cyvcf2\tests
  copying cyvcf2\tests\bug.vcf.gz -&gt; build\lib.win-amd64-3.6\cyvcf2\tests
  copying cyvcf2\tests\decomposed.vcf -&gt; build\lib.win-amd64-3.6\cyvcf2\tests
  copying cyvcf2\tests\empty.vcf -&gt; build\lib.win-amd64-3.6\cyvcf2\tests
  copying cyvcf2\tests\issue_44.vcf -&gt; build\lib.win-amd64-3.6\cyvcf2\tests
  copying cyvcf2\tests\o.vcf.gz -&gt; build\lib.win-amd64-3.6\cyvcf2\tests
  copying cyvcf2\tests\seg.vcf.gz -&gt; build\lib.win-amd64-3.6\cyvcf2\tests
  copying cyvcf2\tests\test-alt-repr.vcf -&gt; build\lib.win-amd64-3.6\cyvcf2\tests
  copying cyvcf2\tests\test-diff.csi -&gt; build\lib.win-amd64-3.6\cyvcf2\tests
  copying cyvcf2\tests\test-format-string.vcf -&gt; build\lib.win-amd64-3.6\cyvcf2\tests
  copying cyvcf2\tests\test-haploidX.vcf -&gt; build\lib.win-amd64-3.6\cyvcf2\tests
  copying cyvcf2\tests\test-hemi.vcf -&gt; build\lib.win-amd64-3.6\cyvcf2\tests
  copying cyvcf2\tests\test-multiallelic-homozygous-alt.vcf.gz -&gt; build\lib.win-amd64-3.6\cyvcf2\tests
  copying cyvcf2\tests\test-multiallelic-homozygous-alt.vcf.gz.tbi -&gt; build\lib.win-amd64-3.6\cyvcf2\tests
  copying cyvcf2\tests\test-strict-gt-option-flag.vcf.gz -&gt; build\lib.win-amd64-3.6\cyvcf2\tests
  copying cyvcf2\tests\test-strict-gt-option-flag.vcf.gz.tbi -&gt; build\lib.win-amd64-3.6\cyvcf2\tests
  copying cyvcf2\tests\test.comp_het.3.vcf -&gt; build\lib.win-amd64-3.6\cyvcf2\tests
  copying cyvcf2\tests\test.snpeff.bcf -&gt; build\lib.win-amd64-3.6\cyvcf2\tests
  copying cyvcf2\tests\test.snpeff.bcf.csi -&gt; build\lib.win-amd64-3.6\cyvcf2\tests
  copying cyvcf2\tests\test.snpeff.vcf -&gt; build\lib.win-amd64-3.6\cyvcf2\tests
  copying cyvcf2\tests\test.vcf.gz -&gt; build\lib.win-amd64-3.6\cyvcf2\tests
  copying cyvcf2\tests\test.vcf.gz.tbi -&gt; build\lib.win-amd64-3.6\cyvcf2\tests
  copying cyvcf2\tests\test_cli.pyc -&gt; build\lib.win-amd64-3.6\cyvcf2\tests
  copying cyvcf2\tests\test_gt_alt_freqs.vcf -&gt; build\lib.win-amd64-3.6\cyvcf2\tests
  copying cyvcf2\tests\test_gt_bases.vcf.gz -&gt; build\lib.win-amd64-3.6\cyvcf2\tests
  copying cyvcf2\tests\test_hemi.pyc -&gt; build\lib.win-amd64-3.6\cyvcf2\tests
  copying cyvcf2\tests\test_reader.pyc -&gt; build\lib.win-amd64-3.6\cyvcf2\tests
  running build_ext
  skipping 'cyvcf2\cyvcf2.c' Cython extension (up-to-date)
  building 'cyvcf2.cyvcf2' extension
  creating build\temp.win-amd64-3.6
  creating build\temp.win-amd64-3.6\Release
  creating build\temp.win-amd64-3.6\Release\cyvcf2
  creating build\temp.win-amd64-3.6\Release\htslib
  creating build\temp.win-amd64-3.6\Release\htslib\cram
  C:\Program Files (x86)\Microsoft Visual Studio\2019\BuildTools\VC\Tools\MSVC\14.20.27508\bin\HostX86\x64\cl.exe /c /nologo /Ox /W3 /GL /DNDEBUG /MD -Ihtslib -Icyvcf2 -Ic:\anaconda3\lib\site-packages\numpy\core\include -Ic:\anaconda3\include -Ic:\anaconda3\include ""-IC:\Program Files (x86)\Microsoft Visual Studio\2019\BuildTools\VC\Tools\MSVC\14.20.27508\include"" ""-IC:\Program Files (x86)\Windows Kits\10\include\10.0.17763.0\ucrt"" ""-IC:\Program Files (x86)\Windows Kits\10\include\10.0.17763.0\shared"" ""-IC:\Program Files (x86)\Windows Kits\10\include\10.0.17763.0\um"" ""-IC:\Program Files (x86)\Windows Kits\10\include\10.0.17763.0\winrt"" ""-IC:\Program Files (x86)\Windows Kits\10\include\10.0.17763.0\cppwinrt"" /Tccyvcf2\cyvcf2.c /Fobuild\temp.win-amd64-3.6\Release\cyvcf2\cyvcf2.obj
  cyvcf2.c
  c:\anaconda3\lib\site-packages\numpy\core\include\numpy\npy_1_7_deprecated_api.h(12) : Warning Msg: Using deprecated NumPy API, disable it by #defining NPY_NO_DEPRECATED_API NPY_1_7_API_VERSION
  C:\Users\Nelson\AppData\Local\Temp\pip-install-d8uogmra\cyvcf2\cyvcf2\relatedness.h(90): warning C4244: '=': conversion from 'int' to 'float', possible loss of data
  C:\Users\Nelson\AppData\Local\Temp\pip-install-d8uogmra\cyvcf2\cyvcf2\relatedness.h(98): warning C4244: '=': conversion from 'int' to 'float', possible loss of data
  C:\Users\Nelson\AppData\Local\Temp\pip-install-d8uogmra\cyvcf2\cyvcf2\relatedness.h(100): warning C4244: '=': conversion from 'double' to 'float', possible loss of data
  C:\Users\Nelson\AppData\Local\Temp\pip-install-d8uogmra\cyvcf2\cyvcf2\relatedness.h(103): warning C4244: '=': conversion from 'double' to 'float', possible loss of data
  C:\Users\Nelson\AppData\Local\Temp\pip-install-d8uogmra\cyvcf2\cyvcf2\relatedness.h(80): warning C4244: 'initializing': conversion from 'double' to 'float', possible loss of data
  C:\Users\Nelson\AppData\Local\Temp\pip-install-d8uogmra\cyvcf2\cyvcf2\relatedness.h(215): warning C4244: '=': conversion from 'double' to 'float', possible loss of data
  C:\Users\Nelson\AppData\Local\Temp\pip-install-d8uogmra\cyvcf2\cyvcf2\relatedness.h(222): warning C4244: 'return': conversion from 'double' to 'float', possible loss of data
  C:\Users\Nelson\AppData\Local\Temp\pip-install-d8uogmra\cyvcf2\htslib\htslib\hts.h(810): warning C4244: 'return': conversion from 'int64_t' to 'int', possible loss of data
  C:\Users\Nelson\AppData\Local\Temp\pip-install-d8uogmra\cyvcf2\htslib\htslib\kstring.h(152): warning C4267: 'return': conversion from 'size_t' to 'int', possible loss of data
  C:\Users\Nelson\AppData\Local\Temp\pip-install-d8uogmra\cyvcf2\htslib\htslib\kstring.h(183): warning C4267: 'return': conversion from 'size_t' to 'int', possible loss of data
  C:\Users\Nelson\AppData\Local\Temp\pip-install-d8uogmra\cyvcf2\htslib\htslib\kstring.h(191): warning C4146: unary minus operator applied to unsigned type, result still unsigned
  C:\Users\Nelson\AppData\Local\Temp\pip-install-d8uogmra\cyvcf2\htslib\htslib\kstring.h(220): warning C4146: unary minus operator applied to unsigned type, result still unsigned
  htslib\htslib/hfile.h(123): warning C4244: 'return': conversion from '__int64' to 'off_t', possible loss of data
  htslib\htslib/hfile.h(245): warning C4018: '&lt;': signed/unsigned mismatch
  cyvcf2\cyvcf2.c(7896): warning C4090: 'function': different 'const' qualifiers
  cyvcf2\cyvcf2.c(7938): warning C4090: 'function': different 'const' qualifiers
  cyvcf2\cyvcf2.c(8454): warning C4090: 'function': different 'const' qualifiers
  cyvcf2\cyvcf2.c(8752): warning C4090: 'function': different 'const' qualifiers
  cyvcf2\cyvcf2.c(8805): warning C4090: 'function': different 'const' qualifiers
  cyvcf2\cyvcf2.c(9225): warning C4090: 'function': different 'const' qualifiers
  cyvcf2\cyvcf2.c(10510): warning C4090: 'function': different 'const' qualifiers
  cyvcf2\cyvcf2.c(12167): warning C4090: 'function': different 'const' qualifiers
  cyvcf2\cyvcf2.c(12901): warning C4090: '=': different 'const' qualifiers
  cyvcf2\cyvcf2.c(13010): warning C4090: 'function': different 'const' qualifiers
  cyvcf2\cyvcf2.c(13010): warning C4090: '=': different 'const' qualifiers
  cyvcf2\cyvcf2.c(13093): warning C4090: '=': different 'const' qualifiers
  cyvcf2\cyvcf2.c(16274): warning C4244: '=': conversion from 'Py_ssize_t' to 'int', possible loss of data
  cyvcf2\cyvcf2.c(18493): warning C4244: '=': conversion from 'Py_ssize_t' to 'int', possible loss of data
  cyvcf2\cyvcf2.c(19538): warning C4244: '=': conversion from 'Py_ssize_t' to 'int', possible loss of data
  cyvcf2\cyvcf2.c(20187): warning C4305: 'function': truncation from 'double' to 'float'
  cyvcf2\cyvcf2.c(20605): warning C4244: '=': conversion from 'Py_ssize_t' to 'int', possible loss of data
  cyvcf2\cyvcf2.c(20968): warning C4244: '=': conversion from 'int32_t' to 'float', possible loss of data
  cyvcf2\cyvcf2.c(21014): warning C4244: '=': conversion from 'int32_t' to 'float', possible loss of data
  cyvcf2\cyvcf2.c(24122): warning C4018: '&lt;': signed/unsigned mismatch
  cyvcf2\cyvcf2.c(26133): warning C4090: '=': different 'const' qualifiers
  cyvcf2\cyvcf2.c(28265): warning C4244: '=': conversion from 'npy_intp' to 'int', possible loss of data
  cyvcf2\cyvcf2.c(28288): warning C4244: '=': conversion from 'npy_intp' to 'int', possible loss of data
  cyvcf2\cyvcf2.c(29622): warning C4244: '=': conversion from 'double' to 'float', possible loss of data
  cyvcf2\cyvcf2.c(33695): warning C4018: '&lt;': signed/unsigned mismatch
  cyvcf2\cyvcf2.c(33960): warning C4018: '&lt;': signed/unsigned mismatch
  cyvcf2\cyvcf2.c(34174): warning C4018: '&lt;': signed/unsigned mismatch
  cyvcf2\cyvcf2.c(36344): warning C4090: '=': different 'const' qualifiers
  cyvcf2\cyvcf2.c(36447): warning C4244: 'function': conversion from 'Py_ssize_t' to 'int', possible loss of data
  cyvcf2\cyvcf2.c(39339): warning C4018: '&gt;=': signed/unsigned mismatch
  cyvcf2\cyvcf2.c(39387): warning C4090: '=': different 'const' qualifiers
  cyvcf2\cyvcf2.c(40370): warning C4090: '=': different 'const' qualifiers
  cyvcf2\cyvcf2.c(41024): warning C4090: 'function': different 'const' qualifiers
  cyvcf2\cyvcf2.c(41052): warning C4090: 'function': different 'const' qualifiers
  cyvcf2\cyvcf2.c(41120): warning C4090: 'function': different 'const' qualifiers
  cyvcf2\cyvcf2.c(41292): warning C4090: 'function': different 'const' qualifiers
  cyvcf2\cyvcf2.c(41330): warning C4090: 'function': different 'const' qualifiers
  cyvcf2\cyvcf2.c(41389): warning C4090: 'function': different 'const' qualifiers
  cyvcf2\cyvcf2.c(68042): warning C4244: 'initializing': conversion from 'double' to 'float', possible loss of data
  cyvcf2\cyvcf2.c(68048): warning C4244: 'initializing': conversion from 'double' to 'float', possible loss of data
  C:\Program Files (x86)\Microsoft Visual Studio\2019\BuildTools\VC\Tools\MSVC\14.20.27508\bin\HostX86\x64\cl.exe /c /nologo /Ox /W3 /GL /DNDEBUG /MD -Ihtslib -Icyvcf2 -Ic:\anaconda3\lib\site-packages\numpy\core\include -Ic:\anaconda3\include -Ic:\anaconda3\include ""-IC:\Program Files (x86)\Microsoft Visual Studio\2019\BuildTools\VC\Tools\MSVC\14.20.27508\include"" ""-IC:\Program Files (x86)\Windows Kits\10\include\10.0.17763.0\ucrt"" ""-IC:\Program Files (x86)\Windows Kits\10\include\10.0.17763.0\shared"" ""-IC:\Program Files (x86)\Windows Kits\10\include\10.0.17763.0\um"" ""-IC:\Program Files (x86)\Windows Kits\10\include\10.0.17763.0\winrt"" ""-IC:\Program Files (x86)\Windows Kits\10\include\10.0.17763.0\cppwinrt"" /Tchtslib\bcf_sr_sort.c /Fobuild\temp.win-amd64-3.6\Release\htslib\bcf_sr_sort.obj
  bcf_sr_sort.c
  htslib\bcf_sr_sort.c(27): fatal error C1083: Cannot open include file: 'strings.h': No such file or directory
  error: command 'C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\BuildTools\\VC\\Tools\\MSVC\\14.20.27508\\bin\\HostX86\\x64\\cl.exe' failed with exit status 2
  ----------------------------------------
  ERROR: Failed building wheel for cyvcf2
  Running setup.py clean for cyvcf2
Failed to build cyvcf2
Installing collected packages: cyvcf2
  Running setup.py install for cyvcf2 ... error
    ERROR: Complete output from command 'c:\anaconda3\python.exe' -u -c 'import setuptools, tokenize;__file__='""'""'C:\\Users\\Nelson\\AppData\\Local\\Temp\\pip-install-d8uogmra\\cyvcf2\\setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' install --record 'C:\Users\Nelson\AppData\Local\Temp\pip-record-stdp98ur\install-record.txt' --single-version-externally-managed --compile:
    ERROR: running install
    running build
    running build_py
    creating build
    creating build\lib.win-amd64-3.6
    creating build\lib.win-amd64-3.6\cyvcf2
    copying cyvcf2\cli.py -&gt; build\lib.win-amd64-3.6\cyvcf2
    copying cyvcf2\__init__.py -&gt; build\lib.win-amd64-3.6\cyvcf2
    copying cyvcf2\__main__.py -&gt; build\lib.win-amd64-3.6\cyvcf2
    creating build\lib.win-amd64-3.6\cyvcf2\tests
    copying cyvcf2\tests\test_cli.py -&gt; build\lib.win-amd64-3.6\cyvcf2\tests
    copying cyvcf2\tests\test_hemi.py -&gt; build\lib.win-amd64-3.6\cyvcf2\tests
    copying cyvcf2\tests\test_reader.py -&gt; build\lib.win-amd64-3.6\cyvcf2\tests
    copying cyvcf2\tests\__init__.py -&gt; build\lib.win-amd64-3.6\cyvcf2\tests
    running egg_info
    writing cyvcf2.egg-info\PKG-INFO
    writing dependency_links to cyvcf2.egg-info\dependency_links.txt
    writing entry points to cyvcf2.egg-info\entry_points.txt
    writing requirements to cyvcf2.egg-info\requires.txt
    writing top-level names to cyvcf2.egg-info\top_level.txt
    reading manifest file 'cyvcf2.egg-info\SOURCES.txt'
    reading manifest template 'MANIFEST.in'
    writing manifest file 'cyvcf2.egg-info\SOURCES.txt'
    copying cyvcf2\cyvcf2.c -&gt; build\lib.win-amd64-3.6\cyvcf2
    copying cyvcf2\cyvcf2.pxd -&gt; build\lib.win-amd64-3.6\cyvcf2
    copying cyvcf2\cyvcf2.pyx -&gt; build\lib.win-amd64-3.6\cyvcf2
    copying cyvcf2\helpers.c -&gt; build\lib.win-amd64-3.6\cyvcf2
    copying cyvcf2\helpers.h -&gt; build\lib.win-amd64-3.6\cyvcf2
    copying cyvcf2\relatedness.h -&gt; build\lib.win-amd64-3.6\cyvcf2
    copying cyvcf2\tests\__init__.pyc -&gt; build\lib.win-amd64-3.6\cyvcf2\tests
    copying cyvcf2\tests\bug.vcf.gz -&gt; build\lib.win-amd64-3.6\cyvcf2\tests
    copying cyvcf2\tests\decomposed.vcf -&gt; build\lib.win-amd64-3.6\cyvcf2\tests
    copying cyvcf2\tests\empty.vcf -&gt; build\lib.win-amd64-3.6\cyvcf2\tests
    copying cyvcf2\tests\issue_44.vcf -&gt; build\lib.win-amd64-3.6\cyvcf2\tests
    copying cyvcf2\tests\o.vcf.gz -&gt; build\lib.win-amd64-3.6\cyvcf2\tests
    copying cyvcf2\tests\seg.vcf.gz -&gt; build\lib.win-amd64-3.6\cyvcf2\tests
    copying cyvcf2\tests\test-alt-repr.vcf -&gt; build\lib.win-amd64-3.6\cyvcf2\tests
    copying cyvcf2\tests\test-diff.csi -&gt; build\lib.win-amd64-3.6\cyvcf2\tests
    copying cyvcf2\tests\test-format-string.vcf -&gt; build\lib.win-amd64-3.6\cyvcf2\tests
    copying cyvcf2\tests\test-haploidX.vcf -&gt; build\lib.win-amd64-3.6\cyvcf2\tests
    copying cyvcf2\tests\test-hemi.vcf -&gt; build\lib.win-amd64-3.6\cyvcf2\tests
    copying cyvcf2\tests\test-multiallelic-homozygous-alt.vcf.gz -&gt; build\lib.win-amd64-3.6\cyvcf2\tests
    copying cyvcf2\tests\test-multiallelic-homozygous-alt.vcf.gz.tbi -&gt; build\lib.win-amd64-3.6\cyvcf2\tests
    copying cyvcf2\tests\test-strict-gt-option-flag.vcf.gz -&gt; build\lib.win-amd64-3.6\cyvcf2\tests
    copying cyvcf2\tests\test-strict-gt-option-flag.vcf.gz.tbi -&gt; build\lib.win-amd64-3.6\cyvcf2\tests
    copying cyvcf2\tests\test.comp_het.3.vcf -&gt; build\lib.win-amd64-3.6\cyvcf2\tests
    copying cyvcf2\tests\test.snpeff.bcf -&gt; build\lib.win-amd64-3.6\cyvcf2\tests
    copying cyvcf2\tests\test.snpeff.bcf.csi -&gt; build\lib.win-amd64-3.6\cyvcf2\tests
    copying cyvcf2\tests\test.snpeff.vcf -&gt; build\lib.win-amd64-3.6\cyvcf2\tests
    copying cyvcf2\tests\test.vcf.gz -&gt; build\lib.win-amd64-3.6\cyvcf2\tests
    copying cyvcf2\tests\test.vcf.gz.tbi -&gt; build\lib.win-amd64-3.6\cyvcf2\tests
    copying cyvcf2\tests\test_cli.pyc -&gt; build\lib.win-amd64-3.6\cyvcf2\tests
    copying cyvcf2\tests\test_gt_alt_freqs.vcf -&gt; build\lib.win-amd64-3.6\cyvcf2\tests
    copying cyvcf2\tests\test_gt_bases.vcf.gz -&gt; build\lib.win-amd64-3.6\cyvcf2\tests
    copying cyvcf2\tests\test_hemi.pyc -&gt; build\lib.win-amd64-3.6\cyvcf2\tests
    copying cyvcf2\tests\test_reader.pyc -&gt; build\lib.win-amd64-3.6\cyvcf2\tests
    running build_ext
    skipping 'cyvcf2\cyvcf2.c' Cython extension (up-to-date)
    building 'cyvcf2.cyvcf2' extension
    creating build\temp.win-amd64-3.6
    creating build\temp.win-amd64-3.6\Release
    creating build\temp.win-amd64-3.6\Release\cyvcf2
    creating build\temp.win-amd64-3.6\Release\htslib
    creating build\temp.win-amd64-3.6\Release\htslib\cram
    C:\Program Files (x86)\Microsoft Visual Studio\2019\BuildTools\VC\Tools\MSVC\14.20.27508\bin\HostX86\x64\cl.exe /c /nologo /Ox /W3 /GL /DNDEBUG /MD -Ihtslib -Icyvcf2 -Ic:\anaconda3\lib\site-packages\numpy\core\include -Ic:\anaconda3\include -Ic:\anaconda3\include ""-IC:\Program Files (x86)\Microsoft Visual Studio\2019\BuildTools\VC\Tools\MSVC\14.20.27508\include"" ""-IC:\Program Files (x86)\Windows Kits\10\include\10.0.17763.0\ucrt"" ""-IC:\Program Files (x86)\Windows Kits\10\include\10.0.17763.0\shared"" ""-IC:\Program Files (x86)\Windows Kits\10\include\10.0.17763.0\um"" ""-IC:\Program Files (x86)\Windows Kits\10\include\10.0.17763.0\winrt"" ""-IC:\Program Files (x86)\Windows Kits\10\include\10.0.17763.0\cppwinrt"" /Tccyvcf2\cyvcf2.c /Fobuild\temp.win-amd64-3.6\Release\cyvcf2\cyvcf2.obj
    cyvcf2.c
    c:\anaconda3\lib\site-packages\numpy\core\include\numpy\npy_1_7_deprecated_api.h(12) : Warning Msg: Using deprecated NumPy API, disable it by #defining NPY_NO_DEPRECATED_API NPY_1_7_API_VERSION
    C:\Users\Nelson\AppData\Local\Temp\pip-install-d8uogmra\cyvcf2\cyvcf2\relatedness.h(90): warning C4244: '=': conversion from 'int' to 'float', possible loss of data
    C:\Users\Nelson\AppData\Local\Temp\pip-install-d8uogmra\cyvcf2\cyvcf2\relatedness.h(98): warning C4244: '=': conversion from 'int' to 'float', possible loss of data
    C:\Users\Nelson\AppData\Local\Temp\pip-install-d8uogmra\cyvcf2\cyvcf2\relatedness.h(100): warning C4244: '=': conversion from 'double' to 'float', possible loss of data
    C:\Users\Nelson\AppData\Local\Temp\pip-install-d8uogmra\cyvcf2\cyvcf2\relatedness.h(103): warning C4244: '=': conversion from 'double' to 'float', possible loss of data
    C:\Users\Nelson\AppData\Local\Temp\pip-install-d8uogmra\cyvcf2\cyvcf2\relatedness.h(80): warning C4244: 'initializing': conversion from 'double' to 'float', possible loss of data
    C:\Users\Nelson\AppData\Local\Temp\pip-install-d8uogmra\cyvcf2\cyvcf2\relatedness.h(215): warning C4244: '=': conversion from 'double' to 'float', possible loss of data
    C:\Users\Nelson\AppData\Local\Temp\pip-install-d8uogmra\cyvcf2\cyvcf2\relatedness.h(222): warning C4244: 'return': conversion from 'double' to 'float', possible loss of data
    C:\Users\Nelson\AppData\Local\Temp\pip-install-d8uogmra\cyvcf2\htslib\htslib\hts.h(810): warning C4244: 'return': conversion from 'int64_t' to 'int', possible loss of data
    C:\Users\Nelson\AppData\Local\Temp\pip-install-d8uogmra\cyvcf2\htslib\htslib\kstring.h(152): warning C4267: 'return': conversion from 'size_t' to 'int', possible loss of data
    C:\Users\Nelson\AppData\Local\Temp\pip-install-d8uogmra\cyvcf2\htslib\htslib\kstring.h(183): warning C4267: 'return': conversion from 'size_t' to 'int', possible loss of data
    C:\Users\Nelson\AppData\Local\Temp\pip-install-d8uogmra\cyvcf2\htslib\htslib\kstring.h(191): warning C4146: unary minus operator applied to unsigned type, result still unsigned
    C:\Users\Nelson\AppData\Local\Temp\pip-install-d8uogmra\cyvcf2\htslib\htslib\kstring.h(220): warning C4146: unary minus operator applied to unsigned type, result still unsigned
    htslib\htslib/hfile.h(123): warning C4244: 'return': conversion from '__int64' to 'off_t', possible loss of data
    htslib\htslib/hfile.h(245): warning C4018: '&lt;': signed/unsigned mismatch
    cyvcf2\cyvcf2.c(7896): warning C4090: 'function': different 'const' qualifiers
    cyvcf2\cyvcf2.c(7938): warning C4090: 'function': different 'const' qualifiers
    cyvcf2\cyvcf2.c(8454): warning C4090: 'function': different 'const' qualifiers
    cyvcf2\cyvcf2.c(8752): warning C4090: 'function': different 'const' qualifiers
    cyvcf2\cyvcf2.c(8805): warning C4090: 'function': different 'const' qualifiers
    cyvcf2\cyvcf2.c(9225): warning C4090: 'function': different 'const' qualifiers
    cyvcf2\cyvcf2.c(10510): warning C4090: 'function': different 'const' qualifiers
    cyvcf2\cyvcf2.c(12167): warning C4090: 'function': different 'const' qualifiers
    cyvcf2\cyvcf2.c(12901): warning C4090: '=': different 'const' qualifiers
    cyvcf2\cyvcf2.c(13010): warning C4090: 'function': different 'const' qualifiers
    cyvcf2\cyvcf2.c(13010): warning C4090: '=': different 'const' qualifiers
    cyvcf2\cyvcf2.c(13093): warning C4090: '=': different 'const' qualifiers
    cyvcf2\cyvcf2.c(16274): warning C4244: '=': conversion from 'Py_ssize_t' to 'int', possible loss of data
    cyvcf2\cyvcf2.c(18493): warning C4244: '=': conversion from 'Py_ssize_t' to 'int', possible loss of data
    cyvcf2\cyvcf2.c(19538): warning C4244: '=': conversion from 'Py_ssize_t' to 'int', possible loss of data
    cyvcf2\cyvcf2.c(20187): warning C4305: 'function': truncation from 'double' to 'float'
    cyvcf2\cyvcf2.c(20605): warning C4244: '=': conversion from 'Py_ssize_t' to 'int', possible loss of data
    cyvcf2\cyvcf2.c(20968): warning C4244: '=': conversion from 'int32_t' to 'float', possible loss of data
    cyvcf2\cyvcf2.c(21014): warning C4244: '=': conversion from 'int32_t' to 'float', possible loss of data
    cyvcf2\cyvcf2.c(24122): warning C4018: '&lt;': signed/unsigned mismatch
    cyvcf2\cyvcf2.c(26133): warning C4090: '=': different 'const' qualifiers
    cyvcf2\cyvcf2.c(28265): warning C4244: '=': conversion from 'npy_intp' to 'int', possible loss of data
    cyvcf2\cyvcf2.c(28288): warning C4244: '=': conversion from 'npy_intp' to 'int', possible loss of data
    cyvcf2\cyvcf2.c(29622): warning C4244: '=': conversion from 'double' to 'float', possible loss of data
    cyvcf2\cyvcf2.c(33695): warning C4018: '&lt;': signed/unsigned mismatch
    cyvcf2\cyvcf2.c(33960): warning C4018: '&lt;': signed/unsigned mismatch
    cyvcf2\cyvcf2.c(34174): warning C4018: '&lt;': signed/unsigned mismatch
    cyvcf2\cyvcf2.c(36344): warning C4090: '=': different 'const' qualifiers
    cyvcf2\cyvcf2.c(36447): warning C4244: 'function': conversion from 'Py_ssize_t' to 'int', possible loss of data
    cyvcf2\cyvcf2.c(39339): warning C4018: '&gt;=': signed/unsigned mismatch
    cyvcf2\cyvcf2.c(39387): warning C4090: '=': different 'const' qualifiers
    cyvcf2\cyvcf2.c(40370): warning C4090: '=': different 'const' qualifiers
    cyvcf2\cyvcf2.c(41024): warning C4090: 'function': different 'const' qualifiers
    cyvcf2\cyvcf2.c(41052): warning C4090: 'function': different 'const' qualifiers
    cyvcf2\cyvcf2.c(41120): warning C4090: 'function': different 'const' qualifiers
    cyvcf2\cyvcf2.c(41292): warning C4090: 'function': different 'const' qualifiers
    cyvcf2\cyvcf2.c(41330): warning C4090: 'function': different 'const' qualifiers
    cyvcf2\cyvcf2.c(41389): warning C4090: 'function': different 'const' qualifiers
    cyvcf2\cyvcf2.c(68042): warning C4244: 'initializing': conversion from 'double' to 'float', possible loss of data
    cyvcf2\cyvcf2.c(68048): warning C4244: 'initializing': conversion from 'double' to 'float', possible loss of data
    C:\Program Files (x86)\Microsoft Visual Studio\2019\BuildTools\VC\Tools\MSVC\14.20.27508\bin\HostX86\x64\cl.exe /c /nologo /Ox /W3 /GL /DNDEBUG /MD -Ihtslib -Icyvcf2 -Ic:\anaconda3\lib\site-packages\numpy\core\include -Ic:\anaconda3\include -Ic:\anaconda3\include ""-IC:\Program Files (x86)\Microsoft Visual Studio\2019\BuildTools\VC\Tools\MSVC\14.20.27508\include"" ""-IC:\Program Files (x86)\Windows Kits\10\include\10.0.17763.0\ucrt"" ""-IC:\Program Files (x86)\Windows Kits\10\include\10.0.17763.0\shared"" ""-IC:\Program Files (x86)\Windows Kits\10\include\10.0.17763.0\um"" ""-IC:\Program Files (x86)\Windows Kits\10\include\10.0.17763.0\winrt"" ""-IC:\Program Files (x86)\Windows Kits\10\include\10.0.17763.0\cppwinrt"" /Tchtslib\bcf_sr_sort.c /Fobuild\temp.win-amd64-3.6\Release\htslib\bcf_sr_sort.obj
    bcf_sr_sort.c
    htslib\bcf_sr_sort.c(27): fatal error C1083: Cannot open include file: 'strings.h': No such file or directory
    error: command 'C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\BuildTools\\VC\\Tools\\MSVC\\14.20.27508\\bin\\HostX86\\x64\\cl.exe' failed with exit status 2
    ----------------------------------------
ERROR: Command ""'c:\anaconda3\python.exe' -u -c 'import setuptools, tokenize;__file__='""'""'C:\\Users\\Nelson\\AppData\\Local\\Temp\\pip-install-d8uogmra\\cyvcf2\\setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' install --record 'C:\Users\Nelson\AppData\Local\Temp\pip-record-stdp98ur\install-record.txt' --single-version-externally-managed --compile"" failed with error code 1 in C:\Users\Nelson\AppData\Local\Temp\pip-install-d8uogmra\cyvcf2\


I spent all day trying to figure this out. When I did a conda install on my remote linux machine it worked just fine. I don't know what the issue is on my PC.
",1,843,"maybe this comment is a bit comes a bit late. I faced the same problem and I solved it using the following bioconda command:
conda install -c bioconda/label/cf201901 cyvcf2

Let me know if it works
",,
matplotlib inconsistent output,https://stackoverflow.com/questions/63797535,"Confusion Matrix ValueError: Found input variables with inconsistent numbers of samples: [3, 360]",,-3,1092,,,
matplotlib inconsistent result,https://stackoverflow.com/questions/33158726,"Fitting data to multimodal distributions with scipy, matplotlib","I have a dataset that I would like to fit to a known probability distribution. The intention is to use the fitted PDF in a data generator - such that I can sample data from the known (fitted) PDF. Data will be used for simulation purposes. At the moment I am just sampling from a normal distribution, which is inconsistent with the real-data, therefore simulation results are not accurate. 

I first wanted to use the following method : 
Fitting empirical distribution to theoretical ones with Scipy (Python)?

My first thought was to fit it to a weibull distribution, but the data is actually multimodal (picture attached). So I guess I need to combine multiple distributions and then fit the data to the resulting dist, is that right ? Maybe combine a gaussian AND a weibull distirbution ?

How can I use the scipy fit() function with a mixed/multimodal distribution ?

Also I would want to do this in Python (i.e. scipy/numpy/matplotlib), as the data generator is written in Python.

Many thanks !


",9,15411,"I would suggest Kernel Density Estimation (KDE). It gives you a solution as a mixture of PDF.

SciPy has only Gaussian kernel (which lookes fine for your specific histogram), but you can find other kernels in the statsmodels or scikit-learn packages.

For reference, those are the relevant functions:

from sklearn.neighbors import KernelDensity
from scipy.stats import gaussian_kde
from statsmodels.nonparametric.kde import KDEUnivariate
from statsmodels.nonparametric.kernel_density import KDEMultivariate


A great resource for KDE in Python is here.
",,
matplotlib inconsistent result,https://stackoverflow.com/questions/42191668,Matplotlib - Dynamic (bar) chart height based on data?,"After struggling with matplotlib for longer than I'd like to admit by trying to do something that's a breeze in pretty much any other plotting library I ever used, I've decided to ask the Stackiverse for some insight. In a nutshell, what I need is to create multiple horizontal bar charts, all sharing the x axis, with different number of values on the y axis and with all the bars having the same height, while the charts themselves would adjust to the amount of entries. A simplified data structure of what I need to plot would be something like:

[
    {""name"": ""Category 1"", ""entries"": [
        {""name"": ""Entry 1"", ""value"": 5},
        {""name"": ""Entry 2"", ""value"": 2},
    ]},
    {""name"": ""Category 2"", ""entries"": [
        {""name"": ""Entry 1"", ""value"": 1},
    ]},
    {""name"": ""Category 3"", ""entries"": [
        {""name"": ""Entry 1"", ""value"": 1},
        {""name"": ""Entry 2"", ""value"": 10},
        {""name"": ""Entry 3"", ""value"": 4},
    ]},    
]


And the closesest I got to what I'd like as a result is using:

import matplotlib.pyplot as plt

def plot_data(data):
    total_categories = len(data)  # holds how many charts to create
    max_values = 1  # holds the maximum number of bars to create
    for category in data:
        max_values = max(max_values, len(category[""entries""]))
    fig = plt.figure(1)
    ax = None
    for index, category in enumerate(data):
        entries = []
        values = []
        for entry in category[""entries""]:
            entries.append(entry[""name""])
            values.append(entry[""value""])
        if not entries:
            continue  # do not create empty charts
        y_ticks = range(1, len(entries) + 1)
        ax = fig.add_subplot(total_categories, 1, index + 1, sharex=ax)
        ax.barh(y_ticks, values)
        ax.set_ylim(0, max_values + 1)  # limit the y axis for fixed height
        ax.set_yticks(y_ticks)
        ax.set_yticklabels(entries)
        ax.invert_yaxis()
        ax.set_title(category[""name""], loc=""left"")
    fig.tight_layout()


This will keep the bar height the same (at least across the figure) no matter how many entries have a certain category, thanks to the y limit (set_ylim()) set to the highest number of bars across the data. However, it will also leave nasty gaps in categories that have less than max number of entries. Or to put everything in a visual perspective, I'm trying to get it from Actual to Expected: 



I've tried removing the gaps through gridspec and different scales in dependence of number of entries but that only ended up looking even more 'skewed' and inconsistent. I tried generating multiple charts and manipulating the figure size then stitching them together in post-process but I couldn't find a way to reliably have the bar height remain the same no matter the data. I'm certain there is a way to extract the needed metrics for precise scaling from some obscure object in matplotlib but at this point I'm afraid I'll go on another wild-goose chase if I try to trace the whole drawing procedure.

Furthermore, if individual subplots can be collapsed around the data, how could I make the figure grow based on the data? For example, if I were to add a fourth category to the above data instead of having the figure 'grow' in height by another chart, it will actually shrink all the charts to fit everything on the default figure size. Now, I think I understand the logic behind matplotlib with axis units and all that, and I know I can set the figure size to increase the overall height but I've no idea how to keep it consistent across the charts, namely how to have the bar height exactly the same regardless of the data?

Do I really need to plot everything manually to get what I want? If so, I might just dump the whole matplotlib package and create my own SVGs from scratch. With hindsight, given the amount of time I've spent on this, I probably should've done that in the first place but now I'm way too stubborn to give it up (or I am a victim of the dreaded sunk cost fallacy).

Any ideas?

Thanks
",6,5889,"I think the only way to have at the same time equal bar width (width in vertical direction) and differing subplotsizes is really to manually position the axes in the figure.

To this end you can specify how large in inches a bar should be and how much spacing you want to have between the subplots in units of this bar width. From those numbers together with the amount of data to plot, you can calculate the total figure height in inches. 
Each of the subplots is then positioned (via fig.add_axes) according to the amount of data and the spacing in the previous subplots. Thereby you nicely fill up the plot. 
Adding a new set of data will then make the figure larger.

data = [
    {""name"": ""Category 1"", ""entries"": [
        {""name"": ""Entry 1"", ""value"": 5},
        {""name"": ""Entry 2"", ""value"": 2},
    ]},
    {""name"": ""Category 2"", ""entries"": [
        {""name"": ""Entry 1"", ""value"": 1},
    ]},
    {""name"": ""Category 3"", ""entries"": [
        {""name"": ""Entry 1"", ""value"": 1},
        {""name"": ""Entry 2"", ""value"": 10},
        {""name"": ""Entry 3"", ""value"": 4},
    ]}, 
    {""name"": ""Category 4"", ""entries"": [
        {""name"": ""Entry 1"", ""value"": 6},
    ]},
]

import matplotlib.pyplot as plt
import numpy as np

def plot_data(data,
              barwidth = 0.2, # inch per bar
              spacing = 3,    # spacing between subplots in units of barwidth
              figx = 5,       # figure width in inch
              left = 4,       # left margin in units of bar width
              right=2):       # right margin in units of bar width

    tc = len(data) # ""total_categories"", holds how many charts to create
    max_values = []  # holds the maximum number of bars to create
    for category in data:
        max_values.append( len(category[""entries""]))
    max_values = np.array(max_values)

    # total figure height:
    figy = ((np.sum(max_values)+tc) + (tc+1)*spacing)*barwidth #inch

    fig = plt.figure(figsize=(figx,figy))
    ax = None
    for index, category in enumerate(data):
        entries = []
        values = []
        for entry in category[""entries""]:
            entries.append(entry[""name""])
            values.append(entry[""value""])
        if not entries:
            continue  # do not create empty charts
        y_ticks = range(1, len(entries) + 1)
        # coordinates of new axes [left, bottom, width, height]
        coord = [left*barwidth/figx, 
                 1-barwidth*((index+1)*spacing+np.sum(max_values[:index+1])+index+1)/figy,  
                 1-(left+right)*barwidth/figx,  
                 (max_values[index]+1)*barwidth/figy ] 

        ax = fig.add_axes(coord, sharex=ax)
        ax.barh(y_ticks, values)
        ax.set_ylim(0, max_values[index] + 1)  # limit the y axis for fixed height
        ax.set_yticks(y_ticks)
        ax.set_yticklabels(entries)
        ax.invert_yaxis()
        ax.set_title(category[""name""], loc=""left"")


plot_data(data)
plt.savefig(__file__+"".png"")
plt.show()



",,
matplotlib inconsistent result,https://stackoverflow.com/questions/53171228,Matplotlib: Inconsistent results with images,"I am trying to plot multiple images in a figure using matplotlib. 

Basically, I read the images using PIl library, convert it to numpy array and do some operation on it (setting the elements in a row to zero). Everything works fine till this point. But when I try to save the results using matplotlib, I get inconsistent results.

Please have a look at my code. 

Importing the libraries

import numpy as np
import matplotlib.pyplot as plt
import PIL.Image as PI


Loading the file



fileName = 'n01978287_43.jpg'
img = PI.open(fileName)
size = 224
img = img.resize((size, size))
img = np.asarray(img, dtype=np.uint8).astype(np.float32)
img = img/255


Result 1

temp_img = np.copy(img)
temp_img[51, :, :] = 0*temp_img[51, :, :]
fig = plt.figure()
ax1 = plt.subplot(1, 6, 1)
ax1.imshow(img, interpolation='none')
ax2 = plt.subplot(1, 6, 2)
ax2.imshow(temp_img, interpolation='none')
plt.savefig('test_516.png') 
plt.close(fig) 




Result 2

temp_img = np.copy(img)
temp_img[52, :, :] = 0*temp_img[52, :, :]
fig = plt.figure()
ax1 = plt.subplot(1, 6, 1)
ax1.imshow(img, interpolation='none')
ax2 = plt.subplot(1, 6, 2)
ax2.imshow(temp_img, interpolation='none')
plt.savefig('test_526.png') 
plt.close(fig) 




Result 3

temp_img = np.copy(img)
temp_img[51, :, :] = 0*temp_img[51, :, :]
fig = plt.figure()
ax1 = plt.subplot(1, 2, 1)
ax1.imshow(img, interpolation='none')
ax2 = plt.subplot(1, 2, 2)
ax2.imshow(temp_img, interpolation='none')
plt.savefig('test_512.png') 
plt.close(fig)




Result 4

temp_img = np.copy(img)
temp_img[56, :, :] = 0*temp_img[56, :, :]
fig = plt.figure()
ax1 = plt.subplot(1, 2, 1)
ax1.imshow(img, interpolation='none')
ax2 = plt.subplot(1, 2, 2)
ax2.imshow(temp_img, interpolation='none')
plt.savefig('test_562.png') 
plt.close(fig)




Now, if you look at the results, you would notice the inconsistency. 


Firstly, for first two images (figure with 6 axes), you see the black    line only in one of the image. (There is a pattern to this if you    zero out all the rows (one at a time) and then try to save the  results).
In the last two images, black line gets thicker. (I didn't find any      pattern in this case).


System Setup - Python3, Matplotlib3, PIL, Numpy

Update:

After looking for ways to save a figure with the desired resolution (224*224 in this case), I wrote the following code (using multiple resources from web). 

Importing libraries and loading the image file

import numpy as np
import matplotlib.pyplot as plt
from PIL import Image, ImageDraw, ImageFont

fileName = 'n01978287_43.jpg'
img = Image.open(fileName)
size = 224
img = img.resize((size, size))
img = np.asarray(img, dtype=np.uint8).astype(np.float32)
img = img/255


Function to plot the grid of images

def plt_save(grid, idx, name):

    nRows = len(grid)
    nCols = len(grid[0])
    print('Clearing figure')
    plt.rcParams.update({'font.size': 8})
    wFig = (nCols+2) # Figure width (two more than nCols because I want to add ylabels on the very left and very right of figure)
    hFig = (nRows+1) # Figure height (one more than nRows becasue I want to add xlabels to the top of figure)
    fig = plt.figure(figsize=( wFig, hFig )) 
    fig.subplots_adjust(left=0, bottom=0, right=1, top=1, wspace=0, hspace=0)

    fig.patch.set_facecolor('grey')

    for r in range(nRows):
        for c in range(nCols):

            ax = plt.subplot2grid( shape=[hFig, wFig], loc=[r+1, c+1] )  
            im= ax.imshow(grid[r][c], interpolation='none')
            ax.spines['bottom'].set_visible(False)
            ax.spines['top'].set_visible(False)
            ax.spines['right'].set_visible(False)
            ax.spines['left'].set_visible(False)
            ax.set_xticks([])
            ax.set_yticks([])
            #fig.colorbar(im, ax=ax)
            #ax.set_aspect('auto')

            if not r:
                ax.set_title('Image',
                             rotation=22.5,
                             horizontalalignment='left',
                             verticalalignment='bottom')

            if not c:
                ax.set_ylabel('leftLabel',
                             rotation=0,
                             horizontalalignment='right',
                             verticalalignment='center')

            if c == wFig-3:
                ax2 = ax.twinx()
                #ax2.axis('off')
                ax2.set_xticks([])
                ax2.set_yticks([])
                ax2.spines['top'].set_visible(False)
                ax2.spines['right'].set_visible(False)
                ax2.spines['bottom'].set_visible(False)
                ax2.spines['left'].set_visible(False)
                ax2.set_ylabel( 'rightLabel',
                                rotation=0,
                                verticalalignment='center',
                                horizontalalignment='left' )

    print('Saving file')
    plt.savefig( ( str(idx) + '_' + name + '_' + fileName.split('.')[0] + '.png'), 
                 orientation='landscape', 
                 #bbox_inches='tight', 
                 facecolor = fig.get_facecolor(),
                 dpi=224, # DPI is 224 becasue the axis size is 1x1 inch and I want 224x224 pixels in each axis
                 transparent=True, 
                 frameon=False )
    plt.close(fig)


Loop to zero out the rows of the image (one at a time)

for i in range(0, 224):
    temp_img = np.copy(img)
    temp_img[i, :, :] = 0*temp_img[i, :, :]
    # 1*4 Grid of images (can vary based on the requirement)
    grid = [img, temp_img, img, temp_img]
    grid = [grid, grid] #2*4 grid of images
    plt_save(grid, i, 'PLT_')


Here is how one of the 224 images looks like. 


The thing is that it works perfectly as long as I stick with this kind of plot. But the moment I try to make some changes (like adding a colorbar, having some spaces between each axis etc), the image resolution changes. If I use bbox_inches = 'tight' while saving my figure, it adjusts everything but changes the original resolution while keeping the figure size constant. 

Is there any other way similar to bbox_inches='tight' such that it can keep the axis resolution fixed while adjusting the figure size accordingly. Or if there is no such thing in matplotlib, could you suggest me any other way to incorporate colorbar (small spaces between axis, ylabel for each axis etc) while keeping the image resolution fixed. 
",3,310,"The image you start with has 224 pixels in height.  

In the first two cases you distribute those over 72 pixels in the resulting image. This means any row of the image has a 72/224=32% chance of showing up in the final plot. In row number 52 you are lucky and hit this one third chance.

In the second two cases the resulting image is 226 pixels in height (i.e. just slightly larger than the original). Here you have a 2/224=0.9% chance that one row will occupy two pixels. In the case of row no. 56 you hit that unlucky chance.
",,
matplotlib inconsistent result,https://stackoverflow.com/questions/36763853,Plotting a set of given points to form a closed curve in matplotlib,"I have (tons of) coordinates of points for  closed curve(s) sorted in x-increasing order. 

When plot it in the regular way the result i get is this:

(circle as an example only, the shapes I currently have can be, at best, classified as amoeboids)


But the result I am looking for is something like this:


I have looked through matplotlib and I couldn't find anything. (Maybe I had my keywords wrong...?)

I have tried to reformat the data in the following ways:


Pick a point at random, find its nearest neighbor and then the next nearest neighbor and so on..
It fails at the edges where, sometimes the data isn't too consistent (the nearest neighbor maybe on the opposite side of the curve).
To account for inconsistent data, I tried to check if the slope between two points (which are being considered as nearest neighbors) matches with the previously connected slope - Fails, for reasons I could not find. (spent considerable number of hours before I gave up)
Pick x_minimum and x_maximum (and corresponding y coordinates) and draw an imaginary line and sort for points on either side of the line. - Fails when you have a curve that looks like a banana.


Is there a python package/library that can help me get to where I want.? Or can you help me with ideas to sort my data points better.? Thanks in advance.

EDIT:

Tried the ConcaveHull on the circle I had, any idea why the lines are overlapping at places.? Here's the image:


EDIT2:
The problem was sorted out by changing part of my code as suggested by @Reblochon Masque in the comment section in his answer.
",3,8332,"If you don't know how your points are set up (if you do I recommend you follow that order, it will be faster) you can use Convex Hull from scipy:

import matplotlib.pyplot as plt
from scipy.spatial import ConvexHull

# RANDOM DATA
x = np.random.normal(0,1,100)
y = np.random.normal(0,1,100)
xy = np.hstack((x[:,np.newaxis],y[:,np.newaxis]))

# PERFORM CONVEX HULL
hull = ConvexHull(xy)

# PLOT THE RESULTS
plt.scatter(x,y)
plt.plot(x[hull.vertices], y[hull.vertices])
plt.show()


, which in the example above results is this:



Notice this method will create a bounding box for your points.
","Here is an example that will maybe do what you want and solve your problem:
more info here

import numpy as np
import matplotlib.pyplot as plt
from scipy.spatial import ConvexHull

points = np.random.rand(30, 2)   # 30 random points in 2-D
hull = ConvexHull(points)

#xs = np.array([point[0] for point in points]) 
#ys = np.array([point[1] for point in points]) 

#xh = np.array([point[0] for point in hull.points]) 
#yh = np.array([point[1] for point in hull.points]) 

plt.plot(points[:,0], points[:,1], 'o')
for simplex in hull.simplices:
    plt.plot(points[simplex, 0], points[simplex, 1], 'k-')


plt.plot(points[hull.vertices,0], points[hull.vertices,1], 'r--', lw=2)
plt.plot(points[hull.vertices[0],0], points[hull.vertices[0],1], 'ro')
plt.show()


The points on the of the convex hull are plotted separately and joined to form a polygon. You can further manipulate them if you want.



I think this is maybe a good solution (easy and cheap) to implement in your case. It will work well if your shapes are convex. 

In case your shapes are not all convex, one approach that might be successful could be to sort the points according to which neighbor is closest, and draw a polygon from this sorted set.
",
matplotlib inconsistent result,https://stackoverflow.com/questions/32964051,Inconsistent results using matplotlib,"What I would like to achieve is having a plot like



So, if the green line should have the line color of value 19, it should be the same color as between 19 and 20 on the right hand side. I also know exactly the range where I will have values (here from 17 to 25).

The approach below sometimes work, but strangely it does only work about each second time. When I execute it again, suddenly I get a completely blue line (rgb 0 0 255). Is there something wrong with my approach?

import matplotlib
import matplotlib.pyplot as plt
import numpy as np

fig, ax = plt.subplots()

cmap = plt.get_cmap('cubehelix')

minval = 17
maxval = 25
bounds = np.arange(minval, maxval+1)

mynorm = matplotlib.colors.Normalize(vmin = minval, vmax = maxval)
sm = matplotlib.cm.ScalarMappable(norm=mynorm, cmap=cmap)

color = sm.to_rgba(20)
ax.plot([0, 100], [0, 100], c=color, lw=2)

ax2 = fig.add_axes([0.90, 0.1, 0.03, 0.8])
cb = matplotlib.colorbar.ColorbarBase(ax2, cmap=cmap, norm=mynorm, spacing='proportional', ticks=bounds, boundaries=bounds, format='%1i')

plt.show()


I'm using Python 3.5.0 with matplotlib 1.4.3.
",3,388,"I have found that the c parameter can sometimes produce non-intuitive results. However, simply using the color parameter tends to be much more consistent.

Does this code fix your issue?

import matplotlib
import matplotlib.pyplot as plt
import numpy as np

fig, ax = plt.subplots()

cmap = plt.get_cmap('cubehelix')

minval = 17
maxval = 25
bounds = np.arange(minval, maxval+1)

mynorm = matplotlib.colors.Normalize(vmin = minval, vmax = maxval)
sm = matplotlib.cm.ScalarMappable(norm=mynorm, cmap=cmap)

color = sm.to_rgba(20)
ax.plot([0, 100], [0, 100], color=color, lw=2) # Using 'color' instead of 'c'

ax2 = fig.add_axes([0.90, 0.1, 0.03, 0.8])
cb = matplotlib.colorbar.ColorbarBase(
    ax2, cmap=cmap, norm=mynorm, spacing='proportional', 
    ticks=bounds, boundaries=bounds, format='%1i')

plt.show()


This issue is referenced here, https://github.com/matplotlib/matplotlib/issues/5197 it will be fixed in 1.5.0
","Your code looks fine. I ran it on Python 2.7.10 |Anaconda 2.3.0 (x86_64)|OS X 10.10.5 without getting your reported error.
",
matplotlib inconsistent result,https://stackoverflow.com/questions/62957597,Matplotlib patches contains_point() returns inconsistent results,"Why does changing the plotting parameters of matplotlib.patches contains_point() change the value to be nonsensical? For example:
import matplotlib.patches as mpatches
print(mpatches.Circle((0,0), radius=1, ec='r').contains_point((0,1.5)))
print(mpatches.Circle((0,0), radius=1).contains_point((0,1.5)))

returns True for the first line, and False for the second - when obviously a unit circle centered at (0,0) does not contain (0, 1.5).
",1,357,"In the contains_point function you could specify a radius parameter so that all points enclosed between the borders of the patch and half the radius specified are also considered in the area. When you specify an edgecolor a default linewidth is set to 1 and this is going to modify the radius parameter in the contains_point function. In the source code of this function in fact you can see that there's a call to a _process_radius function that modify the radius parameter of contains_point to linewidth. This behaviour is documented in the docs of the contains_point function of the general path object:

The path is extended tangentially by radius/2; i.e. if you would draw the path with a linewidth of radius, all points on the line would still be considered to be contained in the area. Conversely, negative values shrink the area: Points on the imaginary line will be considered outside the area.

From this we can understand the behaviour you are experimenting. To solve you can:

set_edgecolor after checked for points
always specify radius=0 in the contains_point function

###### Solution 1 ######
import matplotlib.patches as mpatches
import matplotlib.pyplot as plt

circle_ec = mpatches.Circle((0,0), radius=1)
circle = mpatches.Circle((0,0), radius=1)
point = (0,1.5)
print(circle_ec.contains_point(point, radius = 0))         # prints False
print(circle.contains_point(point))                        # prints False
circle_ec.set_edgecolor('r')

###### Solution 2 #######
import matplotlib.patches as mpatches
import matplotlib.pyplot as plt

circle_ec = mpatches.Circle((0,0), radius=1, ec='r')
circle = mpatches.Circle((0,0), radius=1)
point = (0,1.5)
print(circle_ec.contains_point(point, radius = 0))         # prints False
print(circle.contains_point(point))                        # prints False


Anyway I think that is an unwanted behaviour since linewidth is specified in points and not in data coords. Opened an issue here.
",,
matplotlib inconsistent result,https://stackoverflow.com/questions/36264305,Matplotlib Multi-colored Title (Text) - in practice,"There is an example here for how to create a multi-colored text title.

However, I want to apply this to a plot that already has a figure in it. 

For example, if I apply it to this (same code as with the example minus a few extras and with another figure)...:

plt.rcdefaults()
import matplotlib.pyplot as plt
%matplotlib inline
from matplotlib import transforms

fig = plt.figure(figsize=(4,3), dpi=300)

def rainbow_text(x,y,ls,lc,**kw):

    t = plt.gca().transData
    fig = plt.gcf()
    plt.show()

    #horizontal version
    for s,c in zip(ls,lc):
        text = plt.text(x,y,"" ""+s+"" "",color=c, transform=t, **kw)
        text.draw(fig.canvas.get_renderer())
        ex = text.get_window_extent()
        t = transforms.offset_copy(text._transform, x=ex.width, units='dots')

plt.figure()
rainbow_text(0.5,0.5,""all unicorns poop rainbows ! ! !"".split(), 
        ['red', 'orange', 'brown', 'green', 'blue', 'purple', 'black'],
        size=40)


...the result is 2 plots with the title enlarged.
This sort of makes sense to me because I'm using plt. two times. 
But how do I integrate it so that it only refers to the first instance of plt. in creating the title?

Also, about this line:        

t = transforms.offset_copy(text._transform, x=ex.width, units='dots')


I notice it can alter the spacing between words, but when I play with the values of x, results are not predictable (spacing is inconsistent between words). 
How can I meaningfully adjust that value?

And finally, where it says ""units='dots'"", what are the other options? Are 'dots' 1/72nd of an inch (and is that the default for Matplotlib?)?

How can I convert units from dots to inches? 

Thanks in advance!
",0,8935,"In fact the bounding box of the text comes in units unlike the ones used, for example, in scatterplot. Text is a different kind of object that gets somehow redraw if you resize the window or change the ratio. By having a stabilized window you can ask the coordinates of the bounding box in plot units and build your colored text that way:

a = ""all unicorns poop rainbows ! ! !"".split()
c = ['red', 'orange', 'brown', 'green', 'blue', 'purple', 'black']

f = plt.figure(figsize=(4,3), dpi=120)
ax = f.add_subplot(111)

r = f.canvas.get_renderer()
space = 0.1
w = 0.5
counter = 0
for i in a:
    t = ax.text(w, 1.2, a[counter],color=c[counter],fontsize=12,ha='left')
    transf = ax.transData.inverted()
    bb = t.get_window_extent(renderer=f.canvas.renderer)
    bb = bb.transformed(transf)
    w = w + bb.xmax-bb.xmin + space
    counter = counter + 1
plt.ylim(0.5,2.5)
plt.xlim(0.6,1.6)
plt.show()


, which results in:



This, however, is still not ideal since you need to keep controlling the size of your plot axis to obtain the correct spaces between words. This is somewhat arbitrary but if you manage to do your program with such a control it's feasible to use plot units to achieve your intended purpose.

ORIGINAL POST:

plt. is just the call to the library. In truth you are creating an instance of plt.figure in the global scope (so it can be seen in locally in the function). Due to this you are overwriting the figure because you use the same name for the variable (so it's just one single instance in the end). To solve this try controlling the names of your figure instances. For example:

import matplotlib.pyplot as plt
#%matplotlib inline
from matplotlib import transforms

fig = plt.figure(figsize=(4,3), dpi=300)
#plt.show(fig)

def rainbow_text(x,y,ls,lc,**kw):

    t = plt.gca().transData
    figlocal = plt.gcf()

    #horizontal version
    for s,c in zip(ls,lc):
        text = plt.text(x,y,"" ""+s+"" "",color=c, transform=t, **kw)
        text.draw(figlocal.canvas.get_renderer())
        ex = text.get_window_extent()
        t = transforms.offset_copy(text._transform, x=ex.width, units='dots')

    plt.show(figlocal) #plt.show((figlocal,fig))

#plt.figure()
rainbow_text(0.5,0.5,""all unicorns poop rainbows ! ! !"".split(), 
        ['red', 'orange', 'brown', 'green', 'blue', 'purple', 'black'],
        size=40,)


I've commented several instructions but notice I give a different name for the figure local to the function (figlocal). Also notice that in my examples of show I control directly which figure should be shown.

As for your other questions notice you can use other units as can be seen in the function documentation:

Return a new transform with an added offset.
      args:
        trans is any transform
      kwargs:
        fig is the current figure; it can be None if units are 'dots'
        x, y give the offset
        units is 'inches', 'points' or 'dots'


EDIT: Apparently there's some kind of problem with the extents of the bounding box for text that does not give the correct width of the word and thus the space between words is not stable. My advise is to use the latex functionality of Matplotlib to write the colors in the same string (so only one call of plt.text). You can do it like this:

import matplotlib
import matplotlib.pyplot as plt
matplotlib.use('pgf')
from matplotlib import rc

rc('text',usetex=True)
rc('text.latex', preamble=r'\usepackage{color}')

a = ""all unicorns poop rainbows ! ! !"".split()
c = ['red', 'orange', 'brown', 'green', 'blue', 'purple', 'black']
st = ''
for i in range(len(a)):
    st = st + r'\textcolor{'+c[i]+'}{'+a[i]+'}'
plt.text(0.5,0.5,st)
plt.show()


This however is not an ideal solution. The reason is that you need to have Latex installed, including the necessary packages (notice I'm using the color package). Take a look at Yann answer in this question: Partial coloring of text in matplotlib
","Here is an example where there are 2 plots and 2 instances of using the function for posterity:

import matplotlib.pyplot as plt
%matplotlib inline
dpi=300
f_width=4
f_height=3
f = plt.figure(figsize=(f_width,f_height), dpi=dpi)

ax1 = plt.subplot2grid((100,60), (0,0), rowspan=95, colspan=30)
ax2 = plt.subplot2grid((100,60), (0,30), rowspan=95, colspan=30)

f=f #Name for figure
string = str(""Group 1 ,vs. ,Group 2 (,sub1,) and (,sub2,)"").split(',')
color = ['black','red','black','green','black','blue','black']
xpos = .5
ypos = 1.2
axis=ax1
#No need to include space if incuded between delimiters above
#space = 0.1 
def colortext(f,string,color,xpos,ypos,axis):
#f=figure object name (i.e. fig, f, figure)
    r = f.canvas.get_renderer()
    counter = 0
    for i in string:
        t = axis.text(xpos, ypos, string[counter],color=color[counter],fontsize=12,ha='left')
        transf = axis.transData.inverted()
        bb = t.get_window_extent(renderer=f.canvas.renderer)
        bb = bb.transformed(transf)
        xpos = xpos + bb.xmax-bb.xmin
        counter = counter + 1
colortext(f,string,color,xpos,ypos,axis)

string2 = str(""Group 1 part 2 ,vs. ,Group 2 (,sub1,) and (,sub2,)"").split(',')
ypos2=1.1
colortext(f,string2,color,xpos,ypos2,axis)

plt.show()

","@armatita: I think your answer actually does what I need. I thought I needed display coordinates instead, but it looks like I can just use axis 1 coordinates, if that's what this is (I'm planning on using multiple axes via subplot2grid). Here's an example:

import matplotlib.pyplot as plt
%matplotlib inline
dpi=300
f_width=4
f_height=3
f = plt.figure(figsize=(f_width,f_height), dpi=dpi)

ax1 = plt.subplot2grid((100,115), (0,0), rowspan=95, colspan=25)
ax2 = plt.subplot2grid((100,115), (0,30), rowspan=95, colspan=20)
ax3 = plt.subplot2grid((100,115), (0,55), rowspan=95, colspan=35)
ax4 = plt.subplot2grid((100,115), (0,95), rowspan=95, colspan=20)
r = f.canvas.get_renderer()
t = ax1.text(.5, 1.1, 'a lot of text here',fontsize=12,ha='left')
space=0.1
w=.5

transf = ax1.transData.inverted()
bb = t.get_window_extent(renderer=f.canvas.renderer)
bb = bb.transformed(transf)

e = ax1.text(.5+bb.width+space, 1.1, 'text',fontsize=12,ha='left')

print(bb)
plt.show()


I'm not sure what you mean about controlling the axis size, though. Are you referring to using the code in different environments or exporting the image in different sizes? I plan on having the image used in the same environment and in the same size (per instance of using this approach), so I think it will be okay. Does my logic make sense? I have a weak grasp on what's really going on, so I hope so. I would use it with a function (via splitting the text) like you did, but there are cases where I need to split on other characters (i.e. when a word in parentheses should be colored, but not the parentheses). Maybe I can just put a delimiter in there like ','? I think I need a different form of .split() because it didn't work when I tried it. 
At any rate, if I can implement this across all of my charts, it will save me countless hours. Thank you so much!
"
matplotlib inconsistent issue,https://stackoverflow.com/questions/25511550,Python Matplotlib: Remove black background when rasterizing part of plot in EPS?,"This question is related to a comment on another question.

In matplotlib/python, I am trying to rasterize a specific element in my image and save it to eps. The issue is that when saving to EPS (but not SVG or PDF), a black background appears behind the rasterized element.

Saving to PDF and converting to EPS does not seem to be a reasonable solution, as there are weird pdf2ps and pdftops conversion issues that make understanding bounding boxes very ... scary (or worse, seemingly inconsistent). My current work around involves a convoluted process of saving in svg and export in Inkscape, but this should also not be required.

Here is the sample code needed to reproduce the problem. Matplotlib and Numpy will be needed. If the file is saved to mpl-issue.py, then it can be run with:

python mpl-issue.py

#!/usr/bin/env python
# mpl-issue.py

import numpy as np
import matplotlib as mpl

# change backend to agg
# must be done prior to importing pyplot
mpl.use('agg')

import matplotlib.pyplot as plt

def transparencytest():
    # create a figure and some axes
    f = plt.figure()
    a = {
        'top': f.add_subplot(211),
        'bottom': f.add_subplot(212),
    }

    # create some test data
    # obviously different data on the subfigures
    # just for demonstration
    x = np.arange(100)
    y = np.random.rand(len(x))
    y_lower = y - 0.1
    y_upper = y + 0.1

    # a rasterized version with alpha
    a['top'].fill_between(x, y_lower, y_upper, facecolor='yellow', alpha=0.5, rasterized=True)

    # a rasterized whole axis, just for comparison
    a['bottom'].set_rasterized(True)
    a['bottom'].plot(x, y)

    # save the figure, with the rasterized part at 300 dpi
    f.savefig('testing.eps', dpi=300)
    f.savefig('testing.png', dpi=300)
    plt.close(f)

if __name__ == '__main__':
    print plt.get_backend()
    transparencytest()


The testing.png image looks like this:


The testing.eps image ends up looks like this (in converted pdf versions and the figure-rasterized png):


The black backgrounds behind the rasterized elements are not supposed to be there. How can I remove the black backgrounds when saving an eps figure with rasterized elements in it?

This has been tested with a bunch of other mpl backends, so it does not appear to be a specific problem with agg. Mac OS X 10.9.4, Python 2.7.8 built from MacPorts, Matplotlib 1.3.1.
",0,1955,"This was a known bug which has been fixed.

This is due to the fact that eps does not know about transparency and the default background color for the rasterization was (0, 0, 0, 0) (black which is fully transparent).

I also had this problem (https://github.com/matplotlib/matplotlib/issues/2473) and it is fixed (https://github.com/matplotlib/matplotlib/pull/2479) in matplotlib 1.4.0 which was released last night.
",,
