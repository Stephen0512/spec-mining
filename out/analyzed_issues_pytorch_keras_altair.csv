ISSUE_LINK,AI_VERDICT,REASON,LIBRARY_NAME,API_NAME,ISSUE_DESCRIPTION,NORMAL_CONDITIONS,TRIGGER_CONDITIONS,REASON_FOR_DIFFICULTY_IN_DETECTION,ISSUE_TITLE,ISSUE_BODY,ANSWER_1,ANSWER_2,ANSWER_3,HUMAN_CLASSIFICATION,HUMAN_REASON,ISSUE_SCORE,ISSUE_VIEWS
https://stackoverflow.com/questions/60240434,true,The issue involves unexpected behavior with the PyTorch MaxPool2D API when setting padding=1.,PyTorch,MaxPool2D,"When using MaxPool2D with padding=1, the expected result is a tensor with zero-padding around the input tensor. However, the actual result is the input tensor itself without any padding.",The MaxPool2D API works as expected when padding is set to 0 or any other value.,The issue is triggered when setting padding=1 in the MaxPool2D API.,This issue might be challenging to detect during development and testing as it requires careful observation of the expected padding behavior and comparison with the actual output.,PyTorch MaxPool2D unexpected behavior with padding=1,"I was playing around with MaxPool2D in PyTorch and discovered strange behavior when setting padding=1. Here is what I got:

Code:

import torch
from torch.nn.functional import max_pool2d

TEST = 1


def test_maxpool(negative=False, tnsr_size=2, kernel_size=2, stride=2, padding=0):
    """"""Test MaxPool2D.
    """"""
    global TEST
    print(f'=== TEST {TEST} ===')
    print(*[f'{i[0]}: {i[1]}' for i in locals().items()], sep=' | ')

    inp = torch.arange(1., tnsr_size ** 2 + 1).reshape(1, tnsr_size, tnsr_size)
    inp = -inp if negative else inp

    print('In:')
    print(inp)

    out = max_pool2d(inp, kernel_size, stride, padding=padding)
    print('Out:')
    print(out)
    print()
    TEST += 1


test_maxpool()
test_maxpool(True)

test_maxpool(padding=1)
test_maxpool(True, padding=1)


Out:

=== TEST 1 ===
negative: False | tnsr_size: 2 | kernel_size: 2 | stride: 2 | padding: 0
In:
tensor([[[1., 2.],
         [3., 4.]]])
Out:
tensor([[[4.]]])

=== TEST 2 ===
negative: True | tnsr_size: 2 | kernel_size: 2 | stride: 2 | padding: 0
In:
tensor([[[-1., -2.],
         [-3., -4.]]])
Out:
tensor([[[-1.]]])

=== TEST 3 ===
negative: False | tnsr_size: 2 | kernel_size: 2 | stride: 2 | padding: 1
In:
tensor([[[1., 2.],
         [3., 4.]]])
Out:
tensor([[[1., 2.],
         [3., 4.]]])

=== TEST 4 ===
negative: True | tnsr_size: 2 | kernel_size: 2 | stride: 2 | padding: 1
In:
tensor([[[-1., -2.],
         [-3., -4.]]])
Out:
tensor([[[-1., -2.],
         [-3., -4.]]])


Tests 1, 2, 3 are fine but Test 4 is weird, I expected to get [[0 0], [0 0]] tensor:

In:
[[-1 -2]
 [-3 -4]]

+ padding -&gt; 

[[ 0  0  0  0]
 [ 0 -1 -2  0]
 [ 0 -3 -4  0]
 [ 0  0  0  0]]

-&gt; kernel_size=2, stride=2 -&gt;

[[0 0]
 [0 0]]


According to Test 3 zero padding was used but Test 4 produced controversial result.

What kind of padding (if any) was that? Why does MaxPool2D behave like that?

pytorch 1.3.1
","The documentation is still incorrect in the Parameters section, where it says:

padding – implicit zero padding to be added on both sides

","This was expected behavior since negative infinity padding is done by default.
The documentation for MaxPool is now fixed. See this PR: Fix MaxPool default pad documentation #59404 .
",,false,,,
https://stackoverflow.com/questions/68727252,true,The issue involves unexpected behavior in the manual calculation of cross entropy loss with weight in PyTorch.,PyTorch,cross_entropy,"When manually implementing cross entropy loss with weights, the calculated loss differs from the loss obtained using the PyTorch cross_entropy function with the same weights.",The PyTorch cross_entropy function works as expected when calculating the loss without weights.,The issue is triggered when manually calculating the cross entropy loss with weights.,This issue might be challenging to detect as it requires a deep understanding of the mathematical calculations involved in cross entropy loss and the correct application of weights.,cross entropy loss with weight manual calculation,"Hi just playing around with code, I got the unexpected result of cross entropy loss weight implementation.
pred=torch.tensor([[8,5,3,2,6,1,6,8,4],[2,5,1,3,4,6,2,2,6],[1,1,5,8,9,2,5,2,8],[2,2,6,4,1,1,7,8,3],[2,2,2,7,1,7,3,4,9]]).float()
label=torch.tensor([[3],[7],[8],[2],[5]],dtype=torch.int64)
weights=torch.tensor([1,1,1,10,1,6,1,1,1],dtype=torch.float32)

with this kind of sample variables, pytorch's cross entropy loss gives out 4.7894
loss = F.cross_entropy(pred, label, weight=weights,reduction='mean')
&gt; 4.7894

I manually implemented the cross entropy loss code as below
one_hot = torch.zeros_like(pred).scatter(1, label.view(-1, 1), 1)
log_prb = F.log_softmax(pred, dim=1)
loss = -(one_hot * log_prb).sum(dim=1).mean()

this kind of implementation gives same result with pytorch's cross entropy function if given without weight value. However with weight value
one_hot = torch.zeros_like(pred).scatter(1, label.view(-1, 1), 1)
log_prb = F.log_softmax(pred, dim=1)
loss = -(one_hot * log_prb)*weights.sum(dim=1).sum()/weights.sum()
&gt; 3.9564

it gives out different loss value with pytorch module(4.7894).
I can roughly estimate that my understanding of loss's weight have some problem here, but I can't find out the exact reason for this kind of discrepancy.
Can anybody help me handling this issue?
","I found out the problem. It was quite simple...
I shouldn't have divided with the whole sum of weights.
Instead with dividing with wt.sum() (wt=one_hot*weight) got me 4.7894.
&gt;&gt;&gt; wt = one_hot*weights
&gt;&gt;&gt; loss = -(one_hot * log_prb * weights).sum(dim=1).sum() / wt.sum()
4.7894

The denominator was only with 'related' weight value, not whole.
",,,false,,,
https://stackoverflow.com/questions/59219635,false,The issue does not involve unexpected behavior or unpredictable outcomes related to a specific API. It is a question about achieving a specific vectorized computation using PyTorch.,,,,,,,PyTorch vectorise sum lookup quantity into buckets,"Using PyTorch, I have figured out the following code for calculating totals of an item's property by some ""bucket index"":

DATASET_SIZE = 10
NUM_BUCKETS = 4
bucket_assignment = torch.tensor([0,1,2,3,0,1,2,3,0,1], dtype = torch.long)
values_to_add = torch.tensor([1,2,3,4,5,6,7,8,9,10], dtype = torch.float)
buckets = torch.zeros(NUM_BUCKETS, dtype = torch.float)
buckets.index_add_(0, bucket_assignment, values_to_add)

# Buckets is now tensor([15., 18., 10., 12.])


In my case this is specifically to check allocation bounds on a problem, and later code checks that no bucket is under- or over- allocated.

I would like to check multiple different possible assignments at once (and later pick a best option, code not shown). I thought I could do this by adding another dimension to bucket_assignment plus to buckets and have each row be a different set of assignments. However, this does not work as intended, because the second argument of index_add_ must be a simple vector, I cannot pass in any higher rank tensor.

E.g.

BATCH_SIZE = 2
DATASET_SIZE = 5
NUM_BUCKETS = 3
bucket_assignment = torch.tensor([[0,1,2,0,1], [1,1,1,2,1]], dtype = torch.long)
values_to_add = torch.tensor([1,2,3,4,5], dtype = torch.float)
buckets = torch.zeros(BATCH_SIZE, NUM_BUCKETS, dtype = torch.float)
buckets.index_add_(0, bucket_assignment, values_to_add)


I would like to get this result:

tensor([[5., 7., 3.], [ 0., 11.,  4.]])


Instead, I get an error:

RuntimeError: invalid argument 3: Index is supposed to be a vector at ../aten/src/TH/generic/THTensorEvenMoreMath.cpp:733


That's not unexpected due to the limitations of .index_add, but I don't know how to progress.

I am not sure what other approach would allow me to solve this problem in PyTorch - is there some other torch method I could use that would allow me to achieve the same thing. The main goal here is vectorisation and avoiding loops in Python, as in reality the batch sizes are large and I will be taking advantage of GPU acceleration.
","If the batch size is the problem you could use torch.masked_select to get the values to add up for each bucket torch.masked_select(values_to_add, bucket_assignment == bucket_num), where PyTorch will broadcast the values_to_add and then only iterate over the buckets in plain python like so:

def bucket_sizes(bucket_num):
    mask = bucket_assignment == bucket_num
    buckets = torch.masked_select(values_to_add, mask)
    buckets = torch.split(buckets, list(mask.sum(dim=1)))
    return [bucket.sum() for bucket in buckets]

torch.tensor([bucket_sizes(i) for i in range(NUM_BUCKETS)]).T

",,,false,,,
https://stackoverflow.com/questions/61081303,false,The issue does not involve unexpected behavior or unpredictable outcomes related to a specific API. It is a question about the calculation of gradients in PyTorch.,,,,,,,PyTorch gradient calculation gives unexpected results,"The following code is intended to explain how the PyTorch gradient calculation works, and it should, IMO return the weights matrix, but it doesn't:

# the code calculates T x W + B ---&gt; K1
# compute the mean of K1 --&gt; km
# compute the gradient of km relative to T
#
import torch
torch.manual_seed(0)
t = torch.rand(2,3)
w = torch.rand(3,4)
b = torch.rand(1,4)
#
k1 = torch.mm(t, w) + b
#
#torch.set_grad_enabled(True)
print('k1_grad_fn ',k1.grad_fn)
#
print('t grad ',t.grad)
#
#
km = k1.mean()
km.requires_grad_(True)
print('k1 mean=',km)
km.backward()
print('t grad ',t.grad)
print('k1 grad ',k1.grad_fn)


The results are:

t grad  None
k1 mean= tensor(1.0396, requires_grad=True)
t grad  None
k1 grad  None```


","It's not strange, it is expected as indicated by Autograd Mechanics in PyTorch documentation:


  Backward computation is never performed in the subgraphs, where all
  Tensors didn’t require gradients.


Setting km.required_grad_(True) creates a subgraph (with only single operation being a mean of k1 tensor) so you get k1_mean.grad. Please notice k1 grad would also be None in this case as it doesn't require gradient.

By default tensor has required_grad set to False so the graph doesn't have to perform unnecessary operations. To get your desired behaviour change your t to:

t = torch.rand((2, 3), requires_grad=True)


(each tensor function has requires_grad argument). This gives you, as you probably expected:

k1_grad_fn  &lt;AddBackward0 object at 0x7fe3454d6100&gt;
t grad  None
k1 mean= tensor(1.0396, grad_fn=&lt;MeanBackward0&gt;)
t grad  tensor([[0.3093, 0.1177, 0.2888],
        [0.3093, 0.1177, 0.2888]])
k1 grad  &lt;AddBackward0 object at 0x7fe3454d6100&gt;

",,,false,,,
https://stackoverflow.com/questions/55047065,true,The issue involves unexpected behavior when loading a saved model in PyTorch due to key mismatches in the state_dict.,PyTorch,load_state_dict,"When loading a saved model using load_state_dict, the error 'Unexpected key(s) in state_dict' is raised, indicating that the keys in the saved state_dict do not match the keys in the model's state_dict.",The load_state_dict API works as expected when the keys in the saved state_dict match the keys in the model's state_dict.,The issue is triggered when attempting to load a saved model with mismatched keys in the state_dict.,This issue might be challenging to detect as it requires careful inspection of the saved model's state_dict and the model's state_dict to identify key mismatches.,"Unexpected key(s) in state_dict: &quot;model&quot;, &quot;opt&quot;","I'm currently using fast.ai to train an image classifier model.

data = ImageDataBunch.single_from_classes(path, classes, ds_tfms=get_transforms(), size=224).normalize(imagenet_stats)
learner = cnn_learner(data, models.resnet34)

learner.model.load_state_dict(
    torch.load('stage-2.pth', map_location=""cpu"")
)


which results in :


  torch.load('stage-2.pth', map_location=""cpu"")   File
  ""/usr/local/lib/python3.6/site-packages/torch/nn/modules/module.py"",
  line 769, in load_state_dict
      self.class.name, ""\n\t"".join(error_msgs))) RuntimeError: Error(s) in loading state_dict for Sequential:
  
  ...
  
  Unexpected key(s) in state_dict: ""model"", ""opt"". 


I have looked around in SO and tried to use the following solution:

# original saved file with DataParallel
state_dict = torch.load('stage-2.pth', map_location=""cpu"")
# create new OrderedDict that does not contain `module.`
from collections import OrderedDict
new_state_dict = OrderedDict()
for k, v in state_dict.items():
    name = k[7:] # remove `module.`
    new_state_dict[name] = v
# load params
learner.model.load_state_dict(new_state_dict)


which results in :


  RuntimeError: Error(s) in loading state_dict for Sequential:
  
  Unexpected key(s) in state_dict: """".


I'm using Google Colab to train my model and then port the trained model into docker and try to host in in a local server.

What could be the issue? Could it be the different version of pytorch which results in model mismatch?

In my docker config:

# Install pytorch and fastai
RUN pip install torch_nightly -f https://download.pytorch.org/whl/nightly/cpu/torch_nightly.html
RUN pip install fastai


While my Colab is using the following:

 !curl -s https://course.fast.ai/setup/colab | bash

","My strong guess is that stage-2.pth contains two top-level items: the model itself (its weights) and the final state of the optimizer which was used to train it. To load just the model, you need only the former. Assuming things were done in the idiomatic PyTorch way, I would try

learner.model.load_state_dict(
    torch.load('stage-2.pth', map_location=""cpu"")['model']
)


Update: after applying my first round of advice it becomes clear that you're loading a savepoint create with a different (perhaps differently configured?) model than the one you're loading it into. As you can see in the pastebin, the savepoint contains weights for some extra layers, not present in your model, such as bn3, downsample, etc. 


  ""0.4.0.bn3.running_var"", ""0.4.0.bn3.num_batches_tracked"", ""0.4.0.downsample.0.weight""


at the same time some other key names match, but the tensors are of different shapes.


  size mismatch for 0.5.0.downsample.0.weight: copying a param with shape torch.Size([512, 256, 1, 1]) from checkpoint, the shape in current model is torch.Size([128, 64, 1, 1]).


I see a pattern that you consistently try to load a parameter of shape [2^(x+1), 2^x, 1, 1] in place of [2^(x), 2^(x-1), 1, 1]. Perhaps you're trying to load a model of different depth (ex. loading vgg-16 weights for vgg-11?). Either way, you need to figure out the exact architecture used to create your savepoint and then recreate it before loading the savepoint.

PS. In case you weren't sure - savepoints contain model weights, along with their shapes and (autogenerated) names. They do not contain the full specification of the architecture itself - you need to assure yourself, that you're calling model.load_state_dict with model being of exactly the same architecture as was used to create the savepoint. Otherwise you will likely have weight names mismatching.
",,,false,,,
https://stackoverflow.com/questions/46613117,false,The issue does not involve unexpected behavior or unpredictable outcomes related to a specific API. It is a question about installing and using the tk package in a Docker container.,,,,,,,Why does conda install tk not work in my docker container even though it says its installed?,"I was having issues with tk in my python 3 docker container.

I tried:

conda install tk


but it says it did install it

root@36602e2cd649:/home_simulation_research/overparametrized_experiments/pytorch_experiments# conda install tk
Fetching package metadata ...........
Solving package specifications: .

# All requested packages already installed.
# packages in environment at /opt/conda:
#
tk                        8.6.7                h5979e9b_1


but when I go to python and try to import it it does not work:

&gt;&gt;&gt; import Tkinter
Traceback (most recent call last):
  File ""&lt;stdin&gt;"", line 1, in &lt;module&gt;
ImportError: No module named 'Tkinter'


and other errors:

&gt;&gt;&gt; import tkinter
Traceback (most recent call last):
  File ""&lt;stdin&gt;"", line 1, in &lt;module&gt;
  File ""/opt/conda/envs/pytorch-py35/lib/python3.5/tkinter/__init__.py"", line 35, in &lt;module&gt;
    import _tkinter # If this fails your Python may not be configured for Tk
ImportError: libX11.so.6: cannot open shared object file: No such file or directory


when I run a script that needs it:

Traceback (most recent call last):
  File ""bulk_experiment_dispatcher.py"", line 18, in &lt;module&gt;
    from data_file import *
  File ""/home_simulation_research/overparametrized_experiments/pytorch_experiments/data_file.py"", line 16, in &lt;module&gt;
    import matplotlib.pyplot as plt
  File ""/opt/conda/envs/pytorch-py35/lib/python3.5/site-packages/matplotlib/pyplot.py"", line 115, in &lt;module&gt;
    _backend_mod, new_figure_manager, draw_if_interactive, _show = pylab_setup()
  File ""/opt/conda/envs/pytorch-py35/lib/python3.5/site-packages/matplotlib/backends/__init__.py"", line 32, in pylab_setup
    globals(),locals(),[backend_name],0)
  File ""/opt/conda/envs/pytorch-py35/lib/python3.5/site-packages/matplotlib/backends/backend_tkagg.py"", line 6, in &lt;module&gt;
    from six.moves import tkinter as Tk
  File ""/opt/conda/envs/pytorch-py35/lib/python3.5/site-packages/six.py"", line 92, in __get__
    result = self._resolve()
  File ""/opt/conda/envs/pytorch-py35/lib/python3.5/site-packages/six.py"", line 115, in _resolve
    return _import_module(self.mod)
  File ""/opt/conda/envs/pytorch-py35/lib/python3.5/site-packages/six.py"", line 82, in _import_module
    __import__(name)
  File ""/opt/conda/envs/pytorch-py35/lib/python3.5/tkinter/__init__.py"", line 35, in &lt;module&gt;
    import _tkinter # If this fails your Python may not be configured for Tk
ImportError: libX11.so.6: cannot open shared object file: No such file or directory


I tried apt-get install python-tk (from Install tkinter for Python) but it did not work:

root@36602e2cd649:/home_simulation_research/overparametrized_experiments/pytorch_experiments# apt-get install python-tk
Reading package lists... Done
Building dependency tree
Reading state information... Done
E: Unable to locate package python-tk




I tried running ENTERYPOINT as one of the answers suggested but it threw me some more errors:

/path/fake_gui.sh: 8: /home_simulation_research/overparametrized_experiments/docker_files/runtime/fake_gui.sh: source: not found
/path/fake_gui.sh: 12: /home_simulation_research/overparametrized_experiments/docker_files/runtime/fake_gui.sh: function: not found
/path/fake_gui.sh: 13: kill: invalid signal number or name: SIGTERM
/path/fake_gui.sh: 15: /home_simulation_research/overparametrized_experiments/docker_files/runtime/fake_gui.sh: Syntax error: ""}"" unexpected


but not sure what to do...



Helpful questions:

How to install python-tk in my docker image
","Ok so once I put a dummy screen in the image it stopped crashing:

RUN apt-get update
RUN apt-get install -y xvfb
#RUN Xvfb :1 -screen 0 1024x768x16 &amp;&gt; xvfb.log  &amp;


when I ran my docker image.
","You need to use a Docker container that has a virtual framebuffer installed and running.

This blog post explains the ""Why and How"" of putting X11 into a docker container.

You can see how they're doing it in the Selenium Docker container via their entry_point.sh:

#!/bin/bash
#
# IMPORTANT: Change this file only in directory Standalone!

source /opt/bin/functions.sh

export GEOMETRY=""$SCREEN_WIDTH""""x""""$SCREEN_HEIGHT""""x""""$SCREEN_DEPTH""

function shutdown {
  kill -s SIGTERM $NODE_PID
  wait $NODE_PID
}

if [ ! -z ""$SE_OPTS"" ]; then
  echo ""appending selenium options: ${SE_OPTS}""
fi

SERVERNUM=$(get_server_num)

rm -f /tmp/.X*lock

xvfb-run -n $SERVERNUM --server-args=""-screen 0 $GEOMETRY -ac +extension RANDR"" \
  java ${JAVA_OPTS} -jar /opt/selenium/selenium-server-standalone.jar \
  ${SE_OPTS} &amp;
NODE_PID=$!

trap shutdown SIGTERM SIGINT
wait $NODE_PID


Source for the above code is from this repository on GitHub, SeleniumHQ/docker-selenium.
","With python 3, you must import as follows:

import tkinter   # with a small caps 't'

",false,,,
https://stackoverflow.com/questions/59815381,false,The issue does not meet the criteria for deeper analysis as it does not indicate any unexpected behavior or failure related to a specific API.,,,,,,,pytorch 1D Dropout leads to unstable learning,"I'm implementing an Inception-like CNN in pytorch.  After the blocks of convolution layers, I have three fully-connected linear layers followed by a sigmoid activation to give me my final regression output.  I'm testing the effects of dropout layers in this network, but it's giving me some unexpected results.

Here is the code:

class MyInception(nn.Module):
    def __init__(self, in_channels, verbose=False):
        super(MyInception, self).__init__()
        self.v = verbose
        ic=in_channels; oc=16
        self.inceptionBlock1 = InceptionBlock(in_channels=ic, out_channels=oc, maxpool=False, verbose=verbose) 
        self.inceptionBlock2 = InceptionBlock(in_channels=oc * 6, out_channels=oc, maxpool=False, verbose=verbose) 
        self.pool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)

        self.regressor = nn.Sequential(
            nn.Linear(oc * 6 * 35 * 35, 1024, bias=True),
            nn.ReLU(inplace=True),
            nn.Dropout(p=0.2, inplace=False),  # &lt;--- Dropout 1
            nn.Linear(1024, 128, bias=True),
            nn.ReLU(inplace=True),
            nn.Dropout(p=0.2, inplace=False),  # &lt;--- Dropout 2
            nn.Linear(128, 1, bias=True),
            nn.Sigmoid()
        )

    def forward(self, x):
        x = self.inceptionBlock1(x)
        x = self.inceptionBlock2(x)
        x = self.pool(x)
        x = torch.flatten(x, 1)
        x = self.regressor(x)
        return x


def train(epochs=10, dot_every=25):
    running = pd.DataFrame(columns=['Epoch','Round','TrainLoss','TestLoss','LearningRate'])
    for epoch in range(epochs):
        train_losses = []
        model.train()
        counter = 0

        for images, targets in train_loader:
            images = images.to(device)
            targets = targets.to(device)

            optimizer.zero_grad()
            outputs = model(images)
            loss = loss_fn(torch.flatten(outputs), targets)
            train_losses.append( loss.item() )
            loss.backward()
            optimizer.step()

            counter += 1
            if counter % dot_every == 0: 
                print(""."",  end='.', flush=True)
                test_loss = test()
            else:
                test_loss = -1.
            lr = np.squeeze(scheduler.get_lr())
            running = running.append(pd.Series([epoch, counter, loss.item(), test_loss, lr], index=running.columns), ignore_index=True)

        test_loss = test()
        train_loss = np.mean(np.asarray(train_losses))
        running = running.append(pd.Series([epoch, counter, train_loss, test_loss, lr], index=running.columns), ignore_index=True)
        print("""")
        print(f""Epoch {epoch+1}, Train Loss: {np.round(train_loss,4)}, Test Loss: {np.round(test_loss, 4)}, Learning Rate: {np.format_float_scientific(lr, precision=4)}"")
    return running


def test():
    model.eval()
    test_losses = []
    for i, (images,targets) in enumerate(test_loader):
        images = images.to(device)
        targets = targets.to(device)
        outputs = model(images)
        loss = loss_fn(torch.flatten(outputs), targets)
        test_losses.append( loss.item() )

    mean_loss = np.mean(np.asarray(test_losses))
    return mean_loss

# instantiate the model
model = MyInception(in_channels=4, verbose=False).to(device)
# define the optimizer and loss function
optimizer = Adam(model.parameters(), lr=0.001, weight_decay=0.0001)
loss_fn = nn.MSELoss()

# run it
results = train(epochs=10, dot_every=20)



Here is a plot of the MSE losses for the training data.  (red = no dropout, green = second dropout only, blue = first dropout only, purple = both dropouts)
Runs with dropout have big increases in losses at the epoch boundaries (dashed vertical lines), with the double dropout even having a big jump in loss at the start of epoch 10.



The important thing is the test loss.  That is much more stable and not too different between either condition after the 5th epoch, so maybe I shouldn't care. But I would like to understand what is going on.


","I cracked the case.  I realized that I flip model.train() to model.eval() in the test call without setting it back to train() after.  Since Dropout behaves differently in train and eval modes, adding in Dropout revealed the bug.
",,,false,,,
https://stackoverflow.com/questions/74944881,false,The issue does not meet the criteria for deeper analysis as it is related to the structure and contents of the built libtorch library rather than any specific API exhibiting unexpected behavior.,,,,,,,Building libtorch from source gives unexpected contents,"I'm trying to build libtorch (i.e. the headers and shared object files for pytorch) from source, but get an unexpected result. I expected something that looks like the libtorch downloads from pytorch.org:
bin/
include/
lib/
share/

but instead got this:
abi-check    Caffe2Config.cmake         cmake_uninstall.cmake   empty.cpp             modules/               TorchConfig.cmake
aten/        Caffe2ConfigVersion.cmake  compile_commands.json   FXdiv/                nccl/                  TorchConfigVersion.cmake
bin/         CMakeCache.txt             confu-deps/             include/              nccl_external-prefix/
build.ninja  CMakeFiles/                CTestTestfile.cmake     install_manifest.txt  sleef/
caffe2/      cmake_install.cmake        detect_cuda_version.cc  lib/                  third_party/

It also contains lots of .o files and other junk, which I didn't expect. And it's missing many of the headers, such as the contents of torch/csrc/autograd/.
I followed the instructions for setting up my environment with Conda. Here's the approximate command I used to build:
export BUILD_CAFFE2=1 # probably don't need this
export BUILD_TEST=0
export CMAKE_PREFIX_PATH=${CONDA_PREFIX:-""$(dirname $(which conda))/../""}
export DEBUG=0
export MAX_JOBS=16
export USE_CUDA=1
export USE_CUDNN=1
export CUDA_HOME=...
export NVCC_EXECUTABLE=...
export CUDNN_LIB_DIR=...
export CUDNN_INCLUDE_DIR=...
export CUDNN_LIBRARY=...

python ../tools/build_libtorch.py

I'm doing this on Linux with Python 3.8.
How can I get it to build the usual libtorch package structure?
","I found a way to create the intended libtorch structure by directly using cmake, following this document:
cmake \
  -DBUILD_SHARED_LIBS:BOOL=ON \
  -DCMAKE_BUILD_TYPE:STRING=Release \
  -DPYTHON_EXECUTABLE:PATH=`which python3` \
  -DCMAKE_INSTALL_PREFIX:PATH=../pytorch-install \
  -DBUILD_CAFFE2=1 \
  -DCMAKE_PREFIX_PATH=${CONDA_PREFIX:-""$(dirname $(which conda))/../""} \
  -DUSE_CUDA=1 \
  -DUSE_CUDNN=1 \
  -DCUDNN_INCLUDE_DIR=... \
  -DCUDNN_LIBRARY=... \
  ../pytorch

cmake --build . --target install -- -j 16

This produces libtorch in ../pytorch-install.
",,,false,,,
https://stackoverflow.com/questions/63533029,false,The issue does not meet the criteria for deeper analysis as it is related to a specific error occurring during the execution of a MEX file and does not involve any specific API exhibiting unexpected behavior.,,,,,,,Unexpected Standard exception from MEX file (pytorch model forward),"When I call mex api from Matlab, I got an unexpected standard exception.
I exported 2 pytorch DNN models to 'A.pt' and 'B.pt' files.
And I implemented c++ functions that load models from the '.pt 'files and run models (forward).
The c++ implementation works fine, I can get proper results from the models.
I built the load &amp; run forward function into '.dll' library,
and I implemented a mex api function can call them.
When I call the mex api in Matlab environment,
the 2 models are loaded normally, and the first model runs forward properly.
However, when run forward the 2nd model, I got the following exception.

Unexpected Standard exception from MEX file
What():The following operation failed in the TorchScript interpreter.
Traceback of TorchScript (most recent call last):
RuntimeError: The following operation failed in the TorchScript interpreter.
Traceback of TorchScript (most recent call last):
RuntimeError: error in LoadLibraryA

I have no clue why the c++ implementation works fine but the exception occurs when call it through mex api from Matlab.
Because the load &amp; run forward functions are unchanged I expected the exactly same results.
It is more difficult to debug because there is no call-stack print.
Is there any way to get call-stack ?
Please give me any advice.
Thanks in advance.
-environment-------------------------------
c++ compiler : visual studio 2017 community
matlab : R2020a
libtorch : 1.6
pytorch : 1.5
python : 3.6
cuda : 10.2

","From Mr. Cris Luengo's comments, I solved this problem by copying all libtorch dlls into Matlab's own bin folder. There are several duplicated files but I overwrote them. I'm not sure it is safe or not, so may be backup of previous dlls is good choice. Thank you Mr. Cris Luengo.
",,,false,,,
https://stackoverflow.com/questions/76788649,true,The issue involves the use of PyTorch Lightning's learning rate tuners and the observed behavior of the loss curve. This issue meets the criteria for deeper analysis as it pertains to the behavior of a specific API (PyTorch Lightning's learning rate tuner) under certain conditions.,,,,,,,Pytorch Lightning Learning Rate Tuners Giving unexpected results,"I'm trying to find an optimal learning rate using python pl.tuner.Tuner but results aren't as expected
The model I am running is a linear classifier on top of a BertForSequenceClassification Automodel
I want to find the optimum learning rate when the bert model is frozen.
To do this I am running this code:
  
    tuner = pl.tuner.Tuner(trainer)
    results = tuner.lr_find(
        model, 
        # optimizer = optimizer,
        
        train_dataloaders=data_module, 
        min_lr=10e-8,
        max_lr=10.0,
    )
    # Plot with
    fig = results.plot(suggest=True)
    fig.show()

My optimizer is configured like this in the model:
   def configure_optimizers(self):
        """"""
        :return:
        """"""
        optimizer = torch.optim.AdamW(self.parameters(), lr=self.learning_rate)

        scheduler = get_linear_schedule_with_warmup(
            optimizer,
            num_warmup_steps=self.n_warmup_steps,
            num_training_steps=self.n_training_steps,
        )
        return dict(optimizer=optimizer, lr_scheduler=dict(scheduler=scheduler, interval=""step""))

This produces:
Chart of loss against learning rate
I am confused as to why the loss is increasing at lower learning rates, and this is not what I was expecting.
I have tried:

removing the scheduler
freezing/ unfreezing the weights
Changing the initial learning rate

I was expecting a chart like this: https://github.com/comhar/pytorch-learning-rate-tuner/blob/master/images/learning_rate_tuner_plot.png
Any help appreciated
Many thanks
","I'm getting a similar plot to you, and there is a similar question on Stackoverflow: Unusual Learning Rate Finder Curve: Loss Lowest at Smallest Learning Rate
It may be due to the issue reported here: https://github.com/Lightning-AI/pytorch-lightning/issues/14167
i.e., there may be some moving average smoothing applied, which starts at 0, so the first few loss values are averaged along with 0 leading to the low loss observed on the plot.
However, that doesn't explain why there are many images online of results without this behaviour, unless they were generated with different versions of Lightning. If it is the cause, though, I guess we just have to ensure that the lowest learning rate tested is much too low to be near the optimal and then ignore the left-hand side of the resulting plot.
","I'm not sure if you have solved this problem, but I suggest you to use a larger num_training in tuner.lr_find()
Accoriding to the source code, the default value is 100
def _lr_find(
trainer: ""pl.Trainer"",
model: ""pl.LightningModule"",
min_lr: float = 1e-8,
max_lr: float = 1,
num_training: int = 100,
mode: str = ""exponential"",
early_stop_threshold: Optional[float] = 4.0,
update_attr: bool = False,
attr_name: str = """", 
) -&gt; Optional[_LRFinder]:

",,false,,,
https://stackoverflow.com/questions/44522661,false,The issue does not meet the criteria for deeper analysis as it is related to PyTorch-specific inspection issues in the PyCharm IDE and does not involve any specific API exhibiting unexpected behavior.,,,,,,,PyTorch specific inspection issues in PyCharm,"Has anyone been able to resolve PyTorch specific inspection issues in PyCharm? Previous posts for non-PyTorch related issues suggest upgrading PyCharm, but I'm currently at the latest version. One option is to of course disable certain inspections entirely, but I'd rather avoid that.

Example: torch.LongTensor(x) gives me ""Unexpected argument..."", whereas both call signatures (with and without x) are both supported.
","I believe it's because torch.LongTensor has no __init__ method for pycharm to find.

According to this source that I found thanks to this SO post :


  Use __new__ when you need to control the creation of a new instance.
  Use __init__ when you need to control initialization of a new instance.
  
  __new__ is the first step of instance creation.  It's called first,
  and is responsible for returning a new instance of your class.  In
  contrast, __init__ doesn't return anything; it's only responsible for
  initializing the instance after it's been created.
  
  In general, you shouldn't need to override __new__ unless you're
  subclassing an immutable type like str, int, unicode or tuple.


Since Tensors are types, it makes sense to define only new and no init.

You can experiment this behavior by testing the following classes :

torch.LongTensor(1)  # Unexpected arguments


Produces the warning while the following doesn't.

class MyLongTensor(torch.LongTensor):
    def __init__(self, *args, **kwargs):
        pass

MyLongTensor(1)  # No error


To confirm that the absence of __init__ is the culprit try :

class Example(object):
    pass

Example(0)  # Unexpected arguments


To find out by yourself, use pycharm to Ctrl+click on LongTensor then _TensorBase and look at the defined methods.
",,,false,,,
https://stackoverflow.com/questions/61510810,false,The issue does not meet the criteria for deeper analysis as it is related to pickling PyTorch models with lambda functions and does not involve any specific API exhibiting unexpected behavior.,,,,,,,How does one pickle arbitrary pytorch models that use lambda functions?,"I currently have a neural network module:

import torch.nn as nn

class NN(nn.Module):
    def __init__(self,args,lambda_f,nn1, loss, opt):
        super().__init__()
        self.args = args
        self.lambda_f = lambda_f
        self.nn1 = nn1
        self.loss = loss
        self.opt = opt
        # more nn.Params stuff etc...

    def forward(self, x):
        #some code using fields
        return out


I am trying to checkpoint it but because pytorch saves using state_dicts it means I can't save the lambda functions I was actually using if I checkpoint with the pytorch torch.save etc. I literally want to save everything without issue and re-load to train on GPUs later. I currently am using this:

def save_ckpt(path_to_ckpt):
    from pathlib import Path
    import dill as pickle
    ## Make dir. Throw no exceptions if it already exists
    path_to_ckpt.mkdir(parents=True, exist_ok=True)
    ckpt_path_plus_path = path_to_ckpt / Path('db')

    ## Pickle args
    db['crazy_mdl'] = crazy_mdl
    with open(ckpt_path_plus_path , 'ab') as db_file:
        pickle.dump(db, db_file)


currently it throws no errors when I chekpoint it and it saved it. 

I am worried that when I train it there might be a subtle bug even if no exceptions/errors are trained or something unexpected might happen (e.g. weird saving on disks in the clusters etc who knows).

Is this safe to do with pytorch classes/nn models? Especially if we want to resume training with GPUs?

Cross posted:


How does one pickle arbitrary pytorch models that use lambda functions?
https://discuss.pytorch.org/t/how-does-one-pickle-arbitrary-pytorch-models-that-use-lambda-functions/79026
https://www.reddit.com/r/pytorch/comments/gagpjg/how_does_one_pickle_arbitrary_pytorch_models_that/?
https://www.quora.com/unanswered/How-does-one-pickle-arbitrary-PyTorch-models-that-use-lambda-functions

","I'm the dill author.  I use dill (and klepto) to save classes that contain trained ANNs inside of lambda functions.  I tend to use combinations of mystic and sklearn, so I can't speak directly to pytorch, but I can assume it works the same.  The place where you have to be careful is if you have a lambda that contains a pointer to an object external to the lambda... so for example y = 4; f = lambda x: x+y.  This might seem obvious, but dill will pickle the lambda, and depending on the rest of the code and the serialization variant, may not serialize the value of y.  So, I've seen many cases where people serialize a trained estimator inside some function (or lambda, or class) and then the results aren't ""correct"" when they restore the function from serialization. The overarching cause is because the function wasn't encapsulated so all objects required for the function to yield the correct results are stored in the pickle. However, even in that case you can get the ""correct"" results back, but you'd just need to create the same environment you had when you pickled the estimator (i.e. all the same values it depends on in the surrounding namespace).  The takeaway should be, try to make sure that all variables used in the function are defined within the function.  Here's a portion of a class I've recently started to use myself (should be in the next release of mystic):

class Estimator(object):
    ""a container for a trained estimator and transform (not a pipeline)""
    def __init__(self, estimator, transform):
        """"""a container for a trained estimator and transform

    Input:
        estimator: a fitted sklearn estimator
        transform: a fitted sklearn transform
        """"""
        self.estimator = estimator
        self.transform = transform
        self.function = lambda *x: float(self.estimator.predict(self.transform.transform(np.array(x).reshape(1,-1))).reshape(-1))
    def __call__(self, *x):
        ""f(*x) for x of xtest and predict on fitted estimator(transform(xtest))""
        import numpy as np
        return self.function(*x)


Note when the function is called, everything that it uses (including np) is defined in the surrounding namespace.  As long as pytorch estimators serialize as expected (without external references), then you should be fine if you follow the above guidelines.
","this is not a good idea. If you do this then if your code changes to a different github repo then it will be hard restore your models that took a lot of time to train. The cycles spent recovering those or retraining is not worth it. I recommend to instead do it the pytorch way and only save the weights as they recommend in pytorch.
","Yes, I think it is safe to use dill to pickle lambda functions etc. I have been using torch.save with dill to save state dict and have had no problems resuming training over GPU as well as CPU unless the model class was changed. Even if the model class was changed (adding/deleting some parameters), I could load state dict, modify it, and load to the model.  

Also, usually, people don't save the model objects but only state dicts i.e parameter values to resume the training along with hyperparameters/model arguments to get the same model object later.

Saving model object can be sometimes problematic as changes to model class (code) can make the saved object useless. If you don't plan on changing your model class/code at all and hence the model object won't be changed then maybe saving objects can work well but generally, it is not recommended to pickle module object. 
",false,,,
https://stackoverflow.com/questions/66936111,true,The issue involves the CustomMNISTDataset subclass not being able to find the dataset files. This indicates a problem with the API behavior.,PyTorch,datasets.MNIST,"The CustomMNISTDataset subclass does not inherit the correct root folder path when calling the super().__init__() method. As a result, it fails to locate the dataset files and raises a ""Dataset not found"" error.",The datasets.MNIST API works as expected when called directly with the correct root folder path.,"The issue is triggered when the CustomMNISTDataset subclass is instantiated without explicitly specifying the root folder path, causing it to use the default root folder path that does not contain the dataset files.","This issue might be challenging to detect during development and testing because it involves the interaction between the subclass and the superclass, and the incorrect behavior is not immediately obvious.",Subclass of PyTorch dataset class cannot find dataset files,"I'm trying to create a subclass of the PyTorch MNIST dataset class, which I call  CustomMNISTDataset, as follows:
import torchvision.datasets as datasets

class CustomMNISTDataset(datasets.MNIST):

    def __init__(self, root='/home/psando'):
        super().__init__(root=root,
                         download=False)

but when I execute:
dataset = CustomMNISTDataset()

it fails with error: ""RuntimeError: Dataset not found. You can use download=True to download it"".
However, when I run the following in the same file:
dataset = datasets.MNIST(root='/home/psando', download=False)
print(len(dataset))

it succeeds and prints ""60000"", as expected.
Since CustomMNISTDataset subclasses datasets.MNIST why is the behavior different? I've verified that the path '/home/psando' contains the MNIST directory with raw and processed subdirectories (otherwise, explicitly calling the constructor for datasets.MNIST() would have failed). The current behavior implies that the call to super().__init__() within CustomMNISTDataset is not calling the constructor for datasets.MNIST which is very strange!
Other details: I'm using Python 3.6.8 with torch==1.6.0 and
torchvision==0.7.0. Any help would be appreciated!
","This requires some source-diving, but your problem is this function. The path to the dataset is dependant on the name of the class, so when you subclass MNIST the root folder changes to /home/psando/CustomMNISTDataset
So if you rename /home/psando/MNIST to /home/psando/CustomMNISTDataset it works.
",,,false,,,
https://stackoverflow.com/questions/66642338,true,"The issue involves unexpected CUDA out of memory behavior in PyTorch, indicating a problem with the API or underlying GPU-related functionality.",PyTorch,CUDA memory management,"The issue occurs when running an image segmentation Unet on a Titan RTX GPU. It throws CUDA out of memory errors at different batch sizes, including cases where the required memory is less than the available free memory.",The PyTorch GPU functionality should be able to allocate the required memory for the image segmentation Unet model and process the specified batch sizes without running out of memory.,"The issue is triggered when running the image segmentation Unet on a Titan RTX GPU, regardless of the batch size used.","This issue might be challenging to detect during development and testing because it involves the interaction between the PyTorch GPU functionality and the specific GPU hardware, and the behavior is not consistent or predictable.",Strange Cuda out of Memory behavior in Pytorch,"Edit: SOLVED- Problem relied on the number of workers, lowered them, problem solved
I am using a 24GB Titan RTX and I am using it for an image segmentation Unet with Pytorch,
it is always throwing Cuda out of Memory at different batch sizes, plus I have more free memory than it states that I need, and by lowering batch sizes, it INCREASES the memory it tries to allocate which doesn't make any sense.
here is what I tried:
Image size = 448, batch size = 8

""RuntimeError: CUDA error: out of memory""

Image size = 448, batch size = 6

""RuntimeError: CUDA out of memory. Tried to allocate 3.12 GiB (GPU 0; 24.00 GiB total capacity; 2.06 GiB already allocated; 19.66 GiB free; 2.31 GiB reserved in total by PyTorch)""

is says it tried to allocate 3.12GB and I have 19GB free and it throws an error??
Image size = 224, batch size = 8

""RuntimeError: CUDA out of memory. Tried to allocate 28.00 MiB (GPU 0; 24.00 GiB total capacity; 2.78 GiB already allocated; 19.15 GiB free; 2.82 GiB reserved in total by PyTorch)""

Image size = 224, batch size = 6

""RuntimeError: CUDA out of memory. Tried to allocate 344.00 MiB (GPU 0; 24.00 GiB total capacity; 2.30 GiB already allocated; 19.38 GiB free; 2.59 GiB reserved in total by PyTorch)""

reduced batch size but tried to allocate more ???
Image size = 224, batch size = 4

""RuntimeError: CUDA out of memory. Tried to allocate 482.00 MiB (GPU 0; 24.00 GiB total capacity; 2.21 GiB already allocated; 19.48 GiB free; 2.50 GiB reserved in total by PyTorch)""

Image size = 224, batch size = 2

""RuntimeError: CUDA out of memory. Tried to allocate 1.12 GiB (GPU 0; 24.00 GiB total capacity; 1.44 GiB already allocated; 19.88 GiB free; 2.10 GiB reserved in total by PyTorch)""

Image size = 224, batch size = 1

""RuntimeError: CUDA out of memory. Tried to allocate 1.91 GiB (GPU 0; 24.00 GiB total capacity; 894.36 MiB already allocated; 20.94 GiB free; 1.03 GiB reserved in total by PyTorch)""

Even with stupidly low image sizes and batch sizes...
","SOLVED- Problem relied on the number of workers, lowered them, problem solved
",,,false,,,
https://stackoverflow.com/questions/54492667,true,"The issue involves a segfault when throwing std::runtime_error in Rcpp, indicating a problem with the Rcpp API or its interaction with the libtorch library.",Rcpp and libtorch,std::runtime_error,The issue occurs when calling a function in Rcpp that throws std::runtime_error. It results in a segfault with different error messages depending on the compiler used.,The Rcpp API should be able to throw and handle std::runtime_error without causing a segfault.,"The issue is triggered when calling the specific function in Rcpp that throws std::runtime_error, regardless of the specific error message or compiler used.","This issue might be challenging to detect during development and testing because it involves the interaction between Rcpp and libtorch, and the segfault is not directly caused by the std::runtime_error itself.",Segfault when throwing std::runtime_error on Ubuntu Xenial with Rcpp,"I am having a very strange behavior with Rcpp together with libtorch. 

I have a file with 2 functions:

#include &lt;torch/torch.h&gt;
#include &lt;Rcpp.h&gt; 

// [[Rcpp::export]]
void test_error () {  
  throw std::runtime_error(""hi this is my error"");
}

// [[Rcpp::export]]
void test_error2 () {  
  Rcpp::Rcout &lt;&lt; torch::arange(1) &lt;&lt; std::endl;
}


When I call test_error() I get a segfault (g++):

terminate called after throwing an instance of 'std::bad_alloc'
  what():  std::bad_alloc
Aborted (core dumped)


The clang++ error is:

terminate called after throwing an instance of 'std::length_error'
  what():  basic_string::_M_create
Aborted (core dumped)


test_error2 works as expected.

This error happens only on Ubuntu Xenial. I tested with Ubuntu Trusty and MacOS and no segfaults. 

If I remove the code for test_error2 from the file, I don't have any error, even if I don't remove the #include &lt;torch/torch.h&gt; line.

Also tested compiling with clang++ and g++. Same error.

I created a smalll repo here with the minimal example I could make. 

Does anyone have any idea of what this could be?


  Note configure file will download and install libtorch automatically from pytorch's website. So don't install the package if you don't want this.

","Turns out that compiling the package with an older version g++ worked fine. 

I installed g++-4.9:

sudo apt-get install g++-4.9.

Edited the .R/Makevars to use g++-4.9:

CXX=g++-4.9
CXX11=g++-4.9


Then recompiled Rcpp and the package.
","Can you try replacing

throw std::runtime_error(""hi this is my error"");


with what our docs suggest you do (in an Rcpp-called function, no less) 

Rcpp::stop(""hi this is my error"");


and see what happens?
",,false,,,
https://stackoverflow.com/questions/63549826,true,"The issue involves CUDA out of memory errors when using stanza lemmatization, indicating a problem with the GPU memory management or the specific implementation of the lemmatization process.",Stanza,stanza.Pipeline,The issue occurs when using stanza.Pipeline for lemmatization on a GPU with limited memory. It throws CUDA out of memory errors even for small input texts.,"The stanza.Pipeline API should be able to handle lemmatization on the specified GPU without running out of memory, even for larger input texts.","The issue is triggered when using stanza.Pipeline for lemmatization on a GPU with limited memory, regardless of the specific input texts or batch sizes used.","This issue might be challenging to detect during development and testing because it involves the interaction between stanza, PyTorch, and the specific GPU hardware, and the behavior is not consistent or predictable.",RuntimeError: CUDA out of memory. Problem with stanza lemmatazation using too much GPU memory,"Good day,
I have 11GB of GPU memory and I run into CUDA memory issue with pretrained lemmatazation.
I used this code:
snlp = stanza.Pipeline(lang=""en"", use_gpu=True) # tried different batch_size/ lemma_batch_size - did not help
nlp = StanzaLanguage(snlp)

def tokenize(text):
     tokens = nlp(text)
     doc_l = [token.lemma_ for token in doc]
     lower_tokens = [t.lower() for t in doc_l]
     alpha_only = [t for t in lower_tokens if t.isalpha()]
     no_stops = [t for t in alpha_only if t not in stopwords]
     #torch.cuda.empty_cache() # Tried this - did not work
     return no_stops

tfidf = TfidfVectorizer(tokenizer=tokenize, min_df=0.1, max_df=0.9)
# Construct the TF-IDF matrix
tfidf_matrix = tfidf.fit_transform(texts)


RuntimeError: CUDA out of memory. Tried to allocate 978.00 MiB (GPU 0;
11.00 GiB total capacity; 6.40 GiB already allocated; 439.75 MiB free; 6.53 GiB reserved in total by PyTorch).

I tried
 [(tokenize(t) for t in test]

It only lasted for 12 texts. They are 200 words on average each. Based on Error message - 'Tried to allocate 978.00 MiB' and this data - SNLP uses 1GiB of GPU memory per step??

This behavior seems strange to me (probably because I dont understand how library works) as model is already pretrained, so it should not get bigger when transforming new texts, right? Why it needs so much GPU memory?
Is there any way to clear memory after each run of lemma_ for each text? (#torch.cuda.empty_cache()-does not work) and batch_size does not work either.

It works on CPU, however allocates all of the available memory (32G of RAM), however. It is much slower on CPU. I need it to make it work on CUDA.
","If you check the full stack trace, there might be a hint which processor runs into the memory issue. For instance, I recently ran into a similar issue with this stack trace:
...
File ""stanza/pipeline/depparse_processor.py"", line 42, in process     
preds += self.trainer.predict(b)   
File ""stanza/models/depparse/trainer.py"", line 74, in predict     
_, preds = self.model(word, word_mask, wordchars,
wordchars_mask, upos, xpos, ufeats, pretrained, lemma, head, deprel,
word_orig_idx, sentlens, wordlens)   
... 
RuntimeError: CUDA out of memory.
Tried to allocate 14.87 GiB (GPU 0; 14.76 GiB total capacity; 460.31 MiB already
allocated; 13.39 GiB free; 490.00 MiB reserved in total by PyTorch)

Which pointed me to the fact that I needed to set depparse_batch_size when calling stanza.Pipeline(...). There are other settings like batch_size and lemma_batch_size that you mentioned, as well as pos_batch_size and ner_batch_size etc. These should really help resolve this issue.
",,,false,,,
https://stackoverflow.com/questions/73629682,true,"The issue involves CUDA out of memory errors when running Stable Diffusion on Windows, indicating a problem with the GPU memory management or the specific implementation of Stable Diffusion.",Stable Diffusion,torch.cuda,"The issue occurs when running Stable Diffusion on Windows and throws CUDA out of memory errors, even though the machine has sufficient GPU memory available.",Stable Diffusion should be able to allocate the required GPU memory and run without running out of memory on the specified Windows machine.,"The issue is triggered when running Stable Diffusion on Windows, regardless of the specific prompt or parameters used.","This issue might be challenging to detect during development and testing because it involves the interaction between Stable Diffusion, PyTorch, and the specific GPU hardware, and the behavior is not consistent or predictable.",Error Running Stable Diffusion from the command line in Windows,"I installed Stable Diffusion v1.4 by following the instructions described in https://www.howtogeek.com/830179/how-to-run-stable-diffusion-on-your-pc-to-generate-ai-images/#autotoc_anchor_2
My machine heavily exceeds the min reqs to run Stable Diffusion:
Windows 11 Pro
11th Gen Intel i7 @ 2.30GHz
Latest NVIDIA GeForce GPU
16GB Memory
1TB SSD
Yet, I get an error when trying to run the test prompt
python scripts/txt2img.py --prompt ""a close-up portrait of a cat by pablo picasso, vivid, abstract art, colorful, vibrant"" --plms --n_iter 5 --n_samples 1
RuntimeError: CUDA out of memory. Tried to allocate 1024.00 MiB (GPU 0; 8.00 GiB total capacity; 6.13 GiB already allocated; 0 bytes free; 6.73 GiB reserved in total by PyTorch) If reserved memory is &gt;&gt; allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Reading a post by Marco Ramos it seems like it relates to the number of workers in PyTorch
Strange Cuda out of Memory behavior in Pytorch
How do I change the number of workers while running Stable Diffusion? And why is it throwing this error if my machine still has lots of memory? Has anyone encountered this same issue while running Stable Diffusion?
","I had the same issue, it's because you're using a non-optimized version of Stable-Diffusion. You have to download basujindal's branch of it, which allows it use much less ram by sacrificing the precision, this is the branch - https://github.com/basujindal/stable-diffusion
Everything else in that guide stays the same just clone from this version. It allow you to even push past 512x512 default resolution, you can use 756x512 to get rectangular images for example (but the results may vary since it was trained on a 512 square set).
the new prompt becomes python optimizedSD/optimized_txt2img.py --prompt ""blue orange"" --H 756 --W 512
Also another note: as of a few days ago an even faster and more optimized version was released by neonsecret (https://github.com/basujindal/stable-diffusion), however I'm having issues installing it, so can't really recommend it but you can try it as well and see if it works for you.
","In addition to the optimized version by basujindal, the additional tags following the prompt allows the model to run properly on a machine with NVIDIA or AMD 8+GB GPU.
So the new prompt would look like this
&gt;&gt; python optimizedSD/optimized_txt2img.py --prompt ""a close-up portrait of a cat by pablo picasso, vivid, abstract art, colorful, vibrant"" --H 512 --W 512 --seed 27 --n_iter 2 --n_samples 10 --ddim_steps 50
",,false,,,
https://stackoverflow.com/questions/57982376,true,"The issue involves incorrect tensor multiplication results in PyTorch, indicating a problem with the tensor multiplication API or its implementation.",PyTorch,Tensor multiplication,"The issue occurs when multiplying two PyTorch tensors, resulting in incorrect output. The expected result is 9939754.4, but the actual result is 9939755.0.",The PyTorch tensor multiplication API should be able to correctly multiply the specified tensors and produce the expected result.,The issue is triggered when performing the specific tensor multiplication operation with the given tensor values.,This issue might be challenging to detect during development and testing because it involves the precision of floating-point arithmetic and the specific values used in the tensors.,Pytorch tensor multiplication with Float tensor giving wrong answer,"I am seeing some strange behavior when i multiply two pytorch tensors.

x = torch.tensor([99397544.0])
y = torch.tensor([0.1])
x * y


This outputs 

tensor([9939755.])


However, the answer should be 9939754.4
","In default, the tensor dtype is torch.float32 in pytorch. Change it to torch.float64 will give the right result. 

x = torch.tensor([99397544.0], dtype=torch.float64)
y = torch.tensor([0.1], dtype=torch.float64)
x * y
# tensor([9939754.4000])


The mismatched result for torch.float32 caused by rounding error if you do not have enough precision to calculate (represent) it. 

What Every Computer Scientist Should Know About Floating-Point Arithmetic
",,,false,,,
https://stackoverflow.com/questions/61966466,true,"The issue involves a matching distribution error when installing torch==1.0.0 from requirements.txt, indicating a problem with the availability or compatibility of the specified PyTorch version.",PyTorch,torch,"The issue occurs when trying to install torch==1.0.0 from requirements.txt, resulting in a matching distribution error.",The specified PyTorch version (1.0.0) should be available and compatible for installation from requirements.txt.,The issue is triggered when attempting to install torch==1.0.0 from requirements.txt.,This issue might be challenging to detect during development and testing because it depends on the availability and compatibility of the specified PyTorch version in the package repository or environment.,No matching distribution found for torch==1.0.0 (from -r requirements.txt (line 25)),"No matching distribution found for torch==1.0.0

I was instructed to use to use pytorch==1.0.0 for training. Higher versions of pytorch have strange bilinear warping behavior, because of it model diverge. but i cannot install it from requirement.txt
","Try looking here: https://pytorch.org/get-started/previous-versions/#v100

This answer may not help at all, I don't know a ton about how requirement.txt files work. However, I do know that you can make a new environment and install older versions, commands are given at the website I gave above.
",,,false,,,
https://stackoverflow.com/questions/63939096,true,"The issue involves non-deterministic behavior in training a neural network on GPU using PyTorch, indicating a problem with the GPU-related functionality or the interaction between PyTorch and the specific GPU hardware.",PyTorch,GPU functionality,"The issue occurs when training a neural network on different GPUs with the same random seed. The final accuracy varies between machines, indicating non-deterministic behavior.",The PyTorch GPU functionality should produce consistent results when training a neural network with the same random seed on different GPUs.,The issue is triggered when training a neural network on different GPUs with the same random seed.,"This issue might be challenging to detect during development and testing because it involves the interaction between PyTorch, the GPU functionality, and the specific GPU hardware, and the behavior is not consistent or predictable.",Non-deterministic behavior for training a neural network on GPU implemented in PyTorch and with a fixed random seed,"I observed a strange behavior of the final Accuracy when I run exactly the same experiment (the same code for training neural net for image classification) with the same random seed on different GPUs (machines). I use only one GPU. Precisely, When I run the experiment on one machine_1 the Accuracy is 86,37. When I run the experiment on machine_2 the Accuracy is 88,0.
There is no variability when I run the experiment multiple times on the same machine. PyTorch and CUDA versions are the same. Could you help me to figure out the reason and fix it?
Machine_1:
NVIDIA-SMI 440.82       Driver Version: 440.82       CUDA Version: 10.2
Machine_2:
NVIDIA-SMI 440.100      Driver Version: 440.100      CUDA Version: 10.2
To fix random seed I use the following code:
random.seed(args.seed)
os.environ['PYTHONHASHSEED'] = str(args.seed)
np.random.seed(args.seed)
torch.manual_seed(args.seed)
torch.cuda.manual_seed(args.seed)
torch.backends.cudnn.benchmark = False
torch.backends.cudnn.deterministic = True

","This is what I use:
import torch
import os
import numpy as np
import random

def set_seed(seed):
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False
    np.random.seed(seed)
    random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)

set_seed(13)

Make sure you have a single function that set's the seeds from once. If you are using Jupyter notebooks cell execution timing may cause this. Also the order of functions inside may be important. I never had problems with this code. You may call set_seed() often in code.
",,,false,,,
https://stackoverflow.com/questions/66542007,true,"The issue involves transformer model outputs being NaN values in PyTorch, indicating a problem with the transformer model or its implementation.",PyTorch,transformer model,"The issue occurs when training transformer models in PyTorch, where the model outputs NaN values after several iterations. This happens even with fixed random seeds and valid input data.",The transformer model in PyTorch should be able to produce valid and non-NaN outputs when trained with fixed random seeds and valid input data.,"The issue is triggered during the training of transformer models in PyTorch, regardless of the specific architecture or input data used.","This issue might be challenging to detect during development and testing because it involves the interaction between the transformer model, PyTorch, and the specific training process, and the NaN outputs may not be immediately obvious or predictable.",Transformer Model Output Nan Values in Pytorch,"2021-03-09
I trained my transformer models in pytrorch. In the first few batches, the loss calculation and gradient updates were all performing well. However, the output of the model turned out to be nan values after several iterations. I am confident that there are no flawed data in the dataset. Besides, it's not a classification problem, the labels are float numbers.

2021-03-10
Follow-up:
What an interesting story! When I ran this transformer model with a larger architecture (like 6 encoder layers, 8 heads, etc.). The NAN values disappeared. It seems that the gradient explosion only existed in tiny models.
Solutions:
I searched the Pytorch forum and Stackoverflow and found out the accurate reason for this NAN instance. First, since the NAN loss didn't appear at the very beginning. We can conclude that the model might be well defined. The cause might be the data or the training process. I ran torch.autograd.set_detect_anomaly(True) as told in https://discuss.pytorch.org/t/gradient-value-is-nan/91663/2. It returned that the RuntimeError: Function ‘StdBackward1’ returned nan values in its 0th output.
According to the similar question in https://discuss.pytorch.org/t/gradient-of-standard-deviation-is-nan/14713, I double-checked the output in each layer inside the transformer. Strangely, after dozens of iterations, the positional embedding layer outputted a vector full of zeros. As a result, the LayerNorm that does the normalization job cannot backward the loss well, since it calculated the standard deviations and the standard deviation has no gradient at zero (or you can say it's infinite)! The possible solution is to add x.std(unbiased=False) if you are using pytorch.
This's my encounter with the NAN loss and mse. I hope my experience can give some insights to you when you meet this circumstance!
Relative Questions: Deep-Learning Nan loss reasons
","For what it's worth, I had this problem and it turned out that I had forgot to initialize an embedding vector, so it was just whatever torch.empty() happened to come upon (likely a lot of zeros.)
",,,false,,,
https://stackoverflow.com/questions/64024312,false,The issue does not involve an API exhibiting unexpected failures or unpredictable behaviors under specific runtime conditions. It is a question about how to compute the Hessian matrix for all parameters in a network in PyTorch.,,,,,,,How to compute hessian matrix for all parameters in a network in pytorch?,"Suppose vector \theta is all the parameters in a neural network, I wonder how to compute hessian matrix for \theta in pytorch.
Suppose the network is as follows:
class Net(Module):
    def __init__(self, h, w):
        super(Net, self).__init__()
        self.c1 = torch.nn.Conv2d(1, 32, 3, 1, 1)
        self.f2 = torch.nn.Linear(32 * h * w, 5)

    def forward(self, x):
        x = self.c1(x)
        x = x.view(x.size(0), -1)
        x = self.f2(x)
        return x

I know the second derivative can be calculated by calling torch.autograd.grad() twice, but the parameters in pytorch is organized by net.parameters(), and I don't know how to compute the hessian for all parameters.
I have tried to use torch.autograd.functional.hessian() in pytorch 1.5 as follows:
import torch
import numpy as np
from torch.nn import Module
import torch.nn.functional as F


class Net(Module):
    def __init__(self, h, w):
        super(Net, self).__init__()
        self.c1 = torch.nn.Conv2d(1, 32, 3, 1, 1)
        self.f2 = torch.nn.Linear(32 * h * w, 5)

    def forward(self, x):
        x = self.c1(x)
        x = x.view(x.size(0), -1)
        x = self.f2(x)
        return x


def func_(a, b c, d):
    p = [a, b, c, d]
    x = torch.randn(size=[8, 1, 12, 12], dtype=torch.float32)
    y = torch.randint(0, 5, [8])
    x = F.conv2d(x, p[0], p[1], 1, 1)
    x = x.view(x.size(0), -1)
    x = F.linear(x, p[2], p[3])
    loss = F.cross_entropy(x, y)
    return loss


if __name__ == '__main__':
    net = Net(12, 12)

    h = torch.autograd.functional.hessian(func_, tuple([_ for _ in net.parameters()]))
    print(type(h), len(h))


h is a tuple, and the results are in strange shape. For example, the shape of \frac{\delta Loss^2}{\delta c1.weight^2} is [32,1,3,3,32,1,3,3]. It seems like I can combine them into a complete H, but I don't know which part it is in the whole Hessian Matrix and the corresponding order.
","Here is one solution, I think it's a little too complex but could be instructive.
Considering about these points:

First, about torch.autograd.functional.hessian() the first argument must be a function, and the second argument should be a tuple or list of tensors. That means we cannot directly pass a scalar loss to it. (I don't know why, because I think there is no large difference between a scalar loss or a function that returns a scalar)
Second, I want to obtain a complete Hessian matrix, which is the second derivative of all parameters, and it should be in an appropriate order.

So here is the solution:
import torch
import numpy as np
from torch.nn import Module
import torch.nn.functional as F

class Net(Module):
    def __init__(self, h, w):
        super(Net, self).__init__()
        self.c1 = torch.nn.Conv2d(1, 32, 3, 1, 1)
        self.f2 = torch.nn.Linear(32 * h * w, 5)

    def forward(self, x):
        x = self.c1(x)
        x = x.view(x.size(0), -1)
        x = self.f2(x)
        return x

def haha(a, b, c, d):
    p = [a.view(32, 1, 3, 3), b, c.view(5, 32 * 12 * 12), d]
    x = torch.randn(size=[8, 1, 12, 12], dtype=torch.float32)
    y = torch.randint(0, 5, [8])
    x = F.conv2d(x, p[0], p[1], 1, 1)
    x = x.view(x.size(0), -1)
    x = F.linear(x, p[2], p[3])
    loss = F.cross_entropy(x, y)
    return loss


if __name__ == '__main__':
    net = Net(12, 12)

    h = torch.autograd.functional.hessian(haha, tuple([_.view(-1) for _ in net.parameters()]))
    
    # Then we just need to fix tensors in h into a big matrix

I build a new function haha that works in the same way with the neural network Net. Notice that arguments a, b, c, d are all expanded into one-dimensional vectors, so that the shapes of tensors in h are all two dimensional, in good order and easy to be combined into a large hessian matrix.
In my example, the shapes of tensors in h is
# with relation to c1.weight and c1.weight, c1.bias, f2.weight, f2.bias
[288,288]
[288,32]
[288,23040]
[288,5]

# with relation to c2.bias and c1.weight, c1.bias, f2.weight, f2.bias
[32, 288]
[32, 32]
[32, 23040]
[32, 5]
...


So it is easy to see the meaning of the tensors and which part it is. All we need to do is to allocate a (288+32+23040+5)*(288+32+23040+5) matrix and fix the tensors in h into the corresponding locations.
I think the solution still could be improved, like we don't need to build a function works the same way with neural network, and transform the shape of parameters twice. But for now I don't have better ideas, if there is any better solution, please let me know.
",,,false,,,
https://stackoverflow.com/questions/22591174/pandas-multiple-conditions-while-indexing-data-frame-unexpected-behavior,false,The issue does not involve an API exhibiting unexpected failures or unpredictable behaviors under specific runtime conditions. It is a question about the behavior of logical operators and conditions in pandas.,,,,,,,,,,,,false,,,
https://stackoverflow.com/questions/57285077,false,The issue does not involve an API exhibiting unexpected failures or unpredictable behaviors under specific runtime conditions. It is a question about performing element-wise matrix multiplication in numpy and PyTorch.,,,,,,,Derivative in both arguments of torch.nn.BCELoss(),"When using a torch.nn.BCELoss() on two arguments that are both results of some earlier computation, I get some curious error, which this question is about:

RuntimeError: the derivative for 'target' is not implemented


The MCVE is as follows: 

import torch
import torch.nn.functional as F

net1 = torch.nn.Linear(1,1)
net2 = torch.nn.Linear(1,1)
loss_fcn = torch.nn.BCELoss()

x = torch.zeros((1,1))

y = F.sigmoid(net1(x)) #make sure y is in range (0,1)
z = F.sigmoid(net2(y)) #make sure z is in range (0,1)

loss = loss_fcn(z, y) #works if we replace y with y.detach()

loss.backward()


It turns out if we call .detach() on y the error disappears. But this results in a different computation, now in the .backward()-pass, the gradients with respect to the second argument of the BCELoss will not be computed.

Can anyone explain what I'm doing wrong in this case? As far as I know all pytorch modules in torch.nn should support computing gradients. And this error message seems to tell me that the derivative is not implemented for y, which is somehow strange, as you can compute the gradient of y, but not of y.detach() which seems to be contradictory.
","I met the same problem too. As far as I know, the second argument of BCELoss(input, target),target should be a tensor without gradient attribute. It means that target.requires_grad should be False. But I don't know why.

Usually, the target(we can also call it Ground Truth) doesn't have a gradient attribute. But the target(y in your code) was calculated by F.sigmoid(net1(x)), which means the target (output of net1) has been a tensor with gradient attribute.

so, you should try:


  loss = loss_fcn(z, y.detach())


or:


  loss = loss_fcn(z, y.data)


maybe this?

import torch
import torch.nn.functional as F

net1 = torch.nn.Linear(1,1)
net2 = torch.nn.Linear(1,1)
loss_fcn = torch.nn.BCELoss()

x = torch.zeros((1,1))

y = F.sigmoid(net1(x)) #make sure y is in range (0,1)
z = F.sigmoid(net2(y)) #make sure z is in range (0,1)

y.retain_grad()
a = y

loss = loss_fcn(z, a.detach()) #works if we replace y with y.detach()

loss.backward()

print(y.grad)

","It seems I misunderstood the error message. It is not y that doesn't allow the computation for gradients, it is BCELoss() that doesn't have the ability to compute gradients with respect to the second argument. A similar problem was discussed here.
",,false,,,
https://stackoverflow.com/questions/56342193,false,The issue does not involve an API exhibiting unexpected failures or unpredictable behaviors under specific runtime conditions. It is a question about using torch.nn.BCELoss() on two arguments that are results of earlier computations.,,,,,,,Matrix multiplication (element-wise) from numpy to Pytorch,"I got two numpy arrays (image and and environment map), 

MatA
MatB


Both with shapes (256, 512, 3)

When I did the multiplication (element-wise) with numpy:

prod = np.multiply(MatA,MatB)


I got the wanted result (visualize via Pillow when turning back to Image)

But when I did it using pytorch, I got a really strange result(not even close to the aforementioned).

I did it with the following code:

MatATensor = transforms.ToTensor()(MatA)
MatBTensor = transforms.ToTensor()(MatB)

prodTensor = MatATensor * MatBTensor


For some reasons, the shape for both MatATensor and MatBtensor is 

torch.Size([3, 256, 512])


Same for the prodTensor too.
When I tried to reshape to (256,512,3), I got an error.

Is there a way to get the same result? 

I am new to pytorch, so any help would be appreciated.
","I suggest you use torch.from_numpy, which will easily convert your ndarrays to torch tensors. As in:

In[1]: MatA = np.random.rand(256, 512, 3)
In[2]: MatB = np.random.rand(256, 512, 3)

In[3]: MatA_torch = torch.from_numpy(MatA)
In[4]: MatB_torch = torch.from_numpy(MatB)

In[5]: mul_np = np.multiply(MatA, MatB)
In[6]: mul_torch = MatA_torch * MatB_torch

In[7]: torch.equal(torch.from_numpy(mul_np), mul_torch)
Out[7]: True

In[8]: mul_torch.shape
Out[8]: torch.Size([256, 512, 3])


If you want it back to numpy, just do:

mul_torch.numpy()

","If you read the documentation of transforms.ToTensor() you'll see this transformation does not only convert a numpy array to torch.FloatTensor, but also transpose its dimensions from HxWx3 to 3xHxW.
To ""undo"" this you'll need to

 prodasNp = (prodTensor.permute(2, 0, 1) * 255).to(torch.uint8).numpy()


See permute for more information.
",,false,,,
https://stackoverflow.com/questions/61116433,false,The issue does not involve an API exhibiting unexpected failures or unpredictable behaviors under specific runtime conditions. It is a question about property setters not working as expected in PyTorch.,,,,,,,"Maybe I found something strange on pytorch, which result in property setter not working","Maybe I found something strange on pytorch, which result in property setter not working. Below is a minimal example that demonstrates this:

import torch.nn as nn

class A(nn.Module):
    def __init__(self):
        super(A, self).__init__()
        self.aa = 1
        self.oobj = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]
    @property
    def obj(self):
        print('get attr [obj]: {0}'.format(self.oobj))
        return self.oobj
    @obj.setter
    def obj(self, val):
        print('set attr [obj] to {0}'.format(val))
        self.oobj = val

class B(nn.Module):
    def get_attr(self):
        print('no any attr.')

class C:
    def get_attr(self):
        print('no any attr.')

b = A()             # set obj, and prints my setter message

b.obj               # get obj using my getter

# respectively run the following 3 lines, only the last line not call the setter I defined explicitly.
b.obj = C()         # set obj, and prints my setter message

# b.obj = [1, 2, 3]   # set obj, and prints my setter message

# b.obj = B()         # set obj, but it doesn't print my setter message



The last line doesn't call property setter I defined on class A, but call setter on torch.nn.Module. Because A
    regard B as a nn.Module, call the setter on nn.Module to set attr [obj] as a Module, but it still strange, why
    not call the setter I explicitly defined on class A?

And my project needs to set a nn.Module attribute via setter I defined explicitly, which causes BUG( because it failed). Now I change my code solved the BUG, but still puzzle with the problem.
","It may not look obvious at first, but up until you set b.obj as a nn.Module object, you are defining a normal attribute; but once you set b.obj as a nn.Module object, then you can ""only"" replace b.obj with another nn.Module, because you registered it to _modules. Let me walk you through the code and you'll get it.

nn.Module()'s __setattr__ implementation can be found here.

First, you defined a new nn.Module:

b = A()  # btw, why not a = A() :)


Then, you set (I'll skip unnecessary steps to reproduce the behavior):

b.obj = [1, 2, 3]


In this case, because


[1,2,3] is not a nn.Parameter;
You haven't set a nn.Parameter as attribute before;
[1,2,3] is not a nn.Module;
You haven't set a nn.Module as attribute before;
You haven't registered a buffer before;


Then, this line will be execute:

object.__setattr__(self, name, value)


which is nothing but a normal attribute set, which calls your setter.

Now, when you set:

b.obj = B()


Then, because B() is a nn.Module, the following block will be executed instead:

modules = self.__dict__.get('_modules')
if isinstance(value, Module):
    if modules is None:
        raise AttributeError(
            ""cannot assign module before Module.__init__() call"")
    remove_from(self.__dict__, self._parameters, self._buffers)
    modules[name] = value


So, now you are actually registering a nn.Module to self.__dict__.get('_modules') (print it before and after and you'll see... do it before and after setting [1,2,3] as well).

After this point, if you are not setting a nn.Parameter, and you try to set .obj again, then it will fall into this block:

elif modules is not None and name in modules:
    if value is not None:
        raise TypeError(""cannot assign '{}' as child module '{}' ""
                        ""(torch.nn.Module or None expected)""
                        .format(torch.typename(value), name))
    modules[name] = value


That is: you already have modules['obj'] set to something and from now on you need to provide another nn.Module or None if you want to set it again. And, as you can see, because you are providing a list if you try to set b.obj = [1,2,3] again, you'll get the error message in the block above, and that is what you get.

If you really want set it to something else, then you have to delete it before:

b.obj = B()
del b.obj
b.obj = [1,2,3]

",,,false,,,
https://stackoverflow.com/questions/55828687,true,"The issue involves the usage of nn.DataParallel in PyTorch, which exhibits unexpected behavior in terms of the output shape when used in conjunction with the to(device) method.",PyTorch,nn.DataParallel,"The issue arises when using nn.DataParallel to parallelize model training across multiple GPUs. It leads to unexpected output shapes, specifically resulting in a tensor of shape [2, 8] instead of the expected [1, 8].",The nn.DataParallel API is typically used to parallelize model training across multiple GPUs.,"The issue is triggered when nn.DataParallel is used with a model that has multiple layers and the output shape is expected to be [1, 8].","This issue might be challenging to detect during development and testing, especially for users who are not familiar with the specific behavior of nn.DataParallel and its impact on the output shape.",Pytorch 1.0: what does net.to(device) do in nn.DataParallel?,"The following code from the tutorial to pytorch data paraleelism reads strange to me:

device = torch.device(""cuda:0"" if torch.cuda.is_available() else ""cpu"")

model = Model(input_size, output_size)
if torch.cuda.device_count() &gt; 1:
  print(""Let's use"", torch.cuda.device_count(), ""GPUs!"")
  # dim = 0 [30, xxx] -&gt; [10, ...], [10, ...], [10, ...] on 3 GPUs
  model = nn.DataParallel(model)

model.to(device)


According to my best knowledge, mode.to(device) copy the data to GPU. 


  DataParallel splits your data automatically and sends job orders to multiple models on several GPUs. After each model finishes their job, DataParallel collects and merges the results before returning it to you.


If the DataParallel does the job of copying, what does the to(device) do here?
","They add few lines in the tutorial to explain nn.DataParallel. 


  DataParallel splits your data automatically, and send job orders to multiple models on different GPUs using the data. After each model finishes their job, DataParallel collects and merges the results for you.


The above quote can be understood that nn.DataParallel is just a wrapper class to inform model.cuda() should make a multiple copies to GPUs.

In my case, I don't have any GPU on my laptop. I still call nn.DataParallel() without any problem.

import torch
import torchvision

model = torchvision.models.alexnet()
model = torch.nn.DataParallel(model)
# No error appears if I don't move the model to `cuda`

",,,false,,,
https://stackoverflow.com/questions/63629112,true,"The issue involves the neural network always predicting the same class in each epoch, indicating a potential problem with the model's training or architecture.",Keras,DNN,"The issue occurs when training a deep neural network (DNN) for MNIST classification. The model consistently predicts only one number correctly in each epoch, while incorrectly predicting other numbers. This behavior suggests a problem with the model's training or architecture.","In a well-trained DNN model, the predictions should be distributed across all possible classes, not limited to a single class in each epoch.",The issue is triggered during the training process of the DNN model for MNIST classification.,"This issue might be challenging to detect during development and testing, especially if the model's training progress is not closely monitored or if the issue is specific to the training data or architecture.",Neural network always predicts the same class in each epoch,"I'm trying to implement a mnist classifier with DNN. However, the result I got is quite strange.
enter image description here
In this epoch, this model can only predict number '0' correctly, and incorrect prediction for all the other numbers. This model could only predict a specific number for each epoch. (such predicted number is different in each epoch)
This is how I get the dataset.
from sklearn.datasets import fetch_openml
from keras.utils.np_utils import to_categorical
import numpy as np
from sklearn.model_selection import train_test_split
import time

x, y = fetch_openml('mnist_784', version=1, return_X_y=True)
x = (x/255.).astype('float32')
y = to_categorical(y)

x_train, x_val, y_train, y_val = train_test_split(x, y, test_size=0.15, random_state=42)

For this part, this is my model. A two-hidden-layers DNN with activation functions of Relu and softmax, Cross entropy loss for the error function.
I'm not really sure if my backpropagation is correct or not. I think something is wrong here.
import numpy as np


class NN():
    def __init__(self, input_size, hidden_1_size, hidden_2_size, output_size):
        self.input_data = np.random.randn(1, input_size)
        self.w1 = np.random.randn(input_size, hidden_1_size)
        self.b1 = np.random.randn(1, hidden_1_size)
        
        self.w2 = np.random.randn(hidden_1_size, hidden_2_size)
        self.b2 = np.random.randn(1, hidden_2_size) 

        self.w3 = np.random.randn(hidden_2_size, output_size)
        self.b3 = np.random.randn(1, output_size)


    def Sigmoid(self, z):
        return np.clip(1 / (1.0 + np.exp(-z)), 1e-8, 1 - (1e-7))

    def Softmax(self, z):
        y_logit = np.exp(z - np.max(z, 1, keepdims=True))
        y = y_logit / np.sum(y_logit, 1, keepdims=True)
        return y

    def Relu(self, z):
        return np.maximum(z, 0)

    def acc_test(self, input_data):
        tmp_h1 = self.Relu(input_data.dot(self.w1) + self.b1)
        tmp_h2 = self.Relu(self.h1_out.dot(self.w2) + self.b2)
        tmp_out = self.Softmax(self.h2_out.dot(self.w3) + self.b3)
        return tmp_out

    # Feed Placeholder

    def forward(self, input_data):

        self.input_data = input_data
        self.h1_out = self.Relu(input_data.dot(self.w1) + self.b1)
        self.h2_out = self.Relu(self.h1_out.dot(self.w2) + self.b2)
        self.output_layer = self.Softmax(self.h2_out.dot(self.w3) + self.b3)

    # Backward Propagation

    def backward(self, target):

        # corss_entropy loss derivative
        Loss_to_z_grad = (self.output_layer - target) # correct

        self.b3_grad = Loss_to_z_grad
        self.w3_grad = self.h2_out.T.dot(Loss_to_z_grad) # correct



        Activation_2＿grad = Loss_to_z_grad.dot(self.w3.T) # correct
        Activation_2_grad[Activation_2_grad&lt;0] = 0

        self.b2_grad = Activation_2＿grad
        self.w2_grad = self.h1_out.T.dot(Activation_2＿grad)

        
        Activation_1＿grad = Activation_2＿grad.dot(self.w2.T)
        Activation_1_grad[Activation_1_grad&lt;0] = 0     

        self.b1_grad = Activation_1＿grad
        self.w1_grad = self.input_data.T.dot(Activation_1＿grad)


    # Update Weights
    def update(self, learning_rate=1e-06):
        self.w1 = self.w1 - learning_rate * self.w1_grad
        self.b1 = self.b1 - learning_rate * self.b1_grad

        self.w2 = self.w2 - learning_rate * self.w2_grad
        self.b2 = self.b2 - learning_rate * self.b2_grad

        self.w3 = self.w3 - learning_rate * self.w3_grad
        self.b3 = self.b3 - learning_rate * self.b3_grad

    # Loss Functions
    def cross_entropy(Y, Y_prediction):
        return -(np.matmul(Y, np.log(Y_prediction)) + np.matmul((1-Y), np.log(1-Y_prediction)))

    def print_accuracy(self):
        correct = 0
        loss = 0
        for i in range(y_val.shape[0]):
            self.acc_test(x_val[i])
            index = self.output_layer
            one_hot = 0
            for check in range(y_val[i].shape[0]):
                if y_val[i][check] == 1:
                    one_hot = check
                    break
            if np.argmax(index) == one_hot:
                correct += 1
                # print('correct: ',check)
            # else:
                # print('incorrect: ', check)
        print('accuracy = ', correct/y_val.shape[0])

import random
 mnist_nn = NN(input_size = 784, hidden_1_size = 200, hidden_2_size = 200,output_size = 10)


 
for i in range(1000):
    for j in range(2000):
        index = random.randint(0,x_train.shape[0]-1)
        mnist_nn.forward(x_train[[index]])
        mnist_nn.backward(y_train[index])
        mnist_nn.update()
    print(i)
    mnist_nn.print_accuracy()

The accuracy is terribly low since it can only predict one number.
I've seen this article, Neural network always predicts the same class and I did change Relu to leaky Relu, but it doesn't really work.
I think my dataset should be ok cause I use the same dataset to train a DNN with pytorch, and it works. Also, the initial value of weights and bias are random values.
","I've had a quick look over your code and if I understand it correctly, then there may be some issues:

It seems like you want it to do multi-class classification with 10 classes, however I believe your cross entropy function looks like binary cross entropy, instead of general cross entropy. Also, you're using matrix multiplication, whereas I think you want to sum y * log(y_pred) over the 10 output probabilities and then take the mean across the batch, so you end up with a scalar valued loss.
When doing the ReLU gradient, you should clip where the actual activation is negative, not where the gradient is negative, I think. So Activation_2_grad[Activation_2_grad&lt;0] = 0 should be Activation_2_grad[self.h2_out &lt; 0] = 0.
The rest of the backprop looks okay.

",,,false,,,
https://stackoverflow.com/questions/72690448,true,"The issue involves the usage of the num_layers parameter in a PyTorch LSTM model, which leads to unexpected output dimensions.",PyTorch,nn.LSTM,"The issue arises when using the num_layers parameter in an LSTM model. Instead of the expected output shape of [1, 8], the output shape becomes [2, 8].",The nn.LSTM API is typically used to create LSTM models for sequence data processing.,"The issue is triggered when nn.LSTM is used with the num_layers parameter set to 2, resulting in an unexpected output shape of [2, 8].","This issue might be challenging to detect during development and testing, especially for users who are not familiar with the specific behavior of nn.LSTM and its impact on the output shape.",How should I use num_layers in pytorch LSTM model?,"Hi I am new bee with RNN and pytorch. I am implementing an model to predict data. I first only use single layer and the result was fine. Now I want to improve the accurancy of the model and want to use 2 layers in LSTM. However, the output of LSTM is very strange to me.
I was expecting have a [1, 8] output. However, with num_layers=2 I get a [2, 8] result. What does this result mean? Which should I use as the result of LSTM?
Here is my code:
class LSTM(nn.Module):

    def __init__(self, input_dim, hidden_dim, num_layers, output_dim):

        super(LSTM, self).__init__()
        self.input_dim = input_dim
        self.output_dim = output_dim
        self.hidden_dim = hidden_dim
        self.num_layers = num_layers

        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_dim,
                            num_layers=num_layers, batch_first=True)
        
        self.fc = nn.Linear(hidden_dim, output_dim)

    
    def forward(self, x):

        #x = Variable(torch.Tensor(x).cuda())
        
        h_0 = Variable(torch.zeros(
            self.num_layers, x.size(0), self.hidden_dim).cuda())
        
        c_0 = Variable(torch.zeros(
            self.num_layers, x.size(0), self.hidden_dim).cuda())
        
        
        # Propagate input through LSTM
        ula, (h_out, _) = self.lstm(x, (h_0, c_0))
         
        
        out = self.fc(h_out).cuda()
        
        return out

","h_out has the shape of (hidden_size,numlayers). If you want to pass it to the output layer fc, you need to reshape it/flatten and also increase the size of the output layer to hidden_size*numlayers.
class LSTM(nn.Module):

    def __init__(self, input_dim, hidden_dim, num_layers, output_dim):

        super(LSTM, self).__init__()
        self.input_dim = input_dim
        self.output_dim = output_dim
        self.hidden_dim = hidden_dim
        self.num_layers = num_layers

        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_dim,
                            num_layers=num_layers, batch_first=True)
        
        self.fc = nn.Linear(hidden_dim*num_layers, output_dim)

    
    def forward(self, x):

        #x = Variable(torch.Tensor(x).cuda())
        
        h_0 = Variable(torch.zeros(
            self.num_layers, x.size(0), self.hidden_dim).cuda())
        
        c_0 = Variable(torch.zeros(
            self.num_layers, x.size(0), self.hidden_dim).cuda())
        
        
        # Propagate input through LSTM
        ula, (h_out, _) = self.lstm(x, (h_0, c_0))
         
        out = h_out.transpose(0,1)
        out = out.reshape(-1,hidden_dim*num_layers)
        out = self.fc(out).cuda()
        
        return out

",,,false,,,
https://stackoverflow.com/questions/70970106,false,The issue is related to the installation of a Python wheel file in Conda and does not involve any API exhibiting unexpected failures or behaviors.,,,,,,,Cannot install python wheel file in conda without sudo,"I'm trying to build PyTorch from a .whl file on a jetson nano.
I am able to build and install the file but only while using sudo, attempting to pip install the file without sudo results in this error:
ERROR: torch-1.10.0a0+git36449ea-cp36-cp36m-linux_aarch64.whl is not a supported wheel on this platform.

This is strange as with admin I have no issues installing this file, but I can then only use the library by using the sudo command before going into or running with the python command.
I should note that this is in a conda environment, but even in the base conda environment this issue still occurs.
It seems like I can also install the package by using conda deactivate to deactivate conda.
I am using Python 3.7 in the conda environment and Python 3.6 outside.
","
I am using Python 3.7 in the conda environment and Python 3.6 outside.

This is the issue. You have a cp36 whl file, so python 3.6. I am suspecting that when you run sudo pip, your systems pip is invoked, whereas when you run pip, then pip from your conda env is used, and cannot install a python 3.6 whl to a python 3.7 env.
Either you need to get the cp37 whl or create a conda env that has python 3.6 installed
",,,false,,,
https://stackoverflow.com/questions/69351441,false,The issue is related to the calculation of an exponential moving covariance matrix and does not involve any API exhibiting unexpected failures or behaviors.,,,,,,,Exponential moving covariance matrix,"I have time series data of a specific dimensionality (e.g. [T, 32]). I filter the data in an online fashion using an exponential moving average and variance (according to Wikipedia and this paper):
mean_n = alpha * mean_{n-1} + (1-alpha) * sample
var_n = alpha * (var_{n-1} + (1-alpha) * (sample - mean_{n-1}) * (sample - mean_{n-1}))

I wanted to replace the moving variance with a moving covariance matrix in order to capture the correlation between the data dimensions (e.g. 32). So I have simply replaced the element-wise variance calculation with an outer product:
covmat_n = alpha * (covmat_{n-1} + (1-alpha) * np.outer((sample - mean_{n-1}), (sample - mean_{n-1})))

However this does not seem to give correct results. For example, when I try to initialize a pytorch multivariate gaussian distribution with such a covariance matrix, it sometimes throws an error saying that the covariance matrix contains invalid values. Since it is always symmetric I assume it breaks the positive-definiteness constraint. Other downstream tasks suggested  the same (e.g. computing the KL-Divergence between two gaussians with such covariance matrices sometimes gave negative results).
Does anyone know what I am doing wrong here? Where is the error in my calculations?
And as a bonus question: Are my calculations for the simple moving variance correct? It seeems strange to multiply the new sample variance with alpha again, but the sources suggest that it is the correct way.
","I have found the answer myself. It seemed to be a numerical problem. Since the eigenvalues of a positive definite matrix must be positive, I could solve it by applying an eigenvalue decomposition to every sample's covariance matrix and ensure that its eigenvalues are larger than zero:
diff = sample - last_mean
sample_covmat = np.outer(diff, diff)
w, v = np.linalg.eigh(sample_covmat)
w += 1e-3 # Note: Could also achieve this by e.g. w = np.maximum(w, 0)
sample_covmat = v @ np.diag(w) @ np.linalg.inv(v)

",,,false,,,
https://stackoverflow.com/questions/47461625,false,"The issue is related to the usage of torch.addmm and the understanding of the output tensor types, but does not involve any API exhibiting unexpected failures or behaviors.",,,,,,,torch.addmm received an invalid combination of arguments,"In the official webpage of pytorch I saw the following code with answers: 

&gt;&gt; a = torch.randn(4, 4)
&gt;&gt; a

0.0692  0.3142  1.2513 -0.5428
0.9288  0.8552 -0.2073  0.6409
1.0695 -0.0101 -2.4507 -1.2230
0.7426 -0.7666  0.4862 -0.6628
torch.FloatTensor of size 4x4]

&gt;&gt;&gt; torch.max(a, 1)
(
 1.2513
 0.9288
 1.0695
 0.7426
[torch.FloatTensor of size 4]
,
 2
 0
 0
 0
[torch.LongTensor of size 4]
)


I know that the first result corresponds to the maximum number per row, however I do not get the second tensor (LongTensor)

I tried other random example and after a pytorch.max, I came to find these results 

0.9477  1.0090  0.8348 -1.3513
-0.4861  1.2581  0.3972  1.5751
-1.2277 -0.6201 -1.0553  0.6069
 0.1688  0.1373  0.6544 -0.7784
[torch.FloatTensor of size 4x4]

(
 1.0090
 1.5751
 0.6069
 0.6544
[torch.FloatTensor of size 4]
, 
 1
 3
 3
 2
[torch.LongTensor of size 4]
)


Can anyone tell me what does it really mean these LongTensor data? I thought it was a strange casting between tensors, however after a simple casting of a float tensor, I see that it just cuts decimals

Thanks
","It just tells the index of the max element in your original tensor along the queried dimension.

E.g.

0.9477  1.0090  0.8348 -1.3513
-0.4861  1.2581  0.3972  1.5751
-1.2277 -0.6201 -1.0553  0.6069
 0.1688  0.1373  0.6544 -0.7784
[torch.FloatTensor of size 4x4]

# torch.max(a, 1)
(
 1.0090
 1.5751
 0.6069
 0.6544
[torch.FloatTensor of size 4]
, 
 1
 3
 3
 2
[torch.LongTensor of size 4]
)


In the above example in torch.LongTensor, 

1 is the index of 1.0090 in your original tensor (torch.FloatTensor) 
3 is the index of 1.5751 in your original tensor (torch.FloatTensor) 
3 is the index of 0.6069 in your original tensor (torch.FloatTensor) 
2 is the index of 0.6544 in your original tensor (torch.FloatTensor)

along dimension 1.



Instead, if you'd have requested torch.max(a, 0), the entries in torch.LongTensor would correspond to the indices of max elements in your original tensor along dimension 0.
",,,false,,,
https://stackoverflow.com/questions/65107933,false,The issue is related to a CPU memory leak during PyTorch model training and does not involve any API exhibiting unexpected failures or behaviors.,,,,,,,Pytorch model training CPU Memory leak issue,"When I trained my pytorch model on GPU device,my python script was killed out of blue.Dives into OS log files , and I find script was killed by OOM killer because my CPU ran out of memory.It’s very strange that I trained my model on GPU device but I ran out of my CPU memory.
Snapshot of OOM killer log file

In order to  debug this issue,I install python memory profiler. Viewing log file from memory profiler, I find when column wise -= operation  occurred, my CPU memory gradually increased until OOM killer killed my
program.
Snapshot of Python memory profiler

It’s very strange, I try many ways to solve this issue.Finally, I found before assignment operation,I detach Tensor first.Amazingly,it solves this issue.But I don’t understand clearly why it works.Here is my original function code.
def GeneralizedNabla(self, image):
        pad_size = 2
        affinity = torch.zeros(image.shape[0], self.window_size**2, self.h, self.w).to(self.device)
        h = self.h+pad_size
        w = self.w+pad_size
        #pad = nn.ZeroPad2d(pad_size)
        image_pad = self.pad(image)
        for i in range(0, self.window_size**2):
            affinity[:, i, :, :] = image[:, :, :].detach()  # initialization
            dy = int(i/5)-2
            dx = int(i % 5)-2
            h_start = pad_size+dy
            h_end = h+dy  # if 0 &lt;= dy else h+dy
            w_start = pad_size+dx
            w_end = w+dx  # if 0 &lt;= dx else w+dx
            affinity[:, i, :, :] -= image_pad[:, h_start:h_end, w_start:w_end].detach()
        self.Nabla=affinity
        return

If everyone has any ideas,I will appreciate very much, thank you.
","Previously when you did not use the .detach() on your tensor, you were also accumulating the computation graph as well and as you went on, you kept acumulating more and more until you ended up exuasting your memory to the point it crashed.
When you do a detach(), you are effectively getting the data without the previously entangled history thats needed for computing the gradients.
",,,false,,,
https://stackoverflow.com/questions/66817006,false,The issue does not meet the criteria for deeper analysis as it does not involve an API exhibiting unexpected failures or unpredictable behaviors under specific runtime conditions.,,,,,,,PyTorch CPU memory leak but only when running on a specific machine,"I'm running a model and I've noticed that the RAM usage slowly increases during the training of the model. It's around 200mb-400mb per epoch, but over time it fills up all the RAM on my machine which eventually leads the OS to kill the job. However, the strange thing about this is it's only when running on a specific machine. I've run the same code on a different machine and there's no memory leak whatsoever.
The difference between the two machines is one is running PyTorch 1.7.1 with cuda 10.2 (the machine without the memory leak) and the other machine (the one with the memory leak) is running PyTorch 1.8.1· Both are run with conda and only on the CPU. I have tried using older versions of PyTorch on the machine with the memory leak, but the memory leak still exists so I doubt it's due to a PyTorch version.
I've been using psutil to monitor the RAM usage on the CPU and I've been using the tracemalloc package to print out snapshots during the training loop to see how the memory usage changes from one epoch to the next. Yet when printing out these differences there isn't any within my code that matches anywhere near the 200mb-400mb RAM increase...
An example of this would something like...
Epoch     25/  1000 Loss 1428.8508 RAM:  9.910GB
~/Model.py:46: size=1136 B (-96 B), count=2 (+0), average=568 B
~/utils.py:14: size=1040 B (+24 B), count=8 (+1), average=130 B
~/calc_loss.py:79: size=2128 B (+0 B), count=24 (+0), average=89 B
~/calc_loss.py:78: size=2056 B (+0 B), count=23 (+0), average=89 B
~/utils.py:6: size=1920 B (+0 B), count=21 (+0), average=91 B
Epoch     26/  1000 Loss 1426.0033 RAM: 10.254GB
~/Model.py:46: size=1232 B (+96 B), count=2 (+0), average=616 B
~/utils.py:14: size=1016 B (-24 B), count=7 (-1), average=145 B
~/calc_loss.py:79: size=2128 B (+0 B), count=24 (+0), average=89 B
~/calc_loss.py:78: size=2056 B (+0 B), count=23 (+0), average=89 B
~/utils.py:6: size=1920 B (+0 B), count=21 (+0), average=91 B
~/Layers.py:71: size=992 B (+0 B), count=11 (+0), average=90 B
Epoch     27/  1000 Loss 1436.8241 RAM: 10.606GB
~/utils.py:14: size=1040 B (+24 B), count=8 (+1), average=130 B
~/calc_loss.py:79: size=2128 B (+0 B), count=24 (+0), average=89 B
~/calc_loss.py:78: size=2056 B (+0 B), count=23 (+0), average=89 B
~/utils.py:6: size=1920 B (+0 B), count=21 (+0), average=91 B
~/Model.py:46: size=1232 B (+0 B), count=2 (+0), average=616 B
~/Layers.py:71: size=992 B (+0 B), count=11 (+0), average=90 B
Epoch     28/  1000 Loss 1428.6560 RAM: 10.968GB
~/calc_loss.py:79: size=2128 B (+0 B), count=24 (+0), average=89 B
~/calc_loss.py:78: size=2056 B (+0 B), count=23 (+0), average=89 B
~/utils.py:6: size=1920 B (+0 B), count=21 (+0), average=91 B
~/Model.py:46: size=1232 B (+0 B), count=2 (+0), average=616 B
~/utils.py:14: size=1040 B (+0 B), count=8 (+0), average=130 B
Epoch     29/  1000 Loss 1435.2988 RAM: 11.321GB
~/calc_loss.py:79: size=2128 B (+0 B), count=24 (+0), average=89 B
~/calc_loss.py:78: size=2056 B (+0 B), count=23 (+0), average=89 B
~/utils.py:6: size=1920 B (+0 B), count=21 (+0), average=91 B
~/Model.py:46: size=1232 B (+0 B), count=2 (+0), average=616 B
~/utils.py:14: size=1040 B (+0 B), count=8 (+0), average=130 B
~/Layers.py:71: size=992 B (+0 B), count=11 (+0), average=90 B

The information printed out in between the lines showing the current Epoch are created via the use of this function
def compare_snaps(snap1, snap2, limit=50):
  top_stats=snap1.compare_to(snap2, ""lineno"")
  
  for stat in top_stats[:limit]:
    line=str(stat)
    if(""~/"" in line): #filter only lines from my own code
      print(line)

this function takes two snapshots from  tracemalloc.take_snapshot() from the current epoch and the previous epoch and compares how the memory usage changes. It takes the top 50 memory intensive operations, and filters only the ones that I've written (i.e. it excludes any changes from within anaconda3) and prints these changes to the screen. As can be seen, the changes in memory are negligible. In fact, when comparing the snap shotoutput from both machines, they're near identical.
It seems really weird that PyTorch code would have a memory leak on one machine and not on another... Could this perhaps be a conda environemnt issue? I have tried running this from a pip install and the leak still persists. I understand that this a bit vague (as there's no source code) but are there any ways to check where the leaks coming from? Things like Valgrind comes to mind for compile code but not much for interpretative code. This memory leak issue seems to be outside of my skill-set, so any help would be appreciated.
Thank you! :)
","Try incorporating this in your process:
import gc

# add this after computing one complete operation
gc.collect()

",,,false,,,
https://stackoverflow.com/questions/64601301,true,"The issue involves an incorrect tensor size being passed to the PyTorch Conv1D API, leading to a runtime error. Further analysis is required to provide detailed information about the API and the conditions under which the unexpected behavior occurs.",PyTorch,nn.Conv1d,"The Conv1D API in PyTorch expects a 3-dimensional input tensor, but the issue involves passing a 2-dimensional input tensor instead. This results in a runtime error.",The Conv1D API is applied to a 3-dimensional input tensor.,The issue is triggered when the Conv1D API is applied to a 2-dimensional input tensor.,This issue might be challenging to detect during development and testing if the input tensor shape is not thoroughly validated or if the expected input shape is not clearly documented.,Pytorch input tensor size with wrong dimension Conv1D," def train(epoch):
      model.train()
      train_loss = 0

  for batch_idx, (data, _) in enumerate(train_loader):
    data = data[None, :, :]
    print(data.size())    # something seems to change between here

    data = data.to(device)
    optimizer.zero_grad()
    recon_batch, mu, logvar = model(data) # and here???

    loss = loss_function(recon_batch, data, mu, logvar)
    loss.backward()
    train_loss += loss.item()

    optimizer.step()

    if batch_idx % 1000 == 0:
            print('Train Epoch: {} [{}/{} ({:.0f}%)]\tLoss: {:.6f}'.format(
                epoch, batch_idx * len(data), len(train_loader.dataset),
                100. * batch_idx / len(train_loader),
                loss.item() / len(data)))


  print('====&gt; Epoch: {} Average loss: {:.4f}'.format(epoch, train_loss / len(train_loader.dataset)))

for epoch in range(1, 4):
        train(epoch)

This is very strange looking at the training loop it does recognize that the size is [1,1,1998] but then something changes after it is sent to the device?
    torch.Size([1, 1, 1998])
---------------------------------------------------------------------------
RuntimeError                              Traceback (most recent call last)
&lt;ipython-input-138-70cca679f91a&gt; in &lt;module&gt;()
     27 
     28 for epoch in range(1, 4):
---&gt; 29         train(epoch)

5 frames
/usr/local/lib/python3.6/dist-packages/torch/nn/modules/conv.py in forward(self, input)
    255                             _single(0), self.dilation, self.groups)
    256         return F.conv1d(input, self.weight, self.bias, self.stride,
--&gt; 257                         self.padding, self.dilation, self.groups)
    258 
    259 

RuntimeError: Expected 3-dimensional input for 3-dimensional weight [12, 1, 1], but got 2-dimensional input of size [1, 1998] instead

Also here is my model (I recognize there is likely a couple of other issues here but I am asking about the tensor size not registering)
class VAE(nn.Module):
  def __init__(self):
    super(VAE, self).__init__()

    self.conv1 = nn.Conv1d( 1,12, kernel_size=1,stride=5,padding=0)
    self.conv1_drop = nn.Dropout2d()
    self.pool1 = nn.MaxPool1d(kernel_size=3, stride=2)

    self.fc21 = nn.Linear(198, 1)
    self.fc22 = nn.Linear(198, 1)

    self.fc3 = nn.Linear(1, 198)
    self.fc4 = nn.Linear(198, 1998)

  def encode(self, x):
    h1 = self.conv1(x)
    h1 = self.conv1_drop(h1)
    h1 = self.pool1(h1)
    h1 = F.relu(h1)
    h1 = h1.view(1, -1) # 1 is the batch size
    return self.fc21(h1), self.fc22(h1)
  
  def reparameterize(self, mu, logvar):
    std = torch.exp(0.5*logvar)
    eps = torch.rand_like(std)
    return mu + eps*std 
  
  def decode(self, z):
    h3 = F.relu(self.fc3(z))
    return torch.sigmoid(self.fc4(h3))
  
  def forward(self, x):
    mu, logvar = self.encode(x.view(-1, 1998))
    z = self.reparameterize(mu, logvar)
    return self.decode(z), mu, logvar

So why doesn't Pytorch keep the dimensions after reshaping and would that be the correct tensor size if it did?
","I just found my mistake when I call forward() I am doing self.encode(x.view(-1,1998)) which is reshaping the tensor.
",,,false,,,
https://stackoverflow.com/questions/55558978,false,The issue does not meet the criteria for deeper analysis as it does not involve an API exhibiting unexpected failures or unpredictable behaviors under specific runtime conditions.,,,,,,,PyTorch will not fit straight line to two data points,"I'm facing issues in fitting a simple y= 4x1 line with 2 data points using pytorch. While running the inference code, the model seems to output same value to any input which is strange. Pls find the code attached along with the data files used by me. Appreciate any help here.

import torch
import numpy as np
import pandas as pd

df = pd.read_csv('data.csv')
test_data = pd.read_csv('test_data.csv')

inputs = df[['x1']]
target = df['y']
inputs = torch.tensor(inputs.values).float()
target = torch.tensor(target.values).float()

test_data = torch.tensor(test_data.values).float()
#Defining Network Architecture
import torch.nn as nn
import torch.nn.functional as F

class Net(nn.Module):

  def __init__(self):
    super(Net,self).__init__()

    hidden1 = 3
#     hidden2 = 5 

    self.fc1 = nn.Linear(1,hidden1)
    self.fc3 = nn.Linear(hidden1,1)


  def forward(self,x):
    x = F.relu(self.fc1(x))
    x = self.fc3(x)
    return x

#instantiate the model

model = Net()
print(model)

criterion = nn.MSELoss()
optimizer = torch.optim.SGD(model.parameters(),lr=0.01)

model.train()

#epochs
epochs = 100


for x in range(epochs):
  #initialize the training loss to 0
  train_loss = 0
  #clear out gradients
  optimizer.zero_grad() 

  #calculate the output
  output = model(inputs)

  #calculate loss
  loss = criterion(output,target)

  #backpropagate
  loss.backward() 

  #update parameters
  optimizer.step()

  if ((x%5)==0):
    print('Training Loss after epoch {:2d} is {:2.6f}'.format(x,loss))

#set the model in evaluation mode
model.eval()

#Test the model on unseen data

test_output = model(test_data)

print(test_output)


Below is the model output

#model output
tensor([[56.7579],
        [56.7579],
        [56.7579],
        [56.7579],
        [56.7579],
        [56.7579],
        [56.7579],
        [56.7579],
        [56.7579],
        [56.7579],
        [56.7579],
        [56.7579],
        [56.7579],
        [56.7579],
        [56.7579],
        [56.7579],
        [56.7579],
        [56.7579],
        [56.7579],
        [56.7579]], grad_fn=&lt;AddmmBackward&gt;)

","Your model is collapsing. You can probably see that based on the prints. You may want to use a lower learning rate (1e-5, 1e-6, etc.). Switching from SGD(...)to Adam(...) may be easier if you do not have experience and want less trouble fine-tuning these hparams. Also, maybe 100 epochs is not enough. As you did not share an MCVE, I cannot tell you for sure what it is. Here is an MCVE of linefitting using the same Net you used:

import torch
import numpy as np
import torch.nn as nn
import torch.nn.functional as F

epochs = 1000
max_range = 40
interval = 4

# DATA
x_train = torch.arange(0, max_range, interval).view(-1, 1).float()
x_train += torch.rand(x_train.size(0), 1) - 0.5  # small noise
y_train = (4 * x_train) 
y_train += torch.rand(x_train.size(0), 1) - 0.5  # small noise

x_test  = torch.arange(interval // 2, max_range, interval).view(-1, 1).float()
y_test  = 4 * x_test

class Net(nn.Module):
  def __init__(self):
    super(Net, self).__init__()
    hidden1 = 3
    self.fc1 = nn.Linear(1, hidden1)
    self.fc3 = nn.Linear(hidden1, 1)

  def forward(self, x):
    x = F.relu(self.fc1(x))
    x = self.fc3(x)
    return x

model = Net()
print(model)

criterion = nn.MSELoss()
optimizer = torch.optim.SGD(model.parameters(), lr=1e-5)

# TRAIN
model.train()
for epoch in range(epochs):
  optimizer.zero_grad()
  y_pred = model(x_train)
  loss = criterion(y_pred, y_train)
  loss.backward()
  optimizer.step()

  if epoch % 10 == 0:
    print('Training Loss after epoch {:2d} is {:2.6f}'.format(epoch, loss))

# TEST
model.eval()
y_pred = model(x_test)
print(torch.cat((x_test, y_pred, y_test), dim=-1))


This is what the data looks like:



And this is what the training looks like:

Training Loss after epoch  0 is 7416.805664
Training Loss after epoch 10 is 6645.655273
Training Loss after epoch 20 is 5792.936523
Training Loss after epoch 30 is 4700.106445
Training Loss after epoch 40 is 3245.384277
Training Loss after epoch 50 is 1779.370728
Training Loss after epoch 60 is 747.418579
Training Loss after epoch 70 is 246.781311
Training Loss after epoch 80 is 68.635155
Training Loss after epoch 90 is 17.332235
Training Loss after epoch 100 is 4.280161
Training Loss after epoch 110 is 1.170808
Training Loss after epoch 120 is 0.453974
...
Training Loss after epoch 970 is 0.232296
Training Loss after epoch 980 is 0.232090
Training Loss after epoch 990 is 0.231888


And this is what the output looks like:

|  x_test |  y_pred  |  y_test  |
|:-------:|:--------:|:--------:|
|  2.0000 |   8.6135 |   8.0000 |
|  6.0000 |  24.5276 |  24.0000 |
| 10.0000 |  40.4418 |  40.0000 |
| 14.0000 |  56.3303 |  56.0000 |
| 18.0000 |  72.1884 |  72.0000 |
| 22.0000 |  88.0465 |  88.0000 |
| 26.0000 | 103.9047 | 104.0000 |
| 30.0000 | 119.7628 | 120.0000 |
| 34.0000 | 135.6210 | 136.0000 |
| 38.0000 | 151.4791 | 152.0000 |

",,,false,,,
https://stackoverflow.com/questions/72223776,true,The issue involves a memory depletion problem when deploying a PyTorch model on test data after each training epoch. Further analysis is required to provide detailed information about the API and the conditions under which the unexpected behavior occurs.,PyTorch,torch.cuda.empty_cache,The issue involves a rapid depletion of GPU memory when deploying the model on test data after each training epoch. The exact cause of the memory depletion needs to be investigated further.,The torch.cuda.empty_cache API is used to release GPU memory.,The issue is triggered when deploying the model on test data after each training epoch.,This issue might be challenging to detect during development and testing if the memory usage is not closely monitored or if the specific conditions leading to the memory depletion are not well understood.,Google Colab GPU RAM depletes quickly on test data but not so on training data,"I am training my neural network built with PyTorch under Google Colab Pro+ (Tesla P100-PCIE GPU) but encounters the following strange phenomenon:

The amount of free memory (Mem Free) stays nearly constant when I train the model on training data, so there is no GPU memory issue during training (even over multiple epochs, where I use torch.cuda.empty_cache() at the beginning of each epoch).

However, Mem Free quickly depletes as soon as I deploy the model on test data after each training epoch. Everything except the input data to the network stays the same during the process.


To be more precise, the code during which Mem Free depletes looks like
self.batch_training(train=True)  # no problem
self.batch_training(train=False) # issue occurs,

where self is a class object which stores the model, data, and the training method.
The function batch_training looks like
def batch_training(self, train=True):
    if train:
       X, Y = self.X_train, self.Y_train
    else:
       X, Y = self.X_test, self.Y_test
    ...
    (make prediction and does gradient descent if train == True, otherwise, just compute customized losses)

Below is a screenshot that illustrates the situation (just ignores the training loss_g and in each batch), where I tried the code on a subset of training data so it moves on to the test data quickly. We can see the issue as soon as Test metrics are computed.
Screenshot of the issue (sorry could not make it inline due to ''not enough Reputation'')
Thank you very much for your help!
","For anyone who might encounter similar issues:
As explained in PyTorch FAQ,

Don’t accumulate history across your training loop. By default, computations involving variables that require gradients will keep history. This means that you should avoid using such variables in computations which will live beyond your training loops, e.g., when tracking statistics. Instead, you should detach the variable or access its underlying data.

In my case, adding with torch.no_grad() before my self.batch_training(train=False) resolves the issue.
",,,false,,,
https://stackoverflow.com/questions/68839035,false,The issue does not meet the criteria for deeper analysis as it does not involve an API exhibiting unexpected failures or unpredictable behaviors under specific runtime conditions.,,,,,,,"Activation function of tf.math.pow(x, 0.5) leading to NaN losses","I'm trying to use a custom square root activation function for my Keras sequential model (specifically for the MNIST dataset). When I use tf.math.sqrt(x), training goes smoothly and the model is quite accurate. However, when I try using tf.math.pow(x, 0.5), the model fails to train and the losses go to NaN.
I'm really unsure why this is happening because I would think that the two alternatives are identical.
Square root function
def tfsqrt(x):
    cond = tf.greater_equal(x, 0)
    return tf.where(cond, tf.math.sqrt(x), -tf.math.sqrt(-x))

Power function
def pwsqrt(x):
  cond = tf.greater_equal(x, 0)
  return tf.where(cond, tf.math.pow(x, 0.5), -tf.math.pow(-x, 0.5))

If anybody could explain this unexpected behavior, that would be much appreciated. Thanks!
","Functions are correct:
x=tf.Variable([-2.0,-3.0,0.0, 1.0,2.0])
y=tfsqrt(x)
y
y=pwsqrt(x)
y

The functions work fine in google colab, maybe there are some nan values in data.
Maybe there is some problem in model loss or metric.
",,,false,,,
https://stackoverflow.com/questions/55559944,false,The issue does not meet the criteria for deeper analysis as it does not involve an API exhibiting unexpected failures or unpredictable behaviors under specific runtime conditions.,,,,,,,Keras.util.to_categorical show unexpected behaviour,"Keras .To_Categorical shows unexpected behavior: 

Contrary to documentation in https://keras.io/utils/#to_categorical
Specifying dtype returns an error

from keras.utils import to_categorical
import numpy as np

labels= np.array([0, 2, 1, 2, 0])
to_categorical(labels,dtype='int32')


produce:

TypeError: to_categorical() got an unexpected keyword argument 'dtype'

","No, this is in the code, but the issue is that the documentation corresponds to the github master version, while you don't that version installed. You most likely have a keras release installed.

Currently there is no Keras release that contains the commits for this parameter.
",,,false,,,
https://stackoverflow.com/questions/39001104,false,The issue does not meet the criteria for deeper analysis as it does not involve an API exhibiting unexpected failures or unpredictable behaviors under specific runtime conditions.,,,,,,,"In Keras, If samples_per_epoch is less than the &#39;end&#39; of the generator when it (loops back on itself) will this negatively affect result?","I'm using Keras with Theano to train a basic logistic regression model.

Say I've got a training set of 1 million entries, it's too large for my system to use the standard model.fit() without blowing away memory.


I decide to use a python generator function and fit my model using model.fit_generator().  
My generator function returns batch sized chunks of the 1M training examples (they come from a DB table, so I only pull enough records at a time to satisfy each batch request, keeping memory usage in check).  
It's an endlessly looping generator, once it reaches the end of the 1 million, it loops and continues over the set


There is a mandatory argument in fit_generator() to specify samples_per_epoch.  The documentation indicates


  samples_per_epoch: integer, number of samples to process before going to the next epoch.


I'm assuming the fit_generator() doesn't reset the generator each time an epoch runs, hence the need for a infinitely running generator.

I typically set the samples_per_epoch to be the size of the training set the generator is looping over.

However, if samples_per_epoch this is smaller than the size of the training set the generator is working from and the nb_epoch &gt; 1:


Will you get odd/adverse/unexpected training resulting as it seems the epochs will have differing sets training examples to fit to?
If so, do you 'fastforward' you generator somehow?

",,,,false,,,
https://stackoverflow.com/questions/53779968,true,The issue involves the behavior of the fit_generator function in Keras and how it loads data before actually training the model.,,,,,,,why Keras fit_generator() load before actually &quot;training&quot;,"sorry to bother:

I am confused by keras function : fit_generator

I use custom generator to generate (image,seg_image)
for training 

Look carefully you can see inside of get_seg() function 

I put the print(path) ann the path is just the
path to read image 

from data ,the one more intention is I would like to know how 

fit_generator() get the data from generator 

#import all the stuff
def get_seg(#parameters ):
    print(path) #to track when this function is called 
    return seg_image     #for training 

#pre-processing image 
def getimage(#parameters):
    #do something to image
    return the imgage      #for training

def data_generator():
#load all the data for training 
    zipped =cycle(zip(images,segmentations))
    while True:
        X = []
        Y = []
        for _ in range(batch_size) :
            im , seg = next(zipped)
            X.append(getimage(#parameters))  
            Y.append(get_seg(#parameters))
    yield np.array(X) , np.array(Y) 

#create an generator 
G = data_generator(#parameters) 
#start training
for ep in range( epochs ):
    m.fit_generator( G , steps_per_epoch=512, 
                     epochs=1,workers=1)


While I start training, I get the really unexpected result ,As it goes 

through training:The terminal looks like:
it print out 24 set of path 

fist which it take the data from custom data_generator

data/train/0000_mask.png
data/train/0001_mask.png
data/train/0002_mask.png
data/train/0003_mask.png
data/train/0004_mask.png
data/train/0005_mask.png
data/train/0006_mask.png
data/train/0007_mask.png
data/train/0008_mask.png
data/train/0009_mask.png
data/train/0010_mask.png
data/train/0011_mask.png
data/train/0012_mask.png
data/train/0013_mask.png
data/train/0014_mask.png
data/train/0015_mask.png
data/train/0016_mask.png
data/train/0017_mask.png
data/train/0018_mask.png
data/train/0019_mask.png
data/train/0020_mask.png
data/train/0021_mask.png
data/train/0022_mask.png
data/train/0023_mask.png


And then :
I do believe the training starts here .

1/512 [..............................] - ETA: 2:14:34 - loss: 2.5879 - acc: 0.1697


load the data(image) again 

data/train/0024_mask.png
data/train/0025_mask.png




After 512(steps_per_epoch) which means the next round training 

begins,it would just print next 24 path before training ....

I would like to know why this is happening? Is this is how keras 

works? To load data before actually pass is through the network?

Or I am misunderstand or miss something basic knowledge?
","Yes, this is how Keras works.

Training and loading are two parallel actions. One does not see how the other is going.

In the fit_generator method there is a max_queue_size argument, usually equal to 10 by default. This means the generator will load data at full speed until the queue is full. So you're loading many images in advance (this is good to avoid that the model gets slowed by loading)    

And the training just checks: are there items in the queue? Good, train. 

You're printing more than your batches because you call get_seg inside a loop but only call yield outside this loop. 
",,,false,,,
https://stackoverflow.com/questions/61886910,true,The issue involves manipulating data in a custom loss function in Keras for a CNN.,,,,,,,Manipulating data in keras custom loss function for CNN,"I'm trying to write a custom loss function in Keras for a CNN I'm working on. Y_true and Y_pred will both be tensors of grayscale images, so I expect a shape of [a, x, y, 1], where x and y are the dimensions of my images and a is the batch size.

The plan is to:


Threshold each image of Y_true by its mean pixel intensity
Use the non-zero elements of this mask to get an array of pixel values from Y_true and Y_pred
Measure the cosine similarity (using the built-in Keras loss function) of these arrays and return the average result of the batch as the loss


My main question is how I can efficiently implement this process?
Does the cosine_similarity function work on 1D arrays?

I know that I should avoid for loops to maintain efficiency but it's the only way I can think of implementing this function. Is there a more efficient way to implement this function using the Keras backend or numpy?

EDIT

Basic implementation and an unexpected error when compiling the model with this function:

def masked_cosine_similarity(y_true, y_pred):
    loss = 0
    for i in range(y_true.shape[0]):
        true_y = y_true[i,:,:,0]
        pred_y = y_pred[i,:,:,0]
        mask = true_y &gt; np.mean(true_y)
        elements = np.nonzero(mask)
        true_vals = np.array([true_y[x,y] for x, y in zip(elements[0], elements[1])])
        pred_vals = np.array([pred_y[x,y] for x, y in zip(elements[0], elements[1])])
        loss += cosine_similarity(true_vals, pred_vals)
    return loss / y_true.shape[0]


Error message:

     64     loss = 0
---&gt; 65     for i in range(y_true.shape[0]):
     66         true_y = y_true[i,:,:,0]
     67         pred_y = y_pred[i,:,:,0]

TypeError: 'NoneType' object cannot be interpreted as an integer

","The shape of a tensor in Keras/TF is usually [None, height, width, channels].
This is due to the support of an arbitrary batch size, you don't want to build a model that only works for a specific batch size. For that reason, your code collapses on:

for i in range(y_true.shape[0]):


since y_true.shape[0] == None.

Why do you loop over the batch? You don't need to do it.
For example, given some element-wize loss function (MSE/cosine loss etc.) you can do something like:

def my_loss(y_true, y_pred):
    mask = tf.keras.backend.cast(y_true &gt;= tf.math.reduce_mean(y_true, axis=[1,2], keepdims=True), 'float32') 
    masked_loss = K.sum(mask * elementwize_loss(y_true, y_pred), axis=-1)
    num_valid_pixels = K.maximum(1.0, K.cast(K.sum(mask), 'float32'))
    return masked_loss / num_valid_pixels

",,,false,,,
https://stackoverflow.com/questions/43720702,false,The issue does not meet the criteria for deeper analysis as it does not involve an API exhibiting unexpected failures or unpredictable behaviors under specific runtime conditions.,,,,,,,Fitting a sine wave with Keras and PYMC3 yields unexpected results,"I've been trying to fit a sine curve with a keras (theano backend) model using pymc3. I've been using this [http://twiecki.github.io/blog/2016/07/05/bayesian-deep-learning/] as a reference point.

A Keras implementation alone fit using optimization does a good job, however Hamiltonian Monte Carlo and Variational sampling from pymc3 is not fitting the data. The trace is stuck at where the prior is initiated. When I move the prior the posterior moves to the same spot. The posterior predictive of the bayesian model in cell 59 is barely getting the sine wave, whereas the non-bayesian fit model gets it near perfect in cell 63. I created a notebook here: https://gist.github.com/tomc4yt/d2fb694247984b1f8e89cfd80aff8706 which shows the code and the results. 

Here is a snippet of the model below...

class GaussWeights(object):
    def __init__(self):
        self.count = 0

    def __call__(self, shape, name='w'):
        return pm.Normal(
            name, mu=0, sd=.1,
            testval=np.random.normal(size=shape).astype(np.float32),
            shape=shape)


def build_ann(x, y, init):
    with pm.Model() as m:

        i = Input(tensor=x, shape=x.get_value().shape[1:])
        m = i
        m = Dense(4, init=init, activation='tanh')(m)
        m = Dense(1, init=init, activation='tanh')(m)

        sigma = pm.Normal('sigma', 0, 1, transform=None)
        out = pm.Normal('out', 
                         m, 1,
                         observed=y, transform=None)

    return out



 with pm.Model() as neural_network:
    likelihood = build_ann(input_var, target_var, GaussWeights())

#     v_params = pm.variational.advi(
#         n=300, learning_rate=.4
#     )
#     trace = pm.variational.sample_vp(v_params, draws=2000)
    start = pm.find_MAP(fmin=scipy.optimize.fmin_powell)
    step = pm.HamiltonianMC(scaling=start)
    trace = pm.sample(1000, step,   progressbar=True)

",,,,false,,,
https://stackoverflow.com/questions/72126226,true,The issue involves using model.fit in a for loop for K-fold cross-validation in Keras.,,,,,,,"model.fit in a for loop, for K-fold cross validation","I am trying to code a K-fold cross validation with LSTM architecture.
But I got an this error (edit):
Traceback (most recent call last):
  File ""/Users/me/Desktop/dynamicsoundtreatments/DST-features-RNN.py"", line 58, in &lt;module&gt;
    model.fit(training_data, training_label, epochs=100, batch_size=nbr_de_son)
  File ""/Users/me/miniforge3/lib/python3.9/site-packages/keras/utils/traceback_utils.py"", line 67, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File ""/Users/me/miniforge3/lib/python3.9/site-packages/keras/engine/training.py"", line 1395, in fit
    raise ValueError('Unexpected result of `train_function` '
ValueError: Unexpected result of `train_function` (Empty logs). Please use `Model.compile(..., run_eagerly=True)`, or `tf.config.run_functions_eagerly(True)` for more information of where went wrong, or file a issue/bug to `tf.keras`.

I tried to add run_eagerly=True but got the same error again.
I tried few alternatives such as def train(training_data, training_label): model.fit(training_data, training_label, epochs=100, batch_size=nbr_de_son) outside of the for loop. Got same error.
I was wondering if I should use Functional API, but I am very new to datascience. I really don't why I got this error. Thanks for your help.
nbr_anal = int(6)
nbr_de_son = int(samples.shape[0]/nbr_anal)
sequence = int(samples.shape[1])
input_shape=(nbr_anal, sequence)

# ------------------------------------------------------------------------
# PREPROCESSING

# batch size, sequence length, features
samples = samples.reshape(nbr_de_son, nbr_anal, sequence)
labels_extrait = np.argmax(labels_extrait, axis=1)
print(labels_extrait.shape)

# ------------------------------------------------------------------------
# K-Fold
k = 4
num_validation_samples =  len(samples) // k
num_validation_labels = len(labels_extrait) // k
validation_scores = []

model = Sequential()
model.add(LSTM(sequence,input_shape=input_shape))
model.add(Dropout(0.3))
model.add(Dense(8, activation='softmax'))
model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics='sparse_categorical_accuracy', run_eagerly=True)

for fold in range(k):
    rng_state = np.random.get_state()
    np.random.shuffle(samples)
    np.random.set_state(rng_state)
    np.random.shuffle(labels_extrait)

    validation_data = samples[num_validation_samples * fold:num_validation_samples * (fold + 1)]
    print(validation_data.shape)
    validation_label = labels_extrait[num_validation_labels * fold:num_validation_labels * (fold + 1)]
    print(validation_label.shape)

    training_data = samples[:num_validation_samples * (fold + 1)] + samples[num_validation_samples * (fold)]
    training_label = labels_extrait[:num_validation_labels * (fold + 1)] + labels_extrait[num_validation_labels * (fold)]

    model.fit(training_data, training_label, epochs=100, batch_size=nbr_de_son)

    validation_score = evaluate(validation_data, validation_label)
    validation_scores.append(validation_score)

validation_score = np.average(validation_scores)
print(validation_score)

","You can use StratifiedKFold from the sklearn package to do the cross validation. It is much clearer and is the standard way to do it. You should also reset the model weights at each fold before fitting the model, otherwise you will start with the weights initialized in the previous call of the fit method,
The modified code :
from sklearn.model_selection import StratifiedKFold
# K-Fold cross validation
k = 4
skf = StratifiedKFold(n_splits=k)
validation_scores = []

# store initial model's weights
weights_init = model.get_weights()

for train_index, test_index in skf.split(samples, labels_extrait):
    training_data = samples[train_index]
    training_label = labels_extrait[train_index]
    validation_data = samples[test_index]
    validation_label = labels_extrait[test_index]

    # reset mdoel's weights
    model.set_weights(weights_init)
    # fit
    model.fit(training_data, training_label, epochs=100, batch_size=nbr_de_son)

    validation_score = model.evaluate(validation_data, validation_label)
    validation_scores.append(validation_score)


validation_score = np.average(validation_scores)
print(validation_score) 

I don't know exactly where your error comes from, but the above code works
",,,false,,,
https://stackoverflow.com/questions/72023307,false,The issue does not meet the criteria for deeper analysis as it does not involve an API exhibiting unexpected failures or unpredictable behaviors under specific runtime conditions.,,,,,,,How do I solve LSTM error on predict functions,"This is the code attached below. I'm trying to run it and am getting this error below. Need help resolving it. Graph is defined as the variable name.
graph = Sequential()
graph.add(LSTM(50, return_sequences=True, input_shape = (60, 1)))
graph.add(LSTM(50, return_sequences=False))
graph.add(Dense(25))
graph.add(Dense(1))

graph.compile(optimizer='adam', loss='mean_squared_error')
graph.fit(x_train, y_train, batch_size=1, epochs=1)

test_data = train_data[training_data_length - 60: , : ]
x_test = []
y_test =  dataset[training_data_length : , : ]
for i in range(60,len(test_data)):
    x_test.append(test_data[i-60:i,0])
x_test = np.array(x_test)
x_test = np.reshape(x_test, (x_test.shape[0],x_test.shape[0],1))
predictions = graph.predict(x_test)
predictions = transfer.inverse_transform(predictions)

Getting this error
ValueError                                Traceback (most recent call last)
&lt;ipython-input-40-4624ea9a4af1&gt; in &lt;module&gt;()
      6 x_test = np.array(x_test)
      7 x_test = np.reshape(x_test, (x_test.shape[0],x_test.shape[0],1))
----&gt; 8 predictions = graph.predict(x_test)
      9 predictions = transfer.inverse_transform(predictions)

1 frames
/usr/local/lib/python3.7/dist-packages/keras/engine/training.py in predict(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)
   1995             callbacks.on_predict_batch_end(end_step, {'outputs': batch_outputs})
   1996       if batch_outputs is None:
-&gt; 1997         raise ValueError('Unexpected result of `predict_function` '
   1998                          '(Empty batch_outputs). Please use '
   1999                          '`Model.compile(..., run_eagerly=True)`, or '

ValueError: Unexpected result of `predict_function` (Empty batch_outputs). Please use `Model.compile(..., run_eagerly=True)`, or `tf.config.run_functions_eagerly(True)` for more information of where went wrong, or file a issue/bug to `tf.keras`.

","Your test_data always have length of 60 as per your construction
test_data = train_data[training_data_length - 60: , : ]

So, your for loop is never carried out as it is always equal to for i in range(60,60). Consequently, x_test remains [] and causes the error.
I'm guessing that you want to construct a set of length-60 series, each offset the next by 1 timestep. In that case, the test data should've been constructed as
for i in range(60,len(train_data)):
    x_test.append(train_data[i-60:i,0])

Also, your np.reshape usage seems wrong. The number of elements after reshape will be x_test.shape[0] * x_test.shape[0], but the original x_test have x_test.shape[0] * x_test.shape[1], so unless x_test.shape[0] == x_test.shape[1], you'll always get an error. You don't need reshape anyway, as your x_test should've already been in the right shape.
",,,false,,,
https://stackoverflow.com/questions/72023307,false,The issue does not meet the criteria for deeper analysis as it does not involve an API exhibiting unexpected failures or unpredictable behaviors under specific runtime conditions.,,,,,,,How do I solve LSTM error on predict functions,"This is the code attached below. I'm trying to run it and am getting this error below. Need help resolving it. Graph is defined as the variable name.
graph = Sequential()
graph.add(LSTM(50, return_sequences=True, input_shape = (60, 1)))
graph.add(LSTM(50, return_sequences=False))
graph.add(Dense(25))
graph.add(Dense(1))

graph.compile(optimizer='adam', loss='mean_squared_error')
graph.fit(x_train, y_train, batch_size=1, epochs=1)

test_data = train_data[training_data_length - 60: , : ]
x_test = []
y_test =  dataset[training_data_length : , : ]
for i in range(60,len(test_data)):
    x_test.append(test_data[i-60:i,0])
x_test = np.array(x_test)
x_test = np.reshape(x_test, (x_test.shape[0],x_test.shape[0],1))
predictions = graph.predict(x_test)
predictions = transfer.inverse_transform(predictions)

Getting this error
ValueError                                Traceback (most recent call last)
&lt;ipython-input-40-4624ea9a4af1&gt; in &lt;module&gt;()
      6 x_test = np.array(x_test)
      7 x_test = np.reshape(x_test, (x_test.shape[0],x_test.shape[0],1))
----&gt; 8 predictions = graph.predict(x_test)
      9 predictions = transfer.inverse_transform(predictions)

1 frames
/usr/local/lib/python3.7/dist-packages/keras/engine/training.py in predict(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)
   1995             callbacks.on_predict_batch_end(end_step, {'outputs': batch_outputs})
   1996       if batch_outputs is None:
-&gt; 1997         raise ValueError('Unexpected result of `predict_function` '
   1998                          '(Empty batch_outputs). Please use '
   1999                          '`Model.compile(..., run_eagerly=True)`, or '

ValueError: Unexpected result of `predict_function` (Empty batch_outputs). Please use `Model.compile(..., run_eagerly=True)`, or `tf.config.run_functions_eagerly(True)` for more information of where went wrong, or file a issue/bug to `tf.keras`.

","Your test_data always have length of 60 as per your construction
test_data = train_data[training_data_length - 60: , : ]

So, your for loop is never carried out as it is always equal to for i in range(60,60). Consequently, x_test remains [] and causes the error.
I'm guessing that you want to construct a set of length-60 series, each offset the next by 1 timestep. In that case, the test data should've been constructed as
for i in range(60,len(train_data)):
    x_test.append(train_data[i-60:i,0])

Also, your np.reshape usage seems wrong. The number of elements after reshape will be x_test.shape[0] * x_test.shape[0], but the original x_test have x_test.shape[0] * x_test.shape[1], so unless x_test.shape[0] == x_test.shape[1], you'll always get an error. You don't need reshape anyway, as your x_test should've already been in the right shape.
",,,false,,,
https://stackoverflow.com/questions/71951540,true,"The issue stems from attempting to apply tf.image.rgb_to_grayscale on a tensor that does not have the expected three channels. This API is designed to work with RGB images, but an error occurs when it is used on an image or mask with a different number of channels.",TensorFlow,tf.image.rgb_to_grayscale,"When applying tf.image.rgb_to_grayscale on a segmentation mask (which typically has a single channel), TensorFlow raises a ""dimensions must be equal"" error. This is because the API expects an input tensor with three channels (RGB), but the input mask only has one channel.","The tf.image.rgb_to_grayscale is applied to a tensor with a shape that ends in 3 (i.e., an RGB image).","The issue is triggered when tf.image.rgb_to_grayscale is applied to a tensor with a shape that does not end in 3 (i.e., not an RGB image).",This issue might be challenging to detect for users unfamiliar with the specific requirements of the tf.image.rgb_to_grayscale function or those who assume that it can handle inputs with any number of channels.,"ValueError: Unexpected result of `predict_function` (Empty batch_outputs). Please use `Model.compile(..., run_eagerly=True)`,","I tried using GridSearchCV() but I also got errors so I wrote a very time consuming code that returns the metrics I want to evaluate my model. Here it is :
def df_to_new(df,window_size): 
    df_as_np = df.to_numpy()
    X = []
    y = []
    for i in range(len(df_as_np)-window_size):
        row = [[a] for a in df_as_np[i:i+window_size]]
        X.append(row)
        label = df_as_np[i+window_size]
        y.append(label)
        
    return np.array(X),np.array(y)  

from tensorflow.keras.models import load_model
import os
checkpointpath = 'C:\\Users\\USER\\trainingFORLOOP_daily/cp.ckt'
cp = ModelCheckpoint(checkpointpath, save_best_only=True,verbose=1)     
EPOCH = [30,150,300]
learningRates = [0.0001,0.00001]
batchSize = [15,20,40]
win_size = [5,15,25]
dropout_rate = 0.2
num_features = 1
for i in learningRates:
    for j in EPOCH:
        for k in batchSize:
            for l in win_size:
                X,y = df_to_new(Ac13,l)

                #Split the data
                perc_train = 0.8
                limit_train = int(np.floor(len(Ac13)*perc_train))
                xtrain,ytrain = X[:limit_train],y[:limit_train]
                xval,yval = X[limit_train:],y[limit_train:]
                
                #create the model
                model1 = Sequential() 
                model1.add(InputLayer((l,1))) 
                model1.add(LSTM(128))
                model1.add(Dropout(dropout_rate))
                model1.add(Dense(86,'relu'))
                model1.add(Dropout(dropout_rate))
                model1.add(Dense(1,'linear'))
                model1.summary()
                model1.compile(loss=MeanSquaredError(),optimizer = 
                       Adam(learning_rate=i),
                       metrics=[RootMeanSquaredError()],run_eagerly=True)
                model1.fit(xtrain,ytrain,validation_data=(xval,yval),batch_size=k,epochs=j,callbacks=[cp],shuffle=False)
                model1.save(""my_model"")
                model1 = load_model(""my_model"")
                           
                train_predictions = model1.predict(xtrain).flatten() 
                train_results = pd.DataFrame(data={'TrainPredictions':train_predictions,'Actual values':ytrain})
                train_results
                scale = len(train_predictions)
                val_predictions = model1.predict(xval).flatten()
                val_results = pd.DataFrame(data='ValidatePredictions':val_predictions,'Validation values':yval}) 

                

I am getting the following error (Full traceback):
    ValueError                                Traceback (most recent call last)
~\AppData\Local\Temp/ipykernel_19052/292377201.py in &lt;module&gt;
     51                 plt.legend(bbox_to_anchor =(0.75, 1.15), ncol = 2)
     52                 plt.show()
---&gt; 53                 val_predictions = model1.predict(xval).flatten() # flatten() removes the brackets inside the data
     54                 val_results = pd.DataFrame(data={'Validate Predictions':val_predictions,'Validation values':yval}) #yval are the actual values
     55                 val_results

~\anaconda3\envs\tf-gpu-cuda8\lib\site-packages\keras\utils\traceback_utils.py in error_handler(*args, **kwargs)
     65     except Exception as e:  # pylint: disable=broad-except
     66       filtered_tb = _process_traceback_frames(e.__traceback__)
---&gt; 67       raise e.with_traceback(filtered_tb) from None
     68     finally:
     69       del filtered_tb

~\anaconda3\envs\tf-gpu-cuda8\lib\site-packages\keras\engine\training.py in predict(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)
   1995             callbacks.on_predict_batch_end(end_step, {'outputs': batch_outputs})
   1996       if batch_outputs is None:
-&gt; 1997         raise ValueError('Unexpected result of `predict_function` '
   1998                          '(Empty batch_outputs). Please use '
   1999                          '`Model.compile(..., run_eagerly=True)`, or '

ValueError: Unexpected result of `predict_function` (Empty batch_outputs). Please use `Model.compile(..., run_eagerly=True)`, or `tf.config.run_functions_eagerly(True)` for more information of where went wrong, or file a issue/bug to `tf.keras`.

Any suggestions?
I did the same approach but with hourly data and it worked quite well without any errors. The hourly code is the same as the daily code(this one) the only thing that changed is that the data in the hourly code was summed in a day and the daily code data was obtained.
","There are a few things wrong with your code.

You are creating your model inside the training loop. So that’s wrong. Unless you want to train a new version at each epoch. Your model should be defined and compiled before you start training.

Depending on which version of tensorflow you you are using you should enable eager execution before you declare the model. That lets you train the model without creating a session and a scope.


Hope this helps!
",,,false,,,
https://stackoverflow.com/questions/22591174/pandas-multiple-conditions-while-indexing-data-frame-unexpected-behavior,false,The behavior is due to a misunderstanding of logical operators and conditions in pandas rather than a problem with the API itself.,,,,,,,,,,,,false,,,
https://stackoverflow.com/questions/67448600,true,The issue is related to a library error caused by the absence of the 'six' package. Installing the 'six' package can resolve the issue.,Keras,load_model,The error occurs when trying to load a Keras model due to the absence of the 'six' package. This package is required by Keras and can be installed using 'pip install six'.,,,,Error while loading .h5 model in Flask using keras,"I have built a horse human detector using keras CNN on Google colab the model worked and loaded perfectly on colab. Now I am building a flask application while loading he .h5 model file I was getting error
TypeError: __init__() got an unexpected keyword argument 'ragged'

I reinstall keras  2.3.1 using pip and now I am getting a library error
 NameError: name 'six' is not defined

my App.py
#Import necessary libraries
from flask import Flask, render_template, request

import numpy as np
import os

from keras.preprocessing.image import load_img
from keras.preprocessing.image import img_to_array
from keras.models import load_model
 

#load model
model = load_model(""predictor.h5"" )
print(""@@ model loaded"")

def pred_human_horse(model , horse_or_human):
  test_image = load_img(horse_or_human , target_size=(150,150)) #resize
  print(""@@ Got Image for predicton"")
  test_image = img_to_array(test_image)/255 #numpy array  between 0-1
  test_image = np.expand_dims(test_image,axis=0) #4 dimension

  result= model.predict(test_image).round(3) #rounding off
  pred =np.argmax(result)
  print(""@@ Raw results = "",result)
  print(""@@ class = "", pred)

  if pred==0:
    return ""Horse""

  else:
    return ""Human""

# Crate flask app
app = Flask(__name__)
    
@app.route(""/"",methods=[""GET"",""POST""])
def home():
    return render_template(""index.html"")


@app.route(""/predict"",methods=[""GET"",""POST""])
def predict():
    if request.method==""POST"":
        #get input image file
        file = request.files[""image""]
        filename= file.filename
        print(""@@ File recieved"",filename)
        
        #save the file
        file_path= os.path.join(""static/user_uploaded"",filename)
        file.save(file_path)
        
        print(""@@ Prediction..."")
        pred=pred_human_horse(horse_or_human=file_path )
        
        return render_template(""predict.html"" ,pred_output= pred , user_image=file_path )


if __name__==""__main__"":
    app.run(threaded=False)

Error I am getting
runfile('F:/INTERNSHIP/Crisp-Metric-MAY21/Human-horse-prediction/app.py', wdir='F:/INTERNSHIP/Crisp-Metric-MAY21/Human-horse-prediction')
Traceback (most recent call last):

  File ""&lt;ipython-input-26-df590f092cb6&gt;"", line 1, in &lt;module&gt;
    runfile('F:/INTERNSHIP/Crisp-Metric-MAY21/Human-horse-prediction/app.py', wdir='F:/INTERNSHIP/Crisp-Metric-MAY21/Human-horse-prediction')

  File ""C:\Users\DANIA NIAZI\Anaconda3\lib\site-packages\spyder_kernels\customize\spydercustomize.py"", line 827, in runfile
    execfile(filename, namespace)

  File ""C:\Users\DANIA NIAZI\Anaconda3\lib\site-packages\spyder_kernels\customize\spydercustomize.py"", line 110, in execfile
    exec(compile(f.read(), filename, 'exec'), namespace)

  File ""F:/INTERNSHIP/Crisp-Metric-MAY21/Human-horse-prediction/app.py"", line 13, in &lt;module&gt;
    model = load_model(""predictor.h5"" )

  File ""C:\Users\DANIA NIAZI\Anaconda3\lib\site-packages\keras\engine\saving.py"", line 492, in load_wrapper

  File ""C:\Users\DANIA NIAZI\Anaconda3\lib\site-packages\keras\engine\saving.py"", line 582, in load_model

  File ""C:\Users\DANIA NIAZI\Anaconda3\lib\site-packages\keras\utils\io_utils.py"", line 211, in is_supported_type

NameError: name 'six' is not defined

","Maybe you should try installing the six package which will be installed when installing Django. Anyway you can install it using:
pip install six
",,,false,,,
https://stackoverflow.com/questions/63427411,false,The issue is related to the model's predicted values not aligning with the expected values. This is likely due to a problem with the model architecture or training process rather than an issue with the API itself.,,,,,,,TypeError: __init__() got an unexpected keyword argument &#39;ragged&#39;? when load model,"I'm using keras to load the model which stored in h5 file, but got this error, I tried to google around but got no result.
please help.
face_classifier  = cv2.CascadeClassifier(cv2.data.haarcascades +'haarcascade_frontalface_alt.xml')
classifier = load_model('facial_expression.h5')

class_labels = ['Angry', 'Disgust', 'Fear', 'Happy', 'Neutral', 'Sad', 'Surprise']

cap = cv2.VideoCapture(0)

while True:

ret, frame = cap.read()
labels = []

gray = cv2.cvtColor(frame,cv2.COLOR_BGR2GRAY)
faces = face_classifier.detectMultiScale(gray,1.3,5)

for (x,y,w,h) in faces:
    cv2.rectangle(frame,(x,y),(x+w,y+h),(255,0,0),2)
    roi_gray = gray[y:y+h,x:x+w]
    roi_gray = cv2.resize(roi_gray,(48,48),interpolation=cv2.INTER_AREA)
# rect,face,image = face_detector(frame)


    if np.sum([roi_gray])!=0:
        roi = roi_gray.astype('float')/255.0
        roi = img_to_array(roi)
        roi = np.expand_dims(roi,axis=0)

    # make a prediction on the ROI, then lookup the class

        preds = classifier.predict(roi)[0]
        label=class_labels[preds.argmax()]
        label_position = (x,y)
        cv2.putText(frame,label,label_position,cv2.FONT_HERSHEY_SIMPLEX,2,(0,255,0),3)
    else:
        cv2.putText(frame,'No Face Found',(20,60),cv2.FONT_HERSHEY_SIMPLEX,2,(0,255,0),3)
cv2.imshow('Emotion Detector',frame)
if cv2.waitKey(1) &amp; 0xFF == ord('q'):
    break

cap.release()
cv2.destroyAllWindows()

error like this :
File ""C:\Users\ADMIN\anaconda3\envs\myenv\lib\site-packages\keras\utils\generic_utils.py"", line 147, in deserialize_keras_object
list(custom_objects.items())))
File ""C:\Users\ADMIN\anaconda3\envs\myenv\lib\site-packages\keras\engine\sequential.py"", line 301, in from_config
custom_objects=custom_objects)
File ""C:\Users\ADMIN\anaconda3\envs\myenv\lib\site-packages\keras\layers_init_.py"", line 168, in deserialize
printable_module_name='layer')
File ""C:\Users\ADMIN\anaconda3\envs\myenv\lib\site-packages\keras\utils\generic_utils.py"", line 149, in deserialize_keras_object
return cls.from_config(config['config'])
File ""C:\Users\ADMIN\anaconda3\envs\myenv\lib\site-packages\keras\engine\base_layer.py"", line 1179, in from_config
return cls(**config)
File ""C:\Users\ADMIN\anaconda3\envs\myenv\lib\site-packages\keras\legacy\interfaces.py"", line 91, in wrapper
return func(*args, **kwargs)
TypeError: init() got an unexpected keyword argument 'ragged'
","Instead of importing load_model from keras ,Import it from tf.keras
from tf.keras.models import load_model
classifier = load_model('facial_expression.h5')

",,,false,,,
https://stackoverflow.com/questions/52466932,false,The error occurs because the 'restore_best_weights' argument is not supported in the installed version of Keras. Upgrading to a newer version of Keras or using a different approach is recommended.,,,,,,,restore_best_weights issue keras early stopping,"I am using EarlyStopping from Keras for my deep learning project. The documentations here mentions a very useful idea of restoring best weights. But somehow I am not able to use it yet. I am using Keras 2.2.2/TF 1.10, installed using Anaconda. 
Call is simple as follows. is there any issue? 

es = EarlyStopping(monitor='val_acc', min_delta=1e-4, patience=patience_,verbose=1,restore_best_weights=True)



  __init__() got an unexpected keyword argument 'restore_best_weights'

","Ah, a very common problem. The keras documentation online is produced from the master branch in github, so features that have been introduced only recently are present there, but not yet in any Keras release.

So the restore_best_weights option of the EarlyStopping callback was added on August 18, 2018, while currently the latest Keras release (2.2.2) was made on July 28, 2018. So you will have to use Keras master, wait for a Keras release, or just not use the option.

Update: It is now included in Keras 2.2.3 release.
",,,false,,,
https://stackoverflow.com/questions/53358033,false,The error occurs because the 'include_top' argument is not supported in the installed version of ResNet50. Using the 'require_flatten' argument instead is suggested.,,,,,,,why ResNet50 include_top argument giving error?,"Keras version - 2.2.4

Python - 3.6.6

issue:

from resnet50 import ResNet50



----&gt; 6 model = ResNet50(input_tensor=image_input, 
include_top=True,weights='imagenet')

TypeError: _obtain_input_shape() got an unexpected keyword argument 'include_top'

is there a fix for this?
","Replace: 

model = ResNet50(input_tensor=image_input, include_top=True,weights='imagenet')


With:

model = ResNet50(input_tensor=image_input,require_flatten=True,weights='imagenet')

",,,false,,,
https://stackoverflow.com/questions/50754017,false,The error occurs because the installed version of TensorFlow is too old. Upgrading to a newer version of TensorFlow is recommended.,,,,,,,Keras implementation issue unexpected keyword argument &#39;axis&#39;,"The Keras implementation of MNIST CNN shown here shows the following error:

TypeError: softmax() got an unexpected keyword argument 'axis'


I spent a long time trying to fix it but nothing works. I am running TensorFlow 1.1.0 and Keras 2.1.6. Thank you for help
","You need to use this command to update the version of TensorFlow.

pip install --upgrade tensorflow      # for Python 2.7
pip3 install --upgrade tensorflow     # for Python 3.n

","Your TensorFlow version is too old, you need to upgrade, preferrably to the latest version (currently 1.8).
",,false,,,
https://stackoverflow.com/questions/49301203,false,"The issue does not meet the criteria for deeper analysis as it is related to a misunderstanding of the usage of the 'epochs' parameter in the Keras model compilation, which is not an API-related problem.",,,,,,,keras error got an unexpected keyword argument &#39;epochs&#39;,"I'm trying to train a network in Keras to classify an image and after debugging the last issue got this one of unexpected keywork epochs

muiruri_samuel@training-2:~/google-landmark-recognition-challenge$ python train.py
Using TensorFlow backend.
Found 981214 images belonging to 14951 classes.
Found 237925 images belonging to 14951 classes.
Epoch 1/1
2018-03-15 13:35:19.822304: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instruc
tions that this TensorFlow binary was not compiled to use: AVX2 FMA
Traceback (most recent call last):
  File ""train.py"", line 74, in &lt;module&gt;
    validation_data=validation_generator)
  File ""/home/muiruri_samuel/.local/lib/python2.7/site-packages/keras/legacy/interfaces.py"", line 91, in w
rapper
    return func(*args, **kwargs)
  File ""/home/muiruri_samuel/.local/lib/python2.7/site-packages/keras/models.py"", line 1276, in fit_genera
tor
    initial_epoch=initial_epoch)
  File ""/home/muiruri_samuel/.local/lib/python2.7/site-packages/keras/legacy/interfaces.py"", line 91, in w
rapper
    return func(*args, **kwargs)
  File ""/home/muiruri_samuel/.local/lib/python2.7/site-packages/keras/engine/training.py"", line 2224, in f
it_generator
    class_weight=class_weight)
  File ""/home/muiruri_samuel/.local/lib/python2.7/site-packages/keras/engine/training.py"", line 1883, in t
rain_on_batch
    outputs = self.train_function(ins)
  File ""/home/muiruri_samuel/.local/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py"", line
 2478, in __call__
    **self.session_kwargs)
TypeError: run() got an unexpected keyword argument 'epochs'


at this point I've removed epochs but still getting the same error

from keras.preprocessing.image import ImageDataGenerator
from keras.models import Sequential
from keras.layers import Conv2D, MaxPooling2D
from keras.layers import Activation, Dropout, Flatten, Dense
from keras import backend as K
from keras.callbacks import EarlyStopping
from keras.callbacks import ModelCheckpoint


# dimensions of our images.
img_width, img_height = 150, 150

train_data_dir = 'training_images'
validation_data_dir = 'validation_images'
nb_train_samples = 2000
nb_validation_samples = 800
epochs = 50
batch_size = 16

if K.image_data_format() == 'channels_first':
    input_shape = (3, img_width, img_height)
else:
    input_shape = (img_width, img_height, 3)

model = Sequential()
model.add(Conv2D(32, (3, 3), input_shape=input_shape))
model.add(Activation('relu'))
model.add(MaxPooling2D(pool_size=(2, 2)))

model.add(Conv2D(32, (3, 3)))
model.add(Activation('relu'))
model.add(MaxPooling2D(pool_size=(2, 2)))

model.add(Conv2D(64, (3, 3)))
model.add(Activation('relu'))
model.add(MaxPooling2D(pool_size=(2, 2)))

model.add(Flatten())
model.add(Dense(64))
model.add(Activation('relu'))
model.add(Dropout(0.5))
model.add(Dense(14951, activation=""softmax""))

monitor = EarlyStopping(monitor='val_loss', min_delta=1e-3, patience=5, verbose=0, mode='auto')
checkpointer = ModelCheckpoint(filepath=""best_weights.hdf5"", verbose=0, save_best_only=True) # save best model

model.compile(loss='categorical_crossentropy', optimizer='adam', callbacks=[monitor,checkpointer], epochs=1000, metrics=['accuracy'])

# this is the augmentation configuration we will use for training
train_datagen = ImageDataGenerator(
    rescale=1. / 255,
    shear_range=0.2,
    zoom_range=0.2,
    horizontal_flip=True)

# this is the augmentation configuration we will use for testing:
# only rescaling
test_datagen = ImageDataGenerator(rescale=1. / 255)

train_generator = train_datagen.flow_from_directory(
    train_data_dir,
    target_size=(img_width, img_height),
    batch_size=batch_size,
    class_mode='categorical')

validation_generator = test_datagen.flow_from_directory(
    validation_data_dir,
    target_size=(img_width, img_height),
    batch_size=batch_size,
    class_mode='categorical')

model.fit_generator(
    train_generator,
    validation_data=validation_generator)

model.load_weights('best_weights.hdf5') # load weights from best model
model.save('last_model.h5')


I'd use epochs and batches too but now I'd need it to work first. The model in logic has a folder training_images which has sub folders which are the the classes of the images with the images in these sub folders. Then there's the validation_images which is randomly sampled 20% of the training images to validation.
","model.compile does not take an epochs parameter. Only fit and fit_generator do.
",,,false,,,
https://stackoverflow.com/questions/59035050,false,"The issue does not meet the criteria for deeper analysis as it is related to the training accuracy of an LSTM model in Keras, and does not indicate any unexpected behavior or failure of an API.",,,,,,,"LSTM class, got error: TypeError: __init__() got an unexpected keyword argument &#39;input_shape&#39;","class LSTM:

    scaler_y_train= MinMaxScaler(feature_range=(0,1))
    scaler_x_train= MinMaxScaler(feature_range=(0,1))
    scaler_forecast= MinMaxScaler(feature_range=(0,1))

    def __init__(self, filename, seq_len, forecast_size):
        self._filename= filename
        self._seq_len= seq_len
        self._forecast_size= forecast_size
        self.model= Sequential()


    def load_y_train(self):
        raw_data_values= self._filename.values
        raw_data= self._filename
        print('Initializing the data loading...')
        forecast_out_variable= math.ceil(len(raw_data_values)* self._forecast_size)
        forecast_out_fixed= None

        print('Creating y_train...')
        y= raw_data.iloc[forecast_out_variable:].values
        len_y= len(y)
        data_windows_y= []
        for i in range(len_y - self._seq_len):
            data_windows_y.append(y[i: i + self._seq_len])
        data_windows_y= np.array(data_windows_y).astype(float)
        y_train= data_windows_y[:, -1, [0]]
        return print(y_train.shape), y_train

    def load_x_train(self):
        raw_data_values= self._filename.values
        raw_data= self._filename
        print('Initializing the data loading...')
        forecast_out_variable= math.ceil(len(raw_data_values)* self._forecast_size)
        forecast_out_fixed= None

        print('Creating x_train...')
        x= raw_data.iloc[:-forecast_out_variable].values
        len_x= len(x)
        data_windows_x= []
        for i in range(len_x - self._seq_len):
            data_windows_x.append(x[i: i + self._seq_len])
        data_windows_x= np.array(data_windows_x).astype(float)
        x_train= data_windows_x[:, :-1]
        return x_train

    def load_x_forecast(self):
        raw_data_values= self._filename.values
        raw_data= self._filename
        print('Initializing the data loading...')
        forecast_out_variable= math.ceil(len(raw_data_values)* self._forecast_size)
        forecast_out_fixed= None

        print('Creating x_train...')
        x_forecast = raw_data.iloc[:-forecast_out_variable].values
        len_x_forecast= len(x_forecast)
        data_windows_x= []
        for i in range(len_x_forecast - self._seq_len):
            data_windows_x.append(x_forecast[i: i + self._seq_len])
        data_windows_x= np.array(data_windows_x).astype(float)
        xx_forecast= data_windows_x[:, :-1]
        return print(xx_forecast.shape), xx_forecast

    def build_model(self):
        x_train= self.load_x_train()
        x_train= np.array(x_train).reshape(1050,49,2).astype(float)
        print(x_train.shape)
        print('Model starting compiling...')
        start= time.time()

        self.model.add(LSTM(50, input_shape=(x_train.shape[1], x_train.shape[-1]), 
        return_sequences=True))
        self.model.add(Dropout(0.2))
        self.model.add(LSTM(100, return_sequences=False))
        self.model.add(Dropout(0.2))
        self.model.add(Dense(1, activation = ""linear""))
        self.model.compile(loss='mse', optimizer='adam')
        end= time.time()
        print ('model compiled in: ' +str((end-start)*1000)+ ' ms')
        return model, print(model.summary())




lstm_01= LSTM(ms_unreal, 50, 0.1)
print(lstm_01.load_x_forecast())
print(lstm_01.load_y_train())
print(lstm_01.load_x_train())
print(lstm_01.build_model(model))


Type error is init() got an unexpected keyword argument 'input_shape',
I do not know how to made it work as im calling correctly the libraries. I searched the problem online but im not able to understand the issue. 
The code is a class named LSTM where basically the first function are finalized to get the train and test data, while the last one (in which there is the issue) is to initialize the LSTM keras model. 
","Your class is called LSTM and you call a function from Keras called LSTM. Try renaming your class My_LSTM or some variant. Otherwise you won't be able to call your class without overwriting the Keras implementation.
",,,false,,,
https://stackoverflow.com/questions/49678302,false,"The issue does not meet the criteria for deeper analysis as it is related to the visualization of layers in a Keras model and the usage of the 'pop' method, rather than any unexpected behavior or failure of an API.",,,,,,,Delete layers of keras pretrained model,"I want to use vgg16 pre-trained model of keras. I have notice some strange behavior when trying to change the model. 

1) I have add some layers of the per-trained model. My problem is that tensorboard is showing the layers of the model that I didn't add into the sequence model. This is strange because I have also deleted the imported model. I think this have to do with the dependency between layers so I want to remove this dependencies.  How can I do this?



For example in this picture there is two layers that I didn't add but they are showing in the graph

vgg16_model = keras.applications.vgg16.VGG16()


cnnModel = keras.models.Sequential()

for layer in vgg16_model.layers[0:13]:
    cnnModel.add(layer)

for layer in vgg16_model.layers[14:16]:
    cnnModel.add(layer)

for layer in vgg16_model.layers[17:21]:
    cnnModel.add(layer)

cnnModel.add(keras.layers.Dense(2048, name=""compress_1""))
cnnModel.add(keras.layers.Dense(1024, name=""compress_2""))
cnnModel.add(keras.layers.Dense(512, name=""compress_3""))


for layer in cnnModel.layers[0:4]:
    layer.trainable = False

del vgg16_model


2) the second problem occurs when using cnnModel.pop(). In fact I have add all the layers but I do a pop to the layer I don't want before adding the next one this is the error I get.

Layer block4_conv2 has multiple inbound nodes, hence the notion of ""layer output"" is ill-defined. Use `get_output_at(node_index)` instead.


And this is the code I am using:

for layer in vgg16_model.layers[0:14]:
    cnnModel.add(layer)

cnnModel.pop()

for layer in vgg16_model.layers[14:17]:
    cnnModel.add(layer)

cnnModel.pop()

for layer in vgg16_model.layers[17:21]:
    cnnModel.add(layer)


cnnModel.pop() is working the problem only occurs when trying to add the next layer.

Thank you for your help.
","You can try using Model instead of Sequential, like:

vgg16_model = keras.applications.vgg16.VGG16()

drop_layers = [13, 16]

input_layer = x = vgg16_model.input

for i, layer in enumerate(vgg16_model.layers[1:], 1):
    if i not in drop_layers:
        x = layer(x)

x = keras.layers.Dense(2048, name=""compress_1"")(x)
x = keras.layers.Dense(1024, name=""compress_2"")(x)
x = keras.layers.Dense(512, name=""compress_3"")(x)

cnnModel = keras.models.Model(inputs = input_layer, outputs = x)

for layer in cnnModel.layers[0:4]:
    layer.trainable = False

del vgg16_model

",,,false,,,
https://stackoverflow.com/questions/54283085,false,The issue does not meet the criteria for deeper analysis as it is related to the drawing of images using TensorFlow.js and does not indicate any unexpected behavior or failure of an API.,,,,,,,Why am I getting strange triplication of video using Webcam and Tensorflow.js?,"I have a keras model trained and now I want to run this on the web. I thought this might be a good way to attempt testing out Tensorflow.js. I downloaded the Tesnroflow.js ""Webcam-transfer-learning"" tutorial and then modified it to get what I currently have. The working keras model performs emotion classification after reducing the size of the image to 48x48. Now in the keras model, I take a snapshot of the webcam, copy it and then draw my box and label. I was trying to do the same thing in tf.js, so I setup a canvas, got a reference to it and tried drawing to the canvas after my conversion to gray scale.

I am seeing a strange behavior where it is correctly showing the gray scale image but it is displaying it 3 times across and not sure what I am doing wrong. I have included the areas I believe the problem might reside below.  Should any more info be needed, I can share more. It was my hope that someone that has already tried performing something similar may see right away what I am clearly doing wrong.  Any info would be helpful.  Thanks!

Modified webcam.js by adding function 

preProc() {
return tf.tidy(() =&gt; {
  // Reads the image as a Tensor from the webcam &lt;video&gt; element.
  const webcamImage = tf.fromPixels(this.webcamElement);

  //Resize to our image and get back single channel for greyscale
  const croppedImage = this.cropImage(webcamImage, 1);

  // Expand the outer most dimension so we have a batch size of 1.
  const batchedImage = croppedImage.expandDims(0);

  // Normalize the image between -1 and 1. The image comes in between 0-255,
  // so we divide by 127 and subtract 1.
  return batchedImage.toFloat().div(tf.scalar(127)).sub(tf.scalar(1));
});
}

/**
* Crops an image tensor so we get a square image with no white space.
* @param {Tensor4D} img An input image Tensor to crop.
*/
cropImage(img, dim=3) {
  const size = Math.min(img.shape[0], img.shape[1]);
  const centerHeight = img.shape[0] / 2;
  const beginHeight = centerHeight - (size / 2);
  const centerWidth = img.shape[1] / 2;
  const beginWidth = centerWidth - (size / 2);
  return img.slice([beginHeight, beginWidth, 0], [size, size, dim]);
}


From ui.js I am using drawFrame

export function drawFrame(image, canvas) {
  const [width, height] = [300, 165];
  const ctx = canvas.getContext('2d');
  const imageData = new ImageData(width, height);
  const data = image.dataSync();
  for (let i = 0; i &lt; height * width; ++i) {
    const j = i * 4;
    imageData.data[j + 0] = (data[i * 3 + 0] + 1) * 127;
    imageData.data[j + 1] = (data[i * 3 + 1] + 1) * 127;
    imageData.data[j + 2] = (data[i * 3 + 2] + 1) * 127;
    imageData.data[j + 3] = 255;
  }
  ctx.putImageData(imageData, 0, 0);
}


Finally in index.js, when the predict button is pressed the below handler executes

async function predict() {
while (isPredicting) {
  const predictedClass = tf.tidy(() =&gt; {
    // Capture the frame from the webcam.
    const imgmod = webcam.preProc();
    ui.drawFrame(imgmod, grayframe);


    // Returns the index with the maximum probability. This number corresponds
    // to the class the model thinks is the most probable given the input.
    //return predictions.as1D().argMax();
    return imgmod;
  });

  const classId = (await predictedClass.data())[0];
  predictedClass.dispose();

  //ui.predictClass(classId);
  await tf.nextFrame();
  }
  ui.donePredicting();
}



","drawframe is drawing the image three times.
It has to do with the shape of the input image and the way height and width are used to crop the image. If the input image were of shape [298, 160], the canvas will not be rendered as there will be an error when trying to access index that are not in data. For instance the size of data is 298 * 160 whereas the last element of the loop would try to access the element 3 * 300 * 160. Since there are no error in the code, it indicates that the size of data is bigger than [298, 160]. At any rate, there is a mismatch in data dimension. The image are drawn 3 times because of the three channels, possibly because it was not removed before.

Instead of implementing your own way of drawing the imagedata, you can consider using tf.toPixel method
",,,false,,,
https://stackoverflow.com/questions/71397660,true,"The issue involves the Flatten layer in Keras returning an output shape of (None, None), indicating unexpected behavior. Further analysis is required to determine the cause of this issue.",,,,,,,"Keras Flatten layer returns output shape (None, None)","So, I noticed this strange behavior of Flatten layer of Keras. I'm using TF1.15 and Keras 2.3.0.
Basically the output of the Flatten layer has an unknown shape. It's hard to troubleshoot the model when you can't keep track of the shape. Why is this happening with Flatten layer, and can I do something so it recognizes the shape?
from keras.layers import Flatten
inputs = Input(shape=(3,2,4))
prediction = Flatten()(inputs)
print(inputs.shape, prediction.shape)

(?, 3, 2, 4) (?, ?)


","Try using tf.keras instead of just keras:
import tensorflow as tf
print(tf.__version__)
inputs = tf.keras.layers.Input(shape=(3,2,4))
prediction = tf.keras.layers.Flatten()(inputs)
print(inputs.shape, prediction.shape)

1.15.2
(?, 3, 2, 4) (?, 24)

",,,false,,,
https://stackoverflow.com/questions/59105384,true,"The issue involves the TensorFlow API tf.image.rgb_to_grayscale, which exhibits unexpected behavior when applied to a tensor with a shape that does not end in 3 (i.e., not an RGB image).",TensorFlow,tf.image.rgb_to_grayscale,"When applying tf.image.rgb_to_grayscale on a segmentation mask (which typically has a single channel), TensorFlow raises a ""dimensions must be equal"" error. This is because the API expects an input tensor with three channels (RGB), but the input mask only has one channel.","The tf.image.rgb_to_grayscale is applied to a tensor with a shape that ends in 3 (i.e., an RGB image).","The issue is triggered when tf.image.rgb_to_grayscale is applied to a tensor with a shape that does not end in 3 (i.e., not an RGB image).",This issue might be challenging to detect for users unfamiliar with the specific requirements of the tf.image.rgb_to_grayscale function or those who assume that it can handle inputs with any number of channels.,Tensorboard graph orphan layers,"While building a model that includes transfer learning (from VGG-16).
I encounter this strange behavior. Tensorboard graph shows the layers which are not part of the new model but part of the old, above the point of seperation, and they are just dangling there.





When investigating further, model.summary() does not show these layers, model.get_layer(""block4_conv1"") can't find them either, and the keras tf.keras.utils.plot_model doesn't show them too. but if they are not part of the graph, how would tensorboard know about them?

To build the new model, I used the recommended method. 

Model first stage:

    vgg_input_model = tf.keras.applications.VGG16(weights='imagenet', include_top=False, input_tensor=x)
    final_vgg_kayer = vgg_input_model.get_layer(""block3_pool"")
    input_model = tf.keras.Model(inputs=vgg_input_model.inputs, outputs=final_vgg_kayer.output)
    input_model.trainable = True

    x = tf.keras.layers.Conv2D(512, 1, padding=""same"", activation='relu', name=""stage0_final_conv1"")(input_model.output)
    x = tf.keras.layers.Conv2D(512, 1, padding=""same"", activation='relu', name=""stage0_final_conv2"")(x)
    x = tf.keras.layers.Conv2D(256, 1, padding=""same"", activation='relu', name=""stage0_final_conv3"")(x)
    x = tf.keras.layers.Conv2D(128, 1, padding=""same"", activation='relu', name=""stage0_final_conv4"")(x)




TF:2.1 (nightly-2.x)
PY:3.5
Tensorboard: 2.1.0a20191124

","After trying multiple methods, I came to the conclusion that the recommended way is wrong. doing model_b=tf.keras.Model(inputs=model_a.inputs,outputs=model_a.get_layet(""some_layer"").output) will lead to dangling layers from model_a.
Using tf.keras.backend.clear_session() in between may cleans the keras graph, but tensorboard's graph is left empty then.

The best solution I found is config+weights copy of the required model, layer by layer.
And rebuilding the connections in a new model. that way there is no relationship whatsoever in the Keras graph between the two models.
(This is simple for a sequential model like VGG, but might be more difficult for something like ResNet)



Sample code:

tf.keras.backend.clear_session()

input_shape = (368, 368, 3)  #only the input shape is shared between the models
#transfer learning model definition
input_layer_vgg = tf.keras.layers.Input(shape=input_shape)
vgg_input_model = tf.keras.applications.VGG16(weights='imagenet', include_top=False, input_tensor=input_layer_vgg)
name_last_layer = ""block3_pool"" #the last layer to copy

tf.keras.backend.clear_session() #clean the graph from the transfer learning model

input_layer = tf.keras.layers.Input(shape=input_shape) #define the input layer for the first model
x=input_layer
for layer in vgg_input_model.layers[1:]:    #copy over layers, without the other input layer
    config=layer.get_config()  #get config
    weights=layer.get_weights() #get weights
    #print(config)
    copy_layer=type(layer).from_config(config) #create the new layer from config
    x=copy_layer(x) #connect to previous layers,
                    #required for the proper sizing of the layer,
                    #set_weights will not work without it
    copy_layer.set_weights(weights)    

    if layer.name == name_last_layer:
        break
del vgg_input_model

input_model=tf.keras.Model(inputs=input_layer,outputs=x) #create the new model,
                                                        #if needed x can be used further doen the line

",,,false,,,
https://stackoverflow.com/questions/22591174/pandas-multiple-conditions-while-indexing-data-frame-unexpected-behavior,false,The behavior is due to a misunderstanding of logical operators and conditions in pandas rather than a problem with the API itself.,,,,,,,,,,,,false,,,
https://stackoverflow.com/questions/52880384,false,"The issue is caused by a division by zero in the custom loss function, not due to any unexpected behavior or unpredictable API.",,,,,,,Keras loss becomes nan only at epoch end,"I am observing some strange behavior from Keras. I am training a small model where the training loss becomes nan only at the end of the first epoch.

So if I have 100 batches, and I terminate training at batch 99, then resume for another 99 it trains fine. Otherwise, once it reaches the end of an epoch it always returns nan. 

I am using a custom loss function:

def corr(x, y):
    xc = x - K.mean(x)
    yc = y - K.mean(y)
    r_num = K.mean(xc*yc) 
    r_den = K.std(x)*K.std(y)
    return r_num/r_den


And I have tried all of the standard tricks like dropping my learning rate, clipping the norm and value of my gradient, and increasing batch size. Only in the event of increasing my batch size to something unrealistic like 100,000 (I have 1 million data points) does it actually continue past an epoch, but I would like to understand what is going on at the end that is causing this strange behavior. I also tried different optimizers (currently using Adam), and tried this on different systems to make sure it wasn't a problem on my one computer. 

My input and output is one dimensional and my model is summarized below. 

_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_7 (InputLayer)         (None, 1)                 0         
_________________________________________________________________
dense_7 (Dense)              (None, 100)               200       
_________________________________________________________________
dense_8 (Dense)              (None, 100)               10100     
_________________________________________________________________
dense_9 (Dense)              (None, 1)                 101       
=================================================================
Total params: 10,401
Trainable params: 10,401
Non-trainable params: 0
_________________________________________________________________


Does Keras so something special at the end of an epoch? I couldn't find anything other than the standard logger callback. I also wrote a custom callback which evaluates my model each batch and stores the output, and when I plot it over time it does not appear to blow up or do anything strange. It just looks like it's slowly improving, then the training dies.
","Probably it is caused by a division by zero in the loss function. Make sure the denominator is always positive by adding a small constant to it. You can use K.epsilon() for this purpose:

    return r_num / (r_den + K.epsilon())

",,,false,,,
https://stackoverflow.com/questions/56323567,false,The issue is not related to any API exhibiting unexpected failures or unpredictable behaviors under specific runtime conditions. It appears to be a problem with the model's architecture or training process.,,,,,,,Siamese neural network with two pre-trained ResNet 50 - strange behavior while testing model,"I built siamese neural network, using Keras lib for it.
My model has two inputs with shape (64,64,3), two pre-trained ResNet-50.
Loss function is binary cross entropy.

The model is based on this paper a link

During train I have very good trait/val accuracy, about 0.99/0.98, and low loss 0.01/0.05.

But when I test my saved model, I get bad results. The model can't recognize even two the same pictures. 

Also I noticed strange behavior:
the greater the number of epochs the result is worse. 
For example, comparing two identical images, trained model with 10 epoch  gives prediction:
""8.jpg"": 0.5180479884147644
but the same model trained with 100 epoch gives
""8.jpg"": 5.579867080537926E-13
However for 100 epoch I have better train results.

I've tried different model for CNN: ResNet18, different input shapes, like (224,224,3) or (128,128,3). 

Also I've triad use not pre-train model, only ResNet50/ResNet18 without pre-trained weights. But I have the same bad results while testing real model.

My code is

def create_base_model(image_shape, dropout_rate, suffix=''):
    I1 = Input(shape=image_shape)
    model = ResNet50(include_top=False, weights='imagenet', input_tensor=I1, pooling=None)
    model.layers.pop()
    model.outputs = [model.layers[-1].output]
    model.layers[-1].outbound_nodes = []

    for layer in model.layers:
        layer.name = layer.name + str(suffix)
        layer.trainable = False

    flatten_name = 'flatten' + str(suffix)

    x = model.output
    x = Flatten(name=flatten_name)(x)
    x = Dense(1024, activation='relu')(x)
    x = Dropout(dropout_rate)(x)
    x = Dense(512, activation='relu')(x)
    x = Dropout(dropout_rate)(x)

    return x, model.input


def create_siamese_model(image_shape, dropout_rate):

    output_left, input_left = create_base_model(image_shape, dropout_rate)
    output_right, input_right = create_base_model(image_shape, dropout_rate, suffix=""_2"")

    L1_layer = Lambda(lambda tensors: tf.abs(tensors[0] - tensors[1]))
    L1_distance = L1_layer([output_left, output_right])
    L1_prediction = Dense(1, use_bias=True,
                          activation='sigmoid',
                          kernel_initializer=RandomNormal(mean=0.0, stddev=0.001),
                          name='weighted-average')(L1_distance)

    prediction = Dropout(0.2)(L1_prediction)

    siamese_model = Model(inputs=[input_left, input_right], outputs=prediction)

    return siamese_model

siamese_model = create_siamese_model(image_shape=(64, 64, 3),
                                         dropout_rate=0.2)

siamese_model.compile(loss='binary_crossentropy',
                      optimizer=Adam(lr=0.0001),
                      metrics=['binary_crossentropy', 'acc'])
siamese_model.fit_generator(train_gen,
                            steps_per_epoch=1000,
                            epochs=10,
                            verbose=1,
                            callbacks=[checkpoint, tensor_board_callback, lr_reducer, early_stopper, csv_logger],
                            validation_data=validation_data,
                            max_q_size=3)

siamese_model.save('siamese_model.h5')



# and the my prediction
siamese_net = load_model('siamese_model.h5', custom_objects={""tf"": tf})

X_1 = [image, ] * len(markers)
batch = [markers, X_1]
result = siamese_net.predict_on_batch(batch)

# I've tried also to check identical images 
markers = [image]
X_1 = [image, ] * len(markers)
batch = [markers, X_1]
result = siamese_net.predict_on_batch(batch)


I have some doubts about my prediction method.
Could someone please help me to find what is wrong with predictions?
","What you are getting is expected. I'm not sure what you mean by


  Also I noticed strange behavior: the greater the number of epochs the result is worse.  


But the results you shown are valid and expected. Let's start with what the model is outputting. Your model output is (normalized)distance between the first and second inputs. If the inputs are similar, then the distance should be close to zero. As number of training step increases the model learns to identify the inputs, i.e if the inputs are similar the model learns to output values close to zero, and if the inputs are different the model learns to output values close to one. So, 


  ... trained model with 10 epoch gives prediction: ""8.jpg"": 0.5180479884147644 but the same model trained with 100 epoch gives ""8.jpg"": 5.579867080537926E-13 However for 100 epoch I have better train results.  


, confirms that the model has learned that the two inputs are similar and outputs 5.579867080537926E-13 ~ 0(approximately close to 0). 

Although the model is performing well, there is one issue I've observed in the model definition:- The output layer is dropout layer. Dropout is not valid output layer. What you are doing by this setting is, randomly with probability 0.2 you are setting the output of the model to be zero. 

Let's assume the target variable has 1(the two inputs are different), and model has learnt to identify the images correctly and outputs value close to 1 before the dropout layer. Let's further assume that the dropout layer has decided to set the output to be zero. So the model output will be zero. Even though the layers before dropout layer have performed well, because of the dropout layer, they will be penalized. If this is not what you are looking then remove the last dropout layer.

L1_prediction = Dense(1, use_bias=True,
                    activation='sigmoid',
                    kernel_initializer=RandomNormal(mean=0.0, stddev=0.001),
                    name='weighted-average')(L1_distance)


siamese_model = Model(inputs=[input_left, input_right], outputs=L1_prediction)


However, sometimes this behavior is needed if one want to add noise to the model. This has the same effect with randomly altering the target variable when the value is 1. 
",,,false,,,
https://stackoverflow.com/questions/56029955,true,"The issue involves the custom loss function in Keras, which exhibits unexpected behavior in terms of the loss values. This is an API-related problem as the loss function is a crucial component of the Keras framework.",Keras,Custom loss function,The custom loss function implemented in Keras always starts with almost the same value and does not change significantly during training. This behavior persists even after modifying the loss function or changing its return value. The issue is likely related to the implementation of the loss function or its interaction with other components of the neural network.,The custom loss function should exhibit a decreasing trend during training as the network learns to optimize the specified objective.,The issue is triggered when using the custom loss function in Keras for training a neural network.,"This issue might be challenging to detect during development and testing due to the complex interactions between the loss function, the neural network architecture, and the optimization process.",Extremely Strange behavior of my own loss function written in keras (always output similar values),"I am implementing a CNN based deep learning algorithm to provide continuous ranking of a set of images. My plan is to feed the network with batches of images that is already ranked in the correct order, and evaluate the ranking errors in the custom loss function, hopefully the network can learn how to produce a relative number to indicating the position of a given image in a sequence. 

I have come up with the simple custom loss function as below:

def my_loss_function(y_true, y_pred):
    #diff = y_pred[1:,:] - y_pred[:-1, :] //previous implementation, same problem
    indx1 = tf.range(1, 128)
    indx2 = tf.range(0, 127)
    diff = tf.gather(y_pred, indx1) - tf.gather(y_pred, indx2)
    order_loss = -K.sum(K.log(K.sigmoid(diff)+K.epsilon()))

    return order_loss


As you can see that I actually don't need the y-true data, so I generated a dummy y_true data just to avoid the checking error by keras. 

The strange problem I have now is this loss function always starts with almost the same value when I run it, even after I changed the return to 2*order_loss or change K.sum to K.mean or other functions. In training, the loss can reduce very slowly and will plateau quickly. I could not think of any reason that might cause this problem, did anyone have similar experience or know how to solve this problem?



Update:

Even after I changed the loss function to:

def my_loss_function(y_true, y_pred):
    diff = alpha*y_pred  # alpha is a constant
    order_loss = -K.sum(K.log(K.sigmoid(diff)+K.epsilon()))
    return order_loss


it still has the same problem, when I change the alpha value, the output doesn't change accordingly. The int_shape of y_pred is (none, 1).


","Your question is difficult to answer without exactly working with the problem, but I can suggest you to do some debugging inside the loss function. Here is an example of how you can print some stuff and see what is actually going in there.

def my_loss_function(y_true, y_pred):
    diff = y_pred[1:,:] - y_pred[:-1, :] 
    order_loss = -K.sum(K.log(K.sigmoid(diff)+K.epsilon()))

    # anything you like to print
    print_op = tf.print(""\nDebug : "", diff, order_loss, K.get_variable_shape(y_pred))  
    with tf.control_dependencies([print_op]): # this will force the print_op to be run
        return K.identity(order_loss)
    #return K.identity(order_loss)



I hope this will be helpful for you to see if the shapes and values are as expected and if you see any discrepancies.
Let me know if you find anything fishy, and then we can take it forward.
",,,false,,,
https://stackoverflow.com/questions/55954212,true,"The issue involves the behavior of the ADAM optimizer with AMSGrad, which exhibits unusual behavior compared to other optimizers. This is an API-related problem as it pertains to the specific implementation of the ADAM optimizer in Keras.",Keras,ADAM optimizer with AMSGrad,The ADAM optimizer with AMSGrad shows different behavior compared to other optimizers in terms of training and test accuracies. The training accuracy starts at a relatively high value from the first epoch and changes only slightly after that. This behavior is not observed with other optimizers. The issue is likely related to the specific implementation of the ADAM optimizer with AMSGrad in Keras.,The ADAM optimizer with AMSGrad should exhibit a gradual increase in training and test accuracies during the training process.,The issue is triggered when using the ADAM optimizer with AMSGrad for training a neural network.,This issue might be challenging to detect during development and testing as it requires comparing the behavior of the ADAM optimizer with AMSGrad to other optimizers and analyzing the training and test accuracies.,Unusual behavior of ADAM optimizer with AMSGrad,"I am trying some 1, 2, and 3 layer LSTM networks to classify land cover of some selected pixels from a Landsat time-series spectral data. I tried different optimizers (as implemented in Keras) to see which of them is better, and generally found AMSGrad variant of ADAM doing a relatively better job in my case. However, one strange thing to me is that for the AMSGrad variant, the training and test accuracies start at a relatively high value from the first epoch (instead of increasing gradually) and it changes only slightly after that, as you see in the below graph.

Performance of ADAM optimizer with AMSGrad on

Performance of ADAM optimizer with AMSGrad off

I have not seen this behavior in any other optimizer. Does it show a problem in my experiment? What can be the explanation for this phenomenon?
","Pay attention to the number of LSTM layers. They are notorious for easily overfitting the data. Try a smaller model initially(less number of layers), and gradually increase the number of units in a layer. If you notice poor results, then try adding another LSTM layer, but only after the previous step has been done.

As for the optimizers, I have to admit I have never used AMSGrad. However, the plot with regard to the accuracy does seem to be much better in case of the AMSGrad off. You can see that when you use AMSGrad the accuracy on the training set is much better than that on the test set, which a strong sign of overfitting. 

Remember to keep things simple, experiment with simple models and generic optimizers.
",,,false,,,
https://stackoverflow.com/questions/47798854,true,"The issue involves the constant accuracy and loss values in a Keras CNN model, which is unexpected behavior. This is an API-related problem as it pertains to the behavior of the CNN model implemented using the Keras framework.",Keras,Convolutional Neural Network (CNN),"The Keras CNN model, built using ResNet50 and transfer learning, shows constant accuracy and loss values for every epoch during training. The accuracy and loss values do not change even after adjusting learning rates, changing optimizers, or modifying other parameters. The issue is likely related to the implementation of the CNN model or its interaction with the dataset.",The CNN model should exhibit a decreasing trend in loss and an increasing trend in accuracy during the training process.,The issue is triggered when training a CNN model using the Keras framework.,This issue might be challenging to detect during development and testing as it requires analyzing the behavior of the CNN model and comparing it to expected trends in loss and accuracy.,Keras CNN accuracy and loss are constant,"I am building a keras CNN model using ResNet50 utilizing transfer learning. For some reason my accuracy and loss are exactly the same for every epoch. Strangely, I am seeing the same behavior with similar code but with VGG19. This leads me to believe that the issue is not with the actual model code and somewhere in the pre-processing. I have tried adjusting learning rates, changing optimizer, image resolution, freezing layers etc. and the scores do not change. I went into my image directories to check if my two different classes are mixed, and they are not. What is the issue? I just want to say thank you ahead of time.

P.S. I am training on ~2000 images and have two classes. 

import numpy as np
import pandas as pd

import tensorflow as tf
from keras.preprocessing.image import ImageDataGenerator
from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau
from keras.models import Sequential, Model, load_model
from keras.layers import Conv2D, GlobalAveragePooling2D
from keras.layers import Activation, Dropout, Flatten, Dense
from keras import applications
from keras import optimizers

img_height, img_width, img_channel = 400, 400, 3 #change chanel to 1 instead of three since it is black and white

base_model = applications.ResNet50(weights='imagenet', include_top=False, input_shape=(img_height, img_width, img_channel))

# add a global spatial average pooling layer
x = base_model.output
x = GlobalAveragePooling2D()(x)
# let's add a fully-connected layer
x = Dense(512, activation='relu',name='fc-1')(x)
#x = Dropout(0.5)(x)
x = Dense(256, activation='relu',name='fc-2')(x)
#x = Dropout(0.5)(x)
# and a logistic layer -- let's say we have 2 classes
predictions = Dense(1, activation='softmax', name='output_layer')(x)

model = Model(inputs=base_model.input, outputs=predictions)
model.compile(loss='binary_crossentropy', optimizer=optimizers.SGD(lr=0.1),
              metrics=['accuracy'])

model.summary()

from keras.preprocessing.image import ImageDataGenerator
from keras.callbacks import ModelCheckpoint

batch_size = 6

# prepare data augmentation configuration
train_datagen = ImageDataGenerator(
        rescale=1./255,
        rotation_range=20,
        width_shift_range=0.1,
        height_shift_range=0.1,
        shear_range=0.1,
        zoom_range=0.1,
        horizontal_flip=True,
        vertical_flip=True)


test_datagen = ImageDataGenerator(rescale=1./255)

#possibely resize the image
train_generator = train_datagen.flow_from_directory(
        ""../Train/"",
        target_size=(img_height, img_width),
        batch_size=batch_size,
        class_mode='binary',
        shuffle=True
)

validation_generator = test_datagen.flow_from_directory(
        ""../Test/"",
        target_size=(img_height, img_width),
        batch_size=batch_size,
        class_mode='binary',
        shuffle=True)

epochs = 10

history = model.fit_generator(
        train_generator,
        steps_per_epoch=2046 // batch_size,
        epochs=epochs,
        validation_data=validation_generator,
        validation_steps=512 // batch_size,
        callbacks=[ModelCheckpoint('snapshots/ResNet50-transferlearning.model', monitor='val_acc', save_best_only=True)])


This is the output given by keras:

Epoch 1/10
341/341 [==============================] - 59s 172ms/step - loss: 7.0517 - acc: 0.5577 - val_loss: 7.0334 - val_acc: 0.5588
Epoch 2/10
341/341 [==============================] - 57s 168ms/step - loss: 7.0517 - acc: 0.5577 - val_loss: 7.0334 - val_acc: 0.5588
Epoch 3/10
341/341 [==============================] - 56s 165ms/step - loss: 7.0517 - acc: 0.5577 - val_loss: 7.0334 - val_acc: 0.5588
Epoch 4/10
341/341 [==============================] - 57s 168ms/step - loss: 7.0517 - acc: 0.5577 - val_loss: 7.0334 - val_acc: 0.5588
Epoch 5/10
341/341 [==============================] - 57s 167ms/step - loss: 7.0517 - acc: 0.5577 - val_loss: 7.0334 - val_acc: 0.5588

","predictions = Dense(1, activation='softmax', name='output_layer')(x)


The Dense layer represents how many different classes are there which you want to classify and hence for binary classification you will require 2 where you have written 1.

So change that line to.

 predictions = Dense(2, activation='softmax', name='output_layer')(x)


Just a note, always try to keep a variable to handle number of classes, something like 

predictions = Dense(num_classes, activation='softmax', name='output_layer')(x)


and then define num_classes at the start of your code for better flexibility and readability.

You can check documentation about Dense layer here : https://faroit.github.io/keras-docs/2.0.0/layers/core/
","The final layer should have a 'sigmoid' activation instead of softmax since it is binary classification. 
",,false,,,
https://stackoverflow.com/questions/64119338,true,The issue involves the exploding MAPE (Mean Absolute Percentage Error) metric during training of an LSTM model in Keras. This is an API-related problem as it pertains to the behavior of the MAPE metric in the Keras framework.,Keras,MAPE metric,"During the training of an LSTM model with attention in Keras, the MAPE metric shows an exploding behavior while the MSE (Mean Squared Error) and MAE (Mean Absolute Error) metrics train normally. The MAPE metric continues to increase with each epoch, but it does not hinder the training process as the other metrics still show improvement. The issue is likely related to the implementation of the MAPE metric in Keras or its interaction with the LSTM model.",The MAPE metric should exhibit a decreasing trend during training as the model learns to minimize the percentage error in predictions.,The issue is triggered when using the MAPE metric for evaluating an LSTM model in Keras.,This issue might be challenging to detect during development and testing as it requires analyzing the behavior of the MAPE metric and comparing it to other evaluation metrics such as MSE and MAE.,Why Keras MAPE metric is exploding during training but MSE loss is not?,"I implemented an LSTM with attention in Keras to reproduce this paper. The strange behavior is simple: I have an MSE loss function and an MAPE and MAE as metrics. During training the MAPE is exploding but the MSE and MAE seem to  train normally:
Epoch 1/20
275/275 [==============================] - 191s 693ms/step - loss: 0.1005 - mape: 15794.8682 - mae: 0.2382 - val_loss: 0.0334 - val_mape: 24.9470 - val_mae: 0.1607
Epoch 2/20
275/275 [==============================] - 184s 669ms/step - loss: 0.0099 - mape: 6385.5464 - mae: 0.0725 - val_loss: 0.0078 - val_mape: 11.3268 - val_mae: 0.0803
Epoch 3/20
275/275 [==============================] - 186s 676ms/step - loss: 0.0025 - mape: 5909.3735 - mae: 0.0369 - val_loss: 0.0131 - val_mape: 14.9827 - val_mae: 0.1061
Epoch 4/20
275/275 [==============================] - 187s 678ms/step - loss: 0.0015 - mape: 4746.2788 - mae: 0.0278 - val_loss: 0.0142 - val_mape: 16.1894 - val_mae: 0.1122
Epoch 5/20
 30/275 [==&gt;...........................] - ETA: 2:38 - loss: 0.0012 - mape: 9.3647 - mae: 0.0246

The MAPE is exploding at the end of each epoch. What could be the cause of this specific behavior?
The MAPE is still decreasing with each epoch so is this not really an issue since it is not hindering the training process?
","Your loss and MAPE are decreasing so it sounds good. But if you fear the high values in MAPE you can tell if there is a Y value near zero. Because MAPE is a percentage error.
MAPE results can be misleading. From Wikipedia:

Although the concept of MAPE sounds very simple and convincing, it has
major drawbacks in practical application, and there are many studies
on shortcomings and misleading results from MAPE.

It cannot be used if there are zero values (which sometimes happens for example in demand data) because there would be a division
by zero.
For forecasts which are too low the percentage error cannot exceed 100%, but for forecasts which are too high there is no upper limit to
the percentage error.
MAPE puts a heavier penalty on negative errors, than on positive errors.

To overcome these issues with MAPE, there are some other measures
proposed in literature:

Mean Absolute Scaled Error (MASE)
Symmetric Mean Absolute Percentage Error (sMAPE)
Mean Directional Accuracy (MDA)
Mean Arctangent Absolute Percentage Error (MAAPE)


",,,false,,,
https://stackoverflow.com/questions/63271509,true,The issue involves lagged results in an LSTM model for multivariate time series forecasting using Keras. This is an API-related problem as it pertains to the behavior of the LSTM model and its interaction with the input variables.,Keras,LSTM model,"In an LSTM model for multivariate time series forecasting, the test set predictions exhibit a lagged result that is not present in the training data. This lagged behavior is observed when including the 'pollution (t_1)' variable as part of the inputs. The issue might be related to the handling of lagged variables or the interaction between the LSTM model and the input variables.",The LSTM model should provide accurate predictions for the next month's pollution levels based on the weather conditions and pollution levels of the previous months.,The issue is triggered when using an LSTM model for multivariate time series forecasting in Keras.,This issue might be challenging to detect during development and testing as it requires analyzing the lagged behavior of the predictions and investigating the relationship between the input variables and the LSTM model's performance.,Why do i get lagged results on my LSTM model,"I am new to machine learning and I am performing a Multivariate Time Series Forecast using LSTMs in Keras. I have a monthly timeseries dataset with 4 input variables (temperature, precipitation, Dew and wind_spreed) and 1 output variable (pollution). Using this data i framed a forecasting problem where, given the weather conditions and pollution for prior months, I forecast the pollution at the next month. Below is my code
X = df[['Temperature', 'Precipitation', 'Dew', 'Wind_speed' ,'Pollution (t_1)']].values
y = df['Pollution (t)'].values
y = y.reshape(-1,1)

from sklearn.preprocessing import MinMaxScaler
scaler = MinMaxScaler(feature_range=(0, 1))
scaled = scaler.fit_transform(X)

#dataset has 359 samples in total
train_X, train_y = X[:278], y[:278]
test_X, test_y = X[278:], y[278:]
# reshape input to be 3D [samples, timesteps, features]
train_X = train_X.reshape((train_X.shape[0], 1, train_X.shape[1]))
test_X = test_X.reshape((test_X.shape[0], 1, test_X.shape[1]))
print(train_X.shape, train_y.shape, test_X.shape, test_y.shape) 


model = Sequential()
model.add(LSTM(100, input_shape=(train_X.shape[1], train_X.shape[2])))
model.add(Dropout(0.2))
#    model.add(LSTM(70))
#    model.add(Dropout(0.3))
model.add(Dense(1))
model.compile(loss='mean_squared_error', optimizer='adam')

history = model.fit(train_X, train_y, epochs=700, batch_size=70, validation_data=(test_X, test_y), verbose=2, shuffle=False)

# summarize history for loss
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper right')
plt.show()

To do predictions i use the following code
from sklearn.metrics import mean_squared_error,r2_score
yhat = model.predict(test_X)
mse  = mean_squared_error(test_y, yhat)
rmse = np.sqrt(mse)
r2   = r2_score(test_y, yhat)

print(""test set performance"")
print(""--------------------"")
print(""MSE:"",mse)
print(""RMSE:"",rmse)
print(""R^2: "",r2)

fig, ax = plt.subplots(figsize=(10,5))
ax.plot(range(len(test_y)), test_y, '-b',label='Actual')
ax.plot(range(len(yhat)), yhat, 'r', label='Predicted')
plt.legend()
plt.show()

Running this code i fell into the following issues:

For some reason am getting a lagged result for my test set which is not in my training data as shown on the below image. I do not understand why i have these lagged results (does it have something to do with including 'pollution (t_1)' as part of my inputs)?

Graph Results:


By adding ""pollution (t_1)"" which is a shift by 1 lag of the polution variable as part of my inputs this variable now seems to dominate the prediction as removing the other varibales seems to have no influence on my results (r-squared and rmse) which is strange since all these variables do assit in pollution prediction.

Is there something i am doing wrong in my code which is the reason for these issues? I am new to python so any help to answer the above 2 questions will be greatly appreaciated.
","First of all, I think it is not appropriate to input '1' as Timesteps value, because LSTM model is the one treating timeseries or sequence data.
I think the following script of data mining will work well
def lstm_data(df,timestamps):
    array_data=df.values
    sc=MinMaxScaler()
    array_data_=sc.fit_transform(array_data)

    array=np.empty((0,array_data_.shape[1]))
    range_=array_data_.shape[0]-(timestamps-1)
    for t in range(range_):
        array_data_p=array_data_[t:t+sequenth_length,:]
        array=np.vstack((array,array_data_p))

    array_=array.reshape(-1,timestamps, array.shape[1])
    return array_

#timestamps depend on your objection, but not '1'
x_data=lstm_data(x, timestamps=4) 
y_data=lstm_data(y, timestamps=4)
y_data=y_data.reshape(-1,1)

#Divide each data into train and test
#Input the divided data into your LSTM model

",,,false,,,
https://stackoverflow.com/questions/44716415,false,The behavior observed in approximating the sine function with a neural network using ReLU activation is expected and does not indicate any API-related issues. The shape of the result resembling ReLU is a characteristic of the ReLU activation function and does not signify a problem with the API or the neural network model.,,,,,,,Approximating sine function with Neural Network and ReLU,"I am trying to approximate a sine function with a neural network (Keras).

Yes, I read the related posts :)


Link 1
Link 2
Link 3


Using four hidden neurons with sigmoid and an output layer with linear activation works fine.

But there are also settings that provide results that seem strange to me.

Since I am just started to work with I am interested in what and why things happen, but I could not figure that out so far.

# -*- coding: utf-8 -*-

import numpy as np
np.random.seed(7)

from keras.models import Sequential
from keras.layers import Dense
import pylab as pl
from sklearn.preprocessing import MinMaxScaler

X = np.linspace(0.0 , 2.0 * np.pi, 10000).reshape(-1, 1)
Y = np.sin(X)

x_scaler = MinMaxScaler()
#y_scaler = MinMaxScaler(feature_range=(-1.0, 1.0))
y_scaler = MinMaxScaler()

X = x_scaler.fit_transform(X)
Y = y_scaler.fit_transform(Y)

model = Sequential()
model.add(Dense(4, input_dim=X.shape[1], kernel_initializer='uniform', activation='relu'))
# model.add(Dense(4, input_dim=X.shape[1], kernel_initializer='uniform', activation='sigmoid'))
# model.add(Dense(4, input_dim=X.shape[1], kernel_initializer='uniform', activation='tanh'))
model.add(Dense(1, kernel_initializer='uniform', activation='linear'))

model.compile(loss='mse', optimizer='adam', metrics=['mae'])

model.fit(X, Y, epochs=500, batch_size=32, verbose=2)

res = model.predict(X, batch_size=32)

res_rscl = y_scaler.inverse_transform(res)

Y_rscl = y_scaler.inverse_transform(Y)

pl.subplot(211)
pl.plot(res_rscl, label='ann')
pl.plot(Y_rscl, label='train')
pl.xlabel('#')
pl.ylabel('value [arb.]')
pl.legend()
pl.subplot(212)
pl.plot(Y_rscl - res_rscl, label='diff')
pl.legend()
pl.show()


This is the result for four hidden neurons (ReLU) and linear output activation. 


Why does the result take the shape of the ReLU? 

Does this have something to do with the output normalization? 
","Try adding more hidden layers, each with more hidden units. I used this code:

model = Sequential()
model.add(Dense(50, input_dim=X.shape[1], activation='relu'))
model.add(Dense(50, input_dim=X.shape[1], activation='relu'))
model.add(Dense(1, activation='linear'))


and got these results:


",,,false,,,
https://stackoverflow.com/questions/59855961,true,"The issue stems from attempting to convert a Keras model to a quantized TFLite version using TFLiteConverter. The error occurs due to a mismatch in data types, where the input tensor is expected to be of type FLOAT32 but is of type NOTYPE. This indicates a problem with the conversion process rather than the API itself.",TensorFlow,TFLiteConverter,"During the conversion process, when attempting to set the input tensor, TensorFlow raises an error stating that the tensor has a type of NOTYPE instead of the expected FLOAT32. This error prevents the successful conversion and quantization of the Keras model.",The TFLiteConverter is used to convert a Keras model to a quantized TFLite version.,The issue is triggered when attempting to convert a Keras model to a quantized TFLite version and encountering a mismatch in data types for the input tensor.,"This issue might be challenging to detect during development and testing, as it involves the conversion process and specific requirements for TFLite models.",Converting Keras model with TFLiteConverter to quantized tflite version results in NOTYPE error,"When converting and doing 8-Bit quantization of a keras model I ran into a strange error that did not happen for image data sets.   

import tensorflow.python.keras.backend as K
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras.models import load_model
import numpy as np

x_train = np.array([[0.6171875  0.59791666],[0.6171875  0.59791666],[0.6171875  0.59791666]])
y_train = np.array([[0.6171875  0.59791666],[0.6171875  0.59791666],[0.6171875  0.59791666]])


def representative_dataset_gen():
    for i in range(1):
        # Get sample input data as a numpy array in a method of your choosing.
        sample = np.array([0.5,0.6])
        sample = np.expand_dims(sample, axis=0)
        yield [sample]



model = tf.keras.models.Sequential([
  tf.keras.layers.Flatten(input_shape=(2,)),
  tf.keras.layers.Dense(12, activation='relu'),
  tf.keras.layers.Dense(2, activation='softmax')
])


model.compile(optimizer='adam',
              loss='categorical_crossentropy',
              metrics=['accuracy'])

model.fit(x_train, y_train, epochs=1)

converter = tf.lite.TFLiteConverter.from_keras_model(model)
converter = tf.lite.TFLiteConverter.from_keras_model(model)
converter.optimizations = [tf.lite.Optimize.DEFAULT]
converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]
converter.inference_input_type = tf.float32
converter.inference_output_type = tf.float32
converter.representative_dataset = representative_dataset_gen

tflite_quant_model = converter.convert()



This results in the error

ValueError: Cannot set tensor: Got value of type NOTYPE but expected type FLOAT32 for input 1, name: dense_1_input   

This procedure used to work when using image data but now this happens. Tried different TF version including nightly TF2.1.
","Apparently the problem has to do with the data type of the input tensor which was by default Float64 and not the expected Float32. Since tflite does not know about Float64 it treads it as NOTYPE which is confusing.

Supported TF Lite Types

A cast to float32 solves the problem

sample = sample.astype(np.float32)
",,,false,,,
https://stackoverflow.com/questions/68701979,true,The issue involves the incorrect behavior of recall and precision metrics in a Keras model. This behavior is related to the specific configuration of the model and the usage of the softmax activation function with two output nodes.,Keras,"tf.keras.metrics.Recall, tf.keras.metrics.Precision","The recall and precision metrics are not working correctly in the given Keras model. The model is configured with a sigmoid activation function for binary classification, but the metrics are designed for multi-class classification with softmax activation. This results in unexpected behavior and incorrect metric values.",The recall and precision metrics are typically used in multi-class classification tasks with softmax activation.,The issue is triggered when using the recall and precision metrics in a Keras model with a sigmoid activation function for binary classification.,"This issue might be challenging to detect during development and testing, especially when the model is initially performing reasonably well and the incorrect behavior of the metrics is not immediately apparent.",Recall and precision not working correctly(keras),"I have to build a model in keras. I am really struggling with my actual dataset, hence I am just trying to figure out the basics on a simpler dataset.
model = Sequential([
    Dense(32, input_dim=X_train.shape[1], activation=""sigmoid""),
    Dense(2, activation=""softmax""),
])
metrics=[ 
    tf.keras.metrics.TruePositives(name=""tp""),
    tf.keras.metrics.TrueNegatives(name=""tn""),
    tf.keras.metrics.FalseNegatives(name=""fn""),
    tf.keras.metrics.FalsePositives(name=""fp""),
    tf.keras.metrics.Recall(name=""recall""),
    tf.keras.metrics.Precision(name=""precision"")
]

model.compile(loss=""categorical_crossentropy"", metrics=metrics, optimizer=""sgd"")
model.evaluate(X_test, y_test)
evaluation = model.evaluate(X_test, y_test)
for i, m in model.metrics_names:
    print(m, evaluation[i])

This gets printed out:
loss 0.4604386021425058
tp 2965.5
tn 2965.5
fn 531.25
fp 531.25
recall 0.8480753898620605
precision 0.8480753898620605

Something really strange about this results. I believe it is due to using the softmax with two nodes.
y_train looks something like this:
array([[1., 0.],
       [1., 0.],
       [1., 0.],

       [1., 0.]], dtype=float32)

I tried a sigmoid, but then the whole model breaks down, at least here the fitting works.
Is there a way to configure recall and precision so they consider one output notes as Positive?
","The only solution in your case is to transform the problem into a one-dimensional one, i.e.

Use Dense(1,activation='sigmoid') instead of Dense(2,activation='softmax'); change [0,1] to 0 and [1,0] to 1 as an example.
Use binary_crossentropy instead of categorical_crossentropy.

Otherwise, you can implement a special callback to retrieve those metrics (using scikit-learn, like in the example below):
How to get other metrics in Tensorflow 2.0 (not only accuracy)?
",,,false,,,
https://stackoverflow.com/questions/66245520,true,"The issue involves the training of a Keras model on the IMDB dataset, where the model's behavior is not as expected. The issue is likely related to the model architecture, loss function, or other configuration settings rather than a specific API failure.",Keras,model.fit,"The Keras model trained on the IMDB dataset is not producing the desired results. The loss quickly plateaus, and the model outputs the same value for every input. This behavior is likely due to a problem with the model architecture, loss function, or other configuration settings.",The model.fit function is typically used to train a Keras model on a given dataset.,The issue is triggered when training the Keras model on the IMDB dataset and observing the loss plateau and constant output values.,"This issue might be challenging to detect during development and testing, as it involves multiple factors such as model architecture, loss function, and dataset characteristics.",training on imdb dataset with tensorflow2/keras give strange result,"I'm new to tensorflow2/keras. I was following this tutorial on tensorflow website. Instead of downloading the text data to directory, I use tensorflow_datasets to load imdb dataset directly to tensors/numpy arrays. Below is my code.
import os
import re
import string
import pandas as pd
import numpy as np
import tensorflow as tf
import tensorflow_hub as hub
import tensorflow_datasets as tfds

import matplotlib.pyplot as plt
from tensorflow.keras import layers
from tensorflow.keras.layers.experimental.preprocessing import TextVectorization

print(""Version: "", tf.__version__)
print(""Eager mode: "", tf.executing_eagerly())
print(""Hub version: "", hub.__version__)
print(""GPU is"", ""available"" if tf.config.list_physical_devices('GPU') else ""NOT AVAILABLE"")

train_data, test_data = tfds.load(name=""imdb_reviews"", split=[""train"", ""test""], 
                                  batch_size=-1, as_supervised=True)

X_train, y_train = tfds.as_numpy(train_data)
X_test, y_test = tfds.as_numpy(test_data)

# process text
def custom_standardization(input_data):
  lowercase = tf.strings.lower(input_data)
  stripped_html = tf.strings.regex_replace(lowercase, '&lt;br /&gt;', ' ')
  return tf.strings.regex_replace(stripped_html,
                                  '[%s]' % re.escape(string.punctuation),
                                  '')
max_features = 1000
sequence_length = 50

vectorize_layer = TextVectorization(
    standardize=custom_standardization,
    max_tokens=max_features,
    output_mode='int',
    output_sequence_length=sequence_length)
# Make a text-only dataset (without labels), then call adapt
vectorize_layer.adapt(X_train)

def vectorize_text(text):
  text = tf.expand_dims(text, -1)
  return vectorize_layer(text)

#check data
first_review, first_label = X_train[0], y_train[0]
print(""Review"", first_review)
print(""Vectorized review"", vectorize_text(first_review))
print(""11 ---&gt; "",vectorize_layer.get_vocabulary()[11])
print("" 44 ---&gt; "",vectorize_layer.get_vocabulary()[44])
print('Vocabulary size: {}'.format(len(vectorize_layer.get_vocabulary())))


# vectorize both train and test text data
X_train = vectorize_text(X_train)
X_test = vectorize_text(X_test)


embedding_dim = 16
#define and compile model
model = tf.keras.Sequential([
  layers.Embedding(max_features + 1, embedding_dim),
  layers.GlobalAveragePooling1D(),
  layers.Dropout(0.2),
  layers.Dense(256, activation='relu'),
  layers.Dropout(0.2),
  layers.Dense(1)])
model.summary()
model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
# fit the model
history = model.fit(X_train, y_train, epochs=5, batch_size=32, verbose=2, validation_data=(X_test, y_test))

The output I got is like below:
_________________________________________________________________
Epoch 1/5
782/782 - 4s - loss: 0.0000e+00 - accuracy: 0.5000 - val_loss: 0.0000e+00 - val_accuracy: 0.5000
Epoch 2/5
782/782 - 4s - loss: 0.0000e+00 - accuracy: 0.5000 - val_loss: 0.0000e+00 - val_accuracy: 0.5000
Epoch 3/5
782/782 - 4s - loss: 0.0000e+00 - accuracy: 0.5000 - val_loss: 0.0000e+00 - val_accuracy: 0.5000
Epoch 4/5
782/782 - 4s - loss: 0.0000e+00 - accuracy: 0.5000 - val_loss: 0.0000e+00 - val_accuracy: 0.5000
Epoch 5/5
782/782 - 4s - loss: 0.0000e+00 - accuracy: 0.5000 - val_loss: 0.0000e+00 - val_accuracy: 0.5000

Accuracy are all 50%! Something went wrong.  I'm confused why that happened? I was following the tutorial, train the embedding layer from scratch.  Spent hours trying to figuring out why.
Does anyone know why went wrong? thanks!
","Your dense layer is linear which means you don't have any activation passed into it. There are several solutions, as you are doing binary_classification.
If you want to use Dense(1) as it is, then you should change loss function into:
model.compile(optimizer='adam', loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),
              metrics=['accuracy'])

Or you can use Dense(1, activation = tf.nn.sigmoid), then your loss function should be:
model.compile(optimizer='adam', loss='binary_crossentropy',
              metrics=['accuracy'])

Or another solution if your labels are one hot encoded, you can set Dense(2, activation = tf.nn.softmax) then loss function can be:
model.compile(optimizer='adam', loss='categorical_crossentropy',
              metrics=['accuracy'])

I reproduced the exact code of yours with changings:
  layers.Dense(1, activation = tf.nn.sigmoid)])
  model.compile(optimizer='adam', loss='binary_crossentropy', metrics= 
                                   ['accuracy'])

Got:
782/782 [==============================] - 4s 5ms/step - loss: 0.4836 - accuracy: 0.7675 - val_loss: 0.5065 - val_accuracy: 0.7447

Edit 2: sigmoid squeezes your output into a range of [0,1]. For example to get predictions you need all x_test. X_test[0] will mess up the predictions as it only contains the first sentence. Get predictions by using:
y_hat = model.predict(X_test)
y_hat

array([[0.8105568 ],
       [0.6332975 ],
       [0.20526059],
       ...,
       [0.03132877],
       [0.5318063 ],
       [0.8626927 ]], dtype=float32)

Now set a threshold and convert them into 0 and 1s. This can be done with:
y_pred = [1 * (x[0]&gt;=0.5) for x in y_hat]

y_pred will be containing zeros and ones. Note that threshold is 0.5 here which means if output from sigmoid is bigger than 0.5, than it belongs to second class.
",,,false,,,
https://stackoverflow.com/questions/38991327,true,"The issue involves the initialization of weights and biases in a CNN for image classification. The behavior of the network is not as expected, and the model takes a long time to start producing meaningful results. The issue is likely related to the weight and bias initialization methods rather than a specific API failure.",TensorFlow,tf.Variable,"The CNN model for image classification takes a long time to produce meaningful results and exhibits strange behavior. This is likely due to the initialization of weights and biases, which may not be suitable for the specific problem and network architecture.","The tf.Variable API is used to create trainable variables, such as weights and biases, in TensorFlow models.",The issue is triggered when training the CNN model for image classification and observing slow convergence and strange behavior.,"This issue might be challenging to detect during development and testing, as it involves understanding the impact of weight and bias initialization methods on the training process and network behavior.",Problems with initialisation of weight and biases?,"I use a CNN to do classification (6 classes) of 32x32bit images.

The network is behaving very strangely as it takes 900 steps to do anything different from these results pasted. Afterwards it starts to move to reach reasonable values after several 1000 steps. The ""same network"" done in Theano/Keras with the same data is showing results much faster. I say ""same network"" this because Theano/Keras let me define leakyRELU and some other initialisation methods on variables (init = ""orthogonal"") that let me believe my problem is with initialisation. But I could be far off ;-)

    step        -&gt;  900
    Minibatch loss at step 900: 1.440395
    Minibatch accuracy: 43.0%
    Validation accuracy: 35.5%
    Val F-score: 0.0872979214781    
    confusion matrix 
 [[  0   0   0   0   0   0]
 [  0   0   0   0   0   0]
 [  0   0   0   0   0   0]
 [  0   0   0   0   0   0] 
 [ 21 292 224 398 567  96]
 [  0   0   0   0   0   0]]


my optimizer is as

tf.train.GradientDescentOptimizer(0.1).minimize(loss)


my weights and bias initialisation is (only layer 1 of 6 is shown, but identical)

wc1 = tf.Variable(tf.truncated_normal([2, 2, 1, 16], stddev=0.05), name=""weights_1"")
bc1 = tf.Variable(tf.zeros([16]), name=""bias_1"")


can anybody give me hint towards: 


why it takes 900 steps to get the network to ""do something""
why is the confusion matrix giving me this uniform prediction in the beginning


thanks a lot
Peter
","If you use relu activation function, look at ""kaiming initialisation"" for your weight. The objective is to keep a mean of 0 and a standard deviation of 1 for your output after each layer during the forward pass. 

For relu activation fuction you have to initialize with random normal distribution multiplied by the square root of 2/(number of input for the given layer).

weight_initialisation = random_normal * sqrt(2/(number of input for the layer))


For CNN, I think the number of input will be number of filter * number of cell in the kernel (or 5 * 5 for a [5, 5] kernel)
",,,false,,,
https://stackoverflow.com/questions/69600915,true,"The issue involves the usage of the Keras model.fit() function, where an IndexError occurs during the training process. The issue is likely related to the shape or format of the input data rather than a specific API failure.",Keras,model.fit,"During the training process using the model.fit() function, an IndexError occurs, indicating that the list index is out of range. This issue is likely caused by a problem with the shape or format of the input data.",The model.fit() function is typically used to train a Keras model on a given dataset.,The issue is triggered when using the model.fit() function and encountering an IndexError related to list index out of range.,"This issue might be challenging to detect during development and testing, as it requires careful examination of the input data and understanding of the model's expected input shape.",Keras model.fit() IndexError: list index out of range,"I need some help, I keep getting this strange situation where my Keras model goes out of range
print(np.array(train_x).shape)
print(np.array(train_y).shape)

Returns:
(731, 42)
(731,)

Then:
normalizer = Normalization(input_shape=[42,], axis=None)
normalizer.adapt(train_x[0])

linear_model = Sequential([
    normalizer,
    Dense(units=1)
])
linear_model.summary()

Shows:
Model: ""sequential_1""
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
normalization_5 (Normalizati (None, 42)                3         
_________________________________________________________________
dense_1 (Dense)              (None, 1)                 43        
=================================================================
Total params: 46
Trainable params: 43
Non-trainable params: 3
_________________________________________________________________

Then:
linear_model.compile(
    optimizer=tf.optimizers.Adam(learning_rate=0.1),
    loss='mean_absolute_error')

linear_model.fit(
    train_x,
    train_y,
    epochs=100)

Which results in an IndexError: list index out of range. It looks like my inputs are in the right shape. Any idea what could be causing this?
","train_x and train_y needed to be numpy arrays.
",,,false,,,
https://stackoverflow.com/questions/62944015,true,"The issue involves a Keras model where the output is constant and does not change during training. This behavior is likely due to a problem with the model architecture, activation functions, or other configuration settings rather than a specific API failure.",Keras,model.fit,"The Keras model outputs a constant value for every input during training, indicating a problem with the model architecture, activation functions, or other configuration settings. This behavior prevents the model from learning and producing meaningful results.",The model.fit function is typically used to train a Keras model on a given dataset.,The issue is triggered when training the Keras model and observing a constant output value for every input.,"This issue might be challenging to detect during development and testing, as it requires careful examination of the model architecture, activation functions, and other configuration settings.",Tensorflow/Keras model output is constant,"I am trying to train a CNN in using keras. The input is a 128x128x3 rbg image and output is a single value between 0 and 1 (this is not a classifier model). I have normalised the input. Initially, my model was achieving some reasonable results, getting the mean absolute error to &lt; 0.1. As I tried to tweak the model slightly I found the loss would plateau very quickly to around 0.23. I investigated further and found that it was outputting the same value for every input.
So I reverted my code back to when it was working, but it was no longer working. I eventually found that about 90% of the time it will get stuck at this local minima, outputting a constant value (which I suspect is mean of the training reference values (0.39). The other 10% of the time it will behave nicely and regress down to an error of &lt; 0.1. So it is basically giving qualitatively different behaviour randomly and desired results rarely. The strange thing is, is that I swear it was consistently working before.
I have tried:

Changing the input size
Increasing/decreasing the learning rate by factor of 10
Removing a couple of dense layers
Changing 'relu' to 'leaky relu'
Increasing/removing dropout

def load_data(dir):
    csv_data = get_csv_data()
    xs = []
    ys = []
    for (name, y) in csv_data:
        path = DIR + dir + ""/"" + name
        img = tf.keras.preprocessing.image.load_img(path)
        xs.append(tf.keras.preprocessing.image.img_to_array(img) * (1 / 255.0))
        ys.append(normalize_output(float(y)))
    return np.array(xs).reshape(len(csv_data), IMAGE_DIM, IMAGE_DIM, 3), np.array(ys).reshape(len(csv_data), 1)

def gen_model():
    model = tf.keras.Sequential()
    model.add(tf.keras.layers.Conv2D(filters=64, kernel_size = (5, 5), activation='relu', input_shape=(IMAGE_DIM, IMAGE_DIM, CHAN_COUNT)))
    model.add(tf.keras.layers.MaxPool2D())
    model.add(tf.keras.layers.Conv2D(filters=64, kernel_size = (5, 5), activation='relu'))
    model.add(tf.keras.layers.MaxPool2D())
    model.add(tf.keras.layers.Conv2D(filters=128, kernel_size = (5, 5), activation='relu'))
    model.add(tf.keras.layers.MaxPool2D())
    model.add(tf.keras.layers.Flatten())
    model.add(tf.keras.layers.Dense(256, activation='relu'))
    model.add(tf.keras.layers.Dropout(0.1))
    model.add(tf.keras.layers.Dense(128, activation='relu'))
    model.add(tf.keras.layers.Dropout(0.1))
    model.add(tf.keras.layers.Dense(64, activation='relu'))
    model.add(tf.keras.layers.Dropout(0.1))
    model.add(tf.keras.layers.LeakyReLU())
    model.add(tf.keras.layers.Dense(16, activation='sigmoid'))
    model.add(tf.keras.layers.LeakyReLU())
    model.add(tf.keras.layers.Dense(1, activation='sigmoid'))
    model.compile(loss=keras.losses.MeanSquaredError(),
                  optimizer=tf.keras.optimizers.Adam(),
                  metrics=[keras.metrics.MeanAbsoluteError()])
    return model

def run():
    model = gen_model()

    xs, ys = load_data(""output"")
   
    generator = tf.keras.preprocessing.image.ImageDataGenerator(featurewise_center=False,
                                                                samplewise_center=False,
                                                                featurewise_std_normalization=False,
                                                                samplewise_std_normalization=False,
                                                                validation_split=0.1,
                                                                rotation_range=12,
                                                                horizontal_flip=True,
                                                                vertical_flip=True)

    model.fit(generator.flow(xs, ys, batch_size=32, shuffle=True),
              steps_per_epoch=len(xs) / 32,
              epochs = 10,
              use_multiprocessing=False)

","I rearranged activation on the layers. Please give it a try :
def gen_model():
   model = tf.keras.Sequential()
   model.add(tf.keras.layers.Conv2D(filters=64, kernel_size = (5, 5), activation='relu', input_shape=(IMAGE_DIM, IMAGE_DIM, CHAN_COUNT)))
   model.add(tf.keras.layers.MaxPool2D())
   model.add(tf.keras.layers.Conv2D(filters=64, kernel_size = (5, 5), activation='relu'))
   model.add(tf.keras.layers.MaxPool2D())
   model.add(tf.keras.layers.Conv2D(filters=128, kernel_size = (5, 5), activation='relu'))
   model.add(tf.keras.layers.MaxPool2D())
   model.add(tf.keras.layers.Flatten())
   model.add(tf.keras.layers.Dense(256, activation='relu'))
   model.add(tf.keras.layers.Dropout(0.1))
   model.add(tf.keras.layers.Dense(128, activation='relu'))
   model.add(tf.keras.layers.Dropout(0.1))
   model.add(tf.keras.layers.Dense(64, activation='relu'))
   model.add(tf.keras.layers.Dropout(0.1))
   model.add(tf.keras.layers.Dense(16, activation='relu'))
   model.add(tf.keras.layers.Dense(1, activation='sigmoid'))
   model.compile(loss=keras.losses.MeanSquaredError(),
              optimizer=tf.keras.optimizers.Adam(),
              metrics=[keras.metrics.MeanAbsoluteError()])
   return model

",,,false,,,
https://stackoverflow.com/questions/60931790,false,The issue does not meet the criteria for deeper analysis as it does not involve an API exhibiting unexpected failures or unpredictable behaviors under specific runtime conditions.,,,,,,,Big difference between val-acc and prediction accuracy in Keras Neural Network,"I have a dataset that I used for making NN model in Keras, i took 2000 rows from that dataset to have them as validation data, those 2000 rows should be added in .predict function.

I wrote a code for Keras NN and for now it works good, but I noticed something that is very strange for me. It gives me very good accuracy of more than 83%, loss is around 0.12, but when I want to make a prediction with unseen data (those 2000 rows), it only predicts correct in average of 65%.
When I add Dropout layer, it only decreases accuracy.

Then I have added EarlyStopping, and it gave me accuracy around 86%, loss is around 0.10, but still when I make prediction with unseen data, I get final prediction accuracy of 67%.

Does this mean that model made correct prediction in 87% of situations? Im going with a logic, if I add 100 samples in my .predict function, that program should make good prediction for 87/100 samples, or somewhere in that range (lets say more than 80)? I have tried to add 100, 500, 1000, 1500 and 2000 samples in my .predict function, and it always make correct prediction in 65-68% of the samples.

Why is that, am I doing something wrong?
I have tried to play with number of layers, number of nodes, with different activation functions and with different optimizers but it only changes the results by 1-2%.
My dataset looks like this:

DataFrame shape (59249, 33)
x_train shape (47399, 32)
y_train shape (47399,)
x_test shape (11850, 32)
y_test shape (11850,)
testing_features shape (1000, 32)


This is my NN model:

model = Sequential()
model.add(Dense(64, input_dim = x_train.shape[1], activation = 'relu')) # input layer requires input_dim param
model.add(Dropout(0.2))
model.add(Dense(32, activation = 'relu'))
model.add(Dropout(0.2))
model.add(Dense(16, activation = 'relu'))
model.add(Dense(1, activation='sigmoid')) # sigmoid instead of relu for final probability between 0 and 1

# compile the model, adam gradient descent (optimized)
model.compile(loss=""binary_crossentropy"", optimizer= ""adam"", metrics=['accuracy'])


# call the function to fit to the data training the network)
es = EarlyStopping(monitor='val_loss', min_delta=0.0, patience=1, verbose=0, mode='auto')
model.fit(x_train, y_train, epochs = 15, shuffle = True, batch_size=32, validation_data=(x_test, y_test), verbose=2, callbacks=[es])

scores = model.evaluate(x_test, y_test)
print(model.metrics_names[0], round(scores[0]*100,2), model.metrics_names[1], round(scores[1]*100,2))


These are the results:

Train on 47399 samples, validate on 11850 samples
Epoch 1/15
 - 25s - loss: 0.3648 - acc: 0.8451 - val_loss: 0.2825 - val_acc: 0.8756
Epoch 2/15
 - 9s - loss: 0.2949 - acc: 0.8689 - val_loss: 0.2566 - val_acc: 0.8797
Epoch 3/15
 - 9s - loss: 0.2741 - acc: 0.8773 - val_loss: 0.2468 - val_acc: 0.8849
Epoch 4/15
 - 9s - loss: 0.2626 - acc: 0.8816 - val_loss: 0.2416 - val_acc: 0.8845
Epoch 5/15
 - 10s - loss: 0.2566 - acc: 0.8827 - val_loss: 0.2401 - val_acc: 0.8867
Epoch 6/15
 - 8s - loss: 0.2503 - acc: 0.8858 - val_loss: 0.2364 - val_acc: 0.8893
Epoch 7/15
 - 9s - loss: 0.2480 - acc: 0.8873 - val_loss: 0.2321 - val_acc: 0.8895
Epoch 8/15
 - 9s - loss: 0.2450 - acc: 0.8886 - val_loss: 0.2357 - val_acc: 0.8888
11850/11850 [==============================] - 2s 173us/step
loss 23.57 acc 88.88


And this is for prediction:

#testing_features are 2000 rows that i extracted from dataset (these samples are not used in training, this is separate dataset thats imported)

prediction = model.predict(testing_features , batch_size=32)

res = []
for p in prediction:
    res.append(p[0].round(0))


# Accuracy with sklearn - also much lower 
acc_score = accuracy_score(testing_results, res)
print(""Sklearn acc"", acc_score)    

result_df = pd.DataFrame({""label"":testing_results,
                          ""prediction"":res})


result_df[""prediction""] = result_df[""prediction""].astype(int)

s = 0
for x,y in zip(result_df[""label""], result_df[""prediction""]):
    if x == y:
        s+=1

print(s,""/"",len(result_df))
acc = s*100/len(result_df)
print('TOTAL ACC:', round(acc,2))


The problem is...now I get accuracy with sklearn 52% and my_acc 52%.
Why do I get such low accuracy on validation, when it says that its much larger?
","The training data you posted gives high validation accuracy, so I'm a bit confused as to where you get that 65% from, but in general when your model performs much better on training data than on unseen data, that means you're over fitting. This is a big and recurring problem in machine learning, and there is no method guaranteed to prevent this, but there are a couple of things you can try:


regularizing the weights of your network, e.g. using l2 regularization
using stochastic regularization techniques such as drop-out during training
early stopping
reducing model complexity (but you say you've already tried this)

","I will list the problems/recommendations that I see on your model.


What are you trying to predict? You are using sigmoid activation function in the last layer which seems it is a binary classification but in your loss fuction you used mse which seems strange. You can try binary_crossentropy instead of mse loss function for your model.
Your model seems suffer from overfitting so you can increase the prob. of Dropout and also add new Dropout between other hidden layers or you can remove one of the hidden layers because it seem your model is too complex. 
You can change your neuron numbers in layers like a narrower =&gt; 64 -&gt; 32 -&gt; 16 -&gt; 1 or try different NN architectures. 
Try adam optimizer instead of sgd.
If you have 57849 sample you can use 47000 samples in training+validation and rest of will be your test set. 
Don't use the same sets for your evaluation and validation. First split your data into train and test set. Then when you are fitting your model give validation_split_ratio then it will automatically give validation set from your training set.

",,false,,,
https://stackoverflow.com/questions/59149885,false,The issue does not meet the criteria for deeper analysis as it does not involve an API exhibiting unexpected failures or unpredictable behaviors under specific runtime conditions.,,,,,,,How to calculate the trace of a matrix in Keras backend?,"I am implementing a custom loss function in Keras, and I need to perform a matrix multiplication and then get the trace of the resulting matrix. I could go one of two ways, both of which are giving me trouble in Keras:

Option 1: Iterate over the rows ( and columns ) of the 2 matrices. They both have the same shape, and sum the dot products of the corresponding rows and columns:

# Define custom loss
def custom_loss(y_true,y_pred):
    batch_size = y_true.shape[0]
    X = K.log( K.dot(y_true, K.transpose(y_pred) ) )
    y = 0
    for i in range(0,int(batch_size)):
        y -= X[i,i]
    return y


Error:   


  File ""neural_net.py"", line 71, in custom_loss
      for i in range(0,int(batch_size)): TypeError: int returned non-int (type NoneType)


Strange that the shape of the matrix is returning none.

Option 2: Just do the matrix multiplication and get the trace of the matrix:

# Define custom loss
def custom_loss(y_true,y_pred):
    X = K.log( K.dot(y_true, K.transpose(y_pred) ) )
    return -np.trace(X)


Error: 


  return asanyarray(a).trace(offset=offset, axis1=axis1, axis2=axis2,
  dtype=dtype, out=out) ValueError: diag requires an array of at least
  two dimensions


I am fairly certain that X is a 2-d matrix. My batch size is 32 and there are 47 nodes in the output layer. I've seen output showing that both the matrices are 32 x 47, so the dot product is definitely 2-D. But i was reading that there are some issues about accessing the size of the matrices and using numpy operations inside the keras backend, so I was hoping to find a trace function in the keras backend but was unable to find one.

Does anyone know how to either solve option 1 iteration issue or a way to calculate the trace in the Keras backend?

Thanks!
","I would probably use - appropriate Layer
tf.keras.layers.Lambda(
    function, output_shape=None, mask=None, arguments=None, **kwargs
)

with function as callback (to use in it numpy.trace())... Can not test now...
I also can suppose, that you can try Keras Models With TensorFlow NumPy
","Just wondering, wouldn't this work? 

def custom_loss(y_true,y_pred):
    batch_size = y_true.shape[0]
    X = K.log( K.dot(y_true, K.transpose(y_pred) ) )
    y = - tf.trace(X)
    return y

",,false,,,
https://stackoverflow.com/questions/53920322,false,The issue does not meet the criteria for deeper analysis as it does not involve an API exhibiting unexpected failures or unpredictable behaviors under specific runtime conditions.,,,,,,,Keras model evaluation accuracy on training data set is lower than fitting accuracy,"I get some strange result on evaluating my model with the training data set.
I want to develop a cnn with such structure:

Input --&gt; conv1d --&gt; MaxPool1D --&gt; Flatten --&gt; Dense --&gt; Dense


This is my model:

model = Model(inputs=[inputLayerU, inputLayerM], outputs=outputLayer)
model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=0.01), metrics=['accuracy'])    
model.fit([inputU, inputM],outputY , epochs=100, steps_per_epoch=500)


and this is the result of training the model:

Epoch 95/100
500/500 [==============================] - 329s 659ms/step - loss: 0.5058 - acc: 0.8845
Epoch 96/100
500/500 [==============================] - 329s 659ms/step - loss: 0.4137 - acc: 0.9259
Epoch 97/100
500/500 [==============================] - 329s 659ms/step - loss: 0.3221 - acc: 0.9534
Epoch 98/100
500/500 [==============================] - 329s 659ms/step - loss: 0.2938 - acc: 0.9596
Epoch 99/100
500/500 [==============================] - 330s 659ms/step - loss: 0.4707 - acc: 0.9352
Epoch 100/100
500/500 [==============================] - 329s 659ms/step - loss: 0.4324 - acc: 0.9543


I save the model and weights and then load them and evaluate the model using the same training data set:

loaded_model = model_from_json(loaded_model_json)
loaded_model.load_weights(""GCN-conv1d-acc.h5"")
loaded_model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=0.01), metrics=['accuracy'])
score = loaded_model.evaluate(inputTrain,outTrain,steps=500, verbose=0)


However I get this result:

[7.320816993713379, 0.3042338788509369]


I expected to get some results close to the fitting result but it's too far away.
I checked these postes:

Strange behaviour of the loss function in keras model, with pretrained convolutional base

http://blog.datumbox.com/the-batch-normalization-layer-of-keras-is-broken/

They say Keras has some problems with batch normalization and dropout layers, however I don't use neither of them.
","As far as i know, Adam is the the variation of SGD, which does not take into account the entire data points for model fitting, and pick a batch(subset of rows) for the training,

So this could lead to inconsistency, when you first train model and save weights, 
The next time you run the algo using same weights, the subset on which the weights are being applied could have changed
",,,false,,,
https://stackoverflow.com/questions/53906309,false,The issue does not meet the criteria for deeper analysis as it does not involve an API exhibiting unexpected failures or unpredictable behaviors under specific runtime conditions.,,,,,,,Different between fit and evaluate in keras,"I have used 100000 samples to train a general model in Keras and achieve good performance. Then, for a particular sample, I want to use the trained weights as initialization and continue to optimize the weights to further optimize the loss of the particular sample.

However, the problem occurred. First, I load the trained weight by the keras API easily, then, I evaluate the loss of the one particular sample, and the loss is close to the loss of the validation loss during the training of the model. I think it is normal. However, when I use the trained weight as the inital and further optimize the weight over the one sample by model.fit(), the loss is really strange. It is much higher than the evaluate result and gradually became normal after several epochs. 

I think it is strange that, for the same one simple and loading the same model weight, why the model.fit() and model.evaluate() return different results. I used batch normalization layers in my model and I wonder that it may be the reason. The result of model.evaluate() seems normal, as it is close to what I seen in the validation set before. 

So what cause the different between fit and evaluation? How can I solve it? 
",,,,false,,,
https://stackoverflow.com/questions/50701786,false,The issue does not meet the criteria for deeper analysis as it does not involve an API exhibiting unexpected failures or unpredictable behaviors under specific runtime conditions.,,,,,,,Accuracy lower than AUC for identical Keras models &amp; data,"I have a strange Keras classification behaviour.

I got different accuracy when using cross-validation vs a holdout set.

2 Identical models but with different evaluation methods:


Model1 uses 10-Kfold cross validation (achieving 0.98 mean AUC and lowest AUC 0.89).
Model2 uses hold out set (Accuracy 0.82)


I was expecting to see the worst accuracy of model 2 to be the lowest fold accuracy (0.89 not 0.82).

Data of a small size ~10k x 13

Kfold: 10 folds

Model 1:

def create_baseline():
    # create model
    model = models.Sequential()
    model.add(layers.Dense(64, input_dim=set_1.iloc[:,0:-1].shape[1], activation='relu'))
    model.add(layers.Dense(64, activation='relu'))
    model.add(layers.Dense(1, activation='sigmoid'))
    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
    return model


This the important part of my code (the rest is related to ploting the ROC):

Note: I have tried both with and without standardization

estimators = []
estimators.append(('standardize', MinMaxScaler()))
estimators.append(('mlp', KerasClassifier(build_fn=create_baseline, nb_epoch=1000, batch_size=1000, verbose=0)))
pipeline = Pipeline(estimators)
cv = StratifiedKFold(n_splits=10)
classifier = pipeline
mean_tpr = 0.0
mean_fpr = np.linspace(0, 1, 100)

colors = cycle(['cyan', 'indigo', 'seagreen', 'yellow', 'blue', 'darkorange'])
lw = 2
i = 0
for (train, test), color in zip(cv.split(X, y), colors):
    classifier.fit(X[train], y[train])
    probas_ = classifier.predict_proba(X[test])
    fpr, tpr, thresholds = roc_curve(y[test], probas_[:, 1])
    mean_tpr += interp(mean_fpr, fpr, tpr)
    mean_tpr[0] = 0.0
    roc_auc = auc(fpr, tpr)
    plt.plot(fpr, tpr, lw=lw, color=color,
             label='ROC fold %d (area = %0.2f)' % (i, roc_auc))

    i += 1


Output:


As you can see, I have .98 averag ROC.

Issue:

Model 2:

std = MinMaxScaler()
X_norm = std.fit_transform(X)
X_train_norm, X_test_norm, y_train_norm, y_test_norm = train_test_split(X_norm, y, test_size=0.1, random_state=5)


Keras model

model_2 = models.Sequential()
model_2.add(layers.Dense(64, activation='relu', input_shape=(X_train.shape[1],)))
model_2.add(layers.Dense(64, activation='relu'))
model_2.add(layers.Dense(1, activation='sigmoid'))
model_2.compile(optimizer='adam',
loss='binary_crossentropy',
metrics=['accuracy'])


Runing the model:

history = model_2.fit(X_train_norm,
y_train_norm,
epochs=1000,
batch_size=1000,
validation_data=(X_test_norm, y_test_norm))


Results (last iterations):

8988/8988 [==============================] - 0s - loss: 0.3517 - acc: 0.8249 - val_loss: 0.3701 - val_acc: 0.7954
Epoch 997/1000
8988/8988 [==============================] - 0s - loss: 0.3516 - acc: 0.8238 - val_loss: 0.3699 - val_acc: 0.8059
Epoch 998/1000
8988/8988 [==============================] - 0s - loss: 0.3516 - acc: 0.8250 - val_loss: 0.3694 - val_acc: 0.8038
Epoch 999/1000
8988/8988 [==============================] - 0s - loss: 0.3512 - acc: 0.8241 - val_loss: 0.3692 - val_acc: 0.7975
Epoch 1000/1000
8988/8988 [==============================] - 0s - loss: 0.3504 - acc: 0.8247 - val_loss: 0.3696 - val_acc: 0.7975


​Why the performance of model2 is lower than model1?

Note: 
- same data, type of keras model, and seed but different results!
- I did multiple test with and without standarization and with same and differents seeds, and I still have the same issue.
- I understant that I can use simpler models but my issue is realted to using Keras classifier.

Please correct me if I am doing something wrong.
",,,,false,,,
https://stackoverflow.com/questions/43291488,false,"The issue does not have a description or code snippets provided, making it difficult to determine the API or any unexpected behavior. Therefore, it does not meet the criteria for deeper analysis.",,,,,,,Strange colour cast in Python plt.imshow() image,,,,,false,,,
https://stackoverflow.com/questions/43322683,false,The issue does not meet the criteria for deeper analysis as it does not involve an API exhibiting unexpected failures or unpredictable behaviors under specific runtime conditions.,,,,,,,Python loop taking more time at each iteration,"I made a for loop which strangely increases in duration at each iteration although the amount of variables manipulated remains constant. The code is below with:


X [N*F]: a numpy array with N samples containing F variables
(features);
parts [N]: a numpy array containing the number of the
participant of each sample in X;
model_filename: the model template file
name for each participant (i.e. I have a model per participant)


My goal is to apply the model of participant p to the data of participant p and and to save its output (i.e. N outputs).

outputs = np.full((X.shape[0],), np.nan)
for curr_part in np.unique(parts):
    print(""processing participant {0}"".format(curr_part))
    model = load_model(model_filename.format(curr_part)) # I measured the duration of this call (d0)
    idx = (parts == curr_part)
    outputs[idx] = np.squeeze(model.predict(X[idx,:])); # I measured the duration of this call (d1)


Both d1 and d0 increase at each iteration of the loop (the whole loop take 1.5 seconds at iteration 0 and around 8 seconds at iteration 20). I completely fail to understand why. Also interestingly, if I run the code several times in ipython the duration accumulate as long as I do not restart the kernel (i.e. on the second run iteration 0 takes around 8 seconds). Of course I want to run the code several times so this issue is critical on the long run.

I also tried with the following code which takes approx. the same total duration although I cannot measure the time of each call:

unik_parts = np.unique(parts);
models = [(p, load_model(model_filename.format(p))) for p in unik_parts]
outputs = [np.squeeze(m.predict(X[parts == p,:])) for p,m in models]


Python version 2.7

Models are models from keras
","I've seen this quite a few times when preprocessing data; Typically, in my experience, the memory usage creeps up after each iteration with each following iteration slowing down slightly.
I find that the easiest way to solve this is to separate the tasks into different processes and then use an orchestration process to manage the program flow.
When each task is completed, the associated process is culled and your resources can continue to be allocated to the next task in the flow. This is most helpful for keeping long-running processes crisp.
You could structure the process in this way:
Parent Process
     |_ Pickle Input to Child Proc
     |_ Trigger Child Proc
            |_ Collect Input
            |_ Complete Task
            |_ Pickle Output
     |_ Collect Output



Parent Process -&gt; pickle input -&gt; Child Process
      ^                              |
      |                              |
      ----------------pickle output &lt;-

One of the things you can do to manage the task flow is to create an id and use it to create an empty file, then pass that id to the child process and, once the work is complete, delete it with the child process. This is a simple and convenient way for the parent process to know the child process is complete.
",,,false,,,
https://stackoverflow.com/questions/63464944,false,The issue does not meet the criteria for deeper analysis as it does not involve an API exhibiting unexpected failures or unpredictable behaviors under specific runtime conditions.,,,,,,,Keras loss and metrics values do not match with same function in each,"I am using keras with a custom loss function like below:
def custom_fn(y_true, y_pred):
   # changing y_true, y_pred values systematically
   return mean_absolute_percentage_error(y_true, y_pred)
   

Then I am calling model.compile(loss=custom_fn) and model.fit(X, y,..validation_data=(X_val, y_val)..)
Keras is then saving loss and val_loss in model history. As a sanity check, when the model finishes training, I am using model.predict(X_val) so I can calculate validation loss manually with my custom_fn using the trained model.
I am saving the model with the best epoch using this callback:
callbacks.append(ModelCheckpoint(path, save_best_only=True, monitor='val_loss', mode='min'))

so after calculating this, the validation loss should match keras' val_loss value of the best epoch. But this is not happening.
As another attempt to figure this issue out, I am also doing this:
    model.compile(loss=custom_fn, metrics=[custom_fn])

And to my surprise, val_loss and val_custom_fn do not match (neither loss or loss_custom_fn for that matter).
This is really strange, my custom_fn is essentially keras' built in mape with the y_true and y_pred slightly manipulated. what is going on here?
PS: the layers I am using are LSTM layers and a final Dense layer. But I think this information is not relevant to the problem. I am also using regularisation as hyperparameter but not dropout.
Update
Even removing custom_fn and using keras' built in mape as a loss function and metric like so:
model.compile(loss='mape', metrics=['mape'])

and for simplicity, removing ModelCheckpoint callback is having the same effect; val_loss and val_mape for each epoch are not equivalent. This is extremely strange to me. I am either missing something or there is a bug in Keras code..the former might be more realistic.
","This blog post suggests that keras adds any regularisation used in the training when calculating the validation loss. And obviously, when calculating the metric of choice no regularisation is applied. This is why it occurs with any loss function of choice as stated in the question.
This is something I could not find any documentation on from Keras. However, it seems to hold up since when I remove all regularisation hyperparameters, the val_loss and val_custom_fn match exactly in each epoch.
An easy workaround is to either use the custom_fn as a metric and save the best model based on the metric (val_custom_fn) than on the val_loss. Or else Loop through each epoch manually and calculate the correct val_loss manually after training each epoch. The latter seems to make more sense since there is no reason to include custom_fn both as a metric and as a loss function.
If anyone can find any evidence of this in the Keras documentation that would be helpful.
",,,false,,,
https://stackoverflow.com/questions/55935277,false,The issue does not meet the criteria for deeper analysis as it does not involve an API exhibiting unexpected failures or unpredictable behaviors under specific runtime conditions.,,,,,,,simpleRNN input/output shape,"I have defined a simpleRNN in keras with the following code :

# define RNN architecture
from keras.layers import Input
from keras.models import Model
from keras.layers import SimpleRNN
from keras.models import Sequential

model = Sequential()
model.add(SimpleRNN(units = 10,
                    return_sequences=False, 
                    unroll=True,
                    input_shape=(6, 2)))

model.compile(loss='mse',
              optimizer='rmsprop',
              metrics=['accuracy'])
model.summary()


then I feed it with input data having shape (batch_size, 6, 2) i.e. 6 timesteps each having two features. I therefore expect 6 simpleRNN cells.

When launching the training, I get the following error message : 

Error when checking target: expected simple_rnn_2 to have shape (10,) but got array with shape (1,)


and I don't understand why. 

The point of the RNN (my understanding) is to have its input fed by the previous RNN cell in case it is not the first RNN cell and the new timestep input.

So in this case, I expect the second RNN cell to be fed by the first RNN cell a vector of shape  (10,) since units = 10. How come that it gets a (1,) sized vector ?

What is strange is that as soon as I add a Dense layer in the model, this solves the issue. So the following architecture : 

# define RNN architecture
from keras.layers import Input
from keras.models import Model
from keras.layers import SimpleRNN, Dense
from keras.models import Sequential

model = Sequential()
model.add(SimpleRNN(units = 10,
                    return_sequences=False, 
                    unroll=False,
                    input_shape=(6, 2)))
model.add(Dense(1, activation='relu'))
model.compile(loss='mse',
              optimizer='rmsprop',
              metrics=['accuracy'])
model.summary()


does not throw an error. Any idea why ?
","Assuming you are actually training the model (you did not include that code), the problem is that you are feeding it target outputs of shape (1,) while the SimpleRNN expects input of shape (10,). You can look up the docs here: https://keras.io/layers/recurrent/

The docs clearly state that the output of the SimpleRNN is equal to units, which is 10. Each unit produces one output.

The second sample does work because you have added a Dense layer that reduces the output size to (1,). Now the model can accept your training target outputs and they are backpropped through the network.
",,,false,,,
https://stackoverflow.com/questions/50418332,false,The issue does not meet the criteria for deeper analysis as it does not involve an API exhibiting unexpected failures or unpredictable behaviors under specific runtime conditions.,,,,,,,Got OOM issue with google colab,"I am building a keras model to run some simple image recognition task. If i do everything in raw Keras, I don't hit OOM. However, strangely, when I do it through a mini framework I wrote, which is fairly simple and mainly so that I can keep track of the hyperparameters and setup I used, I hit OOM. Most of the executions should be the same as running the raw Keras. I am guessing somewhere I made some mistakes in my code. Note that this same mini framework had no issue running with CPU on my local laptop. I think I will need to debug. But before that, anyone has any general advice? 

Here's a few lines of the errors I got:

Epoch 1/50
2018-05-18 17:40:27.435366: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2018-05-18 17:40:27.435906: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 0 with properties: name: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235 pciBusID: 0000:00:04.0 totalMemory: 11.17GiB freeMemory: 504.38MiB
2018-05-18 17:40:27.435992: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devices: 0
2018-05-18 17:40:27.784517: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect StreamExecutor with strength 1 edge matrix:
2018-05-18 17:40:27.784675: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0 
2018-05-18 17:40:27.784724: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N 
2018-05-18 17:40:27.785072: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 243 MB memory) -&gt; physical GPU (device: 0, name: Tesla K80, pci bus id: 0000:00:04.0, compute capability: 3.7)
2018-05-18 17:40:38.569609: W tensorflow/core/common_runtime/bfc_allocator.cc:275] Allocator (GPU_0_bfc) ran out of memory trying to allocate 36.00MiB.  Current allocation summary follows.
2018-05-18 17:40:38.569702: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (256):   Total Chunks: 66, Chunks in use: 66. 16.5KiB allocated for chunks. 16.5KiB in use in bin. 2.3KiB client-requested in use in bin.
2018-05-18 17:40:38.569768: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (512):   Total Chunks: 10, Chunks in use: 10. 5.0KiB allocated for chunks. 5.0KiB in use in bin. 5.0KiB client- etc. etc

2018-05-18 17:40:38.573706: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at cwise_ops_common.cc:70 : Resource exhausted: OOM when allocating tensor with shape[18432,512] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc

",,,,false,,,
https://stackoverflow.com/questions/46222755,false,The issue does not meet the criteria for deeper analysis as it does not involve an API exhibiting unexpected failures or unpredictable behaviors under specific runtime conditions.,,,,,,,LSTM results seems to be shifted by one time period back,"With LSTM and Keras I built a simple time series prediction model which works like this:
Providing that I have the data (normalized and prepared for training) for some past 10 time periods, the model predicts the values of the proceeding two time periods.

My Data is shaped similar to this:

X = [[[1,2,3,4,5,6,7,8,9,10],[2,3,4,5,6,7,8,9,10,11],[3,4,5,6,7,8,9,10,11,12]]]

Y = [[11,12],[12,13],[14,15]]

And the model is composed of one LSTM layer with rnn_size number of nodes
as well as one dropout layer.

    model = Sequential()

    model.add(LSTM(rnn_size,
    batch_input_shape=(batch_size, X.shape[1], 
    X.shape[2]),stateful=True,dropout=dropout))

    model.add(Dropout(dropout)) 
    model.add(Dense(y.shape[1]))
    adam_optimizer = keras.optimizers.Adam(clipvalue=5)
    model.compile(loss='mean_squared_error', optimizer=adam_optimizer)
    history = model.fit(X, y, batch_size=batch_size, epochs=nb_epoch, 
                    verbose=2, validation_split=0.1,shuffle=False)
    #################################    
    # Predict
    result = scaler.inverse_transform(
              model.predict_on_batch(test_values[start_date:end_date]
              .values.reshape(1, 1, -1)))


The problem is that the predicted value for time period t+1 - as can be seen in the graph - seems to be somehow just a slightly different value from the one at the time period t.

Is this a (normal) behaviour?
If not, how can I overcome it?

I think I have a relatively good understanding of how LSTM work, however I could not wrap my head around this particular issue.

Edit 1:

I edited the code to use stateless network configurations and edited the shape as suggested by Daniel. However the problem persist.
Strangely also, the validation loss (MSE) is always lower than the training loss.

Edit 2:

Rest of the code added as per Marcin's request

actual value vs. predicted value for the test data set that was not shown to the model before

Validation Loss is less than Training Loss! ( small fluctuation is because of Dropout )
",,,,false,,,
https://stackoverflow.com/questions/42833585,false,The issue does not meet the criteria for deeper analysis as it does not involve an API exhibiting unexpected failures or unpredictable behaviors under specific runtime conditions.,,,,,,,keras 2 - fit_generator on Jupyter notebook broken?,"

https://github.com/fchollet/keras/issues/5818



I've updated to keras v2 yesterday. 

I adapted all my code from version 1 to the new API, following all the warnings I encountered.

However I'm having some very strange problems with fit_generator method of Model.

Using this toy example, wich worked totally fine in version 1:

from keras.models import Model
from keras.layers import Input, Dense, Flatten
from keras.optimizers import SGD
from keras.losses import categorical_crossentropy
from keras.preprocessing.image import ImageDataGenerator

gen = ImageDataGenerator()
train_batches = gen.flow_from_directory(""D:/GitHub/Kaggle/redux/train/"")

inp = Input(shape=(256,256,3))
l1 = Flatten()(inp)
out = Dense(2, activation=""softmax"")(l1)

model = Model(inp, out)

model.compile(loss=categorical_crossentropy, optimizer=SGD(lr=0.01))

model.fit_generator(train_batches, train_batches.samples // train_batches.batch_size)


The output in jupyter notebook is quite strange, printing a unknown symbol until the notebook crashes:

Epoch 1/1
 23/718 [..............................] - ETA: 522s - loss: 8.4146 


Running the code from the terminal don't print those strange symbols.
",,,,false,,,
https://stackoverflow.com/questions/40510703,false,The issue does not meet the criteria for deeper analysis as it does not involve an API exhibiting unexpected failures or unpredictable behaviors under specific runtime conditions.,,,,,,,Implement Siamese Network in Keras Issue,"I was trying to implement a Siamese Network with Keras, I have two input data,

X1 shape: (10000, 52)
X2 shape: (10000, 600)

Each sample in X1 is assumed similar to sample in X2. For example: X1[0] is similar to X2[0]. Thus, I used the ""cosine similarity"" when two inputs are merged. I was trying to use the following code:

def contrastive_loss(y_true, y_pred):
'''Contrastive loss from Hadsell-et-al.'06
http://yann.lecun.com/exdb/publis/pdf/hadsell-chopra-lecun-06.pdf
'''

margin = 1
return K.mean(y_true * K.square(y_pred) + (1 - y_true) * K.square(K.maximum(margin - y_pred, 0)))

left_branch = Sequential()
left_branch.add(Embedding(1000, 32, input_length=52))
left_branch.add(LSTM(64))
left_branch.add(Dense(256))

right_branch = Sequential()
right_branch.add(Embedding(1000, 32, input_length=600))
right_branch.add(LSTM(64))
right_branch.add(Dense(256))

merged = Merge([left_branch, right_branch], mode='cos',dot_axes=1)

final_model = Sequential()
final_model.add(merged)
final_model.summary()

x1 = np.random.randint(1000, size=(10000, 52))
x2 = np.random.randint(1000, size=(10000, 600))
y = np.ones((10000,),dtype=np.int)

final_model.compile(optimizer='adam', loss=contrastive_loss)

final_model.fit([x1, x2], y, nb_epoch=2, batch_size=32)

getoutput_t = K.function([merged.layers[0].layers[0].input], [merged.layers[0].layers[2].output])

getoutput_d = K.function([merged.layers[1].layers[0].input], [merged.layers[1].layers[2].output])

t1 = getoutput_t([x1])[0]

t2 = getoutput_d([x2])[0]


My issue is: after training, t1[0] was very similar to t1[1], but very different from t2[0]. I checked the output of t1 and t2, the generated output were strange:

t1:

array([[ -3.48182112e-01, 6.57344190e-03, -6.75882818e-03,
4.11706511e-03, -2.41146213e-03, -1.14028137e-02,
2.49683809e+00, -9.19755269e-03, -2.37327255e-02....]

[ -3.48182231e-01, 6.57369522e-03, -6.75880583e-03,
4.11719829e-03, -2.41167075e-03, -1.14026833e-02,
2.49683785e+00, -9.19752941e-03, -2.37324722e-02,...]
.......]])

The t1[0] was almost same to t1[1], same issue also was found in t2.

Thus, I have to concerns:

Is there any issue in my Siamese model? (e.g. loss function or layer design)

I don't have negative sampling so far, thus, the target values are ""1"", is this the issue?
",,,,false,,,
https://stackoverflow.com/questions/29985453,false,The issue does not meet the criteria for deeper analysis as it does not involve an API exhibiting unexpected failures or unpredictable behaviors under specific runtime conditions.,,,,,,,Linux error when installing Keras,,,,,false,,,
https://stackoverflow.com/questions/41547497,false,The issue does not meet the criteria for deeper analysis as it does not involve an API exhibiting unexpected failures or unpredictable behaviors under specific runtime conditions.,,,,,,,Keras input issue,,,,,false,,,
