ISSUE_LINK,ai_verdict,REASON,LIBRARY_NAME,API_NAME,ISSUE_DESCRIPTION,NORMAL_CONDITIONS,TRIGGER_CONDITIONS,REASON_FOR_DIFFICULTY_IN_DETECTION,ISSUE_TITLE,ISSUE_BODY,ANSWER_1,ANSWER_2,ANSWER_3,HUMAN_CLASSIFICATION,HUMAN_REASON
https://stackoverflow.com/questions/5675654,false,The issue does not meet the criteria for deeper analysis as it does not involve an API exhibiting unexpected failures or unpredictable behaviors under specific runtime conditions.,,,,,,,How to correctly achieve test isolation with a stateful Python module?,"The project I'm working on is a business logic software wrapped up as a Python package. The idea is that various script or application will import it, initialize it, then use it.

It currently has a top level init() method that does the initialization and sets up various things, a good example is that it sets up SQLAlchemy with a db connection and stores the SA session for later access. It is being stored in a subpackage of my project (namely myproj.model.Session, so other code could get a working SA session after import'ing the model).

Long story short, this makes my package a stateful one. I'm writing unit tests for the project and this stafeful behaviour poses some problems:


tests should be isolated, but the internal state of my package breaks this isolation
I cannot test the main init() method since its behavior depends on the state
future tests will need to be run against the (not yet written) controller part with a well known model state (eg. a pre-populated sqlite in-memory db)


Should I somehow refactor my package because the current structure is not the Best (possible) Practice(tm)? :)

Should I leave it at that and setup/teardown the whole thing every time? If I'm going to achieve complete isolation that'd mean fully erasing and re-populating the db at every single test, isn't that overkill?

This question is really on the overall code &amp; tests structure, but for what it's worth I'm using nose-1.0 for my tests. I know the Isolate plugin could probably help me but I'd like to get the code right before doing strange things in the test suite.
","You have a few options:

Mock the database

There are a few trade offs to be aware of.

Your tests will become more complex as you will have to do the setup, teardown and mocking of the connection.  You may also want to do verification of the SQL/commands sent.  It also tends to create an odd sort of tight coupling which may cause you to spend additonal time maintaining/updating tests when the schema or SQL changes.

This is usually the purest for of test isolation because it reduces a potentially large dependency from testing.  It also tends to make tests faster and reduces the overhead to automating the test suite in say a continuous integration environment.

Recreate the DB with each Test

Trade offs to be aware of.

This can make your test very slow depending on how much time it actually takes to recreate your database.  If the dev database server is a shared resource there will have to be additional initial investment in making sure each dev has their own db on the server.  The server may become impacted depending on how often tests get runs.  There is additional overhead to running your test suite in a continuous integration environment because it will need at least, possibly more dbs (depending on how many branches are being built simultaneously).

The benefit has to do with actually running through the same code paths and similar resources that will be used in production.  This usually helps to reveal bugs earlier which is always a very good thing.

ORM DB swap

If your using an ORM like SQLAlchemy their is a possibility that you can swap the underlying database with a potentially faster in-memory database.  This allows you to mitigate some of the negatives of both the previous options.

It's not quite the same database as will be used in production, but the ORM should help mitigate the risk that obscures a bug.  Typically the time to setup an in-memory database is much shorter that one which is file-backed.  It also has the benefit of being isolated to the current test run so you don't have to worry about shared resource management or final teardown/cleanup.
","Working on a project with a relatively expensive setup (IPython), I've seen an approach used where we call a get_ipython function, which sets up and returns an instance, while replacing itself with a function which returns a reference to the existing instance. Then every test can call the same function, but it only does the setup for the first one.

That saves doing a long setup procedure for every test, but occasionally it creates odd cases where a test fails or passes depending on what tests were run before. We have ways of dealing with that - a lot of the tests should do the same thing regardless of the state, and we can try to reset the object's state before certain tests. You might find a similar trade-off works for you.
","Mock is a simple and powerfull tool to achieve some isolation. There is a nice video from Pycon2011 which shows how to use it. I recommend to use it together with py.test which reduces the amount of code required to define tests and is still very, very powerfull.
",false,
https://stackoverflow.com/questions/7987981,false,The issue does not meet the criteria for deeper analysis as it appears to be related to unicode encoding and decoding rather than an API exhibiting unexpected failures or unpredictable behaviors under specific runtime conditions.,,,,,,,strange python regex behavior - maybe connected to unicode or sqlalchemy,"I'm trying to search for a pattern in sqlalchemy results (actually filter by a 'like' or 'op'('regexp')(pattern) which I believe is implanted with regex somewhere) - the string and the search string are both in hebrew, and presumably (maybe I'm wrong-)-unicode
where r = u'לבן' and c = u'לבן, ורוד,    '
when I do re.search(r,c) I get the SRE.match object
but when I query the db like:

f = session.query(classname)
c = f[0].color


and c gives me:

'\xd7\x9c\xd7\x91\xd7\x9f,\xd7\x95\xd7\xa8\xd7\x95\xd7\x93,'


or print (c):

לבן,ורוד,


practicaly the same but running re.search(r,c) gives me no match object.

Since I suspected a unicode issue I tried to transform to unicode with unicode(c)
and I get an 'UnicodeDecodeError: 'ascii' codec can't decode byte 0xd7 in position 0: ordinal' which I guess means this is already unicode string - so where's the catch here?
I would prefer using the sqlalchemy 'like' but I get the same problem there = where I know for sure (as I showed in my example that the data contains the string)

Should I transform the search string,pattern somehow? is this related to unicode? something else?

The db table (which I'm quering) collation is utf8_unicode_ci
","c = f[0].color


is not returning a Unicode string (or its repr() would show a u'...' kind of string), but a UTF-8 encoded string.

Try 

c = f[0].color.decode(""utf-8"")


which results in

u'\u05dc\u05d1\u05df,\u05d5\u05e8\u05d5\u05d3,'


or

u'לבן,ורוד,'


if your console can display Hebrew characters.
","'\xd7\x9c\xd7\x91\xd7\x9f,\xd7\x95\xd7\xa8\xd7\x95\xd7\x93, is encoded representation of string u'לבן, ורוד, '. So in the second example you should write re.search(r,c.decode('utf-8'))
You're trying to do almost the same except setting encoding parameter. It makes python try ascii encoding
",,false,
https://stackoverflow.com/questions/20933018,true,"The issue involves SQLAlchemy exhibiting unexpected errors, indicating a potential API-related problem.",,,,,,,Random errors with SQLAlchemy,"I'm using a setup with nginx, uwsgi and SQLAlchemy. I recently switched from SQLObject and I'm now seeing strange random errors with SQLAlchemy. For instance:

sqlalchemy.exc.ResourceClosedError: This result object does not return rows. It has been closed automatically.


or:

sqlalchemy.exc.NoSuchColumnError: ""Could not locate column in row for column 'module.id'""


Is this some kind of behavior in SQLAlchemy which I'm not aware of? Can it be related to multiple processes/threads in uwsgi?

My uwsgi config file looks like this:

[uwsgi]
plugins=python
socket = 127.0.0.1:9002
wsgi-file = /thesystem/code/api.py
master = True
processes  = 4
threads = 2
daemonize = /thesystem/logs/uwsgi.log
pidfile = /thesystem/uwsgi.pid

","Very probably you are opening connections in /thesystem/code/api.py entry point.

That means your file descriptors will be inherited in workers and this does not work with sqlalchemy.

Add --lazy-apps (lazy-apps = true in your ini config) to load /thesystem/code/api.py in each worker instead of loading it in the master and then calling fork()
","In addition to the accepted answer, if you do not want to (or cannot) change preforking for lazy-apps, because of the increase in memory usage, for instance, or the changes in your uwsgi reload strategy, you can simply reconnect to the database after forking:

import uwsgi
def setup_db():
    """""" routine that sets up the connection to your database """"""
    ...

uwsgi.post_fork_hook = setup_db

",,false,
https://stackoverflow.com/questions/39298822,true,The issue involves the SQLAlchemy secondary join model and occurs under specific conditions when updating the child's age attribute to the integer value zero.,SQLAlchemy,update,"When inserting a new version with the age attribute set to zero, the insert into the parent_child join table does not happen, and SQLAlchemy raises a warning about the child object not being in the session.",The SQLAlchemy secondary join model works as expected when inserting new versions with non-zero values for the age attribute.,The issue is triggered when inserting a new version with the age attribute set to zero.,"This issue might be challenging to detect during development and testing due to the specific conditions required for it to occur, such as inserting a new version with the age attribute set to zero.",SQLAlchemy secondary join model fails under strange conditions,"I have a strange issue that I simply cannot resolve. Essentially, I have a model and system which works perfectly - except under a very specific (and seemingly arbitrary) set of circumstances.

I'll paste the model in a second but here's the idea. I want certain tables to be versioned. That means for a given table, I break it into two tables, the Master part which has the natural keys for the object, and the Version table which has all the associated data which may change. Then some of my models of course have a relationship, so I create a join table that links versions.

Here are the models:

class Versioned(object):

    def __init__(self, **kwargs):

        super(Versioned, self).__init__(**kwargs)

        self.active = True
        self.created_on = datetime.datetime.now()

    active = Column(BOOLEAN)
    created_on = Column(TIMESTAMP, server_default=func.now())

    def __eq__(self, other):

        return self.__class__ == other.__class__ and \
            all([getattr(self, key) == getattr(other, key)
                for key in self.comparison_keys
                ])

    def __ne__(self, other):

        return not self.__eq__(other)

    comparison_keys = []

class Parent(Base):

    __tablename__ = 'parent'

    id = Column(INTEGER, primary_key=True)

    name = Column(TEXT)

    versions = relationship(""ParentVersion"", back_populates=""master"")

    children = relationship(""Child"", back_populates=""parent"")

    @property
    def current_version(self):
        active_versions = [v for v in self.versions if v.active==True]

        return active_versions[0] if active_versions else None

class ParentVersion(Versioned, Base):

    __tablename__ = 'parent_version'

    id = Column(INTEGER, primary_key=True)

    master_id = Column(INTEGER, ForeignKey(Parent.id))

    address = Column(TEXT)

    master = relationship(""Parent"", back_populates=""versions"")

    children = relationship(""ChildVersion"",
        secondary=lambda : Parent_Child.__table__
    )

class Child(Base):

    __tablename__ = 'child'

    id = Column(INTEGER, primary_key=True)

    parent_id = Column(INTEGER, ForeignKey(Parent.id))

    name = Column(TEXT)

    versions = relationship(""ChildVersion"", back_populates=""master"")

    parent = relationship(""Parent"", back_populates=""children"")

    @property
    def current_version(self):
        active_versions = [v for v in self.versions if v.active==True]

        return active_versions[0] if active_versions else None


class ChildVersion(Versioned, Base):

    __tablename__ = 'child_version'

    id = Column(INTEGER, primary_key=True)

    master_id = Column(INTEGER, ForeignKey(Child.id))

    age = Column(INTEGER)

    fav_toy = Column(TEXT)

    master = relationship(""Child"", back_populates=""versions"")

    parents = relationship(""ParentVersion"",
        secondary=lambda: Parent_Child.__table__,
    )

    comparison_keys = [
        'age',
        'fav_toy',
    ]

class Parent_Child(Base):

    __tablename__ = 'parent_child'

    id = Column(INTEGER, primary_key=True)

    parent_id = Column(INTEGER, ForeignKey(ParentVersion.id))
    child_id = Column(INTEGER, ForeignKey(ChildVersion.id))


Okay, so I know the more recent SQLAlchemy models have some idea of versioning, it's possible that I'm doing this the wrong way. But this fits my use case very well. So humor me and let's assume the model is okay (in the general sense - if there's a minor detail causing the bug that would be good to fix)

Now suppose I want to insert data. I have data from some source, I take it in and build models. Ie, split things into Master/Version, assign the child relationships, assign the version relationships. Now I want to compare it against the data already in my database. For each master object, if I find it, I compare the versions. If the versions are different, you create a new version. The tricky part becomes, if a Child version is different, I want to insert a new Parent version, and update all its relationships. Maybe the code makes more sense to explain this part. search_parent is the object I have created in my pre-parsing stage. It has a version, and children objects, which also have versions.

parent_conds = [
    getattr(search_parent.__class__, name) == getattr(search_parent, name)
    for name, column in search_parent.__class__.__mapper__.columns.items()
    if not column.primary_key
]

parent_match = session.query(Parent).filter(*parent_conds).first()

# We are going to make a new version
parent_match.current_version.active=False
parent_match.versions.append(search_parent.current_version)

for search_child in search_parent.children[:]:

    search_child.parent_id = parent_match.id

    search_conds = [
        getattr(search_child.__class__, name) == getattr(search_child, name)
        for name, column in search_child.__class__.__mapper__.columns.items()
        if not column.primary_key
    ]

    child_match = session.query(Child).filter(*search_conds).first()

    if child_match.current_version != search_child.current_version:
        # create a new version: deactivate the old one, insert the new
        child_match.current_version.active=False
        child_match.versions.append(search_child.current_version)

    else:
        # copy the old version to point to the new parent version
        children = parent_match.current_version.children

        children.append(child_match.current_version)
        children.remove(search_child.current_version)
        session.expunge(search_child.current_version)

    session.expunge(search_child)

session.expunge(search_parent)

session.add(parent_match)

session.commit()


Okay, so once again, this might not be the perfect or even best approach. But it does work. EXCEPT, and this is what I can't figure out. It doesn't work if I'm updating the child's age attribute to the integer value zero. If the child objects start with age 0, and I change it to something else, this works beautifully. If I start with some non-zero integer, and update the age to 0, I get this warning:

SAWarning: Object of type &lt;ChildVersion&gt; not in session, add operation   along 'ParentVersion.children' won't proceed (mapperutil.state_class_str(child), operation, self.prop))


The updated version is inserted, however the insert into the parent_child join table doesn't happen. And it's not that it fails, it's that SQLAlchemy has determined the child object doesn't exist and can't create the join. But it does exist, I know it gets inserted.

Again, this only happens if I'm inserting a new version with age=0. If I'm inserting a new version with any other age, this works exactly as I want it to.

There are other odd things about the bug - it doesn't happen if you don't insert enough children (seems to be around 12 triggers the bug), it doesn't happen depending on other attributes sometimes. I don't think I fully understand the surface area of what causes it.

Thanks for taking the time to read this far. I have a fully working demo complete with source data I'd be happy to share, it just requires some setup so I didn't know if it was appropriate in this post. I hope someone has ideas for what to look at because at this point I'm totally out.

edit: Here is the full stack trace leading to the warning.

  File ""repro.py"", line 313, in &lt;module&gt;
  load_data(session, second_run)
File ""repro.py"", line 293, in load_data
  session.commit()
File ""/Users/me/virtualenvs/dev/lib/python2.7/site-packages/sqlalchemy/orm/session.py"", line 801, in commit
  self.transaction.commit()
File ""/Users/me/virtualenvs/dev/lib/python2.7/site-packages/sqlalchemy/orm/session.py"", line 392, in commit
  self._prepare_impl()
File ""/Users/me/virtualenvs/dev/lib/python2.7/site-packages/sqlalchemy/orm/session.py"", line 372, in _prepare_impl
  self.session.flush()
File ""/Users/me/virtualenvs/dev/lib/python2.7/site-packages/sqlalchemy/orm/session.py"", line 2019, in flush
  self._flush(objects)
File ""/Users/me/virtualenvs/dev/lib/python2.7/site-packages/sqlalchemy/orm/session.py"", line 2101, in _flush
  flush_context.execute()
File ""/Users/me/virtualenvs/dev/lib/python2.7/site-packages/sqlalchemy/orm/unitofwork.py"", line 373, in execute
  rec.execute(self)
File ""/Users/me/virtualenvs/dev/lib/python2.7/site-packages/sqlalchemy/orm/unitofwork.py"", line 487, in execute
  self.dependency_processor.process_saves(uow, states)
File ""/Users/me/virtualenvs/dev/lib/python2.7/site-packages/sqlalchemy/orm/dependency.py"", line 1053, in process_saves
  False, uowcommit, ""add""):
File ""/Users/me/virtualenvs/dev/lib/python2.7/site-packages/sqlalchemy/orm/dependency.py"", line 1154, in _synchronize
  (mapperutil.state_class_str(child), operation, self.prop))
File ""/Users/me/virtualenvs/dev/lib/python2.7/site-packages/sqlalchemy/util/langhelpers.py"", line 1297, in warn
  warnings.warn(msg, exc.SAWarning, stacklevel=2)
File ""repro.py"", line 10, in warn_with_traceback
  traceback.print_stack()
/Users/me/virtualenvs/dev/lib/python2.7/site-packages/sqlalchemy/orm/dependency.py:1154: SAWarning: Object of type &lt;ChildVersion&gt; not in session, add operation along 'ParentVersion.children' won't proceed
(mapperutil.state_class_str(child), operation, self.prop))


edit2:
Here is a gist with a python file you can run to see the strange behavior.
https://gist.github.com/jbouricius/2ede420fb1f7a2deec9f557c76ced7f9
","The reason you get this error is that you've inadvertently added objects into the session.

Here is the MVCE:

engine = create_engine(""sqlite://"", echo=False)


def get_data():
    children = [
        Child(name=""Carol"", versions=[ChildVersion(age=0, fav_toy=""med"")]),
        Child(name=""Timmy"", versions=[ChildVersion(age=0, fav_toy=""med"")]),
    ]
    return Parent(
        name=""Zane"", children=children,
        versions=[
            ParentVersion(
                address=""123 Fake St"",
                children=[v for child in children for v in child.versions]
            )
        ]
    )


def main():
    Base.metadata.create_all(engine)

    session = Session(engine)
    parent_match = get_data()
    session.add(parent_match)
    session.commit()

    with session.no_autoflush:
        search_parent = get_data()

        parent_match.versions.append(search_parent.current_version)
        for search_child in search_parent.children[:]:
            child_match = next(c for c in parent_match.children if c.name == search_child.name)

            if child_match.current_version != search_child.current_version:
                child_match.versions.append(search_child.current_version)
            else:
                session.expunge(search_child.current_version)

            session.expunge(search_child)

        session.expunge(search_parent)
        session.commit()


Aside: this is what you needed to provide in the question itself. Providing a tarball with instructions is not the best way to get answers.

The line

parent_match.versions.append(search_parent.current_version)


not only adds search_parent.current_version, it also adds search_parent, which in turn adds all related objects, including the child versions of other children. Judging by the fact that you later expunge other related objects to prevent them from being added to the session, I conclude that you only want to add search_parent.current_version without adding other related objects. Due to the circular nature of your relationships you need to take care to lift only the objects you want out of search_parent before you add them. Here is the fixed MVCE:

with session.no_autoflush:
    search_parent = get_data()

    current_parent_version = search_parent.current_version
    search_parent.versions.remove(current_parent_version)
    current_parent_version.children = []  # &lt;--- this is key
    for search_child in search_parent.children[:]:
        child_match = next(c for c in parent_match.children if c.name == search_child.name)

        if child_match.current_version != search_child.current_version:
            current_child_version = search_child.current_version
            search_child.versions.remove(current_child_version)
            child_match.versions.append(current_child_version)
            current_parent_version.children.append(current_child_version)

    parent_match.versions.append(current_parent_version)

    session.commit()

",,,false,
https://stackoverflow.com/questions/66896139,false,The issue does not involve an API exhibiting unexpected failures or unpredictable behaviors under specific runtime conditions. It appears to be related to accessing updated data from the database within an executor function.,,,,,,,Flask-Executor and Flask-SQLAlchemy: Can&#39;t get updated data from DB inside of executor function,"I'm adding a Flask-based API to my web application to control the start and stop of some network automation functions. I've ran into a strange behavior where functions called by a Flask-Executor .submit() method are seemingly unable to get new or updated data from the database.
I know this question is very involved, so thank you to anyone who shares their time and input. See the end of this question for an overview of my project structure.
The flask-executor documentation says:

When calling submit() or map() Flask-Executor will wrap ThreadPoolExecutor callables with a copy of both the current application context and current request context

I don't quite fully understand what it means by context, but I feel that it might be a good hint about why this should or shouldn't work. (I am using the ThreadPoolExecutor, by the way). I assume that the db SQLAlchemy object is part of the application context, and as such a copy of db should be made available in the executor function. This didn't seem to be the case because I still had to import db in the file containing the function called by the executor, as you'll see later on in this post.
My front end has simple start and stop buttons which send a POST to the following API route:

file: app/api.py

from flask import request
from flask_login import login_required
from app import app, db, executor
from app.models import Project
from datetime import datetime
from automation.Staging import control

@app.route('/api/staging/control', methods=['POST'])
@login_required
def staging_control():
    data = request.json
    project_id = data['project-id']
    action = data['staging-control']

    project = Project.query.get(project_id)
    sp = project.staging_profile
    current_status = sp.status

    if action == 'start':
        if current_status == 'STARTED':
            return {'response': 200, 'message': 'Job already running!'}
        else:
            sp.status = 'STARTED'
            db.session.commit()
            # The executor only spawns the thread if the task status was not already started.
            executor.submit(control.start_staging, project_id)
        
    
    elif action == 'stop':
        if current_status == 'STARTED':
            sp.status = 'STOPPED'
            db.session.commit()

    return {'response' : 200, 'message': 'OK'}

Background
The status of the job is stored in a DB model.  If a start action is POSTed, the DB model's status column is updated. Likewise, if a stop action is POSTed, the DB model's status is updated.
The executor's function call to control.start_staging spawns a thread that begins an infinite loop which does some work and then sleeps for X seconds.  At the start of each time through the loop, I am trying to check the DB model's status column to determine whether or not to break from the loop and close the thread.
Starting the thread works just fine. The database model gets updated, the executor spawns the thread, and my while loop begins.
Sending the stop action from my frontend works just fine too. The status in the DB is set to STOPPED, and I can see this with manual queries to in my DB shell.
However, the control.start_staging function originally started by the executor still thinks the status is set to STARTED, even though it will actually be updated to STOPPED at some time during the thread's operation. I have attempted to get the updated value as many ways as I can think of from inside the thread. I've seen this similar question.
Here is the control.start_staging function. I've shared a few of the different ways that I've tried to get the updated status in the excerpt below as comments:

file: automation/Staging/control.py

from app import db
from app.models import Project, Staging_Profile
from app.config import STAGING_DURATION_MINS
from datetime import datetime, timedelta
from time import sleep


def start_staging(project_id):    
    project = Project.query.get(project_id)
    print(f""Received START for project {project.project_name}"")
    sp = project.staging_profile
    sp.last_activity = datetime.utcnow()
    db.session.commit()
    status = sp.status

    # Staging Loop Start
    while True:

        # This just serves as a force-stop if the job runs for more than STAGING_DURATION_MINUTES minutes.
        if sp.last_activity + timedelta(minutes=STAGING_DURATION_MINS) &gt; datetime.utcnow():
            print(f""Status is: {sp.status}"")

            # ATTEMPT 1: does not get updated data
            # status = sp.status

            # ATTEMPT 2: does not get updated data
            # status = Staging_Profile.query.get(project.staging_profile_id).status

            # ATTEMPT 3: does not get updated data
            all_profiles = db.session.query(Staging_Profile).all()
            this_profile = [profile for profile in all_profiles if profile.id == sp.id][0]

            if this_profile.status == 'STOPPED':
                print(""Status is STOPPED. Returning"")
                break
            
            else:
                print(f""Status is {this_profile.status}"")

            # Do work
            do_some_stuff()

        else:
            break
        sleep(5)

    return

Now, what's really puzzling is that I can write data to the database from inside the executor function. The line sp.last_activity = datetime.utcnow() followed by db.session.commit() successfully writes the current time when the thread is started.
My Suspicions
I have built this application in a very modular style approach, and I feel that perhaps this is the source of the issue.
Here is an overview of the relevant parts of my application structure:
app/
├─ __init__.py   # This is where my db &amp; executor are instantiated
├─ api.py        # This is where the /api/staging/control route lives
├─ models.py     # This holds my SQLAlchemy DB classes
├─ routes.py     # This holds my regular front-end routes
├─ config.py     # General config parameters

automation/
├─ Staging/
│  ├─ control.py    # This is where the function passed to the executor is defined
│  ├─ __init__.py   # Empty
├─ __init__.py      # Empty

Thanks again. I will post a resolution or a workaround to this issue when I find one.
","use update
Project.query.filter_by(id=project_id).update({
 'last_activity': datetime.utcnow()
})

",,,false,
https://stackoverflow.com/questions/46440663,false,The issue is not relevant for deeper analysis as it does not meet the criteria of exhibiting unexpected behavior under specific runtime conditions. It appears to be a question about the behavior of the filter function in flask-sqlalchemy.,,,,,,,Strange filter behavior in flask-sqlalchemy,"I have a query in flask-sqlalchemy and filter is behaving strange:

q.filter(Transaction.transaction_id == ReconciledTransaction.safe_withdraw_id).all()


It works fine, but:

q.filter(Transaction.transaction_id != ReconciledTransaction.safe_withdraw_id).all()


Doesn't work correctly! What seems to be the problem?

UPD
My models:
Reconciled transaction model:

class ReconciledTransactionModel(db.Model):
    """"""Reconciled Transaction model""""""

    __tablename__ = 'ReconciledTransaction'

    id = db.Column('id', db.Integer, primary_key=True, nullable=False)
    balance_entry_id = db.Column('BalanceEntry_id', db.Integer, db.ForeignKey(""BalanceEntry.id""), nullable=False)
    safe_withdraw_id = db.Column('Transaction_id', db.String, nullable=False)
    datetime = db.Column('datetime', db.Date(), nullable=False)
    balance_entry_amount = db.Column('BalanceEntry_amount', db.Float)
    reconciled_amount = db.Column('ReconciledAmount', db.Float)
    currency = db.Column('currency', db.String)
    reconciliation_status = db.Column('reconciliation_status', db.String, nullable=False)
    status_code = db.Column('status_code', db.Integer, nullable=False)


Transaction Model:

class TransactionModel(db.Model):
    """"""Transaction SA model.""""""

    __tablename__ = 'Transaction'

    id = db.Column('id', db.Integer, primary_key=True)
    till_id = db.Column('Till_id', db.Integer, db.ForeignKey(""Till.id""),
                        nullable=False)
    till = relationship(""Till"", foreign_keys=[till_id], backref=""transactions"", enable_typechecks=False)
    establishment_id = db.Column('Establishment_id', db.Integer,
                                 db.ForeignKey(""Establishment.id""),
                                 nullable=False)
    establishment = relationship(""Establishment"",
                                 foreign_keys=[establishment_id],
                                 backref=""transactions"",
                                 enable_typechecks=False)
    employee_id = db.Column('Employee_id', db.Integer,
                            db.ForeignKey(""Employee.id""),
                            nullable=False)
    employee = relationship(""Employee"",
                            foreign_keys=[employee_id],
                            backref=""transactions"",
                            enable_typechecks=False)
    local_time = db.Column('local_time', db.DateTime, nullable=False)
    create_time = db.Column('create_time', db.TIMESTAMP(timezone=True),
                            nullable=False)
    send_time = db.Column('send_time', db.TIMESTAMP(timezone=True),
                          nullable=False)
    receive_time = db.Column('receive_time', db.TIMESTAMP(timezone=True),
                             nullable=False)
    total_value = db.Column('total_value', db.Integer, nullable=False)
    amount = db.Column('amount', db.Float, nullable=False)
    discrepancy = db.Column('discrepancy', db.Float, nullable=False)
    type = db.Column('type', db.Enum('shift',
                                     'payment',
                                     'skimming',
                                     'withdraw',
                                     'refund',
                                     'till',
                                     'till_deposit',
                                     'safe_deposit',
                                     'safe_withdraw',
                                     'till_reset',
                                     name='transaction_type'),
                     nullable=False)
    status = db.Column('status',
                       db.Enum('start', 'end', name='transaction_status'),
                       nullable=False)
    receipt_id = db.Column('receipt_id', db.String(32), server_default=None)
    transaction_id = db.Column('transaction_id', db.String(32),
                               server_default=None)
    parent_transaction = db.Column('parent_transaction', db.String(32),
                                   server_default=None)
    discrepancy_reason = db.Column('discrepancy_reason', db.String(1024))
    resolve_discrepancy_reason = db.Column('resolve_discrepancy_reason',
                                           db.String(1024))
    accounted = db.Column('accounted', db.Boolean, default=False)


And here is my query:

_transactions = db.session.query(Transaction,
                                 status_sq.c.count,
                                 end_transaction_sq.c.discrepancy,
                                 end_transaction_sq.c.discrepancy_reason,
                                 end_transaction_sq.c.resolve_discrepancy_reason,
                                 end_transaction_sq.c.amount,
                                 ). \
    filter(Transaction.establishment_id.in_(store_ids)). \
    filter(Transaction.amount != 0). \
    filter_by(status='start')

transactions = _transactions. \
    filter(Transaction.type.in_(transaction_types)). \
    outerjoin(status_sq,
              Transaction.transaction_id == status_sq.c.transaction_id). \
    outerjoin(end_transaction_sq,
              Transaction.transaction_id == end_transaction_sq.c.transaction_id)

# check possible values for sorting and pages
if sort_field not in allowed_sort_fields:
    sort_field = Transaction.default_sort_field
if sort_dir not in (ASCENDING, DESCENDING):
    sort_dir = Transaction.default_sort_dir
if per_page &gt; 100:  # hard limit
    per_page = Transaction.default_per_page

if sort_dir == ASCENDING:
    order = allowed_sort_fields[sort_field].desc()
else:
    order = allowed_sort_fields[sort_field].desc()

q = transactions.\
    join(Establishment).\
    join(Employee, Transaction.employee_id == Employee.id). \
    outerjoin(Currency). \
    group_by(Transaction,
             status_sq.c.count,
             end_transaction_sq.c.discrepancy,
             end_transaction_sq.c.discrepancy_reason,
             end_transaction_sq.c.resolve_discrepancy_reason,
             end_transaction_sq.c.amount,
             allowed_sort_fields[sort_field]).\
    order_by(order)
items = q.filter(Transaction.transaction_id == ReconciledTransaction.safe_withdraw_id).limit(per_page).offset((page - 1) * per_page).all()


'Doesn't work correctly' means that in second case(when I place !=, and wanna take transactions only, which are not in ReconciledTransaction table) filter gets ignored, but when filter contains ==, all works correctly(I have only matched transactions).
","When you use query like this:

q = db.session.query(Transaction). \
    filter(Transaction.transaction_id != ReconciledTransaction.safe_withdraw_id)


it transforms into SQL query:

SELECT Transaction.* FROM Transaction, ReconciledTransaction
WHERE Transaction.transaction_id != ReconciledTransaction.safe_withdraw_id


which means you will get all Transaction rows with all ReconciledTransaction rows except those with matching ids.

If you need to get all Transaction objects which are not in ReconciledTransaction table you can first get all ReconciledTransaction ids:

r_query = db.session.query(ReconciledTransaction.safe_withdraw_id). \
    group_by(ReconciledTransaction.safe_withdraw_id)
r_ids = [x[0] for x in r_query]


and then use NOT IN filter in your Transaction query:

q = q.filter(Transaction.transaction_id.notin_(r_ids))


Or your can use subquery:

q = q.filter(Transaction.transaction_id.notin_(
    db.session.query(ReconciledTransaction.safe_withdraw_id)
))


Edit: as Ilja Everilä stated NOT EXISTS operator performance might be better than NOT IN. SQLAlchemy query will look like this:

q = q.filter(~session.query(ReconciledTransaction). \
    filter(ReconciledTransaction.safe_withdraw_id == Transaction.id).exists())

",,,false,
https://stackoverflow.com/questions/56317578,false,The issue is not relevant for deeper analysis as it does not involve an API exhibiting unexpected failures or unpredictable behaviors under specific runtime conditions. It seems to be a question about SQLAlchemy not updating model instances with external changes.,,,,,,,SQLAlchemy does not update/expire model instances with external changes,"Recently I came across strange behavior of SQLAlchemy regarding refreshing/populating model instances with the the changes that were made outside of the current session. I created the following minimal working example and was able to reproduce problem with it.


from time import sleep

from sqlalchemy import orm, create_engine, Column, BigInteger, Integer
from sqlalchemy.ext.declarative import declarative_base

DATABASE_URI = ""postgresql://{user}:{password}@{host}:{port}/{name}"".format(
    user=""postgres"",
    password=""postgres"",
    host=""127.0.0.1"",
    name=""so_sqlalchemy"",
    port=""5432"",
)


class SQLAlchemy:
    def __init__(self, db_url, autocommit=False, autoflush=True):
        self.engine = create_engine(db_url)
        self.session = None

        self.autocommit = autocommit
        self.autoflush = autoflush

    def connect(self):
        session_maker = orm.sessionmaker(
            bind=self.engine,
            autocommit=self.autocommit,
            autoflush=self.autoflush,
            expire_on_commit=True
        )
        self.session = orm.scoped_session(session_maker)

    def disconnect(self):
        self.session.flush()
        self.session.close()
        self.session.remove()
        self.session = None


BaseModel = declarative_base()


class TestModel(BaseModel):
    __tablename__ = ""test_models""

    id = Column(BigInteger, primary_key=True, nullable=False)
    field = Column(Integer, nullable=False)


def loop(db):
    while True:
        with db.session.begin():
            t = db.session.query(TestModel).with_for_update().get(1)
            if t is None:
                print(""No entry in db, creating..."")
                t = TestModel(id=1, field=0)
                db.session.add(t)
                db.session.flush()

            print(f""t.field value is {t.field}"")
            t.field += 1
            print(f""t.field value before flush is {t.field}"")
            db.session.flush()
            print(f""t.field value after flush is {t.field}"")

        print(f""t.field value after transaction is {t.field}"")
        print(""Sleeping for 2 seconds."")
        sleep(2.0)


def main():
    db = SQLAlchemy(DATABASE_URI, autocommit=True, autoflush=True)
    db.connect()
    try:
        loop(db)
    except KeyboardInterrupt:
        print(""Canceled"")


if __name__ == '__main__':
    main()



My requirements.txt file looks like this:

alembic==1.0.10
psycopg2-binary==2.8.2
sqlalchemy==1.3.3


If I run the script (I use Python 3.7.3 on my laptop running Ubuntu 16.04), it will nicely increment a value every two seconds as expected:

t.field value is 0
t.field value before flush is 1
t.field value after flush is 1
t.field value after transaction is 1
Sleeping for 2 seconds.
t.field value is 1
t.field value before flush is 2
t.field value after flush is 2
t.field value after transaction is 2
Sleeping for 2 seconds.
...


Now at some point I open postgres database shell and begin another transaction:

so_sqlalchemy=# BEGIN;
BEGIN
so_sqlalchemy=# UPDATE test_models SET field=100 WHERE id=1;
UPDATE 1
so_sqlalchemy=# COMMIT;
COMMIT


As soon as I press Enter after the UPDATE query, the script blocks as expected, as I'm issuing SELECT ... FOR UPDATE query there. However, when I commit the transaction in the database shell, script continues from the previous value (say, 27) and does not detect that external transaction has changed the value of field in database to 100.

My question is, why does this happen at all? There are several factors that seem to contradict the current behavior:


I'm using expire_on_commit setting set to True, which seems to imply that every model instance that has been used in transaction will be marked as expired after the transaction has been committed. (Quoting documentation, ""When True, all instances will be fully expired after each commit(), so that all attribute/object access subsequent to a completed transaction will load from the most recent database state."").
I'm not accessing some old model instance but rather issue completely new query every time. As far as I understand, this should lead to direct query to the database and not access cached instance. I can confirm that this is indeed the case if I turn sqlalchemy debug log on.


The quick and dirty fix for this problem is to call db.session.expire_all() right after the transaction has begun, but this seems very inelegant and counter-intuitive. I would be very glad to understand what's wrong with the way I'm working with sqlalchemy here.
","I ran into a very similar situation with MySQL. I needed to ""see"" changes to the table that were coming from external sources in the middle of my code's database operations. I ended up having to set autocommit=True in my session call and use the begin() / commit() methods of the session to ""see"" data that was updated externally.

The SQLAlchemy docs say this is a legacy configuration:


  Warning
  
  “autocommit” mode is a legacy mode of use and should not be considered for new projects.


but also say in the next paragraph:


  Modern usage of “autocommit mode” tends to be for framework integrations that wish to control specifically when the “begin” state occurs


So it doesn't seem to be clear which statement is correct.
",,,false,
https://stackoverflow.com/questions/17315422,true,The issue involves unexpected behavior when storing datetime.datetime.max using SQLAlchemy==0.8.1 with the mysql-python==1.2.4 driver. The change in behavior between the two versions of the driver suggests a potential bug or compatibility issue.,,,,,,,Unable to store datetime.datetime.max using SQLAlchemy==0.8.1 with the mysql-python==1.2.4 driver,"I've noticed a change in behavior for storing datetime.datetime.max via SQLAlchemy==SQLAlchemy==0.8.1 and going from mysql-python==1.2.3 to mysql-python==1.2.4. By only changing the driver from 1.2.3 to 1.2.4 I go from being able to store to being unable to store it.

Where do I turn to for help in this matter? SQLAlchemy or mysql-python? Is this expected behaviour or a bug or do I have a bad setup? I fear that a change like this will break a lot of systems out there.

This is my SQLAlchemy setup:

from sqlalchemy import create_engine, Integer, DateTime, Column
from sqlalchemy.orm import sessionmaker
from sqlalchemy.ext.declarative import declarative_base
from datetime import datetime

engine = create_engine('mysql://root@localhost/test_database', echo=True)
Base = declarative_base()

class User(Base):
    __tablename__ = 'users'
    id = Column(Integer, primary_key=True)
    age = Column(DateTime, default=datetime.max)

Base.metadata.create_all(engine)
session = sessionmaker(bind=engine)()
u = User()
session.add(u)
session.commit()


I also have a virtualenv called test. This is what happens when I run the code above.

(test)➜  ~  pip install MySQL-python==1.2.3
(test)➜  ~  python test.py
2013-06-26 10:29:18,885 INFO sqlalchemy.engine.base.Engine SELECT DATABASE()
2013-06-26 10:29:18,885 INFO sqlalchemy.engine.base.Engine ()
2013-06-26 10:29:18,887 INFO sqlalchemy.engine.base.Engine SHOW VARIABLES LIKE 'character_set%%'
2013-06-26 10:29:18,887 INFO sqlalchemy.engine.base.Engine ()
2013-06-26 10:29:18,891 INFO sqlalchemy.engine.base.Engine SHOW VARIABLES LIKE 'sql_mode'
2013-06-26 10:29:18,891 INFO sqlalchemy.engine.base.Engine ()
2013-06-26 10:29:18,896 INFO sqlalchemy.engine.base.Engine DESCRIBE `users`
2013-06-26 10:29:18,896 INFO sqlalchemy.engine.base.Engine ()
2013-06-26 10:29:18,904 INFO sqlalchemy.engine.base.Engine BEGIN (implicit)
2013-06-26 10:29:18,905 INFO sqlalchemy.engine.base.Engine INSERT INTO users (age) VALUES (%s)
2013-06-26 10:29:18,905 INFO sqlalchemy.engine.base.Engine (datetime.datetime(9999, 12, 31, 23, 59, 59, 999999),)
2013-06-26 10:29:18,908 INFO sqlalchemy.engine.base.Engine COMMIT


And the database (test_database) looks like this:

mysql&gt; select * from users;
+----+---------------------+
| id | age                 |
+----+---------------------+
|  1 | 9999-12-31 23:59:59 |
+----+---------------------+
1 row in set (0.00 sec)


This is my expected result so nothing strange here.

However, by simply switching the driver to mysql-python==1.2.4 I get this result.

(test)➜  ~  pip install MySQL-python==1.2.4
(test)➜  ~  python test.py
2013-06-26 10:33:39,544 INFO sqlalchemy.engine.base.Engine SELECT DATABASE()
2013-06-26 10:33:39,544 INFO sqlalchemy.engine.base.Engine ()
2013-06-26 10:33:39,546 INFO sqlalchemy.engine.base.Engine SHOW VARIABLES LIKE 'character_set%%'
2013-06-26 10:33:39,546 INFO sqlalchemy.engine.base.Engine ()
2013-06-26 10:33:39,546 INFO sqlalchemy.engine.base.Engine SHOW VARIABLES LIKE 'sql_mode'
2013-06-26 10:33:39,546 INFO sqlalchemy.engine.base.Engine ()
2013-06-26 10:33:39,547 INFO sqlalchemy.engine.base.Engine DESCRIBE `users`
2013-06-26 10:33:39,547 INFO sqlalchemy.engine.base.Engine ()
2013-06-26 10:33:39,551 INFO sqlalchemy.engine.base.Engine BEGIN (implicit)
2013-06-26 10:33:39,552 INFO sqlalchemy.engine.base.Engine INSERT INTO users (age) VALUES (%s)
2013-06-26 10:33:39,552 INFO sqlalchemy.engine.base.Engine (datetime.datetime(9999, 12, 31, 23, 59, 59, 999999),)
/Users/pelle/.virtualenvs/test/lib/python2.7/site-packages/sqlalchemy/engine/default.py:324: Warning: Datetime function: datetime field overflow
  cursor.execute(statement, parameters)
/Users/pelle/.virtualenvs/test/lib/python2.7/site-packages/sqlalchemy/engine/default.py:324: Warning: Out of range value for column 'age' at row 1
  cursor.execute(statement, parameters)
2013-06-26 10:33:39,553 INFO sqlalchemy.engine.base.Engine COMMIT


And the database looks like this.

mysql&gt; select * from users;
+----+---------------------+
| id | age                 |
+----+---------------------+
|  1 | 0000-00-00 00:00:00 |
+----+---------------------+
1 row in set (0.00 sec)


So now all of the sudden I receive a warning Warning: Datetime function: datetime field overflow and I end up with a nullable value in my database.
","This is a reported bug in the new version of the MySQL server (5.6.X) to do with the rounding of fractional seconds.

See this link for more information:
http://bugs.mysql.com/bug.php?id=68760

The way round this is to round out the milliseconds.
",,,false,
https://stackoverflow.com/questions/20025324,false,The issue does not meet the criteria for deeper analysis as it does not involve an API exhibiting unexpected failures or unpredictable behaviors under specific runtime conditions.,,,,,,,Why does SQLAlchemy insert expression values() method function differently when executing in one statement or two?,"I'm working through the SQLAlchemy core tutorial (http://docs.sqlalchemy.org/en/rel_0_8/core/tutorial.html) and found a strange behavior. In the Insert Expressions section they first create a basic insert expression and print it.

&gt;&gt;&gt; ins = users.insert()
&gt;&gt;&gt; str(ins)
'INSERT INTO users (id, name, fullname) VALUES (:id, :name, :fullname)'


They then perform the same operation, adding values to specific columns, which limits the number of columns in the expression to those listed in the values() call.

&gt;&gt;&gt; ins = users.insert().values(name='jack', fullname='Jack Jones')
&gt;&gt;&gt; str(ins)
'INSERT INTO users (name, fullname) VALUES (:name, :fullname)'


Why is it that if I take the second version and perform it in 2 lines instead of 1, the values() call doesn't take?

&gt;&gt;&gt; ins = users.insert()
&gt;&gt;&gt; ins.values(name='jack', fullname='Jack Jones')
&gt;&gt;&gt; str(ins)
'INSERT INTO users (id, name, fullname) VALUES (:id, :name, :fullname)'


I know the values() call is truly not doing anything because the I tested the params value from a ins.compile().params call and they are all None in the version in 2 lines.

&gt;&gt;&gt; ins = users.insert().values(name='jack', fullname='Jack Jones')
&gt;&gt;&gt; ins.compile().params
{'fullname': 'Jack Jones', 'name': 'jack'}

&gt;&gt;&gt; ins = users.insert()
&gt;&gt;&gt; ins.values(name='jack', fullname='Jack Jones')
&gt;&gt;&gt; ins.compile().params
{'fullname': None, 'password': None, 'id': None, 'name': None}

","The difference between these two:

&gt;&gt;&gt; ins = users.insert().values(name='jack', fullname='Jack Jones')
&gt;&gt;&gt; ins.compile().params
{'fullname': 'Jack Jones', 'name': 'jack'}

&gt;&gt;&gt; ins = users.insert()
&gt;&gt;&gt; ins.values(name='jack', fullname='Jack Jones')
&gt;&gt;&gt; ins.compile().params
{'fullname': None, 'password': None, 'id': None, 'name': None}


is that you are not saving the returned value of ins in the second case.
I haven't tested, but to make them equivalent, it should be something like:

&gt;&gt;&gt; ins = users.insert()
&gt;&gt;&gt; ins = ins.values(name='jack', fullname='Jack Jones')


Otherwise, what you do with ins.values just vanishes
",,,false,
https://stackoverflow.com/questions/32149305,false,The issue does not meet the criteria for deeper analysis as it is related to caching and performance optimization rather than an API exhibiting unexpected failures or unpredictable behaviors.,,,,,,,MySQL data not updating in web app - Python/Flask,"I'm building a Python/Flask Application using a MySQL backend. Many of the views have forms that submit data to a route, and then the route adds the data via db.session.commit(). I can confirm that the MySQL db updates with every commit, but then if I subsequently refresh the page multiple times, the data changes every time (i.e. most recently added items will disappear, reappear).

I've done a couple things to try to fix:

HTML Headers
I know it's not best to add a ton of headers and hope for the best, but I added these headers to fight caching:

&lt;meta http-equiv=""cache-control"" content=""max-age=0"" /&gt;
&lt;meta http-equiv=""cache-control"" content=""private, proxy-revalidate, s-maxage=0, no-cache, no-store, must-revalidate"" /&gt;
&lt;meta http-equiv=""expires"" content=""0"" /&gt;
&lt;meta http-equiv=""expires"" content=""Tue, 01 Jan 1980 1:00:00 GMT"" /&gt;
&lt;meta http-equiv=""pragma"" content=""no-cache"" /&gt;


Flask Headers
I added headers to the Flask app itself:

response.headers[""Cache-Control""] = ""no-cache, no-store, must-revalidate"" # HTTP 1.1.
response.headers[""Pragma""] = ""no-cache"" # HTTP 1.0.
response.headers[""Expires""] = ""0"" # Proxies.


Disable MySQL Caching

SET SESSION query_cache_type=OFF;


Apache2 Disable Caching

&lt;filesMatch ""\.(html|htm|js|css)$""&gt;
    FileETag None
    &lt;ifModule mod_headers.c&gt;
            Header unset ETag
            Header set Cache-Control ""max-age=0, no-cache, no-store, must-revalidate""
            Header set Pragma ""no-cache""
            Header set Expires ""Wed, 11 Jan 1984 05:00:00 GMT""
    &lt;/ifModule&gt;
&lt;/filesMatch&gt;


Some behaviors based on just prying around and facts:


Whenever I execute sudo service apache2 restart, following it everything works fine. Some other posts mentioned that there may be processes that are started that are causing this?
I'm using SQLAlchemy as my ORM, so at one point I thought that maybe it was SQLAlchemy caching information, so following every commit I tried db.session.close() so that the connection would be fresh every time I queried.


Can anyone give me a hand? Like many folks learning Flask, I'm a beginner at web application development.

Many thanks!

EDIT: 


When I run the application by just doing python init.py, it works perfectly fine.
I receive intermittent HTTP 500 errors when refreshing the page at a quick pace.


EDIT 2:
Here's a sample of a route that I'm POSTing to, following which I'm redirecting to itself and displaying the GET option to the user. I would expect that after committing data, the subsequent query should include it, but many times the web application doesn't show the most recently added data. On multiple refreshes, it'll sometimes show and sometimes disappear. Very strange:

@app.route('/projects', methods=['GET','POST'])
@login_required
def projects():
    user = return_current_user()
    if request.method == 'POST':
            if request.form['title'] and request.form['description']:
                    project = Projects(request.form['title'], request.form['description'])
                    if project.title != None and project.description != None:
                            user.projects.append(project)
                            db.session.commit()

            return redirect('/projects')
    elif request.method == 'GET':
            if 'clear-page' in request.args:
                    return redirect('/projects')
            elif 'query' in request.args:
                    projects = Projects.query.filter(Projects.title.like('%'+request.args['query']+'%')).order_by(Projects.timestamp.desc()).all()

            else:
                    projects = Projects.query.order_by(Projects.timestamp.desc()).all()

            return render_template('projects.html', title=""Projects"", user=user, projects=projects)

","I actually figured this out - a solve was to add db.session.commit() before the query. Not sure if it's the ""correct"" way to approach but it does the trick for my small-scale application!
","You might try switching off all the special changes you have made on the apparent assumption that some caching mechanism is responsible for the observed results, and show some of the code of your flask site. Then start to gather documented evidence of what is actually happening.
",,false,
https://stackoverflow.com/questions/53164153,false,"The issue does not meet the criteria for deeper analysis as it is related to different drivers and their specific behavior, rather than an API exhibiting unexpected failures or unpredictable behaviors.",,,,,,,why should we set the local_infile=1 in sqlalchemy to load local file? Load file not allowed issue in sqlalchemy,"I am using sqlalchemy to connect to MySQL database and found a strange behavior.
If I query 

LOAD DATA LOCAL INFILE 
'C:\\\\Temp\\\\JaydenW\\\\iata_processing\\\\icer\\\\rename\\\\ICER_2017-10- 
12T09033
7Z023870.csv    


It pops an error:

sqlalchemy.exc.InternalError: (pymysql.err.InternalError) (1148, u'The used 
command is not allowed with this MySQL versi
on') [SQL: u""LOAD DATA LOCAL INFILE 
'C:\\\\Temp\\\\JaydenW\\\\iata_processing\\\\icer\\\\rename\\\\ICER_2017-10- 
12T090337Z023870.csv' INTO TABLE genie_etl.iata_icer_etl LINES TERMINATED BY 
'\\n' 
IGNORE 1 Lines   (rtxt);""] (Background on this error at: 
http://sqlalche.me/e/2j85)


And I find the reason is that:
I need to set the parameter as

args = ""mysql+pymysql://""+username+"":""+password+""@""+hostname+""/""+database+""? 
local_infile=1""


If I use MySQL official connection library. I do not need to do so.

myConnection = MySQLdb.connect(host=hostname, user=username, passwd=password, db=database)


Can anyone help me to understand the difference between the two mechanisms?
","The reason is that the mechanisms use different drivers.
In SQLAlchemy you appear to be using the pymysql engine, which uses the PyMySQL Connection class to create the DB connection. That one requires the user to explicitly pass the local_infile parameter if they want to use the LOAD DATA LOCAL command.

The other example uses MySQLdb, which is basically a wrapper around the MySQL C API (and to my knowledge not the official connection library; that would be MySQL Connector Python, which is also available on SQLAlchemy as mysqlconnector). This one apparently creates the connection in a way that the LOAD DATA LOCAL is enabled by default.
",,,false,
https://stackoverflow.com/questions/64855545,false,"The issue does not meet the criteria for deeper analysis as it is related to the expected behavior of the rowcount attribute in SQLAlchemy, rather than an API exhibiting unexpected failures or unpredictable behaviors.",,,,,,,SQLalchemy rowcount always -1 for statements,"I was playing around with SQLalchemy and Microsoft SQL Server to get a hang of the functions when I came across a strange behavior. I was taught that the attribute rowcount on the result proxy object will tell how many rows were effected by executing a statement. However, when I select or insert single or multiple rows in my test database, I always get -1. How could this be and how can I fix this to reflect the reality?
connection = engine.connect()
metadata = MetaData()

# Ex1: select statement for all values
student = Table('student', metadata, autoload=True, autoload_with=engine)
stmt = select([student])
result_proxy = connection.execute(stmt)
results = result_proxy.fetchall()
print(result_proxy.rowcount)

# Ex2: inserting single values
stmt = insert(student).values(firstname='Severus', lastname='Snape')
result_proxy = connection.execute(stmt)
print(result_proxy.rowcout)
 
# Ex3: inserting multiple values 
stmt = insert(student)
values_list = [{'firstname': 'Rubius', 'lastname': 'Hagrid'},
               {'firstname': 'Minerva', 'lastname': 'McGonogall'}]
result_proxy = connection.execute(stmt, values_list)
print(result_proxy.rowcount)

The print function for each block seperately run example code prints -1. The Ex1 successfully fetches all rows and both insert statements successfully write the data to the database.
According to the following issue, the rowcount attribute isn't always to be trusted. Is that true here as well? And when, how can I compensate with a Count statement in a SQLalcehmy transaction?
PDO::rowCount() returning -1
","The single-row INSERT … VALUES ( … ) is trivial: If the statement succeeds then one row was affected, and if it fails (throws an error) then zero rows were affected.
For a multi-row INSERT simply perform it inside a transaction and rollback if an error occurs. Then the number of rows affected will either be zero or len(values_list).
To get the number of rows that a SELECT will return, wrap the select query in a SELECT count(*) query and run that first, for example:
select_stmt = sa.select([Parent])
count_stmt = sa.select([sa.func.count(sa.text(""*""))]).select_from(
    select_stmt.alias(""s"")
)
with engine.connect() as conn:
    conn.execution_options(isolation_level=""SERIALIZABLE"")
    rows_found = conn.execute(count_stmt).scalar()
    print(f""{rows_found} row(s) found"")
    results = conn.execute(select_stmt).fetchall()
for item in results:
    print(item.id)

",,,false,
https://stackoverflow.com/questions/65100066,true,"The issue involves the contains_eager functionality of SQLAlchemy, which exhibits unexpected behavior when objects are already loaded in an existing session. This API is designed to load related objects eagerly, but it does not handle the case where the objects are already present in the session and need to be filtered out.",,,,,,,SQLAlchemy loads unrelated cached objects when using contains_eager,"I'm using the contains_eager functionality of SQLAlchemy and I'm seeing strange behavior when objects are already loaded in an existing session. Specifically, it seems that those objects are not filtered out of the relationship collection as they would be when loading data fresh.
Here is a minimal example. The steps are

Create a parent-child relationship.
Add a parent and two children with different values.
Perform a joined, filtered query, using contains_eager to load matching children for the parent. Note that the filter should exclude one of the two children.
Observe that both children have been populated on the children property of the resulting object.
The correct results can be obtained by using a new session, or even by calling session.expire_all(), which indicates that the issue is that the children already exist in the current session.

Is this the expected behavior? And if so, is calling expire_all the right thing to do to avoid this?
More generally, should contains_eager be avoided because of this? It seems like a break in the abstraction if one has to keep track of whether or not a child object already exists before issuing a query. But maybe I am missing something.
from sqlalchemy import and_, Column, create_engine, DateTime, ForeignKey, Integer, String
from sqlalchemy.ext.declarative import declarative_base
from sqlalchemy.orm import contains_eager, relationship, sessionmaker


create_statements = [""""""
    DROP TABLE IF EXISTS child;
    """""", """"""
    DROP TABLE IF EXISTS parent;
    """""", """"""
    CREATE TABLE parent
    (
        id INTEGER NOT NULL PRIMARY KEY,
        name VARCHAR
    );
    """""", """"""
    CREATE TABLE child
    (
        id INTEGER NOT NULL PRIMARY KEY,
        parent_id INTEGER REFERENCES parent(id),
        value INTEGER
    );
    """"""
]

Base = declarative_base()


class Parent(Base):
    __tablename__ = ""parent""
    __table_args__ = {'implicit_returning': False}
    id = Column(Integer, primary_key=True)
    name = Column(String)

    children = relationship(""Child"", back_populates=""parent"")


class Child(Base):
    __tablename__ = ""child""
    __table_args__ = {'implicit_returning': False}
    id = Column(Integer, primary_key=True)
    parent_id = Column(Integer, ForeignKey(Parent.id))
    value = Column(Integer)

    parent = relationship(Parent, back_populates=""children"")


if __name__ == ""__main__"":
    engine = create_engine(f""sqlite:///"")
    session = sessionmaker(bind=engine)()

    for statement in create_statements:
        session.execute(statement)

    p1 = Parent(id=1, name=""A"")
    c1 = Child(id=1, parent=p1, value=10)
    c2 = Child(id=2, parent=p1, value=20)
    session.add_all([p1, c1, c2])
    session.flush()
    # session.expire_all()  # Uncommenting this makes the below work as expected.

    results = session \
        .query(Parent) \
        .join(Child, Parent.id == Child.parent_id) \
        .options(
            contains_eager(Parent.children)
        ).filter(Child.value &lt; 15) \
        .order_by(Parent.id) \
        .all()

    print(len(results[0].children))  # We should only have 1 child.
    print(all(c.value &lt; 15 for c in results[0].children))  # All children should match the above filter condition.

","I asked this question on the SQLAlchemy GitHub page. The solution is to use populate_existing on any query that uses contains_eager and filter. In my specific example, this query does the right thing
session \
        .query(Parent) \
        .join(Child, Parent.id == Child.parent_id) \
        .options(
            contains_eager(Parent.children)
        ).filter(Child.value &lt; 15) \
        .order_by(Parent.id) \
        .populate_existing() \
        .all()

",,,false,
https://stackoverflow.com/questions/71445447,true,"The issue involves the behavior of the SQLAlchemy API when using the `lazy='joined'` option in a one-to-one relationship. The API does perform a join for the parent query, but it does not remember the joined results, leading to additional SQL statements being executed to retrieve the related data.",SQLAlchemy,db.relationship,"When using the `lazy='joined'` option in a one-to-one relationship, SQLAlchemy does not eagerly load the related data during the initial query. Instead, it executes additional SQL statements to retrieve the related data when accessed later.",The `lazy='joined'` option is applied to a one-to-one relationship in SQLAlchemy.,"The issue occurs when accessing the related data of a `lazy='joined'` relationship, causing additional SQL statements to be executed.",This issue might be challenging to detect for users who expect the `lazy='joined'` option to eagerly load the related data during the initial query.,"How can I access the joined results of lazy=’joined’, without executing a second SQL statement, or changing the parent query","Sqlalchemy lazy=’joined’ performs a join for a simple parent query, but does not seem to remember what was joined.
I have this simple one-to-one relationship defined:
class User(Base):
    __tablename__ = 'user'

    email = db.Column(db.Unicode(255), nullable=False, server_default=u'', unique=True)
    password = db.Column(db.String(255), nullable=False, server_default='')
    ...
    userprofile = db.relationship(""Userprofile"",  
                                    uselist=False,
                                    backref=db.backref('userprofile', lazy='joined', innerjoin=True), 
                                    passive_deletes=True)   


class Userprofile(Base):
    __tablename__ = 'userprofile'
    user_id = db.Column(db.Integer, db.ForeignKey('user.id', ondelete='CASCADE'))
    first_name = db.Column(db.Unicode(100), nullable=False, server_default=u'')
    last_name = db.Column(db.Unicode(100), nullable=False, server_default=u'')
    ...
    user = db.relationship(""User"", uselist=False, backref=db.backref('user', lazy='joined', innerjoin=True))

I know the userprofile relationship may have ‘too many’ options specified but I have tried everything I can think of.
As part of a 3rd party user management package, this query is executed for each web page, in order to get the user making the request, resulting in this SQL:
user = User.query.filter(User.id == user_id).one()

sqlalchemy.engine.Engine - INFO - SELECT  user.id AS user_id, user.email AS user_email, user.password AS user_password, userprofile_1.id AS userprofile_1_id, userprofile_1.user_id AS userprofile_1_user_id, userprofile_1.first_name AS userprofile_1_first_name, userprofile_1.last_name AS userprofile_1_last_name ...
FROM user JOIN userprofile AS userprofile_1 ON user.id = userprofile_1.user_id 
WHERE user.id = ?


Then, in the same view, when I want to access a field from user profile, this SQL is executed:
user.userprofile.first_name
  
sqlalchemy.engine.Engine - INFO - SELECT userprofile.id AS userprofile_id,  userprofile.user_id AS userprofile_user_id, userprofile.first_name AS userprofile_first_name, userprofile.last_name AS userprofile_last_name  ...
FROM userprofile 
WHERE ? = userprofile.user_id


Which to me is very strange.  The first query has the userprofile fields already, so why the second SQL statement?
I can’t really change the user query to add something like ‘contains_eager’ to the query, so that approach is not an option.  Also, sqlalchemy complains if I try to use lazy='dynamic' for a one-to-one relationship.
I have 2 questions then:

what can I do to the table definitions, if anything, to eliminate the second SQL statement?  Again, changing the User query is not an option.

Any idea why ‘contains_eager’ is not the default behavior for lazy=’joined’?  It seems like that should be the default.  Or is there an option in the db.relationsip function to request eager loading?


","Gord Thompson solved it for me.
Since I posted the original code, I thought it might be helpful to post the modified code that worked.
class User(Base, UserMixin):
    __tablename__ = 'user'

    email = db.Column(db.Unicode(255), nullable=False, server_default=u'', unique=True)

    userprofile = db.relationship(
        ""Userprofile"",
        back_populates=""user"",
        uselist=False,
        lazy=""joined"",
        innerjoin=True,
        passive_deletes=True)   

class Userprofile(Base):
    __tablename__ = 'userprofile'
    user_id = db.Column(db.Integer, db.ForeignKey('user.id', ondelete='CASCADE'))
    first_name = db.Column(db.Unicode(100), nullable=False, server_default=u'')
    last_name = db.Column(db.Unicode(100), nullable=False, server_default=u'')

user = db.relationship(""User"", back_populates=""userprofile"", uselist=False)



",,,false,
https://stackoverflow.com/questions/19970809,true,The issue involves a version compatibility problem between SQLAlchemy and the cx_Oracle library. The specific version of cx_Oracle being used (5.0.2 10g) is causing an error when executing the `first()` method on a query.,SQLAlchemy,first,"When using the `first()` method on a query with cx_Oracle version 5.0.2 10g, a DatabaseError with the message ""ORA-01036: illegal variable name/number"" is raised.",The `first()` method is used on a query with a compatible version of cx_Oracle.,The issue is triggered when using the `first()` method on a query with cx_Oracle version 5.0.2 10g.,This issue might be challenging to detect for users who are not aware of the version compatibility problem between SQLAlchemy and cx_Oracle.,SQLAlchemy: Unable to get the first item of query,"I'm currently learning SQLAlchemy, and i found this strange thing. I was experimenting with a table which stores a person's name and address, and to get them i use this:

session.query(User)


And to get the first item, i tried:

session.query(User).first()


Which throws a DatabaseError:

Traceback (most recent call last):
  File ""&lt;pyshell#4&gt;"", line 1, in &lt;module&gt;
    session.query(User).first()
  File ""build\bdist.win32\egg\sqlalchemy\orm\query.py"", line 2275, in first
    ret = list(self[0:1])
  File ""build\bdist.win32\egg\sqlalchemy\orm\query.py"", line 2142, in __getitem__
    return list(res)
  File ""build\bdist.win32\egg\sqlalchemy\orm\query.py"", line 2346, in __iter__
    return self._execute_and_instances(context)
  File ""build\bdist.win32\egg\sqlalchemy\orm\query.py"", line 2361, in _execute_and_instances
    result = conn.execute(querycontext.statement, self._params)
  File ""build\bdist.win32\egg\sqlalchemy\engine\base.py"", line 664, in execute
    return meth(self, multiparams, params)
  File ""build\bdist.win32\egg\sqlalchemy\sql\elements.py"", line 272, in _execute_on_connection
    return connection._execute_clauseelement(self, multiparams, params)
  File ""build\bdist.win32\egg\sqlalchemy\engine\base.py"", line 761, in _execute_clauseelement
    compiled_sql, distilled_params
  File ""build\bdist.win32\egg\sqlalchemy\engine\base.py"", line 874, in _execute_context
    context)
  File ""build\bdist.win32\egg\sqlalchemy\engine\base.py"", line 1023, in _handle_dbapi_exception
    exc_info
  File ""build\bdist.win32\egg\sqlalchemy\util\compat.py"", line 185, in raise_from_cause
    reraise(type(exception), exception, tb=exc_tb)
  File ""build\bdist.win32\egg\sqlalchemy\engine\base.py"", line 867, in _execute_context
    context)
  File ""build\bdist.win32\egg\sqlalchemy\engine\default.py"", line 376, in do_execute
    cursor.execute(statement, parameters)
DatabaseError: (DatabaseError) ORA-01036: illegal variable name/number
 'SELECT test_user_uid, test_user_name, test_user_address \nFROM (SELECT test_user.""uid"" AS test_user_uid, test_user.name AS test_user_name, test_user.address AS test_user_address \nFROM test_user) \nWHERE ROWNUM &lt;= :ROWNUM_1' {'ROWNUM_1': 1}


However, i was able to retrieve what i wanted if i select all the rows, and loop through the query object:

users = [user for user in session.query(User)]
user1 = users[0]


That's all, i thought it's strange. Here's my mapping class:

class User(Base):
    __tablename__ = 'test_user'

    uid = Column(Integer, primary_key = True)
    name = Column(String(50))
    address = Column(String(100))

    def __repr__(self):
        return ""&lt;User (%s, %s)""%(self.name, self.address)


My best guess is that Session.query().first() is looking for the first row, with the generated query. However, the working method retrieves all the rows, and select the first one in Python. The problem is clearly from the generated query (invalid query). The main question is, what caused SQLAlchemy to create an invalid query?

Also, i noticed that SQLAlchemy makes things more difficult by making a query with sub-query. Is that behavior intended?

I hope i can get a satisfying answer, thanks!
","Well, it didn't take me long to realize this. It turns out that it's a version problem, i was previously using cx_Oracle version 5.0.2 10g, i tried to upgrade it to version 5.1.2 10g, and things works fine. 

This is probably an undocumented bug in SQLAlchemy, i can't find a place where they mention it. 

Conclusion: If you want to use the latest version of SQLAlchemy (0.9.0b1) with Oracle 10g, you shouldn't use cx_Oracle older than version 5.1.2 10g.

Hope this helps, and thanks for reading the question!
",,,false,
https://stackoverflow.com/questions/51361689,false,The issue does not involve an API exhibiting unexpected failures or unpredictable behaviors under specific runtime conditions. It appears to be a problem related to database integrity constraints and the usage of SQLAlchemy in a Flask project.,,,,,,,FLASK After commit causes odd IntegrityError violates unique constraint,"UPDATE / CLARIFICATION

I confirmed that this strange behavior only occurs on the macOS machine, moving everything to a windows machine (using sqlite and doing a fresh init and migrate) doesn't cause the error... doing the same on my High Sierra box does cause the odd error. 

Is anyone familiar with some known difference between sqlalchemy on Windows and macOS that might help?



Short version... I'm getting an integrity error (unique constraint) after I try to commit ANY entry to the DB, even if there are NO EXISTING entries at all in the table... why?

DETAILS

I've built a FLASK project (roughly based on the Miguel Grinberg Flask Maga Tutorial) using postgresql and sqlalchemy, the front-end has a page to register a user with a confirmation email (which works fine)...  to save time I've written a route (see below) which pre-loads a confirmed user to the Users database, this user is the ONLY user in the Users table and I only visit the route ONE TIME. 

After a successful commit I get an IntegrityError ""duplicate key value violates unique constraint"". This route only adds ONE user to an existing EMPTY Users table. The data IS successfully saved to the DB, the user can log in, but an error gets thrown. I get a similar error (see below) but am focusing on this route as an example because it is shorter than other views I've written.

EXAMPLE OF ROUTE CAUSING UNIQUE CONSTRAINT ERROR 

@main.route('/popme')
#@login_required
def popme():
    ## add user
    u1 = User()
    u1.email = 'user@domain.com'
    u1.username = 'someuser'
    u1.password_hash = 'REMOVED'
    u1.confirmed = '1'
    u1.role_id = 3
    u1.name = 'Some User'
    db.session.add(u1)
    db.session.commit()
    flash('User someuser can now login!')
    return redirect(url_for('main.index'))


I only started getting this error after moving the entire project from a Windows machine to a MacOS machine. I'm running Python 3.6 in a virtual environment, this error occurs if I'm using sqlite3 or postgresql.

I've written a much longer route which pre-fills in about 20 other tables successfully (does on commit() at the end, all data IS stored in the DB), however I get an IntegrityError ""duplicate key value violates unique constraint"" every time for a seemingly random entry. I've destroyed the DB, done an init, migrated... each time when the commit() is called a IntegrityError is thrown, each time on a different table, there is no apparent reasoning. 

BELOW IS USER MODEL

class User(UserMixin, db.Model):
    __tablename__ = 'users'
    id = db.Column(db.Integer, primary_key=True)
    email = db.Column(db.String(64), unique=True, index=True)
    username = db.Column(db.String(64), unique=True, index=True)
    password_hash = db.Column(db.String(128))
    confirmed = db.Column(db.Boolean, default=False)
    role_id = db.Column(db.Integer, db.ForeignKey('roles.id'))
    name = db.Column(db.String(64))
    last_seen = db.Column(db.DateTime(), default=datetime.utcnow)

    def ping(self):
        self.last_seen = datetime.utcnow()
        db.session.add(self)

    @property
    def password(self):
        raise AttributeError('password is not a readable attribute')

    @password.setter
    def password(self, password):
        self.password_hash = generate_password_hash(password)

    def verify_password(self, password):
        return check_password_hash(self.password_hash, password)

    def generate_confirmation_token(self, expiration=3600):
        s = Serializer(current_app.config['SECRET_KEY'], expiration)
        return s.dumps({'confirm': self.id})

    def confirm(self, token):
        s = Serializer(current_app.config['SECRET_KEY'])
        try:
            data = s.loads(token)
        except:
            return False
        if data.get('confirm') != self.id:
            return False
        self.confirmed = True
        db.session.add(self)
        return True

    def generate_reset_token(self, expiration=3600):
        s = Serializer(current_app.config['SECRET_KEY'], expiration)
        return s.dumps({'reset': self.id})

    def reset_password(self, token, new_password):
        s = Serializer(current_app.config['SECRET_KEY'])
        try:
            data = s.loads(token)
        except:
            return False
        if data.get('reset') != self.id:
            return False
        self.password = new_password
        db.session.add(self)
        return True

    def generate_email_change_token(self, new_email, expiration=3600):
        s = Serializer(current_app.config['SECRET_KEY'], expiration)
        return s.dumps({'change_email': self.id, 'new_email': new_email})

    def change_email(self, token):
        s = Serializer(current_app.config['SECRET_KEY'])
        try:
            data = s.loads(token)
        except:
            return False
        if data.get('change_email') != self.id:
            return False
        new_email = data.get('new_email')
        if new_email is None:
            return False
        if self.query.filter_by(email=new_email).first() is not None:
            return False
        self.email = new_email
        db.session.add(self)
        return True

    def can(self, permissions):
        return self.role is not None and (self.role.permissions &amp; permissions) == permissions

    def is_administrator(self):
        return self.can(Permission.ADMINISTRATOR)

    def __init__(self, **kwargs):
        super(User, self).__init__(**kwargs)
        if self.role is None:
            if self.email == current_app.config['FLASKY_ADMIN']:
                self.role = Role.query.filter_by(permissions=0xff).first()
            if self.role is None:
                self.role = Role.query.filter_by(default=True).first()

    def __repr__(self):
        return '&lt;User %r&gt;' % self.username


I've tried Sql-alchemy Integrity error but its my understanding that sqlalchemy does auto-increment primary keys. 

UPDATED INTEGRITY ERROR

sqlalchemy.exc.IntegrityError: (psycopg2.IntegrityError) duplicate key value violates unique constraint ""ix_users_email""
DETAIL:  Key (email)=(worldbmd@gmail.com) already exists.
 [SQL: 'INSERT INTO users (email, username, password_hash, confirmed, role_id, name, last_seen) VALUES (%(email)s, %(username)s, %(password_hash)s, %(confirmed)s, %(role_id)s, %(name)s, %(last_seen)s) RETURNING users.id'] [parameters: {'email': 'user@domain.com', 'username': 'someuser', 'password_hash': 'REMOVED', 'confirmed': '1', 'role_id': 1, 'name': 'Some User', 'last_seen': datetime.datetime(2018, 7, 16, 17, 27, 13, 451593)}]

","I also got into the same problem..as Joost said flask app runs twice. So we need to set it to run only once. We can achieve it by adding use_reloader=False like:
if __name__ = ""__main__"":
    app.run(debug=True, use_reloader=False)

or
we can directly set debug=False,
if __name__ = ""__main__"":
    app.run(debug=False)

","The integrity error is caused by trying to add a blister to the blisters table with a non unique property. I think your model looks something like this:

class Blister(db.Model):
    __tablename__ = 'blisters'
    id = db.Column(db.Integer, primary_key=True)
    name= db.Column(db.String(64), unique=True)
    notes= db.Column(db.String(64))
    cost= db.Column(db.Float)


And you're trying to add a blister with a name Small round dome which is already in the blisters table in the database, therefore causing the Integrity Error.
",,false,
https://stackoverflow.com/questions/61943730,false,The issue does not meet the criteria for deeper analysis as it does not involve an API exhibiting unexpected failures or unpredictable behaviors under specific runtime conditions.,,,,,,,Fields of SQLAlchemy models filled after creating new object,"I am creating a Python application using the Flask framework with SQLAlchemy and now struggling with a strange behavior on creating new model objects.

Lets say I have a model Offer which has some fields, a private property __validation_errors and a public method validate:

from app import db

class Offer(db.Model):
    currency = db.Column(db.String(255))
    price = db.Column(db.Numeric(20, 5))
    __validation_errors = []

    def validate(self, errors: List[Exception]) -&gt; bool:
        self.validate_not_empty()
        self.validate_price()
        for err in self.__validation_errors:
            errors.append(err)
        if len(self.__validation_errors) &gt; 0:
            return False
        return True


the object itself is created in a service class by calling

offer = Offer(currency, price)


and providing data received with a POST request in a router.
If the validation fails, an error message is returned to the user and the object is not written into DB.

And here begins the strange part. If a request with invalid data is received and another request with correct data is sent, the values in the __validation_errors list don't disappear in the offer object, like if the object is reused and not reset. If another invalid request is made, the errors are just appended to the already existing in the list. Of course, it can be fixed by setting __validation_errors = [] every time the validate() method is called, but I would like to understand what is happening here. Do I miss some SQLAlchemy specific features? The app.db object and the Session are created on app and exist all the time.
","So the trouble here is that __validation_errors is bound to the class Offer, rather than an instance of Offer, which means that unless each instance sets its own self.__validation_errors, self.__validation_errors just maps to Offer.__validation_errors for every instance (kind of like accessing an attribute/function of a subclass that only exists on the parent class).

Take for example:

&gt;&gt;&gt; class SomeClass:
...     some_list = []
... 
&gt;&gt;&gt; a = SomeClass()
&gt;&gt;&gt; b = SomeClass()
&gt;&gt;&gt; a.some_list.append(5)
&gt;&gt;&gt; a.some_list
[5]
&gt;&gt;&gt; b.some_list
[5]
&gt;&gt;&gt; a.some_list is b.some_list
True


This is because anything in the class body is an attribute of the class itself (this makes a little more sense considering the fact that methods are also class attributes - unlike in prototype-based languages), rather than a value each instance is assigned after it is created

&gt;&gt;&gt; class SomeClass:
...     x = [1, 2, 3, 4, 5]
...
&gt;&gt;&gt; SomeClass.x
[1, 2, 3, 4, 5]
&gt;&gt;&gt; SomeClass().x
[1, 2, 3, 4, 5]
&gt;&gt;&gt; SomeClass.x is SomeClass().x
True


If you want to bind the attribute to the instance rather than the class, you attach it to self, preferably in __init__

class SomeClass:
    def __init__(self):
        self.x = 5




PS, it's worth noting that this is only really noticeable when you're working with mutable attributes. With immutable attributes, any ""change"" will create a new value on the object, and since &lt;instance&gt;.&lt;attribute&gt; is retrieved instead of &lt;instance&gt;.__class__.&lt;attribute&gt; whenever it exists, it will appear that the instance was separate from the class to begin with.



So, back to your example, I'd suggest initializing __validation_errors at the same time you __init__ialize the rest of the object:

class Offer(db.Model):
    ...
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.__validation_errors = []

    def validate(self, errors: List[Exception]) -&gt; bool:
        ...

",,,false,
https://stackoverflow.com/questions/64770459,false,The issue does not meet the criteria for deeper analysis as it is related to circular import errors rather than API-related problems.,,,,,,,"In Python/Flask, why would &quot;from models import Result&quot; cause a circular import error, while &quot;from models import *&quot; and &quot;import models&quot; both work?","I'm working through the Flask By Example tutorial, and I'm running into a circular import error when setting up the database through SQLAlchemy. When other people run into this problem, it seems to be because of a misplaced import statement in app.py. In my case, the error seems to depend on the way I'm importing the database models instead, which I don't understand. import models and from models import * both work, but from models import Result fails with the following error message:
Traceback (most recent call last):
  File ""app.py"", line 13, in &lt;module&gt;
    from models import Result
  File ""/mnt/c/users/power/desktop/projects/fbe/models.py"", line 1, in &lt;module&gt;
    from app import db
  File ""/mnt/c/users/power/desktop/projects/fbe/app.py"", line 13, in &lt;module&gt;
    from models import Result
ImportError: cannot import name 'Result' from partially initialized module 'models' (most likely due to a circular import) (/mnt/c/users/power/desktop/projects/fbe/models.py)

Any ideas why this might be happening? It seems like there may be some nuances to how Python does imports that I'm unaware of. It's also strange that my code is pretty much identical to the provided code, and and yet I still get an error. Could the behavior with newer versions of Python have changed? I highly doubt it, but for what it's worth I'm using 3.8.5.
My code is below. All files live in the main app directory.
app.py
from flask import Flask
from flask_sqlalchemy import SQLAlchemy
from flask_migrate import Migrate

app = Flask(__name__)
app.config.from_pyfile('config.py')
app.config.from_pyfile('instance/prod-config.py', silent=True)
app.config['SQLALCHEMY_TRACK_MODIFICATIONS'] = False

db = SQLAlchemy(app)
migrate = Migrate(app, db)

### this works ###
import models

### this works too ###
# from models import *

### this fails with the above error ###
# from models import Result

@app.route('/')
def hello():
    return 'Hello world!'

@app.route('/&lt;name&gt;')
def hello_personalized(name):
    return ('Hello ' + name)

if __name__ == '__main__':
    app.run()

models.py
from app import db
from sqlalchemy.dialects.postgresql import JSON

class Result(db.Model):
    __tablename__ = 'results'

    id = db.Column(db.Integer, primary_key=True)
    url = db.Column(db.String())
    result_all = db.Column(JSON)
    result_no_stop_words = db.Column(JSON)

    def __init__(self, url, result_all, result_no_stop_words):
        self.url = url
        self.result_all = result_all
        self.result_no_stop_words = result_no_stop_words

    def __repr__(self):
        return '&lt;id {}&gt;'.format(self.id)

config.py
DEBUG = True
TESTING = True
CSRF_ENABLED = True
SECRET_KEY = 'needs-to-be-changed'
SQLALCHEMY_DATABASE_URI = 'postgresql://username:password@localhost/fbe'

","Tbh, neither of those examples should work. You clearly have a circular import between those files: from app import db and from models import Result.
My guess here is that when you're using import models or even from models import *, you're importing from another package called models that exists on your python environment.
My solution would be to use separation of concerns and remove db initialization from your Flask view declarations. You can take a look on one of the many possible solutions here.
",,,false,
https://stackoverflow.com/questions/65742509,false,The issue does not meet the criteria for deeper analysis as it is related to mapping classes to tables using SQLAlchemy's declarative_base and does not involve an API exhibiting unexpected failures or unpredictable behaviors under specific runtime conditions.,,,,,,,How to get list of objects from multi-value field with SqlAlchemy using ORM?,"I have MS Access DB file (.accdb) from my client and need to describe tables and columns with declarative_base class. As I can see in table constructor - one of column has Integer value and has relationship ""one-to-many"" with another column in some another table (foreign key).
But actually in this foreign key stored not single Integer value, but string with number values separated with semicolons. This technique called as ""multi-value fields"". In fact this is ""many-to-many"" relationship without associative tables.
Very simplified scheme:
Persons
-------------
id - Integer
name - String
vacancy_id - Integer (multi-value, Foreign key of Vacancies.id)

Vacancies
-------------
id - Integer
vacancy_name - String

I tried to map classes to tables using declarative_base parent class. But can't find how to declare ""many-to-many"" relationship without associative table. Now I have such code.
Base = declarative_base()


class Vacancy(Base):
    __tablename__ = 'Vacancies'
    id = sa.Column(sa.Integer, name='id', primary_key=True, autoincrement=True)
    vacancy_name = sa.Column(sa.String(255), name='vacancy_name')


class Person(Base):
    __tablename__ = 'Persons'
    id = sa.Column(sa.Integer, name='id', primary_key=True, autoincrement=True)
    name = sa.Column(sa.String(255), name='name')
    vacancy_id = sa.Column(sa.Integer, ForeignKey(Vacancy.id), name='vacancy_id')
    vacancies = relationship(Vacancy)

During request Person I have strange behavior:

If vacancy_id not specified, I get Person.vacancies as None.
If vacancy_id specified as single value (i.e. ""1""), in Person.vacancies I get single object of Vacancy class.
If vacancy_id specified as multiple value (i.e. ""1;2;3""), in Person.vacancies I also get None.

Of course I can request raw Person.vacancy_id, split it with semicolon, and make request to get Vacancies with list of ID's.
But I wonder - if SqlAlchemy can process ""multi-value fields""? And what the best way to work with such fileds?
UPDATE
At present I made following workaround to automatically parse multi-value columns. This should be added to Persons class:
@orm.reconstructor
def load_on_init(self):
    if self.vacancy_id:
        ids = self.vacancy_id.split(';')
        self.vacancies = [x for x in Vacancy.query.filter(Vacancy.id.in_(ids)).all()]
    else:
        self.vacancies = []

Vacancies class should have fllowing attribute:
query = DBSession.query_property()

Finally we have to prepare session for in-class usage:
engine = create_engine(CONNECTION_URI)
DBSession = scoped_session(sessionmaker(bind=engine))
Base = declarative_base()

","Access ODBC provides very limited support for multi-value lookup fields. Such fields are actually implemented using a hidden association table (with a name like f_1BC9E55B5578456EB5ACABC99BB2FF0B_vacancies) but those tables are not accessible from SQL statements:
SELECT * from f_1BC9E55B5578456EB5ACABC99BB2FF0B_vacancies

results in the error

The Microsoft Access database engine cannot find the input table or query ''. Make sure it exists and that its name is spelled correctly.

As you have discovered, Access ODBC will read the key values of the multiple entries and present them as a semicolon-separated list that we can parse, but we cannot update those values
UPDATE Persons SET vacancies = '1;2' WHERE id = 1

fails with

An UPDATE or DELETE query cannot contain a multi-valued field. (-3209)

So, TL;DR, if you only need to read from the database then your workaround may be sufficient, but if you need to modify those multi-valued fields then Access ODBC is not going to get the job done for you.
",,,false,
https://stackoverflow.com/questions/71630879,true,The issue involves unexpected behavior in SQLAlchemy's session.add() function when adding a new object to the database. The behavior is related to the handling of primary key values and the uniqueness constraint in the database.,SQLAlchemy,session.add,"When adding a new object to the database using session.add(), SQLAlchemy does not check if the primary key value already exists in the table. This leads to a duplicate key violation error when trying to insert a new object with an existing primary key value.",The session.add() function works as expected when adding new objects with unique primary key values.,The issue is triggered when trying to add a new object with a primary key value that already exists in the table.,"This issue might be challenging to detect during development and testing because SQLAlchemy does not perform a check for duplicate primary key values by default, assuming that the user will handle this validation separately.",Strange bevaiour of session.add in sqhalchemy (duplicate key value violates unique constraint in session.add)),"I encountered strange behaviour of SQLAlchemy
I tried to add a new object to the DB. This is done in the function add_tag()
class News(Base):
    __tablename__ = ""news""

    id = Column(Integer, primary_key=True)

    title = Column(String)
    description = Column(String)
    
    def add_tag(self, tag_name, session):
        logging.debug(""adding news tags   "" + tag_name)
        tag = session.query(Tag).filter_by(name=tag_name).first()
        logging.debug(tag)
        if tag:
            nt = NewsTags()
            nt.tag_id = tag.id
           self.tags.append(nt)
        else:
            new_tag = Tag()
            logging.debug('Creating a new tag:  ' + tag_name)
            new_tag.name = tag_name
            session.add(new_tag)
            session.commit()
            nt = NewsTags()
            nt.tag_id = new_tag.id
            self.tags.append(nt)


class NewsTags(Base):
    __tablename__ = ""news_tags""

    news_id = Column(ForeignKey(""news.id""), primary_key=True)
    tag_id = Column(ForeignKey(""tag.id""), primary_key=True)
    news = relationship(""News"", backref=""tags"")



class Tag(Base):

    __tablename__ = ""tag""
    id = Column(Integer, primary_key=True)
    name = Column(String(30))

    news = relationship(NewsTags, backref=""tag"", post_update=True)
 



I first check if the object already exists. If it does not, I try to create a new one and add it to the session (add it to the DB).
I already had some objects in the table (create by other means).
See a copy paste of values in table tag:
id, Name
1   Sports
2   Business
3   Finance
4   World
5   US
6   UK
7   Technology
8   Science
9   Health
10  Video Games
11  IT
12  Startups
13  Europe
14  Apps
15  Space

I think I might have added some the data to the DB manually (via SQL insert), but some of it was created through SQLAlchemy.
I came across very strange behavior when trying to add a new object.
First it tried to create an object with id=1 and failed as it was a duplicate. I did not save the stack trace, but I ran it again, it tried to add the object with id=2, here's the stack trace.
psycopg2.errors.UniqueViolation: duplicate key value violates unique constraint ""tag_pkey""
DETAIL:  Key (id)=(2) already exists.

Full stack trace:
DEBUG:root:adding news tags   AI
2022-03-28 06:52:33,188 INFO sqlalchemy.engine.Engine SELECT tag.id AS tag_id, tag.name AS tag_name 
FROM tag 
WHERE tag.name = %(name_1)s 
 LIMIT %(param_1)s
INFO:sqlalchemy.engine.Engine:SELECT tag.id AS tag_id, tag.name AS tag_name 
FROM tag 
WHERE tag.name = %(name_1)s 
 LIMIT %(param_1)s
2022-03-28 06:52:33,188 INFO sqlalchemy.engine.Engine [cached since 0.005467s ago] {'name_1': 'AI', 'param_1': 1}
INFO:sqlalchemy.engine.Engine:[cached since 0.005467s ago] {'name_1': 'AI', 'param_1': 1}
DEBUG:root:None
DEBUG:root:Creating a new tag:  AI
2022-03-28 06:52:33,191 INFO sqlalchemy.engine.Engine INSERT INTO tag (name) VALUES (%(name)s) RETURNING tag.id
INFO:sqlalchemy.engine.Engine:INSERT INTO tag (name) VALUES (%(name)s) RETURNING tag.id
2022-03-28 06:52:33,191 INFO sqlalchemy.engine.Engine [generated in 0.00015s] {'name': 'AI'}
INFO:sqlalchemy.engine.Engine:[generated in 0.00015s] {'name': 'AI'}
2022-03-28 06:52:33,192 INFO sqlalchemy.engine.Engine ROLLBACK
INFO:sqlalchemy.engine.Engine:ROLLBACK
Traceback (most recent call last):
  File ""/app/.heroku/python/lib/python3.9/site-packages/sqlalchemy/engine/base.py"", line 1808, in _execute_context
    self.dialect.do_execute(
  File ""/app/.heroku/python/lib/python3.9/site-packages/sqlalchemy/engine/default.py"", line 732, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.UniqueViolation: duplicate key value violates unique constraint ""tag_pkey""
DETAIL:  Key (id)=(2) already exists.


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File ""/app/assign.py"", line 59, in &lt;module&gt;
    main()
  File ""/app/assign.py"", line 46, in main
    parse_feeds(session)
  File ""/app/models/VitalNewsFeed.py"", line 77, in parse_feeds
    feed.parse_feed(session)
  File ""/app/models/VitalNewsFeed.py"", line 63, in parse_feed
    new_news = News(entry=e, source=self.source, tags=self.tags, session=session)
  File ""&lt;string&gt;"", line 4, in __init__
  File ""/app/.heroku/python/lib/python3.9/site-packages/sqlalchemy/orm/state.py"", line 480, in _initialize_instance
    manager.dispatch.init_failure(self, args, kwargs)
  File ""/app/.heroku/python/lib/python3.9/site-packages/sqlalchemy/util/langhelpers.py"", line 70, in __exit__
    compat.raise_(
  File ""/app/.heroku/python/lib/python3.9/site-packages/sqlalchemy/util/compat.py"", line 207, in raise_
    raise exception
  File ""/app/.heroku/python/lib/python3.9/site-packages/sqlalchemy/orm/state.py"", line 477, in _initialize_instance
    return manager.original_init(*mixed[1:], **kwargs)
  File ""/app/models/VitalNews.py"", line 178, in __init__
    self.add_tag(tag, session)
  File ""/app/models/VitalNews.py"", line 121, in add_tag
    session.commit()
  File ""/app/.heroku/python/lib/python3.9/site-packages/sqlalchemy/orm/session.py"", line 1431, in commit
    self._transaction.commit(_to_root=self.future)
  File ""/app/.heroku/python/lib/python3.9/site-packages/sqlalchemy/orm/session.py"", line 829, in commit
    self._prepare_impl()
  File ""/app/.heroku/python/lib/python3.9/site-packages/sqlalchemy/orm/session.py"", line 808, in _prepare_impl
    self.session.flush()
  File ""/app/.heroku/python/lib/python3.9/site-packages/sqlalchemy/orm/session.py"", line 3363, in flush
    self._flush(objects)
  File ""/app/.heroku/python/lib/python3.9/site-packages/sqlalchemy/orm/session.py"", line 3503, in _flush
    transaction.rollback(_capture_exception=True)
  File ""/app/.heroku/python/lib/python3.9/site-packages/sqlalchemy/util/langhelpers.py"", line 70, in __exit__
    compat.raise_(
  File ""/app/.heroku/python/lib/python3.9/site-packages/sqlalchemy/util/compat.py"", line 207, in raise_
    raise exception
  File ""/app/.heroku/python/lib/python3.9/site-packages/sqlalchemy/orm/session.py"", line 3463, in _flush
    flush_context.execute()
  File ""/app/.heroku/python/lib/python3.9/site-packages/sqlalchemy/orm/unitofwork.py"", line 456, in execute
    rec.execute(self)
  File ""/app/.heroku/python/lib/python3.9/site-packages/sqlalchemy/orm/unitofwork.py"", line 630, in execute
    util.preloaded.orm_persistence.save_obj(
  File ""/app/.heroku/python/lib/python3.9/site-packages/sqlalchemy/orm/persistence.py"", line 244, in save_obj
    _emit_insert_statements(
  File ""/app/.heroku/python/lib/python3.9/site-packages/sqlalchemy/orm/persistence.py"", line 1237, in _emit_insert_statements
    result = connection._execute_20(
  File ""/app/.heroku/python/lib/python3.9/site-packages/sqlalchemy/engine/base.py"", line 1620, in _execute_20
    return meth(self, args_10style, kwargs_10style, execution_options)
  File ""/app/.heroku/python/lib/python3.9/site-packages/sqlalchemy/sql/elements.py"", line 325, in _execute_on_connection
    return connection._execute_clauseelement(
  File ""/app/.heroku/python/lib/python3.9/site-packages/sqlalchemy/engine/base.py"", line 1487, in _execute_clauseelement
    ret = self._execute_context(
  File ""/app/.heroku/python/lib/python3.9/site-packages/sqlalchemy/engine/base.py"", line 1851, in _execute_context
    self._handle_dbapi_exception(
  File ""/app/.heroku/python/lib/python3.9/site-packages/sqlalchemy/engine/base.py"", line 2032, in _handle_dbapi_exception
    util.raise_(
  File ""/app/.heroku/python/lib/python3.9/site-packages/sqlalchemy/util/compat.py"", line 207, in raise_
    raise exception
  File ""/app/.heroku/python/lib/python3.9/site-packages/sqlalchemy/engine/base.py"", line 1808, in _execute_context
    self.dialect.do_execute(
  File ""/app/.heroku/python/lib/python3.9/site-packages/sqlalchemy/engine/default.py"", line 732, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.IntegrityError: (psycopg2.errors.UniqueViolation) duplicate key value violates unique constraint ""tag_pkey""
DETAIL:  Key (id)=(2) already exists.

[SQL: INSERT INTO tag (name) VALUES (%(name)s) RETURNING tag.id]
[parameters: {'name': 'AI'}]
(Background on this error at: https://sqlalche.me/e/14/gkpj)

I tried running the script again. Now it tired to create with id=3
sqlalchemy.exc.IntegrityError: (psycopg2.errors.UniqueViolation) duplicate key value violates unique constraint ""tag_pkey""
DETAIL:  Key (id)=(3) already exists.

[SQL: INSERT INTO tag (name) VALUES (%(name)s) RETURNING tag.id]
[parameters: {'name': 'AI'}]
(Background on this error at: https://sqlalche.me/e/14/gkpj)

id=4
and so on. Every time it failed and I had to run the script again. As if it did not know the last value of id in the table of Tags. After a few fails it created with id=9, which was not a duplicate and the script continued working fine. But the error was strange. Shouldn't it check if primary key is available before creating a new object?
Now the error does not reproduce on dev. But when I pushed my code to prod the same thing happens again.
I am using PostgreSQL with SQLAlchemy.
","This was not SQLAlchemy's fault. It was PostgresSQL and me inserting into the table with a PostgreSQL client Postico without auto-increment which broke the sequence.
Running SQL command of ""SELECT setval('tag_id_seq', (SELECT MAX(id) FROM tag), true);"" fixed it.
",,,false,
https://stackoverflow.com/questions/25768428,false,The errors experienced with SQLAlchemy and the connection/session are not directly related to an API exhibiting unexpected failures or unpredictable behaviors under specific runtime conditions. The issue seems to be more related to threading and connection management in the web server environment.,,,,,,,SQLAlchemy connection errors,"I'm experiencing some strange bugs which seem to be caused by connections used by Sqlalchemy, which i can't pin down exactly.. i was hoping someone has a clue whats going on here.

We're working on a Pyramid (version 1.5b1) and use Sqlalchemy (version 0.9.6) for all our database connectivity. Sometimes we get errors related to the db connection or session, most of the time this would be a cursor already closed or This Connection is closed error, but we get other related exceptions too:

(OperationalError) connection pointer is NULL
(InterfaceError) cursor already closed
Parent instance &lt;...&gt; is not bound to a Session, and no contextual session is established; lazy load operation of attribute '...' cannot proceed

A conflicting state is already present in the identity map for key (&lt;class '...'&gt;, (1001L,))
This Connection is closed (original cause: ResourceClosedError: This Connection is closed)
(InterfaceError) cursor already closed
Parent instance &lt;...&gt; is not bound to a Session; lazy load operation of attribute '...' cannot proceed
Parent instance &lt;...&gt; is not bound to a Session, and no contextual session is established; lazy load operation of attribute '...' cannot proceed
'NoneType' object has no attribute 'twophase'
(OperationalError) connection pointer is NULL
This session is in 'prepared' state; no further


There is no silver bullet to reproduce them, only by refreshing many times they are bound to happen one at some point. So i made a script using multi-mechanize to spam different urls concurrently and see where and when it happens.

It appears the url triggered doesn't really matter, the errors happen when there are concurrent requests that span a longer time (and other requests get served in between). This seems to indicate there is some kind of threading problem; that either the session or connection is shared among different threads.

After googling for these issues I found a lot of topics, most of them tell to use scoped sessions, but the thing is we do use them already:

db_session = scoped_session(sessionmaker(extension=ZopeTransactionExtension(), autocommit=False, autoflush=False))
db_meta = MetaData()



We have a BaseModel for all our orm objects:

BaseModel = declarative_base(cls=BaseModelObj, metaclass=BaseMeta, metadata=db_meta)
We use the pyramid_tm tween to handle transactions during the request
We hook db_session.remove() to the pyramid NewResponse event (which is fired after everything has run). I also tried putting it in a seperate tween running after pyramid_tm or even not doing it at all, none of these seem to have effect, so the response event seemed like the most clean place to put it.
We create the engine in our main entrypoint of our pyramid project and use a NullPool and leave connection pooling to pgbouncer. We also configure the session and the bind for our BaseModel here:

engine = engine_from_config(config.registry.settings, 'sqlalchemy.', poolclass=NullPool)
db_session.configure(bind=engine, query_cls=FilterQuery)
BaseModel.metadata.bind = engine
config.add_subscriber(cleanup_db_session, NewResponse)
return config.make_wsgi_app()
In our app we access all db operation using:

from project.db import db_session
...
db_session.query(MyModel).filter(...)
db_session.execute(...)
We use psycopg2==2.5.2 to handle the connection to postgres with pgbouncer in between
I made sure no references to db_session or connections are saved anywhere (which could result in other threads reusing them)


I also tried the spamming test using different webservers, using waitress and cogen i got the errors very easily, using wsgiref we unsurprisingly have no errors (which is singlethreaded). Using uwsgi and gunicorn (4 workers, gevent) i didn't get any errors.

Given the differences in the webserver used, I thought it either has to do with some webservers handling requests in threads and some using new processes (maybe a forking problem)? To complicate matters even more, when time went on and i did some new tests, the problem had gone away in waitress but now happened with gunicorn (when using gevent)! I have no clue on how to go debugging this...

Finally, to test what happens to the connection, i attached an attribute to the connection at the start of the cursor execute and tried to read the attribute out at the end of the execute:

@event.listens_for(Engine, ""before_cursor_execute"")
def _before_cursor_execute(conn, cursor, stmt, params, context, execmany):
  conn.pdtb_start_timer = time.time()

@event.listens_for(Engine, ""after_cursor_execute"")
def _after_cursor_execute(conn, cursor, stmt, params, context, execmany):
  print conn.pdtb_start_timer


Surprisingly this sometimes raised an exception: 'Connection' object has no attribute 'pdtb_start_timer'

Which struck me as very strange.. I found one discussion about something similar: https://groups.google.com/d/msg/sqlalchemy/GQZSjHAGkWM/rDflJvuyWnEJ
And tried adding strategy='threadlocal' to the engine, which from what i understand should force 1 connection for the tread. But it didn't have any effect on the errors im seeing.. (besides some unittests failing because i need two different sessions/connections for some tests and this forces 1 connection to be associated)

Does anyone have any idea what might go on here or have some more pointers on how to attack this problem?

Thanks in advance!

Matthijs Blaas
","Update: The errors where caused by multiple commands that where send in one prepared sql statement. Psycopg2 seems to allow this, but apparently it can cause strange issues. The PG8000 connector is more strict and bailed out on the multiple commands, sending one command fixed the issue!
",,,false,
https://stackoverflow.com/questions/14470688,true,The issue involves the SQLAlchemy API and its bidirectional relationship association proxy functionality.,SQLAlchemy,association_proxy,"The issue is related to using association proxies in SQLAlchemy bidirectional relationships. Specifically, when removing an object from the association proxy list, an integrity error occurs as SQLAlchemy tries to set one of the foreign key columns to null.",The SQLAlchemy association_proxy works as expected when adding objects to the association proxy list.,The issue is triggered when removing objects from the association proxy list.,This issue might be challenging to detect during development and testing if users are not familiar with the specific behavior and requirements of association proxies in SQLAlchemy bidirectional relationships.,SQLAlchemy Bidirectional Relationship association proxy,"Update:

For anyone having this issue, with the very latest SQLAlchemy this behaviour has been fixed.

Original issue:

I am having a problem with getting association proxies to update correctly.

Using the example models here: http://docs.sqlalchemy.org/en/rel_0_7/orm/extensions/associationproxy.html#simplifying-association-objects

But changing UserKeyword with this line:

keyword = relationship(""Keyword"", backref=backref(""user_keywords"", cascade=""all, delete-orphan""))


and adding this to Keyword:

users = association_proxy('user_keywords', 'user')


So a keyword instance has a list of users.

The following works as expected:

&gt;&gt;&gt; rory = User(""rory"")
&gt;&gt;&gt; session.add(rory)
&gt;&gt;&gt; chicken = Keyword('chicken')
&gt;&gt;&gt; session.add(chicken)
&gt;&gt;&gt; rory.keywords.append(chicken)
&gt;&gt;&gt; chicken.users
[&lt;__main__.User object at 0x1f1c0d0&gt;]
&gt;&gt;&gt; chicken.user_keywords
[&lt;__main__.UserKeyword object at 0x1f1c450&gt;]


But removals do strange things. Removing from the association proxy lists like so:

&gt;&gt;&gt; rory.keywords.remove(chicken)


Causes an integrity error as SA tries to set one of the foreign key columns to null.

Doing this:

&gt;&gt;&gt; rory.user_keywords.remove(rory.user_keywords[0])


Results in this:

&gt;&gt;&gt; chicken.users
[None]


I have missed something obvious haven't I?
","UserKeyword requires that it be associated with both a Keyword and User at the same time in order to be persisted.   When you associate it with a User and Keyword, but then remove it from the User.user_keywords collection, it's still associated with the Keyword.   

&gt;&gt;&gt; rory.keywords.remove(chicken)

# empty as we expect
&gt;&gt;&gt; rory.user_keywords
[]   

# but the other side, still populated.  UserKeyword 
# has no User, but still has Keyword
&gt;&gt;&gt; chicken.user_keywords
[&lt;__main__.UserKeyword object at 0x101748d10&gt;]

# but the User on that UserKeyword is None
&gt;&gt;&gt; chicken.user_keywords[0].user is None
True

# hence accessing the ""association"" gives us None
# as well
&gt;&gt;&gt; chicken.users
[None]


So if we were to flush() this right now, you've got a UserKeyword object ready to go but it has no User on it, so you get that NULL error.    At INSERT time, the object is not considered to be an ""orphan"" unless it is not associated with any Keyword.user_keywords or User.user_keywords collections.   Only if you were to say, del chicken.user_keywords[0] or equivalent, would you see that no INSERT is generated and the UserKeyword object is forgotten.

If you were to flush the object to the database before removing it from ""rory"", then things change.  The UserKeyword is now persistent, and when you remove ""chicken"" from ""rory.keywords"", a ""delete-orphan"" event fires off which does delete the UserKeyword, even though it still is associated with the Keyword object:

rory.keywords.append(chicken)

session.flush()

rory.keywords.remove(chicken)

session.flush()


you see the SQL:

INSERT INTO ""user"" (name) VALUES (%(name)s) RETURNING ""user"".id
{'name': 'rory'}

INSERT INTO keyword (keyword) VALUES (%(keyword)s) RETURNING keyword.id
{'keyword': 'chicken'}

INSERT INTO user_keyword (user_id, keyword_id, special_key) VALUES (%(user_id)s, %(keyword_id)s, %(special_key)s)
{'keyword_id': 1, 'special_key': None, 'user_id': 1}

DELETE FROM user_keyword WHERE user_keyword.user_id = %(user_id)s AND user_keyword.keyword_id = %(keyword_id)s
{'keyword_id': 1, 'user_id': 1}


Now a reasonable person would ask, ""isn't that inconsistent?""   And at the moment I'd say, ""absolutely"".   I need to look into the test cases to see what the rationale is for this difference in behavior, I've identified in the code why it occurs in this way and I'm pretty sure this difference in how an ""orphan"" is considered for ""pending"" versus ""persistent"" objects is intentional, but in this particular permutation obviously produces a weird result.   I might make a change in 0.8 for this if I can find one that is feasible.

edit: http://www.sqlalchemy.org/trac/ticket/2655 summarizes the issue which I'm going to have to think about.   There is a test for this behavior specifically, need to trace that back to its origin.
",,,false,
https://stackoverflow.com/questions/24231535,false,The issue is not related to an API exhibiting unexpected failures or unpredictable behaviors under specific runtime conditions. It is a question about SQLAlchemy behavior and a possible misunderstanding of the logical flow in the code.,,,,,,,Flask-SQLAlchemy Can&#39;t Create Relationship - SAWarning,"I used fbone to start this project off and I'm using its implementation of a Flask-SQLAlchemy column extension named DenormalizedText. I get the concept of this and how it works (not my issue) but my implementation of the add method is having odd results.

DenormalizedText

class DenormalizedText(Mutable, types.TypeDecorator):
    """"""
    Stores denormalized primary keys that can be
    accessed as a set.

    :param coerce: coercion function that ensures correct
               type is returned

    :param separator: separator character
    """"""

    impl = types.Text

    def __init__(self, coerce=int, separator="" "", **kwargs):

        self.coerce = coerce
        self.separator = separator

    super(DenormalizedText, self).__init__(**kwargs)

    def process_bind_param(self, value, dialect):
        if value is not None:
            items = [str(item).strip() for item in value]
            value = self.separator.join(item for item in items if item)
        return value

    def process_result_value(self, value, dialect):
        if not value:
            return set()
        return set(self.coerce(item) for item in value.split(self.separator))

    def copy_value(self, value):
        return set(value)


My class Person has a DenormalizedText parameter named family

family = Column(DenormalizedText)

and here's the add method

# just adds each object to the other's family relationship
def add_family(self, person):
    self.family.add(person.id)
    person.family.add(self.id)


So here's the strange bits:


I have another relationship for Person done the exact same way for a different class called Residence. This works fine. So I think for a minute maybe there's an issue with a self-referencing implementation. But fbone does this with their provided User class and that works?!
So I wrote a test... and it passed!
Everything leading up to the use of this method works fine. Both Person objects are in session and committed, I double check the ""family member"" before trying to add (make sure they were saved to the db and have an id).


Occasionally I get this rare error:
SAWarning: The IN-predicate on ""persons.id"" was invoked with an empty sequence. This results in a contradiction, which nonetheless can be expensive to evaluate.  Consider alternative strategies for improved performance.



Like I said this works fine for the residences relationship with Person but here's the relevant code from the view and form handler in case:

def create_step2_family():

    client = Person()
    spouse = Person()
    spouse_form = PersonalForm()

    # default action
    try:
        client = Person.get_by_id(session['working_client'])
    except KeyError:
        flash('Please complete this section first.', 'warning')
        return redirect(url_for('client.create_step1_personal'))
    else:

        # spouse form action
        if request.method == 'POST' and spouse_form.validate_on_submit():
            spouse_form.populate_obj(spouse)
            spouse.type_code = SPOUSE

            db.session.add(spouse)
            db.session.commit()

            if Person.get_by_id(spouse.id):
                client.add_family(spouse)
                db.session.add(client)
                db.session.commit()

    return render_template('clients/wizard/step2/family.html', active=""Create"",
    client=client, spouse=spouse, spouse_form=spouse_form, active_tab=""family"")

","I need a vacation

The error ""SAWarning: The IN-predicate on ""persons.id"" was invoked with an empty sequence. This results in a contradiction, which nonetheless can be expensive to evaluate.  Consider alternative strategies for improved performance."" was because I was calling a query on the column that returned nothing, or the list of persons.ids was empty because, well, nothing was being added.

And nothing was being added because I forgot these little lines at the bottom of my add method:

def add_family(self, person):
    self.family.add(person.id)
    person.family.add(self.id)
    self.family=set(self.family)
    person.family=set(person.family)


I'm a mess. Anyway, I won't accept my answer for this if someone can provide a better explanation or improvement to the code.
",,,false,
https://stackoverflow.com/questions/7017058,false,The issue is not related to an API exhibiting unexpected failures or unpredictable behaviors under specific runtime conditions. It is a question about SQLAlchemy relationships and the behavior of backref and lazy loading.,,,,,,,SQLAlchemy intricacies with relationships and uncommitted objects,"I am using SQLAlchemy via Flask, and I want to add simple personal messaging to my webapp.  The model has a User class, a PersonalMessage class and a PersonalMessageUser association class, with the latter setting up relationships to the former two—nothing fancy.  Here is a stripped down version:

import collections
import datetime

from flaskext.sqlalchemy import SQLAlchemy
from . import app

db = SQLAlchemy(app)

def current_ts():
    return datetime.datetime.utcnow()

class User(db.Model):
    id = db.Column(db.Integer, primary_key=True)
    username = db.Column(db.String(127), nullable=False, unique=True)

    def __repr__(self):
        return '&lt;User {0.username!r} (#{0.id})&gt;'.format(self)

class PersonalMessage(db.Model):
    id = db.Column(db.Integer, primary_key=True)
    subject = db.Column(db.String(127), nullable=False)
    body = db.Column(db.Text, nullable=False)
    date = db.Column(db.DateTime, nullable=False, default=current_ts)

    def __repr__(self):
        return '&lt;PersonalMessage {0.subject!r} (#{0.id})&gt;'.format(self)

    def __init__(self, subject, body, from_, to=None, cc=None, bcc=None):
        self.subject = subject
        self.body = body
        if not to and not cc and not bcc:
            raise ValueError, 'No recipients defined'
        self._pm_users.append(PersonalMessageUser(
            message=self, user_type='From', user=from_,
        ))
        for type, values in {'To': to, 'CC': cc, 'BCC': bcc}.items():
            if values is None:
                continue
            if not isinstance(values, collections.Iterable):
                values = [values]
            for value in values:
                self._pm_users.append(PersonalMessageUser(
                    message=self, user=value, user_type=type,
                ))

class PersonalMessageUser(db.Model):
    pm_id = db.Column(db.ForeignKey(PersonalMessage.id), nullable=False,
                      primary_key=True)
    message = db.relationship(PersonalMessage, backref='_pm_users',
                              lazy='subquery')
    user_id = db.Column(db.ForeignKey(User.id), nullable=False,
                        primary_key=True)
    user = db.relationship(User, backref='_personal_messages')
    user_type = db.Column(
        db.Enum('From', 'To', 'CC', 'BCC', name='user_type'),
        nullable=False, default='To', primary_key=True,
    )

    def __repr__(self):
        return (
            '&lt;PersonalMessageUser '
            '{0.user_type}: {0.user.username!r} '
            '(PM #{0.pm_id}: {0.message.subject!r})&gt;'
        ).format(self)


Everything works fine basically, but I noticed something strange when I played around with it in the Python interpreter: when I create a new PersonalMessage with one sender and one recipient, the _pm_users backref actually lists each user twice.  Once the object has been committed to the database, it looks okay, though.  See the following session as an example:

&gt;&gt;&gt; al = User(username='al')
&gt;&gt;&gt; db.session.add(al)
&gt;&gt;&gt; steve = User(username='steve')
&gt;&gt;&gt; db.session.add(steve)
&gt;&gt;&gt; db.session.commit()
BEGIN (implicit)
INSERT INTO user (username) VALUES (?)
('al',)
INSERT INTO user (username) VALUES (?)
('steve',)
COMMIT
&gt;&gt;&gt; pm = PersonalMessage('subject', 'body', from_=al, to=steve)
&gt;&gt;&gt; pm._pm_users
BEGIN (implicit)
SELECT user.id AS user_id, user.username AS user_username 
FROM user 
WHERE user.id = ?
(1,)
SELECT user.id AS user_id, user.username AS user_username 
FROM user 
WHERE user.id = ?
(2,)
[&lt;PersonalMessageUser From: u'al' (PM #None: 'subject')&gt;,
 &lt;PersonalMessageUser From: u'al' (PM #None: 'subject')&gt;,
 &lt;PersonalMessageUser To: u'steve' (PM #None: 'subject')&gt;,
 &lt;PersonalMessageUser To: u'steve' (PM #None: 'subject')&gt;]
&gt;&gt;&gt; len(pm._pm_users)
4
&gt;&gt;&gt; db.session.add(pm)
&gt;&gt;&gt; pm._pm_users
[&lt;PersonalMessageUser From: u'al' (PM #None: 'subject')&gt;,
 &lt;PersonalMessageUser From: u'al' (PM #None: 'subject')&gt;,
 &lt;PersonalMessageUser To: u'steve' (PM #None: 'subject')&gt;,
 &lt;PersonalMessageUser To: u'steve' (PM #None: 'subject')&gt;]
&gt;&gt;&gt; db.session.commit()
INSERT INTO personal_message (subject, body, date) VALUES (?, ?, ?)
('subject', 'body', '2011-08-10 19:48:15.641249')
INSERT INTO personal_message_user (pm_id, user_id, user_type) VALUES (?, ?, ?)
((1, 1, 'From'), (1, 2, 'To'))
COMMIT
&gt;&gt;&gt; pm._pm_users
BEGIN (implicit)
SELECT personal_message.id AS personal_message_id,
    personal_message.subject AS personal_message_subject,
    personal_message.body AS personal_message_body,
    personal_message.date AS personal_message_date 
FROM personal_message 
WHERE personal_message.id = ?
(1,)
SELECT personal_message_user.pm_id AS personal_message_user_pm_id,
    personal_message_user.user_id AS personal_message_user_user_id,
    personal_message_user.user_type AS personal_message_user_user_type 
FROM personal_message_user 
WHERE ? = personal_message_user.pm_id
(1,)
SELECT user.id AS user_id, user.username AS user_username 
FROM user 
WHERE user.id = ?
(1,)
SELECT user.id AS user_id, user.username AS user_username 
FROM user 
WHERE user.id = ?
(2,)
[&lt;PersonalMessageUser From: u'al' (PM #1: u'subject')&gt;,
 &lt;PersonalMessageUser To: u'steve' (PM #1: u'subject')&gt;]


At least the final result is what I expect it to be, but each user showing up twice before committing makes me feel uncomfortable; I’d like to understand what’s going on there.  Do I miss something in my relationship/backref setup, or shall I just ignore this?
","When you call

self._pm_users.append(PersonalMessageUser(
    message=self, user_type='From', user=from_,
))


you have twice append object to _pm_users list

This should work for you:

PersonalMessageUser(
    message=self, user_type='From', user=from_,
)


or

self._pm_users.append(
    PersonalMessageUser(user_type='From', user=from_,)
)       


When set relationship property, sqlalchemy associate objects for you
",,,false,
https://stackoverflow.com/questions/63081680,true,The issue involves the asyncpg library and its handling of date objects.,asyncpg,execute,The issue is caused by passing a string instead of a date object to the 'date' column in the asyncpg insert query. This results in an 'invalid input' error.,The asyncpg execute function works as expected when a valid date object is passed to the 'date' column.,The issue is triggered when a string is passed instead of a date object to the 'date' column.,This issue might be challenging to detect during development and testing if users are not familiar with the specific requirements of the asyncpg execute function or if they assume that passing a string to a date column would automatically convert it to a date object.,Error &#39;str&#39; object has no attribute &#39;toordinal&#39; in asyncpg,"My queries were giving strange results so i debugged a little bit, i change my String, date object to sqlalchemy Date so it raised this error
asyncpg.exceptions.DataError: invalid input for query argument $2: '2020-03-11'
('str' object has no attribute 'toordinal')

Here is my sqlalchemyTable
db = sqlalchemy.Table(
""db"",
metadata,
sqlalchemy.Column(""date"", Date),
sqlalchemy.Column(""data"", JSONB),
)

how i insert values:
query = db.insert().values(
    date=datetime.strptime(key,""%d/%m/%Y"").strftime(""%Y-%m-%d""),
    data=value,
)
try:
    await database.execute(query)
except UniqueViolationError:
    pass

Why did i change the type String to Date,because when i ran a query like
query = f""""""SELECT * FROM db WHERE date BETWEEN SYMMETRIC '{start_at}' AND '{end_at}'""""""
return await database.execute(query)

It was only returning one row and one column like 2020-03-11
","I would say your issue is here:
date=datetime.strptime(key,""%d/%m/%Y"").strftime(""%Y-%m-%d""),
You are passing a string to a date field.  SQLAlchemy is looking for a date object to be passed in, hence the ('str' object has no attribute 'toordinal') error. toordinal being an attribute of a date object. Remove the .strftime(""%Y-%m-%d"") and it should work.
",,,false,
https://stackoverflow.com/questions/47276146,true,The issue involves the SQLAlchemy API and constructing a CASE expression to add a new column to the query output.,SQLAlchemy,case,The issue is related to constructing a CASE expression in SQLAlchemy to add a new column based on an existing column in the query output.,The SQLAlchemy case expression works as expected when constructing conditional expressions in queries.,The issue is triggered when attempting to construct a CASE expression to add a new column based on an existing column in the query output.,This issue might be challenging to detect during development and testing if users are not familiar with the specific syntax and usage of the SQLAlchemy case expression.,How to add a CASE column to SQLAlchemy output?,"So far I've got basically the following:

MyTable.query
    .join(…)
    .filter(…)


The filter has a complicated case insensitive prefix check:

or_(*[MyTable.name.ilike(""{}%"".format(prefix)) for prefix in prefixes])


Now I have to retrieve the matching prefix in the result. In pseudo-SQL:

SELECT CASE
           WHEN strpos(lower(my_table.foo), 'prefix1') = 0 THEN 'prefix1'
           WHEN …
       END AS prefix
 WHERE prefix IS NOT NULL;


The SQLAlchemy documentation demonstrates how to use CASE within WHERE, which seems like a strange edge case, and not how to use it to get a new column based on an existing one.

The goal is to avoid duplicating the logic and prefix names anywhere.
","You know that case is an expression. You can use it in any place where SQL allows it, and link to docs you have provided show how to construct the case expression.

from sqlalchemy import case, literal
from sqlalchemy.orm import Query

q = Query(case([
    (literal(1) == literal(1), True),
], else_ = False))

print(q)

SELECT CASE WHEN (:param_1 = :param_2) THEN :param_3 ELSE :param_4 END AS anon_1

",,,false,
https://stackoverflow.com/questions/67975069,false,The issue does not meet the criteria for deeper analysis as it does not involve an API exhibiting unexpected failures or unpredictable behaviors under specific runtime conditions.,,,,,,,Phantom Queries in PyMSQL + SQLAlchemy on Lambda,"I'm trying to debug an issue that one of our developers recently revealed.
We're using AWS Lambda functions to periodically query our MySQL DB.
These functions are written in Python 3.7, with a Lambda layer added to support using PyMSQL, SQLAlchemy, and Pandas.
The (summarized and redacted) code looks like this:
import pymysql
from sqlalchemy import create_engine
import pandas as pd

mysql_connector = create_engine('mysql+pymysql://' + oltp_username + ':' + oltp_password + '@' + host + '/' + db_name)
query = """"""
        SELECT sum(field) + """"""
        FROM schema.table;
        """"""
df = pd.read_sql(query, mysql_connector)

The works just fine, and returns the expected results, however there is a strange side-effect. Our query-logging software indicates that two queries are being received from this function: the expected query in its proper format, along with a mysterious query that looks like this:
DESCRIBE `
        SELECT sum(field)
        FROM schema.table;
        `

In MySQL this is obviously a malformed query, since DESCRIBE operates on tables. Our query logging software indicates that this error appears for every single query that is run from this Lambda function.
Does anybody know where these phantom queries might be coming from? I presume it's some option in either PyMYSQL or SQLAlchemy, but I can't find anything in the documentation. Also, why are these being sent? Including a DESCRIBE makes no sense. I suspect it's just sending the raw query string to the DESCRIBE function, due to an expression like DATE_FORMAT(datefield,""%%Y-%%m-%%d %%H:%%i:00"") appearing with the consecutive %, instead of being properly escaped, as it is in the correct query.
","snakecharmerb and Ilja Everilä provided a few answers in the comments, all of which work. Thanks!

Upgrade SQLAlchemy to version 1.4
Don't use pandas to execute the SQL.
Use pandas read_sql_query function instead of read_sql.

",,,false,
https://stackoverflow.com/questions/59093282,false,The issue does not meet the criteria for deeper analysis as it appears to be a problem with the Java program's execution rather than an API-related issue.,,,,,,,Why is Oracle Pivot producing non-existent results?,"I manage a database holding a large amount of climate data collected from various stations. It's an Oracle 12.2 DB, and here's a synopsis of the relevant tables:

FACT = individual measurements at a particular time


UTC_START = time in UTC at which the measurement began
LST_START = time in local standard time (to the particular station) at which the measurement began
SERIES_ID = ID of the series to which the measurement belongs (FK to SERIES)
STATION_ID = ID of the station at which the measurement occurred (FK to STATION)
VALUE = value of the measurement


Note that UTC_START and LST_START always have a constant difference per station (the LST offset from UTC). I have confirmed that there are no instances where the difference between UTC_START and LST_START is anything other than what is expected.

SERIES = descriptive data for series of data


SERIES_ID = ID of the series (PK)
NAME = text name of the series (e.g. Temperature)


STATION = descriptive data for stations


STATION_ID = ID of the station (PK)
SITE_ID = ID of the site at which a station is located (most sites have one station, but a handful have 2)
SITE_RANK = rank of the station within the site if there are more than 1 stations.
EXT_ID = external ID for a site (provided to us)


The EXT_ID of a site applies to all stations at that site (but may not be populated unless SITE_RANK == 1, not ideal, I know, but not the issue here), and data from lower ranked stations is preferred. To organize this data into a consumable format, we're using a PIVOT to collect measurements occurring at the same site/time into rows.

Here's the query:

WITH
    primaries AS (
        SELECT site_id, ext_id
        FROM station
        WHERE site_rank = 1
    ),

    data as (
        SELECT d.site_id, d.utc_start, d.lst_start, s.name, d.value FROM (
            SELECT s.site_id, f.utc_start, f.lst_start, f.series_id, f.value,
                 ROW_NUMBER() over (PARTITION BY s.site_id, f.utc_start, f.series_id ORDER BY s.site_rank) as ORDINAL
                 FROM fact f
                      JOIN station s on f.station_id = s.station_id
        ) d
            JOIN series s ON d.series_id = s.series_id
            WHERE d.ordinal = 1
                AND d.site_id = ?
                AND d.utc_start &gt;= ?
                AND d.utc_start &lt; ?
    )

    records as (

        SELECT * FROM data
        PIVOT (
               MAX(VALUE) AS VALUE
               FOR NAME IN (
                   -- these are a few series that we would want to collect by UTC_START
                   't5' as t5,
                   'p5' as p5,
                   'solrad' as solrad,
                   'str' as str,
                   'stc_05' as stc_05,
                   'rh' as rh,
                   'smv005_05' as smv005_05,
                   'st005_05' as st005_05,
                   'wind' as wind,
                   'wet1' as wet1
                   )
                )
    )

SELECT r.*, p.ext_id
FROM records r JOIN primaries p on r.site_id = p.site_id


Here's where things get odd. This query works perfectly in SQLAlchemy, IntelliJ (using OJDBC thin), and Orcale SQL Developer. But when it's run from within our Java program (same JDBC urls, and credentials, using plain old JDBC statments and result sets), it gives results that don't make sense. Specifically for the same station, it will return 2 rows with the same UTC_START, but different LST_START (recall that I have verified that this 100% does not occur anywhere in the FACT table). Just to ensure there was no weird parameter handling going on, we tested hard-coding values in for the placeholders, and copy-and-pasted the exact same query between various clients, and the only one that returns these strange results is the Java program (which is using the exact same OJDBC jar as IntelliJ).

If anyone has any insight or possible causes, it would be greatly appreciated. We're at a bit of a loss right now.
","It turns out that Nathan's comment was correct. Though it seems counter-intuitive (to me, at least), it appears that calling ResultSet.getString on a DATE column will in fact convert to Timestamp first. Timestamp has the unfortunate default behavior of using the system default timezone unless you specify otherwise explicitly.

This default behavior meant that daylight saving's time was taken into account when we didn't intend it to be, leading to the odd behavior described.
",,,false,
https://stackoverflow.com/questions/49360525,false,The issue does not meet the criteria for deeper analysis as it is related to the case sensitivity of the SQLite LIKE operator rather than an API exhibiting unexpected failures or unpredictable behaviors.,,,,,,,SqlAlchemy like filter case insensitive but it should be case sensitive,"I am using SqlAlchemy for a database, but I have a problem when using like function in queries.

My database is SQLite.

My request is like this: 

self.session.query(Value.scan).filter(Value.current_value.like(""%"" + search + ""%""), Value.tag == Tag.tag, Tag.visible == True).distinct().all()


The column Value.current_value is a String, you can see the declaration here:

class Value(Base):
    current_value = Column(String, nullable=False)


search variable is a str coming from a rapid search bar, and is case sensitive (I never call lower or upper on it).

I want to do a case sensitive search, but the results are case insensitive.

I did some research and like should be case sensitive, and ilike case insensitive, so I don't understand why it's case insensitive.

Should I choose another type for my column that has to be case sensitive?

Another strange thing is that I have the same problem when using the function contains on the same column (case insensitive result), but not when using operators like ==, !=, &lt;, or &gt; (case sensitive result)

Does semeone knows why it's case sensitive with operators, but not with like and contains functions?

Best regards

Lucie
","In SQLite, LIKE is by default case insensitive.

What I had to do is activating the case_sensitive_like pragma.

I created a class to activate the pragma like this:

class ForeignKeysListener(PoolListener):
    """"""
    Class to activate the pragma case_sensitive_like, that makes the
    like and contains functions case sensitive
    """"""
    def connect(self, dbapi_con, con_record):
        db_cursor = dbapi_con.execute('pragma case_sensitive_like=ON')


When you create the engine, you just have to add the listener like this:

engine = create_engine(
    'sqlite:///' + os.path.join(self.folder, 'database', 'mia2.db'),
    listeners=[ForeignKeysListener()])

",,,false,
https://stackoverflow.com/questions/75913528,true,The issue involves an API exhibiting unexpected behavior under specific runtime conditions.,,,,,,,AWS lambda does not update rds postgres table but the execution is successful,"I have a postgres DB in RDS. When I connect it from my local and insert into it via sqlalchemy, I can see the updated results but when I run the same code from Lambda, the execution is shown as completed (i.e code runs correctly) but the results are not updated in the DB. In the same code when I print out result of `SELECT * table_name*, I can see the new row addition but when I check from local (using both pgadmin and through code) whether the row was really added or not, it it is missing.
One strange thing I observed is that I have a column 'ID' which is a primary key. When I add the row with ID 25 from lambda followed by adding another row from local, the row added from local is assigned row 27 (instead of 26 which was assigned to row added from lambda, however I can't see the row with id 26).
The following is my code:
from sqlalchemy import create_engine, MetaData, Table, Column, Integer, String, text
engine = create_engine(""postgresql://%s:%s@%s:%s/%s"" % (username, password, host, port, database))
conn = engine.connect()
print(""CONNECTED TO RDS"")

query = 'select * from predictions'
result = conn.execute(text(query)).fetchall()
print(""THE QUERY RESULT IS"", '\n', result)
upload_query = ""INSERT INTO predictions (image_url, image_class) VALUES ('LOCAL_ADD', 'LOCAL_ADD');""
conn.execute(text(upload_query))
query = 'select * from predictions'
result = conn.execute(text(query)).fetchall()
print(""THE QUERY RESULT IS"", '\n', result)

conn.close()

My lambda has no VPC to it. However my RDS DB has 4 security groups with two of them being inbound/outbound rules set to 'All Traffic' with '0.0.0.0'
","You've executed the changes with conn.execute, but if autocommit is not set within the lambda session then the changes won't be issued to the database until you commit them with conn.commit.
So add the line conn.commit() like so:
conn.execute(text(upload_query))
conn.commit()

","Enable autocommit or explicitly COMMIT the transacion in the Lambda function.
Sequence values are updated whenever the next value is requested. This occurs outside of the transaction. It can be viewed as occurring in an independent transaction. This is necessary so that sequences can be used by concurrent transactions without introducing waits or deadlocks.
",,false,
https://stackoverflow.com/questions/73440112,false,The issue does not meet the criteria for deeper analysis as it is unclear whether it involves an API exhibiting unexpected failures or unpredictable behaviors.,,,,,,,MySql: Insert record doesn&#39;t exists or actually never being created?,"Recently I've encountered a strange problem which I couldn't see any ideas based on my current knowledge.
Backend:  Python3, Sqlalchemy,
MySQL Config: read-committed, id auto-increment, cluster with 3 nodes.
Query: insert into xxx values(xxx...) and then db.session.commit()
Expect Result: New record id returned and mysql successfully create one record.
Actually Result: New record id returned and no mysql record created and no binlog found.
I wonder: if something panic, the transcation should've rollback and no id should returned. What I missed?
","Possibly one is overwriting the session with child processes,
Connection problems with SQLAlchemy and multiple processes
",,,false,
https://stackoverflow.com/questions/72293864,false,The issue does not meet the criteria for deeper analysis as it is related to a database integrity constraint violation rather than an API exhibiting unexpected failures or unpredictable behaviors.,,,,,,,Getting sqlalchemy.exc.IntegrityError: (sqlite3.IntegrityError) NOT NULL constraint failed: user.image_file,"I am new to learning Flask. following youtube tutorial, I did the same as the trainer did. But I am getting the Integrity error.
Flask_Blog.Py:
from datetime import datetime
from flask import Flask, render_template, url_for, flash, redirect
from flask_sqlalchemy import SQLAlchemy
from sqlalchemy.com import dynamic

from forms import RegistrationForm, LoginForm

app = Flask(__name__)
app.config['SECRET_KEY'] = '5791628bb0b13ce0c676dfde280ba245'
app.config['SQLALCHEMY_DATABASE_URI'] = 'sqlite:///site.db'
db = SQLAlchemy(app)


class User(db.Model):
    id = db.Column(db.Integer, primary_key=True)
    username = db.Column(db.String(20), unique=True, nullable=False)
    email = db.Column(db.String(120), unique=True, nullable=False)
    image_file = db.Column(db.String(20), nullable=False)
    password = db.Column(db.String(60), nullable=False)
    posts = db.relationship('Post', backref='author', lazy=True)

    def __repr__(self):
        return f""User('{self.username}', '{self.email}', '{self.image_file}')""


class Post(db.Model):
    id = db.Column(db.Integer, primary_key=True)
    title = db.Column(db.String(100), nullable=False)
    date_posted = db.Column(db.DateTime, nullable=False, default=datetime.utcnow)
    content = db.Column(db.Text, nullable=False)
    user_id = db.Column(db.Integer, db.ForeignKey('user.id'), nullable=False)

    def __repr__(self):
        return f""Post('{self.title}', '{self.date_posted}')""


posts = [
    {
        'author': 'Corey Schafer',
        'title': 'Blog Post 1',
        'content': 'First post content',
        'date_posted': 'April 20, 2018'
    },
    {
        'author': 'Jane Doe',
        'title': 'Blog Post 2',
        'content': 'Second post content',
        'date_posted': 'April 21, 2018'
    }
]


@app.route(""/"")
@app.route(""/home"")
def home():
    return render_template('home.html', posts=posts)


@app.route(""/about"")
def about():
    return render_template('about.html', title='About')


@app.route(""/register"", methods=['GET', 'POST'])
def register():
    form = RegistrationForm()
    if form.validate_on_submit():
        flash(f'Account created for {form.username.data}!', 'success')
        return redirect(url_for('home'))
    return render_template('register.html', title='Register', form=form)


@app.route(""/login"", methods=['GET', 'POST'])
def login():
    form = LoginForm()
    if form.validate_on_submit():
        if form.email.data == 'admin@blog.com' and form.password.data == 'password':
            flash('You have been logged in!', 'success')
            return redirect(url_for('home'))
        else:
            flash('Login Unsuccessful. Please check username and password', 'danger')
    return render_template('login.html', title='Login', form=form)

 if __name__ == '__main__':
        app.run(debug=True) 

This the commands I am trying to execute.
Commands:
**from Flask_Blog import db
db.create_all()
from Flask_Blog import User, Post
user_1 = User(username='Strange', email='doctor@strange.com', password='password')
db.session.add(user_1)
db.session.commit()**

After executing commit I am getting below error.

Traceback (most recent call last):
File ""C:\Users\darjunku\PycharmProjects\Working_with_Flask\venv\lib\site-packages\sqlalchemy\engine\base.py"", line 1819, in _execute_context
self.dialect.do_execute(
File ""C:\Users\darjunku\PycharmProjects\Working_with_Flask\venv\lib\site-packages\sqlalchemy\engine\default.py"", line 732, in do_execute
cursor.execute(statement, parameters)
sqlite3.IntegrityError: NOT NULL constraint failed: user.image_file
The above exception was the direct cause of the following exception:
Traceback (most recent call last):
File ""C:\Users\darjunku\AppData\Local\Programs\Python\Python310\lib\code.py"", line 90, in runcode
exec(code, self.locals)
File """", line 1, in 
File """", line 2, in commit
File ""C:\Users\darjunku\PycharmProjects\Working_with_Flask\venv\lib\site-packages\sqlalchemy\orm\session.py"", line 1435, in commit
self._transaction.commit(_to_root=self.future)
File ""C:\Users\darjunku\PycharmProjects\Working_with_Flask\venv\lib\site-packages\sqlalchemy\orm\session.py"", line 829, in commit
self._prepare_impl()
File ""C:\Users\darjunku\PycharmProjects\Working_with_Flask\venv\lib\site-packages\sqlalchemy\orm\session.py"", line 808, in _prepare_impl
self.session.flush()
File ""C:\Users\darjunku\PycharmProjects\Working_with_Flask\venv\lib\site-packages\sqlalchemy\orm\session.py"", line 3367, in flush
self.flush(objects)
File ""C:\Users\darjunku\PycharmProjects\Working_with_Flask\venv\lib\site-packages\sqlalchemy\orm\session.py"", line 3506, in flush
with util.safe_reraise():
File ""C:\Users\darjunku\PycharmProjects\Working_with_Flask\venv\lib\site-packages\sqlalchemy\util\langhelpers.py"", line 70, in exit
compat.raise(
File ""C:\Users\darjunku\PycharmProjects\Working_with_Flask\venv\lib\site-packages\sqlalchemy\util\compat.py"", line 207, in raise
raise exception
File ""C:\Users\darjunku\PycharmProjects\Working_with_Flask\venv\lib\site-packages\sqlalchemy\orm\session.py"", line 3467, in _flush
flush_context.execute()
File ""C:\Users\darjunku\PycharmProjects\Working_with_Flask\venv\lib\site-packages\sqlalchemy\orm\unitofwork.py"", line 456, in execute
rec.execute(self)
File ""C:\Users\darjunku\PycharmProjects\Working_with_Flask\venv\lib\site-packages\sqlalchemy\orm\unitofwork.py"", line 630, in execute
util.preloaded.orm_persistence.save_obj(
File ""C:\Users\darjunku\PycharmProjects\Working_with_Flask\venv\lib\site-packages\sqlalchemy\orm\persistence.py"", line 245, in save_obj
_emit_insert_statements(
File ""C:\Users\darjunku\PycharmProjects\Working_with_Flask\venv\lib\site-packages\sqlalchemy\orm\persistence.py"", line 1238, in _emit_insert_statements
result = connection._execute_20(
File ""C:\Users\darjunku\PycharmProjects\Working_with_Flask\venv\lib\site-packages\sqlalchemy\engine\base.py"", line 1631, in _execute_20
return meth(self, args_10style, kwargs_10style, execution_options)
File ""C:\Users\darjunku\PycharmProjects\Working_with_Flask\venv\lib\site-packages\sqlalchemy\sql\elements.py"", line 325, in _execute_on_connection
return connection._execute_clauseelement(
File ""C:\Users\darjunku\PycharmProjects\Working_with_Flask\venv\lib\site-packages\sqlalchemy\engine\base.py"", line 1498, in _execute_clauseelement
ret = self._execute_context(
File ""C:\Users\darjunku\PycharmProjects\Working_with_Flask\venv\lib\site-packages\sqlalchemy\engine\base.py"", line 1862, in _execute_context
self.handle_dbapi_exception(
File ""C:\Users\darjunku\PycharmProjects\Working_with_Flask\venv\lib\site-packages\sqlalchemy\engine\base.py"", line 2043, in handle_dbapi_exception
util.raise(
File ""C:\Users\darjunku\PycharmProjects\Working_with_Flask\venv\lib\site-packages\sqlalchemy\util\compat.py"", line 207, in raise
raise exception
File ""C:\Users\darjunku\PycharmProjects\Working_with_Flask\venv\lib\site-packages\sqlalchemy\engine\base.py"", line 1819, in _execute_context
self.dialect.do_execute(
File ""C:\Users\darjunku\PycharmProjects\Working_with_Flask\venv\lib\site-packages\sqlalchemy\engine\default.py"", line 732, in do_execute
cursor.execute(statement, parameters)
sqlalchemy.exc.IntegrityError: (sqlite3.IntegrityError) NOT NULL constraint failed: user.image_file
[SQL: INSERT INTO user (username, email, image_file, password) VALUES (?, ?, ?, ?)]
[parameters: ('Strange', 'doctor@strange.com', None, 'password')]
(Background on this error at: https://sqlalche.me/e/14/gkpj)

Traceback (most recent call last):
  File ""C:\Users\darjunku\PycharmProjects\Working_with_Flask\venv\lib\site-packages\sqlalchemy\engine\base.py"", line 1819, in _execute_context
    self.dialect.do_execute(
  File ""C:\Users\darjunku\PycharmProjects\Working_with_Flask\venv\lib\site-packages\sqlalchemy\engine\default.py"", line 732, in do_execute
    cursor.execute(statement, parameters)
sqlite3.IntegrityError: NOT NULL constraint failed: user.image_file
The above exception was the direct cause of the following exception:
Traceback (most recent call last):
  File ""C:\Users\darjunku\AppData\Local\Programs\Python\Python310\lib\code.py"", line 90, in runcode
    exec(code, self.locals)
  File ""&lt;input&gt;"", line 1, in &lt;module&gt;
  File ""&lt;string&gt;"", line 2, in commit
  File ""C:\Users\darjunku\PycharmProjects\Working_with_Flask\venv\lib\site-packages\sqlalchemy\orm\session.py"", line 1435, in commit
    self._transaction.commit(_to_root=self.future)
  File ""C:\Users\darjunku\PycharmProjects\Working_with_Flask\venv\lib\site-packages\sqlalchemy\orm\session.py"", line 829, in commit
    self._prepare_impl()
  File ""C:\Users\darjunku\PycharmProjects\Working_with_Flask\venv\lib\site-packages\sqlalchemy\orm\session.py"", line 808, in _prepare_impl
    self.session.flush()
  File ""C:\Users\darjunku\PycharmProjects\Working_with_Flask\venv\lib\site-packages\sqlalchemy\orm\session.py"", line 3367, in flush
    self._flush(objects)
  File ""C:\Users\darjunku\PycharmProjects\Working_with_Flask\venv\lib\site-packages\sqlalchemy\orm\session.py"", line 3506, in _flush
    with util.safe_reraise():
  File ""C:\Users\darjunku\PycharmProjects\Working_with_Flask\venv\lib\site-packages\sqlalchemy\util\langhelpers.py"", line 70, in __exit__
    compat.raise_(
  File ""C:\Users\darjunku\PycharmProjects\Working_with_Flask\venv\lib\site-packages\sqlalchemy\util\compat.py"", line 207, in raise_
    raise exception
  File ""C:\Users\darjunku\PycharmProjects\Working_with_Flask\venv\lib\site-packages\sqlalchemy\orm\session.py"", line 3467, in _flush
    flush_context.execute()
  File ""C:\Users\darjunku\PycharmProjects\Working_with_Flask\venv\lib\site-packages\sqlalchemy\orm\unitofwork.py"", line 456, in execute
    rec.execute(self)
  File ""C:\Users\darjunku\PycharmProjects\Working_with_Flask\venv\lib\site-packages\sqlalchemy\orm\unitofwork.py"", line 630, in execute
    util.preloaded.orm_persistence.save_obj(
  File ""C:\Users\darjunku\PycharmProjects\Working_with_Flask\venv\lib\site-packages\sqlalchemy\orm\persistence.py"", line 245, in save_obj
    _emit_insert_statements(
  File ""C:\Users\darjunku\PycharmProjects\Working_with_Flask\venv\lib\site-packages\sqlalchemy\orm\persistence.py"", line 1238, in _emit_insert_statements
    result = connection._execute_20(
  File ""C:\Users\darjunku\PycharmProjects\Working_with_Flask\venv\lib\site-packages\sqlalchemy\engine\base.py"", line 1631, in _execute_20
    return meth(self, args_10style, kwargs_10style, execution_options)
  File ""C:\Users\darjunku\PycharmProjects\Working_with_Flask\venv\lib\site-packages\sqlalchemy\sql\elements.py"", line 325, in _execute_on_connection
    return connection._execute_clauseelement(
  File ""C:\Users\darjunku\PycharmProjects\Working_with_Flask\venv\lib\site-packages\sqlalchemy\engine\base.py"", line 1498, in _execute_clauseelement
    ret = self._execute_context(
  File ""C:\Users\darjunku\PycharmProjects\Working_with_Flask\venv\lib\site-packages\sqlalchemy\engine\base.py"", line 1862, in _execute_context
    self._handle_dbapi_exception(
  File ""C:\Users\darjunku\PycharmProjects\Working_with_Flask\venv\lib\site-packages\sqlalchemy\engine\base.py"", line 2043, in _handle_dbapi_exception
    util.raise_(
  File ""C:\Users\darjunku\PycharmProjects\Working_with_Flask\venv\lib\site-packages\sqlalchemy\util\compat.py"", line 207, in raise_
    raise exception
  File ""C:\Users\darjunku\PycharmProjects\Working_with_Flask\venv\lib\site-packages\sqlalchemy\engine\base.py"", line 1819, in _execute_context
    self.dialect.do_execute(
  File ""C:\Users\darjunku\PycharmProjects\Working_with_Flask\venv\lib\site-packages\sqlalchemy\engine\default.py"", line 732, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.IntegrityError: (sqlite3.IntegrityError) NOT NULL constraint failed: user.image_file
[SQL: INSERT INTO user (username, email, image_file, password) VALUES (?, ?, ?, ?)]
[parameters: ('Corey', 'C@demo.com', None, 'password')]
(Background on this error at: https://sqlalche.me/e/14/gkpj)*

","Try:
image_file = db.Column(db.String(20), nullable=True)

",,,false,
https://stackoverflow.com/questions/71451982,false,The issue does not meet the criteria for deeper analysis as it is related to naming conventions in SQLAlchemy migrations rather than an API exhibiting unexpected failures or unpredictable behaviors.,,,,,,,How to make alembic or flask migrate name foreign keys when autogenerating migrations?,"I've been struggling with this issue on and off for quite some time, and strangely could not find a straightforward question/answer combo on this on SO. Related questions here and here. I finally found a solution so I will ask and answer my own question.
In Flask SQLAlchemy (and regular SQLAlchemy), you can have a column like this:
class Character(db.model):
  background_id = db.Column(db.Integer, db.ForeignKey('backgrounds.id'))

When you run flask db migrate, or alembic revision --autogenerate, this will result in an operation that looks like this:
def upgrade():
  op.create_foreign_key(None, 'characters', 'backgrounds', ['background_id'], ['id'])

def downgrade():
  op.drop_constraint(None, 'characters', type_='foreignkey')

The None here is bad. In fact, if you try to downgrade later, this will always fail, because drop_constraint needs the name of the constraint.
You can change this every time you generate a migration, like this:
def upgrade():
  op.create_foreign_key('fk_characters_backgrounds', 'characters', 'backgrounds', ['background_id'], ['id'])

def downgrade():
  op.drop_constraint('fk_characters_backgrounds', 'characters', type_='foreignkey')

Which works!
But if you're like me, you don't want to have to remember to do this every time you autogenerate a revision with a foreign key.
So the question is, how can we make this automatic?
","There is an answer to this question in the best practices suggested here, at the end of the section on The Importance of Naming Conventions. The solution is to add a naming_convention to your sqlalchemy metadata, like this:
convention = {
  ""ix"": ""ix_%(column_0_label)s"",
  ""uq"": ""uq_%(table_name)s_%(column_0_name)s"",
  ""ck"": ""ck_%(table_name)s_%(constraint_name)s"",
  ""fk"": ""fk_%(table_name)s_%(column_0_name)s_%(referred_table_name)s"",
  ""pk"": ""pk_%(table_name)s""
}

metadata = MetaData(naming_convention=convention)

More specifically, with Flask-SQLAlchemy, do this when initializing your db:
from sqlalchemy import MetaData

convention = {
  ""ix"": ""ix_%(column_0_label)s"",
  ""uq"": ""uq_%(table_name)s_%(column_0_name)s"",
  ""ck"": ""ck_%(table_name)s_%(constraint_name)s"",
  ""fk"": ""fk_%(table_name)s_%(column_0_name)s_%(referred_table_name)s"",
  ""pk"": ""pk_%(table_name)s""
}

db = SQLAlchemy(metadata=MetaData(naming_convention=convention))

And voila! If you run autogenerate, you'll get this:
def upgrade():
  op.create_foreign_key(op.f('fk_characters_background_id_backgrounds'), 'characters', 'backgrounds', ['background_id'], ['id'])

def downgrade():
  op.drop_constraint(op.f('fk_characters_background_id_backgrounds'), 'characters', type_='foreignkey')

Thanks (unsurprisingly) to  Miguel Grinberg, creator of Flask Migrate, for having linked to the correct page in the Alembic docs that finally allowed me to solve this problem! Someone had asked about this in an issue on Flask Migrate GitHub, and Miguel correctly pointed out that this was an Alembic issue, not a Flask Migrate issue.
",,,false,
https://stackoverflow.com/questions/69566226,false,The issue does not meet the criteria for deeper analysis as it is related to a parameter error in the query rather than an API exhibiting unexpected failures or unpredictable behaviors.,,,,,,,&quot;MySQLInterfaceError: Python type list cannot be converted&quot; on all sqlalchemy.orm.Query methods,"I am running a Python 3.9.0 server through uvicorn with sqlalchemy connecting to a mysql server.
I'm trying to make queries with a db (session) object that I construct as such, based on examples I've pulled from the docs and web:
database.py
from sqlalchemy.orm import sessionmaker
...
engine = create_engine(
    SQLALCHEMY_DATABASE_URL,
    connect_args={'auth_plugin': 'mysql_native_password'}
)
...
SessionLocal = sessionmaker(autocommit=False, autoflush=False, bind=engine)

main.py
from sqlalchemy.orm import Session
from .database import SessionLocal, engine, Base

def get_db():
    try:
        db = SessionLocal()
        yield db
    finally:
        db.close()

Per the docs, I'm trying to invoke queries as follows:
main.py
db.query(models.OriginalSong).filter_by(original_song_title=original_song_title).first()

However, methods such as first() and all() are throwing the same error:
_mysql_connector.MySQLInterfaceError: Python type list cannot be converted

pointing to the line where the all() or first() method was invoked.
I checked the type of the invocation and they appear to be correct:
print(type(db.query(models.OriginalSong).filter_by(original_song_title=original_song_title)))

&lt;class 'sqlalchemy.orm.query.Query'&gt;

I don't understand what I'm doing differently from what the docs on the query type or the tutorial. There was one stackoverflow question I found with the same error, but trying to join the ""list"" just resulted in the same error. I'm not even sure what the return type of first() or one() are as it errors before I can try to print the type.
How can I access the result of my query without an error?
(I tried to include only the necessary code, but the full repo can be seen on github)
EDIT: Strangely, a prior query works:
query = db.query(models.RemixArtist).filter_by(remix_artist_name=remix_artist_name).first()

I don't see any difference between the two.
","The issue wasn't with the return of the query, the issue was with the parameter passed to the filter_by. Without realizing it, I was passing it a list, when it expected a string. I had to change the argument's value for parameter original_song_title to be original_song_title[0].
",,,false,
https://stackoverflow.com/questions/67858384,false,"The issue is not related to an API exhibiting unexpected failures or unpredictable behaviors under specific runtime conditions. It appears to be a problem with the consistency between the ORM cache and the database, which is not directly related to an API.",,,,,,,SQLAlchemy: How do I avoid this inconsistency between ORM cache and DB,"Here's a simplified version of my problem. I have a program query.py:
import time
from models import Ball, session

time.sleep(1)
r = session.query(Ball).filter(Ball.color=='red').first()
print(f'Red ball color is {r.color}')
time.sleep(2)
b = session.query(Ball).filter(Ball.color=='blue').first()
print(f'Blue ball color is {b.color}')
print(f'Red ball id is {r.id}, blue ball id is {b.id}')

When I run query.py at the same time as modify.py (included below), I get the following output:
$ python modify.py &amp;! python query.py
Red ball color is red                                                                                                                                                   
Blue ball color is red                                                                                                                                                  
Red ball id is 1, blue ball id is 1                                                                                                                  

The problem is that the blue ball is red!
Here is the content of models.py:
import sqlalchemy as sa
import sqlalchemy.orm as sao
import sqlalchemy.ext.declarative as saed

Base = saed.declarative_base()

class Ball(Base):
    __tablename__ = 'ball'
    id = sa.Column(sa.Integer, primary_key=True)
    color = sa.Column(sa.String)

engine = sa.create_engine('sqlite:///test.db')
Base.metadata.create_all(engine)
session = sao.Session(engine)

And here is modify.py:
import time
from models import Ball, session

session.query(Ball).delete()
b = Ball(color='red')
session.add(b)
session.commit()
time.sleep(2)
b.color = 'blue'
session.add(b)
session.commit()

I find it very strange that I get an inconsistency here between my DB query (that sees the latest DB state) and the object returned via the SQLAlchemy identiy map for my DB query (which is stale, reflecting the DB state the first time the row in question was read). I know that restarting my transaction in the query.py process before each query will invalidate the cached objects in the identity map and result in the blue ball being blue here, but that's a non-starter.
I'd be happy if the blue ball were blue -- i.e. if the DB query and object it returned agreed -- or if the blue ball query returned None -- i.e. if the concurrent DB modification was not visible in the query transaction. But I seem to be stuck in the middle.
","It seems the underlying issue is that SQLite support is buggy by default in Python, and SQLAlchemy knowingly inherits this buggy behavior. I eventually figured out how to get both possible correct behaviors, i.e. either make the blue ball blue, by invalidating the identity map/cache without cancelling the current transaction, or make the query for blue balls return None, by running the query.py in a properly isolated transaction.
Achieving isolation / making the blue ball query return None
I found that setting the isolation level, by passing isolation_level=&lt;level&gt; to create_engine had no effect, i.e. this didn't provide a way to make the query.py run in a transaction that is isolated from the writes by modify.py, where the query for blue balls would return None. After reading about isolation levels in SQLite it seems that setting the isolation level to SERIALIZABLE should accomplish this, but it does not. However, the SQLAlchemy documentation warns that by default SQLite transactions are broken:

In the section Database Locking Behavior / Concurrency, we refer to the pysqlite driver’s assortment of issues that prevent several features of SQLite from working correctly. The pysqlite DBAPI driver has several long-standing bugs which impact the correctness of its transactional behavior. In its default mode of operation, SQLite features such as SERIALIZABLE isolation, transactional DDL, and SAVEPOINT support are non-functional, and in order to use these features, workarounds must be taken.

That page goes on to suggest workarounds to get functioning transactions, and those workarounds work for me. Namely, adding the following to the bottom of models.py achieves the isolation behavior where the blue balls query returns None:
@sa.event.listens_for(engine, ""connect"")
def do_connect(dbapi_connection, connection_record):
    # disable pysqlite's emitting of the BEGIN statement entirely.
    # also stops it from emitting COMMIT before any DDL.
    dbapi_connection.isolation_level = None

@sa.event.listens_for(engine, ""begin"")
def do_begin(conn):
    # emit our own BEGIN
    conn.exec_driver_sql(""BEGIN"")

With this change, the output becomes
$ python modify.py &amp;! python query.py
Red ball color is red                                                                                                                                                   
Traceback (most recent call last):                                                                                                                                      
  File ""query.py"", line 10, in &lt;module&gt;
    print(f'Blue ball color is {b.color}')
AttributeError: 'NoneType' object has no attribute 'color'

I.e. the DB write in modify.py that makes the ball blue is not visible in the (implicit) transaction in query.py.
Making the query and returned object agree / making the blue ball blue
On the other hand, to get the behavior where the the blue ball is blue, it's enough to invalidate the cache/identity map before each query, using session.expire_all(). I.e., changing query.py to the following works:
import time
from models import Ball, session

time.sleep(1)
r = session.query(Ball).filter(Ball.color=='red').first()
print(f'Red ball color is {r.color}')
time.sleep(2)
# Added this line:
session.expire_all()
b = session.query(Ball).filter(Ball.color=='blue').first()
print(f'Blue ball color is {b.color}')
print(f'Red ball id is {r.id}, blue ball id is {b.id}')

With this change, the output becomes
$ python modify.py &amp;! python query.py                                                                                                                                
Red ball color is red                                                                                                                                                   
Blue ball color is blue                                                                                                                                                 
Red ball id is 1, blue ball id is 1

",,,false,
https://stackoverflow.com/questions/66004467,false,"The issue is not related to an API exhibiting unexpected failures or unpredictable behaviors under specific runtime conditions. It seems to be a problem with the truncation of BIGINT values in a specific table, which is not directly related to an API.",,,,,,,MySQL BIGINT inconsistent for inserts?,"On ubuntu.. running MySQL v 5.6.
created a python program that performs all my operations.
my app creates tables dynamically. there are many. a few are very similar.. for example, here are two:
create table tst.intgn_party_test_load (
  party_id bigint unsigned NOT NULL,
  party_supertype varchar(15) NOT NULL,
  carrier_party_id bigint unsigned NOT NULL,
  full_name varchar(500),
  lda_actv_ind integer,
  lda_file_id integer,
  lda_created_by varchar(100),
  lda_created_on datetime,
  lda_updated_by varchar(100),
  lda_updated_on datetime, 
  PRIMARY KEY(party_id,party_supertype,carrier_party_id)
) 

and
create table tst.intgn_party_relationship (
  parent_party_id bigint unsigned NOT NULL,
  child_party_id bigint unsigned NOT NULL,
  relationship_type varchar(10),
  lda_actv_ind integer,
  lda_file_id integer,
  lda_created_by varchar(100),
  lda_created_on datetime,
  lda_updated_by varchar(100),
  lda_updated_on datetime, 
  PRIMARY KEY(parent_party_id,child_party_id,relationship_type)
) 

My program also dynamically populates the tables. I construct the party id fields using source data converted to an BIGINT.
For example, the insert it constructs for the first table is:
INSERT INTO intgn_party_test_load (
  party_supertype, 
  carrier_party_id, 
  party_id, 
  full_name, 
  lda_actv_ind, 
  lda_file_id) 
SELECT  
  'Agency' as s0,
  0 as s1,
  CONV(SUBSTRING(CAST(SHA(CONCAT(full_name,ga)) AS CHAR), 1, 16), 16, 10) as s2,
  CONCAT(full_name,'-',ga) as s3, 
  lda_actv_ind, 
  lda_file_id 
FROM tst.raw_listing_20210118175114 
ON DUPLICATE KEY 
UPDATE  
  full_name = VALUES(full_name), 
  lda_actv_ind = VALUES(lda_actv_ind), 
  lda_file_id = VALUES(lda_file_id) ;

and for the second table the insert constructed looks very similar, and is based on the exact same source data:
INSERT INTO tst.intgn_party_relationship (
  parent_party_id,
  relationship_type,
  child_party_id, 
  lda_actv_ind, 
  lda_file_id) 
SELECT (Select party_id 
        from intgn_party 
        where full_name = 'xxx') as s0,
       'Location' as s1,
       CONV(SUBSTRING(CAST(SHA(CONCAT(full_name,ga)) AS CHAR), 1, 16), 16, 10) as s2, 
       lda_actv_ind, 
       lda_file_id 
FROM tst.raw_listing_20210118175114 
ON DUPLICATE KEY 
UPDATE  
  lda_actv_ind = VALUES(lda_actv_ind), 
  lda_file_id = VALUES(lda_file_id) 

Now... the first table (intgn_party_test_load) is the issue. I can drop it, recreate it manually even.. no matter what i do, the data inserted into it via python has the BIGINT party_id truncated to just 16 digits.
EVERY OTHER TABLE that uses the exact same formula to populate the party_id, creates BIGINT numbers that are between 18 and 20 digits long. I can see all the same source records loaded in the tables, and i see the truncated values in the first table (intgn_party_test_load). for example, the first table has a record with party id = 7129232523783260.  the second table (and many others) has the same record loaded with [child]party id  = 7129232523783260081.
The exact same formula, executed the exact same way from python.. but this table gets shorter BIGINTs.
Interestingly, I tried manually running the insert into this table (not using the python program), and it inserts the full BIGINT values.
So I'm confused why the python program has 'chosen' this table to not work correctly, while it works fine on all other tables.
Is there some strange scenario where values get truncated?
BTW, my python program utilizes sqlalchemy to run the creations/inserts. Since it works manually, I have to assume its related to sqlalchemy.. but no idea why it works on all but this table..
[edit]
to add, the sql commands through sqlalchemy are executed using db_connection.execute(sql)
[edit - adding more code detail]
from sqlalchemy import create_engine, exc

engine = create_engine(
            connection_string,
            pool_size=6, max_overflow=10, encoding='latin1', isolation_level='AUTOCOMMIT'
        )
        connection = engine.connect()

sql = ""INSERT INTO intgn_party_test_load (
  party_supertype, 
  carrier_party_id, 
  party_id, 
  full_name, 
  lda_actv_ind, 
  lda_file_id) 
SELECT  
  'Agency' as s0,
  0 as s1,
  CONV(SUBSTRING(CAST(SHA(CONCAT(full_name,ga)) AS CHAR), 1, 16), 16, 10) as s2,
  CONCAT(full_name,'-',ga) as s3, 
  lda_actv_ind, 
  lda_file_id 
FROM tst.raw_listing_20210118175114 
ON DUPLICATE KEY 
UPDATE  
  full_name = VALUES(full_name), 
  lda_actv_ind = VALUES(lda_actv_ind), 
  lda_file_id = VALUES(lda_file_id) ;""

        result = db_connection.execute(sql)

Thats as best i can reduce it too (the code is much more complicated as it dynamically creates the statement amoungst other things).. but from my logging, i see the exact statement it is executing (As above), and i see the result in the BIGINT columns after.  all tables but this one. And only when through the app.
so it doesn't happen to the other tables even through the app..
very confusing.. was hoping someone just knew a bug in mySQL 5.6 around BIGINTs as it pertains to maybe the destination table's key construct or total length of records.. or some other crazy reason.  I do see that interestingly, if i do a distinct on BIGINT column that has &gt;18 digit lengths, it comes back as 16 digits - guess the distinct function doesn't support BIGINT..
was kinda hoping this hints at an issue, but i don't get why the other tables would work fine...
[EDIT - adding some of the things i see sqlalchemy running apparently, around the actual run of my query.. just in the crazy case they impact anything - for the one table?? ]
SET AUTOCOMMIT = 0
SET AUTOCOMMIT = 1
SET NAMES utf8mb4
SHOW VARIABLES LIKE 'sql_mode'
SHOW VARIABLES LIKE 'lower_case_table_names'
SELECT VERSION()
SELECT DATABASE()
SELECT @@tx_isolation
show collation where `Charset` = 'utf8mb4' and `Collation` = 'utf8mb4_bin'
SELECT CAST('test plain returns' AS CHAR(60)) AS anon_1
SELECT CAST('test unicode returns' AS CHAR(60)) AS anon_1
SELECT CAST('test collated returns' AS CHAR CHARACTER SET utf8mb4) COLLATE utf8mb4_bin AS anon_1
ROLLBACK
SET NAMES utf8mb4

hard to say the order or anything like that.. there are a ton that get run at the same microsecond.
","after racking my brains for days.. coming at it from all angles, i could not figure out why 1 table out of many, had issues with truncating the SHA'd value.
In the end, i have redesigned how i hold my Ids, and i no longer bother converting to BIGINT. it all works fine when i leave it as CHAR.
CAST(SHA(CONCAT(full_name,ga)) AS CHAR)

So changed all my Id columns to varchar(40) and use the above style.  All good now. Joins will use varchar instead of bigint - which i'm ok with.
",,,false,
https://stackoverflow.com/questions/61657802,false,The issue does not meet the criteria for deeper analysis as it does not involve an API exhibiting unexpected failures or unpredictable behaviors under specific runtime conditions.,,,,,,,Flask SQLAlchemy Sessions commit() not working &quot;sometimes&quot;,"I've been working in a web app for a while and this is the first time I realize this problem, I think it could be related with how SQLAlchemy sessions are handled, so some clarification in simple term would be helpful.
My configuration for work with flask sqlAlchemy is:

from flask_sqlalchemy import SQLAlchemy
db = SQLAlchemy(app)


My problem: db.session.commit() sometimes doesn't save changes. I wrote some flask endpoints which are reached via the front end requests in the user browser.
In this particular case, I'm editing a hotel ""Booking"" object altering the ""Rooms"" columns which is a Text field. 

the function does the following:

1-Query the Booking object from the dates in the request

2- Edit the Rooms column of this Booking object

3- Commit the changes ""db.session.commit()""

4- If a user has X functionality active, I make some checks calling a second function:

·4.1- This functions make some checks and query and edit another object in the database different from the ""Booking"" object I edited previously.

·4.2- At the end of this secondary function I call db.session.commit() ""Note this changes always got saved correctly in the database""

·4.3- Return the results to the previous function

5- Return results to the front end (""just before this return, I print the Booking.Rooms to make sure it looks as it should, and it does... I even tried to make a second commit after the print but before the return... But after this, sometimes Booking.Rooms are updated as expected but some other times it doesn't... I noted if repeat the action many times it finally works, but given the intermediate function ""described in point 4"" saves all his changes correctly, this causes an inconsistency in the data and drives me mad because if I repeat the action and procedure in the function of point 4 worked correctly, I can't repeat the mod Rooms action...

So, I'm now really confused if this is something I don't understand from flask sessions, for what I understand, whenever I make a new request to flask, it's an isolated session, right?
I mean, if 2 concurrent users are storing some changes in the database, a db.session.commit() from one of the users won't commit the changes from the other one, right?

Same way, if I call db.session.commit() in one request, that changes are stored in the database, and if after that ""in the same request"", I keep modding things, it's like another session, right? And the committed changes are there already safely stored? And I can still use previous objects for further modifications 

Anyway, all of this shouldn't be a problem because after the commit() I print the Booking.Rooms and looks as expected... And some times it works getting stored correctly and some times it doesn't...

Also note: When I return this result to the client, the client makes instantly a second request to the server to request updated Booking data, and then the data is returned without this expected changes committed... I suppose flask handled all the commit() before it gets the second request ""other way it wouldn't have returned the result previously..."" 

Can this be a limitation of the flask development server which can't handle correctly many requests and that when deployed with gunicorn it doesn't happen?

Any hint or clarification about Sessions would be nice, because this is pretty strange behaviour, especially that sometimes works and others don't...

And as requested here is the code, I know is not possible to reproduce, have a lot of setup behind and would need a lot of data to works as intended under same circumstances as in my case, but this should provide an overview of how the functions looks like and where are the commits I mention above. Any ideas of where can be the problem is very helpful.

#Main function hit by the frontend
@users.route('/url_endpoint1', methods=['POST'], strict_slashes=False)
@login_required
def add_room_to_booking_Api():
    try:
        bookingData = request.get_json()
        roomURL=bookingData[""roomSafeURL""]
        targetBooking = bookingData[""bookingURL""]
        startDate = bookingData[""checkInDate""]
        endDate = bookingData[""checkOutDate""]
        roomPrices=bookingData[""roomPrices""]

        booking = Bookings.query.filter_by(SafeURL=targetBooking).first() 
        alojamiento = Alojamientos.query.filter_by(id=reserva.CodigoAlojamiento).first() #owner of the booking
        room=Rooms.query.filter_by(SafeURL=roomURL).first()
        roomsInBooking=ast.literal_eval(reserva.Habitaciones) #I know, should be json.loads() and json.dumps() for better performance probably...

        #if room is available for given days add it to the booking
        if CheckIfRoomIsAvailableForBooking(alojamiento.id, room, startDate, endDate, booking) == ""OK"":

            roomsInBooking.append({""id"": room.id, ""Prices"": roomPrices, ""Guests"":[]}) #add the new room the Rooms column of the booking
            booking.Habitaciones = str(roomsInBooking)#save the new rooms data
            print(booking.Habitaciones) # check changes applied
            room.ReservaAsociada = booking.id  # associate booking and room
            for ocupante in room.Ocupantes: #associate people in the room with the booking
                ocupante.Reserva = reserva.id

            #db.session.refresh(reserva) # test I made to check if something changes but didn't worked
            if some_X_function() == True: #if user have some functionality enabled
                #db.session.begin() #another test which didn't worked
                RType = WuBook_Rooms.query.filter_by(ParentType=room.Tipo).first()
                RType=[RType] #convert to list because I resuse the function in cases with multiple types
                resultAdd = function4(RType, booking.Entrada.replace(hour=0, minute=0, second=0), booking.Salida.replace(hour=0, minute=0, second=0))
                if resultAdd[""resultado""] == True:  # ""resultado"":error, ""casos"":casos
                    return (jsonify({""resultado"": ""Error"", ""mensaje"": resultAdd[""casos""]}))

            print(booking.Habitaciones) #here I still get expected result
            db.session.commit()
            #I get this return of averything correct in my frontend but not really stored in the database
            return jsonify({""resultado"": ""Ok"", ""mensaje"": ""Room "" + str(room.Identificador) + "" added to the booking""})

        else:
            return (jsonify({""resultado"": ""Error"", ""mensaje"": ""Room "" + str(room.Identificador) + "" not available to book in target dates""}))

    except Exception as e:
        #some error handling which is not getting hit now
        db.session.rollback()
        print(e, "": en linea"", lineno())
        excepcion = str((''.join(traceback.TracebackException.from_exception(e).format()).replace(""\n"",""&lt;/br&gt;""), ""&lt;/br&gt;Excepcion emitida ne la línea: "", lineno()))
        sendExceptionEmail(excepcion, current_user)
        return (jsonify({""resultado"":""Error"",""mensaje"":""Error""}))

#function from point 4
def function4(RType, startDate, endDate): 
    delta = endDate - startDate
    print(startDate, endDate)
    print(delta)
    for ind_type in RType: 
        calendarUpdated=json.loads(ind_type.updated_availability_by_date)
        calendarUpdatedBackup=calendarUpdated
        casos={}
        actualizar=False
        error=False
        for i in range(delta.days):
            day = (startDate + timedelta(days=i))
            print(day, i)
            diaString=day.strftime(""%d-%m-%Y"")
            if day&gt;=datetime.now() or diaString==datetime.now().strftime(""%d-%m-%Y""): #only care about present and future dates
                disponibilidadLocal=calendarUpdated[diaString][""local""]
                yaReservadas=calendarUpdated[diaString][""local_booked""]
                disponiblesChannel=calendarUpdated[diaString][""avail""]
                #adjust availability data
                if somecondition==True:
                    actualizar=True
                    casos.update({diaString:""Happened X""})
                else:
                    actualizar=False
                    casos.update({diaString:""Happened Y""})
                    error=""Error""
        if actualizar==True: #this part of the code is hit normally and changes stored correctly
            ind_type.updated_availability_by_date=json.dumps(calendarUpdated)
            wubookproperty=WuBook_Properties.query.filter_by(id=ind_type.PropertyCode).first()
            wubookproperty.SyncPending=True
            ind_type.AvailUpdatePending=True
        elif actualizar==False: #some error occured, revert changes
            ind_type.updated_availability_by_date = json.dumps(calendarUpdatedBackup)

    db.session.commit()#this commit persists 
    return({""resultado"":error, ""casos"":casos}) #return to main function with all this chnages stored

","Realized nothing was wrong at session level, it was my fault in another function client side which sends a request to update same data which is just being updated but with the old data... so in fact, I was getting the data saved correctly in the database but overwrote few milliseconds later. It was just a return statement missing in a javascript file to avoid this outcome...
",,,false,
https://stackoverflow.com/questions/54888558,false,"The issue does not meet the criteria for deeper analysis as it is related to the syntax for defining a self-referential many-to-many relationship in SQLAlchemy, rather than an API exhibiting unexpected failures or unpredictable behaviors under specific runtime conditions.",,,,,,,Syntax for late-binding many-to-many self-referential relationship,"I have found many explanations for how to create a self-referential many-to-many relationship (for user followers or friends) using a separate table or class:
Below are three examples, one from Mike Bayer himself:

Many-to-many self-referential relationship in sqlalchemy
How can I achieve a self-referencing many-to-many relationship on the SQLAlchemy ORM back referencing to the same attribute?
Miguel Grinberg's Flask Megatutorial on followers

But in every example I've found, the syntax for defining the primaryjoin and secondaryjoin in the relationship is an early-binding one:
# this relationship is used for persistence
friends = relationship(""User"", secondary=friendship, 
                       primaryjoin=id==friendship.c.friend_a_id,
                       secondaryjoin=id==friendship.c.friend_b_id,
)

This works great, except for one circumstance: when one uses a Base class to define the id column for all of your objects as shown in Mixins: Augmenting the base from the docs
My Base class and followers table are defined thusly:
from flask_sqlchalchemy import SQLAlchemy
db = SQLAlchemy()

class Base(db.Model):
    __abstract__ = True
    id = db.Column(db.Integer, primary_key=True)

user_flrs = db.Table(
    'user_flrs',
    db.Column('follower_id', db.Integer, db.ForeignKey('user.id')),
    db.Column('followed_id', db.Integer, db.ForeignKey('user.id')))

But now I have trouble with the followers relationship that has served me loyally for a while before I moved the id's to the mixin:
class User(Base):
    __table_name__ = 'user'
    followed_users = db.relationship(
        'User', secondary=user_flrs, primaryjoin=(user_flrs.c.follower_id==id),
        secondaryjoin=(user_flrs.c.followed_id==id),
        backref=db.backref('followers', lazy='dynamic'), lazy='dynamic')

db.class_mapper(User)  # trigger class mapper configuration

Presumably because the id is not present in the local scope, though it seems to throw a strange error for that:

ArgumentError: Could not locate any simple equality expressions involving locally mapped foreign key columns for primary join condition 'user_flrs.follower_id = :follower_id_1' on relationship User.followed_users.  Ensure that referencing columns are associated with a ForeignKey or ForeignKeyConstraint, or are annotated in the join condition with the foreign() annotation. To allow comparison operators other than '==', the relationship can be marked as viewonly=True.

And it throws the same error if I change the parentheses to quotes to take advantage of late-binding. I have no idea how to annotate this thing with foreign() and remote() because I simply don't know what sqlalchemy would like me to describe as foreign and remote on a self-referential relationship that crosses a secondary table! I've tried many combinations of this, but it hasn't worked thus far.
I had a very similar (though not identical) problem with a self-referential relationship that did not span a separate table and the key was simply to convert the remote_side argument to a late-binding one. This makes sense to me, as the id column isn't present during an early-binding process.
If it is not late-binding that I am having trouble with, please advise. In the current scope, though, my understanding is that id is mapped to the Python builtin id() and thus will not work as an early-binding relationship.
Converting id to Base.id in the joins results in the following error:

ArgumentError: Could not locate any simple equality expressions involving locally mapped foreign key columns for primary join condition 'user_flrs.follower_id = ""&lt;name unknown&gt;""' on relationship User.followed_users.  Ensure that referencing columns are associated with a ForeignKey or ForeignKeyConstraint, or are annotated in the join condition with the foreign() annotation. To allow comparison operators other than '==', the relationship can be marked as viewonly=True.

","You can't use id in your join filters, no, because that's the built-in id() function, not the User.id column.

You have three options:


Define the relationship after creating your User model, assigning it to a new User attribute; you can then reference User.id as it has been pulled in from the base:

class User(Base):
    # ...

User.followed_users = db.relationship(
    User,
    secondary=user_flrs,
    primaryjoin=user_flrs.c.follower_id == User.id,
    secondaryjoin=user_flrs.c.followed_id == User.id,
    backref=db.backref('followers', lazy='dynamic'),
    lazy='dynamic'
)

Use strings for the join expressions. Any argument to relationship() that is a string is evaluated as a Python expression when configuring the mapper, not just the first argument:

class User(Base):
    # ...

    followed_users = db.relationship(
        'User',
        secondary=user_flrs,
        primaryjoin=""user_flrs.c.follower_id == User.id"",
        secondaryjoin=""user_flrs.c.followed_id == User.id"",
        backref=db.backref('followers', lazy='dynamic'),
        lazy='dynamic'
    )

Define the relationships as callables; these are called at mapper configuration time to produce the final object:

class User(Base):
    # ...

    followed_users = db.relationship(
        'User',
        secondary=user_flrs,
        primaryjoin=lambda: user_flrs.c.follower_id == User.id,
        secondaryjoin=lambda: user_flrs.c.followed_id == User.id,
        backref=db.backref('followers', lazy='dynamic'),
        lazy='dynamic'
    )



For the latter two options, see the sqlalchemy.orgm.relationship() documentation:


  Some arguments accepted by relationship() optionally accept a callable function, which when called produces the desired value. The callable is invoked by the parent Mapper at “mapper initialization” time, which happens only when mappers are first used, and is assumed to be after all mappings have been constructed. This can be used to resolve order-of-declaration and other dependency issues, such as if Child is declared below Parent in the same file*[.]*
  
  [...]
  
  When using the Declarative extension, the Declarative initializer allows string arguments to be passed to relationship(). These string arguments are converted into callables that evaluate the string as Python code, using the Declarative class-registry as a namespace. This allows the lookup of related classes to be automatic via their string name, and removes the need to import related classes at all into the local module space*[.]*
  
  [...]
  
  
  primaryjoin –  
  
  [...]
  
  primaryjoin may also be passed as a callable function which is evaluated at mapper initialization time, and may be passed as a Python-evaluable string when using Declarative.
  
  
  [...]
  
  
  secondaryjoin –
  
  [...]
  
  secondaryjoin may also be passed as a callable function which is evaluated at mapper initialization time, and may be passed as a Python-evaluable string when using Declarative.
  


Both the string and the lambda define the same user_flrs.c.followed_id == User.id / user_flrs.c.follower_id == User.id expressions as used in the first option, but because they are given as a string and callable function, respectively, you postpone evaluation until SQLAlchemy needs to have those declarations finalised.
",,,false,
https://stackoverflow.com/questions/49154668,true,The issue involves an API exhibiting unexpected behavior. The @hybrid_property decorator in SQLAlchemy is not displaying the returned value as expected in the Mako template after a library update. This behavior change in SQLAlchemy can be considered as an unexpected failure or unpredictable behavior under specific runtime conditions.,,,,,,,SQLAlchemy plus FormAlchemy displays @hybrid_property name and not the returned value,"I have recently updated my SQLAlchemy and FormAlchemy to the newest ones and a strange thing has occurred. 

I do use Mako Templates to display data from my model. 

Python:

class Asset(object):
        id = Column(Integer, primary_key=True, nullable=False)
        ...

        @hybrid_property
        def underscore_name(self):
             return ""x_underscore_name_x""


Mako template:

  &lt;li&gt;Item: ${Asset.underscore_name}&lt;/li&gt;


Before the upgrade the web page rendered text was:

Item: x_underscore_name_x


After the upgrade it shows:

Item: Asset.underscore_name


Important! The method is being executed but the returned result is not being rendered on the webpage. Any ideas?

EDIT:

The library responsible for this behaviour is SQL Alchemy &gt;=1.1.0. Version 1.0.19 does not have this issue. Let's see what's the response from the developers.
","By the courtesy of Michael Bayer who commented on reported issue #4214:
hybrid_property name being displayed rather than the returned value

The value of Asset.underscore_name is a SQL expression, which since the changes described in http://docs.sqlalchemy.org/en/latest/changelog/migration_11.html#hybrid-properties-and-methods-now-propagate-the-docstring-as-well-as-info is derived from the clause_element() method of the returned object that Core understands. The hybrid you illustrate above is not correctly written because it is not returning a Core SQL expression object:

    s = Session()
    print(s.query(Asset.underscore_name))

sqlalchemy.exc.InvalidRequestError: SQL expression, column, or mapped entity expected - got ''x_underscore_name_x''


if you are looking for a class-bound attribute to return a string, just assign it:

class Asset(...):
    underscore_name = ""x_underscore_name_x""


or if you want that to be a method that is callable at the classlevel, use a @classproperty, there's a recipe for that here: https://stackoverflow.com/a/5191224/34549 and SQLAlchemy has one you can copy: 
https://github.com/zzzeek/sqlalchemy/blob/master/lib/sqlalchemy/util/langhelpers.py#L1140

IMO there's no bug here because this is not the correct usage of @hybrid_property

Thank you very much Michael!
",,,false,
https://stackoverflow.com/questions/47710463,false,The issue does not meet the criteria for deeper analysis as it does not exhibit unexpected behavior under specific runtime conditions. It appears to be a concurrency issue related to database locking rather than an API-related problem.,,,,,,,SQLite: the database is locked,"I'm using flask with sqlalchemy and sqlite db. I have 2 ajax that send some data from html to my .py file.

The problem is every time when i do any of these 2 operations, the second one become unavailable because of lock of db. Also, if first chosen action will be deleting, then exception firing no matter what operation will be chosen after. with first choice of adding, we can add without limitations that's strange too, because functions seem similar.

I've tried timeouts, closing sessions in a different ways, the result is always the same.

Here are two functions-handlers:

app = Flask(__name__)
csrf = CSRFProtect(app)

app.config.from_object('config')
db = SQLAlchemy(app)

import forms
import models


@app.route('/delete', methods = ['GET', 'POST'])
def delete():
    if request.method == ""POST"":
        if request.form['type'] == ""delete"":
            print(""delete"")
            engine = create_engine(SQLALCHEMY_DATABASE_URI)
            Session = sessionmaker(bind=engine)
            session = Session()

            try:
                print(""try"")
                requested = request.form['id']
                print(requested)
                models.Income.query.filter(models.Income.id == requested).delete()
                session.commit()
            except:
                print(""rollback"")
                session.rollback()
            finally:
                print(""fin"")
                session.close()

            ellist = models.Income.query.all()
            return render_template(""incomeSection.html"", list=ellist)


@app.route('/add', methods=['GET', 'POST'])
def add():
    if request.method == ""POST"":
        if request.form['type'] == ""add"":
            print('add')
            engine = create_engine(SQLALCHEMY_DATABASE_URI)
            Session = sessionmaker(bind=engine)
            session = Session()

            try:
                print(""try"")
                newItem = models.Income(name=request.form['name'], tag=request.form['tag'],
                                        account=request.form['account'],
                                        date=date(*(int(i) for i in request.form['date'].split(""-""))))
                session.add(newItem)
                session.commit()
            except:
                print('rollback')
                session.rollback()
            finally:
                print(""fin"")
                session.close()

            ellist = models.Income.query.all()
            print(ellist)
            return render_template(""incomeSection.html"", list=ellist)


I've read that this exception caused by non-closed connections, but I have .close() in every finally block. I think the problem might be because of the db = SQLAlchemy(app) but I don't know how to fix if that is the case. Because I use this variable to connect with db in forms.py where I have the form template and in models.py where I defined my tables within db.
","So, aperrently, thete was an issue with number of connections.
The thing that solved my problem was the context manager for sqlalchemy, i used this one:

class SQLAlchemyDBConnection(object):

def __init__(self, connection_string):
    self.connection_string = connection_string
    self.session = None

def __enter__(self):
    engine = create_engine(self.connection_string)
    Session = sessionmaker()
    self.session = Session(bind=engine)
    return self

def __exit__(self, exc_type, exc_val, exc_tb):
    self.session.commit()
    self.session.close()


and in the handler just 

with SQLAlchemyDBConnection(SQLALCHEMY_DATABASE_URI) as db:
            newItem = models.Income(*your params*)
            db.session.add(newItem)


And now it works fine, but i still don't know what was the issue in version that was earlier. They are seem to be same just with or without context manager
",,,false,
https://stackoverflow.com/questions/35247731,true,"The issue involves an SQL query generated by SQLAlchemy that returns more than one row, causing an error. This behavior is related to the usage of a subquery in the SQL statement and the expectation that it should return a single result. The issue is specific to the interaction between the SQLAlchemy query and the underlying database.",,,,,,,more than one row returned by a subquery used as an expression,"All my SQLAlchemy models query just fine, but there is one that gives me:

more than one row returned by a subquery used as an expression


This is the (shortened) SQL generated by SQLAlchemy:

SELECT poi.key
,poi.pok
,poi.noc
,coalesce((
    SELECT CASE 
        WHEN (sum(item.noc) IS NULL)
            THEN NULL
        ELSE :param_1
        END AS anon_1
    FROM item,poi
    WHERE poi.key = item.poi_key
    GROUP BY item.key
    ), poi.noc) AS coalesce_1
,coalesce((
    SELECT sum(soi.noc) AS sum_1
    FROM soi
    WHERE soi.poi_key = poi.key
        AND soi.is_shipped = 0
    ), :param_2) AS coalesce_2
,coalesce((
    SELECT CASE 
        WHEN (sum(item.noc) IS NULL)
            THEN NULL
        ELSE :param_1
        END AS anon_1
    FROM item,poi
    WHERE poi.key = item.poi_key
    GROUP BY item.key
    ), poi.noc) - ee((
    SELECT sum(soi.noc) AS sum_1
    FROM soi
    WHERE soi.poi_key = poi.key
        AND soi.is_shipped = 0
    ), :param_2) AS anon_2
FROM poi


The model is:

class POI(Base):
    key = Column(Integer, primary_key=True)
    pok = Column(Integer, nullable=False)
    noc = Column(Integer, nullable=False)
    __table_args__ = (UniqueConstraint(pok,),
        ForeignKeyConstraint([pok],),{})


I'm using scoped_session since my application is multi-threaded and I thought that could be the problem. But it wasn't. I've tried every variation of using Session, but the problem persist. The strange thing is that upon application initialization, when multiple threads are started that fetch data from the db, only this query throws the error. Invoking only this query (manually) works fine somehow. So the problem is apparently in interaction with other queries.

The error is a bit vague to me, but I think the problem is that a subquery should return one result, not many. I'm at a loss where to start looking for the answer. Is it a threading issue? A Session issue? Something else?
","I believe you don't need FROM item,poi in 2 subqueries and I also remove the group by in those too. Regading the case expression involving :param_1 I am not sure if what I suggest below is funcionally correct or not, but you need to create a SUM() value before you can test if that SUM() IS NULL.

SELECT
      poi.key
    , poi.pok
    , poi.noc
    , COALESCE((
            SELECT SUM(COALESCE(item.noc,:param_1))
            FROM item
            WHERE poi.key = item.poi_key
      )
      , poi.noc) AS coalesce_1
    , COALESCE((
            SELECT
                  SUM(soi.noc) AS sum_1
            FROM soi
            WHERE soi.poi_key = poi.key
                  AND soi.is_shipped = 0
      )
      , @param_2) AS coalesce_2
    , COALESCE((
            SELECT SUM(COALESCE(item.noc,:param_1))
            FROM item
            WHERE poi.key = item.poi_key
      )
      , poi.noc) - ee((
            SELECT
                  SUM(soi.noc) AS sum_1
            FROM soi
            WHERE soi.poi_key = poi.key
                  AND soi.is_shipped = 0
      )
      , :param_2) AS anon_2
FROM poi

",,,false,
https://stackoverflow.com/questions/24375722,false,The issue does not involve an API exhibiting unexpected failures or unpredictable behaviors. It is related to a discrepancy between the expected and observed results when dividing a timedelta column by a float value in SQLAlchemy. This behavior is likely due to the internal representation and precision of timedelta objects in SQLAlchemy.,,,,,,,sqlalchemy: dividing timedelta column by float giving strange results,"I'm trying to perform some manipulation on a column inside the query. I want to divide a timedelta column by 60. Unfortunately it is giving strange results.

In my query I return two datetimes, the timedelta between these two datetimes, and the timedelta between these two datetimes divided by 2. An example of the output:

(datetime.datetime(2013, 1, 1, 0, 0, 8), 
datetime.datetime(2013, 1, 1, 0, 7, 32), 
datetime.timedelta(0, 444), 
Decimal('362.0000'))


However, the timedelta 444 divided by 2 does not equal 362.0000. But this is what sqlalchemy is giving me. Is there anyway to get the correct result?

Its important to do this manipulation in the query because I am also trying to use the above as a subquery and average over the results but I'm getting strange averages back as well.

Here is my query:

    sub = session.query( 
                        merge.c.time_received,
                        merge.c.time_sent,
                        func.timediff(
                            merge.c.time_sent,
                            merge.c.time_received
                        ),
                        func.timediff(
                            merge.c.time_sent,
                            merge.c.time_received
                        ) / 2.0
                    ). \
                    filter(
                        merge.c.time_received != None,
                        merge.c.time_sent != None,
                    ). \
                    limit(1)

","I am not sure what happens when you divide TIME by a number. But maybe converting it to seconds first using TIME_TO_SEC(time) will do the trick.

I assumed mysql` is used.
",,,false,
https://stackoverflow.com/questions/17787042,false,The issue is not related to an API exhibiting unexpected failures or unpredictable behaviors. It is about encoding or escaping a password in a SQLAlchemy connection string to successfully connect to a MySQL database. This is a configuration issue rather than an API-related problem.,,,,,,,SqlAlchemy connection string,"I faced very strange issue - my solution using sqlalchemy cannot connects to database. It depends on password I am using. for example, following records are working perfect:

PWD='123123123'
USR='test_user';
SQLALCHEMY_DATABASE_URI = 'mysql://{}:{}@localhost:3306/test_db'.format(USR, PWD)

#final result is 'mysql://test_user:123123123@localhost:3306/test_db'.format(USR, PWD)


But when I trying to put something serious to password (like '8yq+aB&amp;k$V') connection failed. How to 'escape' or encode password somehow that sqlalchemy passed it to mysql succesfully?
",,,,false,
https://stackoverflow.com/questions/41514960,false,"The issue does not involve an API exhibiting unexpected failures or unpredictable behaviors. It is a question about utilizing existing frameworks (Flask, SQLAlchemy, Marshmallow) to accomplish specific tasks such as importing data models, generating schemas, and formatting JSON responses. This is more of a design and implementation question rather than an API-related problem.",,,,,,,"Use DB data model to generate SQLAlchemy models, schemas, and JSON response","Using Flask and SQLAlchemy for a Python webapp, my goal is to create a system in which I can:


Import data models from an existing PostgreSQL DB, and map these to fields in corresponding SQLAlchemy models
Use these SQLAlchemy models to automatically generate a schema. This schema will then be used to perform data validation on user-submitted data.(I'm currently trying to use Marshmallow, but am open to other suggestions).
Perform JSON response formatting using the schema generated in step 2. (I'm currently trying to format my responses according to JsonAPI's schema - this could change if need be, but I would prefer it).


All of this will be packaged together in a single layer that should allow for simple data access, validation, and response formatting when writing an API, hopefully without ever having to manually define data models or schemas past the definitions that already exist in the DB. This leads me to my question:

Can I utilize existing frameworks to do everything that I'm trying to accomplish? I wouldn't expect that there's one single library that could do everything, but I'm hoping to be able to leverage several existing frameworks. However, I run into direct conflicts, particularly in steps 2 and 3. The stack I've been attempting to use so far is as follows:


Flask (My web framework)
SQLAlchemy (Used to access my data through reflection, see below)
Flask-Marshmallow (To generate a schema for data validation)
Marshmallow-JsonAPI (To format the JSON response according to the JsonAPI spec)


I have a solution for step 1: SQLAlchemy's reflection. This allows me to read the tables in an existing DB and map them to SQLAlchemy models. This works beautifully.

The combination of steps 2 and 3 is where it gets hazy - I'm trying to use Flask-Marshmallow and Marshmallow-JsonAPI. Flask-Marshmallow has the ModelSchema class, which allows you to generate a schema by passing it an existing SQLAlchemy Model. I can pass it my SQLAlchemy model that was generated in step 1, fulfilling my criteria for step 2. As for Marshmallow-JsonAPI, it has functionality such that you can define a schema for your model, and it will automatically create JSONAPI-compliant responses, fulfilling my criteria for step 3. I can individually get each of these frameworks to do what I need.

Unfortunately, since my schema has to inherit from Schema classes in both the Flask-Marshmallow and Marshmallow-JSONAPI, I run into issues. My Model for a ""Dummy"" class looks like this:

import flask
from marshmallow_jsonapi import Schema, fields
from sqlalchemy.ext.declarative import DeferredReflection
from flask_marshmallow import Marshmallow
from flask_sqlalchemy import SQLAlchemy

app = flask.Flask(__name__)
DB = SQLAlchemy()
DB.init_app(app)
MA = Marshmallow(app)
DeferredReflection.prepare(DB.get_engine(app))

class Dummy(DeferredReflection, DB.Model):
    __tablename__ = ""dummy""  # This model will be reflected from the Dummy table


class DummySchema(MA.ModelSchema, Schema):  # The problem line
    class Meta:
        model = Dummy  # Create schema for the Dummy model


Upon trying to start up my API, I get strange errors like this:

api_1        | [2017-01-05 23:16:13,379] ERROR in app: Exception on /api/dummy [POST]
api_1        | Traceback (most recent call last):
api_1        |   File ""/usr/local/lib/python3.5/site-packages/flask/app.py"", line 1612, in full_dispatch_request
api_1        |     rv = self.dispatch_request()
api_1        |   File ""/usr/local/lib/python3.5/site-packages/flask/app.py"", line 1598, in dispatch_request
api_1        |     return self.view_functions[rule.endpoint](**req.view_args)
api_1        |   File ""/usr/local/lib/python3.5/site-packages/flask_restful/__init__.py"", line 477, in wrapper
api_1        |     resp = resource(*args, **kwargs)
api_1        |   File ""/usr/local/lib/python3.5/site-packages/flask/views.py"", line 84, in view
api_1        |     return self.dispatch_request(*args, **kwargs)
api_1        |   File ""/usr/local/lib/python3.5/site-packages/flask_restful/__init__.py"", line 587, in dispatch_request
api_1        |     resp = meth(*args, **kwargs)
api_1        |   File ""/dummyproj/api/src/dummyproj/api/v1/dummy_resource.py"", line 34, in post
api_1        |     dummy_schema = dummy.DummySchema()
api_1        |   File ""/usr/local/lib/python3.5/site-packages/marshmallow_sqlalchemy/schema.py"", line 143, in __init__
api_1        |     super(ModelSchema, self).__init__(*args, **kwargs)
api_1        |   File ""/usr/local/lib/python3.5/site-packages/marshmallow_jsonapi/schema.py"", line 81, in __init__
api_1        |     super(Schema, self).__init__(*args, **kwargs)
api_1        |   File ""/usr/local/lib/python3.5/site-packages/marshmallow/schema.py"", line 358, in __init__
api_1        |     self._update_fields(many=many)
api_1        |   File ""/usr/local/lib/python3.5/site-packages/marshmallow/schema.py"", line 750, in _update_fields
api_1        |     self.__set_field_attrs(ret)
api_1        |   File ""/usr/local/lib/python3.5/site-packages/marshmallow/schema.py"", line 772, in __set_field_attrs
api_1        |     self.on_bind_field(field_name, field_obj)
api_1        |   File ""/usr/local/lib/python3.5/site-packages/marshmallow_jsonapi/schema.py"", line 164, in on_bind_field
api_1        |     field_obj.load_from = self.inflect(field_name)
api_1        |   File ""/usr/local/lib/python3.5/site-packages/marshmallow_jsonapi/schema.py"", line 190, in inflect
api_1        |     return self.opts.inflect(text) if self.opts.inflect else text
api_1        | AttributeError: 'SchemaOpts' object has no attribute 'inflect'


It seems that since my Schema class inherits from two different superclasses, it causes issues. I would expect the two marshmallow libraries to be pretty compatible, but there does seem to be an issue with this.

The upshot of all of this is that I'm not quite sure what to try next. I'm not really expecting any specific advice with the Marshmallow frameworks, more just trying to show my train of thought. Is there any sort of best practice surrounding this sort of design, or am I trying to solve too much at once?

(New to both Python and SO - apologies if any of this is unclear).
","I'm successfully combining marshmallow-jsonapi + marshmallow-sqlalchemy. Here's the magic:

import marshmallow
import marshmallow_jsonapi
import marshmallow_jsonapi.flask
import marshmallow_sqlalchemy

import myapp.database as db


def make_jsonapi_schema_class(model_class):
    class SchemaOpts(marshmallow_sqlalchemy.SQLAlchemyAutoSchemaOpts, marshmallow_jsonapi.SchemaOpts):
        pass

    class Schema(marshmallow_sqlalchemy.SQLAlchemyAutoSchema, marshmallow_jsonapi.flask.Schema):
        OPTIONS_CLASS = SchemaOpts

        @marshmallow.post_load
        def make_instance(self, data, **kwargs):
            # Return deserialized data as a dict, not a model instance
            return data

        # You can add default behavior here, for example
        # id = fields.Str(dump_only=True)

    # https://marshmallow-sqlalchemy.readthedocs.io/en/latest/recipes.html#automatically-generating-schemas-for-sqlalchemy-models
    class Meta:
        # Marshmallow-SQLAlchemy
        model = model_class
        sqla_session = db.session

        # Marshmallow-JSONAPI
        type_ = model_class.__name__.lower()
        self_view = type_ + '_detail'
        self_view_kwargs = {'id': '&lt;id&gt;'}
        self_view_many = type_ + '_list'

    schema_class = type(model_class.__name__ + 'Schema', (Schema,), {'Meta': Meta})
    return schema_class


You then call

FooSchema = make_jsonapi_schema_class(Foo)


to generate the Marshmallow schema class from your SQLAlchemy declarative model. You can then in turn subclass that class if you want to customize it.

(To implement the REST API, I'm using Flask-REST-JSONAPI, which is built on top of Flask + SQLAlchemy + marshmallow-jsonapi.)
",,,false,
https://stackoverflow.com/questions/63435264,true,"The issue involves the use of the filter .in_(list) method in SQLAlchemy with pyodbc, which leads to a 'Maximum number of parameters' error. This behavior is specific to the combination of SQLAlchemy, pyodbc, and SQL Server, and is not a general problem with the API.",SQLAlchemy,filter .in_(),"When using the filter .in_() method with a large list of values in SQLAlchemy with pyodbc and SQL Server, a 'Maximum number of parameters in the SQL query is 2100' error is raised. This error occurs because the SQL Server has a limit on the number of parameters that can be used in a single query.",The filter .in_() method works as expected when the number of parameters in the list is within the limit set by the SQL Server.,The issue is triggered when the number of parameters in the list exceeds the limit set by the SQL Server.,"This issue might be challenging to detect during development and testing because it is specific to the combination of SQLAlchemy, pyodbc, and SQL Server, and may not be encountered until a large number of parameters are used.",&quot;Maximum number of parameters&quot; error with filter .in_(list) using pyodbc,"One of our queries that was working in Python 2 + mxODBC is not working in Python 3 + pyodbc; it raises an error like this: Maximum number of parameters in the sql query is 2100. while connecting to SQL Server. Since both the printed queries have 3000 params, I thought it should fail in both environments, but clearly that doesn't seem to be the case here. In the Python 2 environment, both MSODBC 11 or MSODBC 17 works, so I immediately ruled out a driver related issue.
So my question is:

Is it correct to send a list as multiple params in SQLAlchemy because the param list will be proportional to the length of list? I think it looks a bit strange; I would have preferred concatenating the list into a single string because the DB doesn't understand the list datatype.
Are there any hints on why it would be working in mxODBC but not pyodbc? Does mxODBC optimize something that pyodbc does not? Please let me know if there are any pointers - I can try and paste more info here. (I am still new to debugging SQLAlchemy.)

Footnote: I have seen lot of answers that suggest to chunk the data, but because of 1 and 2, I wonder if I am doing the correct thing in the first place.
(Since it seems to be related to pyodbc, I have raised an internal issue in the official repository.)
import sqlalchemy
import sqlalchemy.orm

from sqlalchemy import MetaData, Table
from sqlalchemy.ext.declarative import declarative_base

from  sqlalchemy.orm.session import Session

Base = declarative_base()

create_tables = """"""
CREATE TABLE products(
    idn NUMERIC(8) PRIMARY KEY
);
""""""

check_tables = """"""   
SELECT * FROM products;
""""""

insert_values = """"""
INSERT INTO products
(idn)
values
(1),
(2);
""""""

delete_tables = """"""
DROP TABLE products;
""""""

engine = sqlalchemy.create_engine('mssql+pyodbc://user:password@dsn')
connection = engine.connect()
cursor = engine.raw_connection().cursor()
Session = sqlalchemy.orm.sessionmaker(bind=connection)
session = Session()

session.execute(create_tables)

metadata = MetaData(connection)

class Products(Base):
   __table__ = Table('products', metadata, autoload=True)

try:
    session.execute(check_tables)
    session.execute(insert_values)
    session.commit()
    query = session.query(Products).filter(
        Products.idn.in_(list(range(0, 3000)))
    )
    query.all()
    f = open(""query.sql"", ""w"")
    f.write(str(query))
    f.close()
finally:
    session.execute(delete_tables)
    session.commit()

","When you do a straightforward .in_(list_of_values) SQLAlchemy renders the following SQL ...
SELECT team.prov AS team_prov, team.city AS team_city 
FROM team 
WHERE team.prov IN (?, ?)

... where each value in the IN clause is specified as a separate parameter value. pyodbc sends this to SQL Server as ...
exec sp_prepexec @p1 output,N'@P1 nvarchar(4),@P2 nvarchar(4)',N'SELECT team.prov AS team_prov, team.city AS team_city, team.team_name AS team_team_name 
FROM team 
WHERE team.prov IN (@P1, @P2)',N'AB',N'ON'

... so you hit the limit of 2100 parameters if your list is very long. Presumably, mxODBC inserted the parameter values inline before sending it to SQL Server, e.g.,
SELECT team.prov AS team_prov, team.city AS team_city 
FROM team 
WHERE team.prov IN ('AB', 'ON')

You can get SQLAlchemy to do that for you with
provinces = [""AB"", ""ON""]
stmt = (
    session.query(Team)
    .filter(
        Team.prov.in_(sa.bindparam(""p1"", expanding=True, literal_execute=True))
    )
    .statement
)
result = list(session.query(Team).params(p1=provinces).from_statement(stmt))

",,,false,
https://stackoverflow.com/questions/41240754,false,"The issue is related to user permissions and access rights on the file system, rather than a problem with the API itself.",,,,,,,Why can I import certain modules in Python only with administrator rights?,"I'm struggling with some strange issues in Python 2.7. I wrote a very long tool where I import different modules, which I had to install first using pip. The tool is to be shared within the company, where different users have different rights on their specific machines.
The problem occurred when another user logged into my machine (I'm having administrator rights there) and tried to use the tool. He was unable to run it, because specific modules could not be imported because of his status as a ""non-admin"". 

The error message is simply ""No module named XY"".
When we looked into the file system, we found that we were not able to look into the folder where the module had been installed, simply because the access was denied by the system. 
We also got this error message when trying to run pip from the cmd; it prints ""Access denied"" and won't do anything.

How is it possible, that some modules can be accessed by anyone, while others can't? And how can I get around this problem?

Specifically, I'm talking about sqlalchemy and pyodbc.

Thanks a lot in advance.

EDIT 1: Oh, and we're talking about Windows here, not Linux...

EDIT 2: Due to company policy it is not possible to set administrator permissions to all users. I tried, as suggested, but it didn't work and I learned that it's not possible within the company.
","Got it...

Following the advice of Nabeel Ahmed, I first uninstalled the packages which caused the issues from my admin account. Then I changed the script to

pip install --user {module_name}


and voila... it works for all users now.

Thanks a lot for you help, guys!
","One simply solution is set permissions for site-package directory (where the packages gt installed) as per usable by all i.e. read and execute permission for all on the directory:

sudo chmod -Rv ugo+rX /usr/lib/python2.7/site-packages/


Also for the lib64 packages - the path to site-packages may vary for various Linux distros.



Edit 1: For windows look into this 'File and Folder Permissions' for setting Read &amp; Execute permissions for all, for a file or folder (i.e. site-packges) 

The path 'd be - C:\Python27\Lib\site-packages



Edit 2: in apropos of:


  EDIT 2: Due to company policy it is not possible to set administrator
  permissions to all users. I tried, as suggested, but it didn't work
  and I learned that it's not possible within the company.


if so, simply install sqlalchemy (or any other package) for specific user using pip:

pip install --user {module_name} 


Source: Per user site-packages directory.
","You should either use virtualenv as stated before or set the proper permissions to the site-packages folder. I should be in C:\Python27\Lib.
",false,
https://stackoverflow.com/questions/19963083,false,"The issue is related to IPython and its interaction with the SQLite database, rather than a problem with the SQLAlchemy API.",,,,,,,SQLAlchemy + SQLite Locking in IPython Notebook,"I'm getting an OperationalError: (OperationalError) database is locked error when connection via SQLAlchemy in an IPython notebook instance and I'm not sure why.

I've written a Python interface to a SQLite database using SQLAlchemy and the Declarative Base syntax. I import the database models into an IPython notebook to explore the data. This worked just fine this morning. Here is the code:

from psf_database_interface import session, PSFTable
query = session.query(PSFTable).first()


But this afternoon after I closed my laptop with IPython running (it restarts the server just fine) I started getting this error. It's strange because I can still open the database from the SQLite3 command line tool and query data. I don't expect any other processes to be connecting to this database and running fuser on the database confirms this. My application is not using any concurrent processes (in the code I've written, IDK if something is buried in SQLAlchemy or IPython), and even if it were I'm just doing a read operation, which SQLite does support concurrently.

I've tried restarting the IPython kernel as well as killing and restarting the IPython notebook server. I've tried creating a backup of the database and replacing the database with the backup as suggested here: https://stackoverflow.com/a/2741015/1216837. Lastly, out of desperation, I tried adding the following to see if I could clean out something stuck in the session somehow:

print session.is_active
session.flush()
session.close()
session.close_all()
print session.is_active


Which returns True and True. Any ideas?

Update: I can run the code snippet that is causing errors from a python file without any issues, the issue only occurs in IPython.
","I faced the same problem. I can run python scripts but the IPython raise the below exception.

You need to check with fuser there is no process which is using this. But if you cannot find anything and your history of commands are not important to you, you can use the following workaround.

When I deleted the /home/my_user/.ipython/profile_default/history.sqlite file, I can start the IPython. The history is empty as I mentioned above.

    $ ipython                                                             
    [TerminalIPythonApp] ERROR | Failed to create history session in /home/my_user/.ipython/profile_default/history.sqlite. History will not be saved.
    Traceback (most recent call last):
    File ""/home/esadrfa/libs/anaconda3/lib/python3.6/site-packages/IPython/core/history.py"", line 543, in __init__
        self.new_session()
    File ""&lt;decorator-gen-22&gt;"", line 2, in new_session
    File ""/home/esadrfa/libs/anaconda3/lib/python3.6/site-packages/IPython/core/history.py"", line 58, in needs_sqlite
        return f(self, *a, **kw)
    File ""/home/esadrfa/libs/anaconda3/lib/python3.6/site-packages/IPython/core/history.py"", line 570, in new_session
        self.session_number = cur.lastrowid
    sqlite3.OperationalError: database is locked
    [TerminalIPythonApp] ERROR | Failed to open SQLite history :memory: (database is locked).

",,,false,
https://stackoverflow.com/questions/39542204,false,"The issue is specific to the combination of SQLAlchemy, cx_Oracle, and Oracle DB, and is not a general problem with the API.",,,,,,,Intermittent &quot;ORA-01458: invalid length inside variable character string&quot; error when inserting data into Oracle DB using SQLAlchemy,"I am working on a Python 3.5 server project and using SQLAlchemy 1.0.12 with cx_Oracle 5.2.1 to insert data into Oracle 11g. I noticed that many of my multi-row table insertions are failing intermittently with ""ORA-01458: invalid length inside variable character string"" error. 

I generally insert a few thousand to a few tens of thousands of rows at a time, and the data is mostly composed of strings, Pandas timestamps, and floating point numbers. I have made the following observations:


The error occurs on both Windows and Linux host OS for the Python server 
The error always occurs intermittently, even when the data doesn't change
If I don't insert floating point numbers, or if I round them, the error happens less often but still happens
If I insert the rows one at a time I don't encounter the error (but this is unacceptable for me performance-wise)


Additionally, I have tried to insert again if I encountered the error. The first thing I tried was to was put a try-except block around where I call execute on the sqlalchemy.engine.base.Connection object like the following:

try:
    connection.execute(my_table.insert(), records)
except DatabaseError as e:
    connection.execute(my_table.insert(), records)


I noticed that using this method the second insertion still often fails. The second thing I tried was to try the same in the implementation of do_executemany of OracleDialect_cx_oracle in the sqlalchemy package (sqlalchemy\dialects\oracle\cx_oracle.py):

def do_executemany(self, cursor, statement, parameters, context=None):
    if isinstance(parameters, tuple):
        parameters = list(parameters)

    # original code
    # cursor.executemany(statement, parameters)

    # new code
    try:
        cursor.executemany(statement, parameters)
    except Exception as e:
        print('trying again')
        cursor.executemany(statement, parameters)


Strangely, when I do it this way the second executemany call will always work if the first one fails. I'm not certain what this means but I believe this points to the cx_Oracle driver being the cause of the issue instead of sqlalchemy. 

I have searched everywhere online and have not seen any reports of the same problem. Any help would be greatly appreciated!
","I had the same problem, the code would some times fail and some times go through with no error. Apparently my chunk size was to big for buffer, the error did not occur anymore once I reduced the chunk size from 10K to 500 rows.
","We found out that if we replace all the float.nan objects with None before we do the insertion the error disappears completely. Very weird indeed!
",,false,
https://stackoverflow.com/questions/33161280,true,The issue involves an unexpected failure when instantiating an object from the SQLAlchemy ORM. It appears to be related to the usage of the declarative base and the metadata associated with it.,SQLAlchemy,declarative_base,"After dropping and recreating the schema, the common declarative base class loses its metadata, leading to an AttributeError when trying to create objects.",The declarative base class should have its metadata intact after dropping and recreating the schema.,"The issue is triggered when the schema is dropped and recreated, causing the declarative base class to lose its metadata.",This issue might be challenging to detect because it involves the interaction between multiple files and the usage of the declarative base class. It requires understanding the internal workings of SQLAlchemy's ORM and the specific behavior of the declarative_base API.,SQLAlchemy instantiate object from ORM fails with AttributeError: mapper,"I've been trying to get a decent-sized project going with SQLAlchemy on the backend. I have table models across multiple files, a declarative base in its own file, and a helper file to wrap common SQLAlchemy functions, and driver file.

I was uploading data, then decided to add a column. Since this is just test data I thought it'd be simplest to just drop all of the tables and start fresh... then when I tried to recreate the schema and tables, the common declarative base class suddenly had empty metadata. I worked around this by importing the class declaration files -- strange, since I didn't need those imports before -- and it was able to recreate the schema successfully.

But now when I try to create objects again, I get an error:

AttributeError: mapper


Now I'm totally confused! Can someone explain what's happening here? It was working fine before I dropped the schema and now I can't get it working.

Here's the skeleton of my setup:

base.py

from sqlalchemy.ext.declarative import declarative_base
Base = declarative_base()


models1.py

from base import Base
class Business(Base):
    __tablename__ = 'business'
    id = Column(Integer, primary_key=True)


models2.py:

from base import Base
class Category(Base):
    __tablename__ = 'category'
    id = Column(Integer, primary_key=True)


helper.py:

from base import Base

# I didn't need these two imports the first time I made the schema
# I added them after I was just getting an empty schema from base.Base
# but have no idea why they're needed now?
import models1
import models2

def setupDB():
    engine = getDBEngine(echo=True) # also a wrapped func (omitted for space)
    #instantiate the schema
    try:
        Base.metadata.create_all(engine, checkfirst=True)
        logger.info(""Successfully instantiated Database with model schema"")
    except:
        logger.error(""Failed to instantieate Database with model schema"")
        traceback.print_exc()

def dropAllTables():
    engine = getDBEngine(echo=True)
    # drop the schema
    try:
        Base.metadata.reflect(engine, extend_existing=True)
        Base.metadata.drop_all(engine)
        logger.info(""Successfully dropped all the database tables in the schema"")
    except:
        logger.error(""Failed to drop all tables"")
        traceback.print_exc()


driver.py:

import models1
import models2

# ^ some code to get to this point
categories []
categories.append(
                models2.Category(alias=category['alias'],
                                 title=category['title']) # error occurs here
                )


stack trace: (for completeness)

File ""./main.py"", line 16, in &lt;module&gt;
yelp.updateDBFromYelpFeed(fname)
  File ""/Users/thomaseffland/Development/projects/health/pyhealth/pyhealth/data/sources/yelp.py"", line 188, in updateDBFromYelpFeed
    title=category['title'])
  File ""&lt;string&gt;"", line 2, in __init__
  File ""/Users/thomaseffland/.virtualenvs/health/lib/python2.7/site-packages/sqlalchemy/orm/instrumentation.py"", line 347, in _new_state_if_none
    state = self._state_constructor(instance, self)
  File ""/Users/thomaseffland/.virtualenvs/health/lib/python2.7/site-packages/sqlalchemy/util/langhelpers.py"", line 747, in __get__
    obj.__dict__[self.__name__] = result = self.fget(obj)
  File ""/Users/thomaseffland/.virtualenvs/health/lib/python2.7/site-packages/sqlalchemy/orm/instrumentation.py"", line 177, in _state_constructor
    self.dispatch.first_init(self, self.class_)
  File ""/Users/thomaseffland/.virtualenvs/health/lib/python2.7/site-packages/sqlalchemy/event/attr.py"", line 256, in __call__
    fn(*args, **kw)
  File ""/Users/thomaseffland/.virtualenvs/health/lib/python2.7/site-packages/sqlalchemy/orm/mapper.py"", line 2825, in _event_on_first_init
    configure_mappers()
  File ""/Users/thomaseffland/.virtualenvs/health/lib/python2.7/site-packages/sqlalchemy/orm/mapper.py"", line 2721, in configure_mappers
    mapper._post_configure_properties()
  File ""/Users/thomaseffland/.virtualenvs/health/lib/python2.7/site-packages/sqlalchemy/orm/mapper.py"", line 1710, in _post_configure_properties
    prop.init()
  File ""/Users/thomaseffland/.virtualenvs/health/lib/python2.7/site-packages/sqlalchemy/orm/interfaces.py"", line 183, in init
    self.do_init()
  File ""/Users/thomaseffland/.virtualenvs/health/lib/python2.7/site-packages/sqlalchemy/orm/relationships.py"", line 1616, in do_init
    self._process_dependent_arguments()
  File ""/Users/thomaseffland/.virtualenvs/health/lib/python2.7/site-packages/sqlalchemy/orm/relationships.py"", line 1673, in     _process_dependent_arguments
    self.target = self.mapper.mapped_table
  File ""/Users/thomaseffland/.virtualenvs/health/lib/python2.7/site-packages/sqlalchemy/util/langhelpers.py"", line 833, in __getattr__
    return self._fallback_getattr(key)
  File ""/Users/thomaseffland/.virtualenvs/health/lib/python2.7/site-packages/sqlalchemy/util/langhelpers.py"", line 811, in _fallback_getattr
    raise AttributeError(key)
AttributeError: mapper


I know this post is long, but I wanted to give the complete picture. First I am confused why the base.Base schema was empty in the first place. Now I am confused why the Categories object is missing a mapper!

Any help/insight/advice is greatly appreciated, thanks!

Edit:

So the model files and helper.py are in a supackage and the driver.py is actually a file in a sibling subpackage and its code is wrapped in a function.  This driver function is called by a package-level main file.  So I don't think it can be because SQLAlchemy hasn't had time to initialize? (If I understand the answer correctly)  here is what the (relevant part of) main file looks like:

main.py:

import models.helper as helper
helper.setupDB(echo=true) # SQLAlchemy echos the correct statements

import driverpackage.driver as driver
driver.updateDBFromFile(fname) # error occurs in here


and driver.py actually looks like:

import ..models.models1
import ..models.models2

def updateDBFromFile(fname):
    # ^ some code to get to this point
    categories []
    categories.append(
                    models2.Category(alias=category['alias'],
                                     title=category['title']) # error occurs here
                    )
    # a bunch more code


Edit 2:
I'm beginning to suspect the underlying issue is the same reason I suddenly need to import all of the models to set up the schema in helper.py.  If I print the tables of the imported model objects, they have no bound MetaData or schema:

print YelpCategory.__dict__['__table__'].__dict__
####
{'schema': None, '_columns': &lt;sqlalchemy.sql.base.ColumnCollection object at 0x102312ef0&gt;, 
'name': 'yelp_category', 'description': 'yelp_category', 
'dispatch': &lt;sqlalchemy.event.base.DDLEventsDispatch object at 0x10230caf0&gt;, 
'indexes': set([]), 'foreign_keys': set([]), 
'columns': &lt;sqlalchemy.sql.base.ImmutableColumnCollection object at 0x10230fc58&gt;, 
'_prefixes': [], 
'_extra_dependencies': set([]), 
'fullname': 'yelp_category', 'metadata': MetaData(bind=None), 
'implicit_returning': True, 
'constraints': set([PrimaryKeyConstraint(Column('id', Integer(), table=&lt;yelp_category&gt;, primary_key=True, nullable=False))]), 'primary_key': PrimaryKeyConstraint(Column('id', Integer(), table=&lt;yelp_category&gt;, primary_key=True, nullable=False))}


I wonder why the metadata from the base that created the database is not gettng bound?
","The question seems to have died off, so I'll post my work around.  It's simple.  I refactored the code to explicitly supply the classes and the classic mappers, instead of using the declarative base and everything worked fine again...
","I guess the error happens because you are executing your code on Python module-level. This code is executed when Python imports the module.


Move your code to a function.
Call the function after SQLAlchemy has been properly initialized.
create_all() needs to call only once when the application is installed, because created tables persistent in the database
You need DBSession.configure(bind=engine) or related which will tell models to which database connection they are related. This is missing from the question.

",,false,
https://stackoverflow.com/questions/57014580,true,The issue involves accessing an SQLAlchemy database on Heroku and encountering an error related to a 'user' model that doesn't exist. This suggests a potential problem with the API or database configuration.,SQLAlchemy,db.Model,"When attempting to access the 'user' table using the 'Post' model, an error occurs stating that the table does not exist. This is unexpected since the 'Post' model is defined with the '__tablename__' attribute set to 'user'.",The 'Post' model should be able to access the 'user' table in the database.,"The issue is triggered when attempting to query the 'user' table using the 'Post' model, possibly due to a misconfiguration or missing migration.","This issue might be challenging to detect during development and testing, especially if the local environment has the 'user' table already created or if the issue only arises when deploying to Heroku.",Can&#39;t access sqlalchemy database from heroku. Strange error about some &#39;user&#39; model that I don&#39;t have not existing,"I have an SQLAlchemy database running on a flask app that works fine locally, but as soon as I run it on Heroku I get a strange issue (See stack trace below).

I don't have a model called 'user' so I don't know what's going on. I've made sure that I've run db.create_all().

Creating a row in my table works fine, but it's only once I try selecting some rows do I get an issue (__init__.py):

@app.route('/blog')
def blog():
    title = 'Blog'
    name = get_name()

    blog_text = []

    for post in sorted(Post.query.filter_by(category=Post.Category.blog),
                       key=lambda p: p.posted_at, reverse=True):
        content = post.content

        blog_text.append(content)

    content = '\n'.join(blog_text)

    return render_template('blog.html', **locals())


And my 'Post' logic in my models.py:


class Post(db.Model):
    __tablename__ = 'user'

    id = db.Column(db.Integer, primary_key=True)

    name = db.Column(db.String(200), nullable=False)
    posted_at = db.Column(db.DateTime, nullable=False, default=datetime.now())
    content = db.Column(db.String(200), nullable=False)
    content_type = db.Column(db.String(200), default=""md"", nullable=False)  # md / html
    category = db.Column(db.String(200), nullable=False, default=""blog"")  # poetry / blog

    class Type:
        md = ""md""
        html = ""html""

    class Category:
        poetry = ""poetry""
        blog = ""blog""

   ...



Here's the error:

2019-07-12T21:59:09.120100+00:00 app[web.1]: 10.63.22.102 - - [12/Jul/2019:21:59:09 +0000] ""GET /blog HTTP/1.1"" 500 290 ""https://secure-savannah-20745.herokuapp.com/"" ""Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/75.0.3770.100 Safari/537.36""
2019-07-12T21:59:09.120443+00:00 heroku[router]: at=info method=GET path=""/blog"" host=secure-savannah-20745.herokuapp.com request_id=c32b2484-c8ff-4910-a4e2-5089eca7c6d3 fwd=""98.0.144.26"" dyno=web.1 connect=2ms service=66ms status=500 bytes=455 protocol=https
2019-07-12T21:59:13.632588+00:00 app[web.1]: 10.63.22.102 - - [12/Jul/2019:21:59:13 +0000] ""GET /projects HTTP/1.1"" 200 2342 ""https://secure-savannah-20745.herokuapp.com/"" ""Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/75.0.3770.100 Safari/537.36""
2019-07-12T21:59:13.837771+00:00 app[web.1]: 10.63.22.102 - - [12/Jul/2019:21:59:13 +0000] ""GET /static/circles.js HTTP/1.1"" 200 0 ""https://secure-savannah-20745.herokuapp.com/projects"" ""Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/75.0.3770.100 Safari/537.36""
2019-07-12T21:59:13.838434+00:00 heroku[router]: at=info method=GET path=""/static/circles.js"" host=secure-savannah-20745.herokuapp.com request_id=8476c020-fc32-47b3-a4e1-7b6faeb115ca fwd=""98.0.144.26"" dyno=web.1 connect=1ms service=6ms status=200 bytes=5449 protocol=https
2019-07-12T21:59:13.633394+00:00 heroku[router]: at=info method=GET path=""/projects"" host=secure-savannah-20745.herokuapp.com request_id=faa3d473-7bda-4e39-810b-e3994eac7390 fwd=""98.0.144.26"" dyno=web.1 connect=1ms service=1494ms status=200 bytes=2504 protocol=https
2019-07-12T21:59:18.074035+00:00 heroku[router]: at=info method=GET path=""/blog"" host=secure-savannah-20745.herokuapp.com request_id=e9748e89-dcce-43db-8db8-a16c7fe657ab fwd=""98.0.144.26"" dyno=web.1 connect=3ms service=12ms status=500 bytes=455 protocol=https
2019-07-12T21:59:18.070372+00:00 app[web.1]: [2019-07-12 21:59:18,069] ERROR in app: Exception on /blog [GET]
2019-07-12T21:59:18.070385+00:00 app[web.1]: Traceback (most recent call last):
2019-07-12T21:59:18.070396+00:00 app[web.1]: File ""/app/.heroku/python/lib/python3.6/site-packages/sqlalchemy/engine/base.py"", line 1244, in _execute_context
2019-07-12T21:59:18.070398+00:00 app[web.1]: cursor, statement, parameters, context
2019-07-12T21:59:18.070403+00:00 app[web.1]: File ""/app/.heroku/python/lib/python3.6/site-packages/sqlalchemy/engine/default.py"", line 550, in do_execute
2019-07-12T21:59:18.070405+00:00 app[web.1]: cursor.execute(statement, parameters)
2019-07-12T21:59:18.070407+00:00 app[web.1]: sqlite3.OperationalError: no such table: user
2019-07-12T21:59:18.070410+00:00 app[web.1]: 
2019-07-12T21:59:18.070412+00:00 app[web.1]: The above exception was the direct cause of the following exception:
2019-07-12T21:59:18.070414+00:00 app[web.1]: 
2019-07-12T21:59:18.070416+00:00 app[web.1]: Traceback (most recent call last):
2019-07-12T21:59:18.070418+00:00 app[web.1]: File ""/app/.heroku/python/lib/python3.6/site-packages/flask/app.py"", line 2292, in wsgi_app
2019-07-12T21:59:18.070421+00:00 app[web.1]: response = self.full_dispatch_request()
2019-07-12T21:59:18.070423+00:00 app[web.1]: File ""/app/.heroku/python/lib/python3.6/site-packages/flask/app.py"", line 1815, in full_dispatch_request
2019-07-12T21:59:18.070425+00:00 app[web.1]: rv = self.handle_user_exception(e)
2019-07-12T21:59:18.070427+00:00 app[web.1]: File ""/app/.heroku/python/lib/python3.6/site-packages/flask/app.py"", line 1718, in handle_user_exception
2019-07-12T21:59:18.070430+00:00 app[web.1]: reraise(exc_type, exc_value, tb)
2019-07-12T21:59:18.070432+00:00 app[web.1]: File ""/app/.heroku/python/lib/python3.6/site-packages/flask/_compat.py"", line 35, in reraise
2019-07-12T21:59:18.070434+00:00 app[web.1]: raise value
2019-07-12T21:59:18.070436+00:00 app[web.1]: File ""/app/.heroku/python/lib/python3.6/site-packages/flask/app.py"", line 1813, in full_dispatch_request
2019-07-12T21:59:18.070438+00:00 app[web.1]: rv = self.dispatch_request()
2019-07-12T21:59:18.070446+00:00 app[web.1]: File ""/app/.heroku/python/lib/python3.6/site-packages/flask/app.py"", line 1799, in dispatch_request
2019-07-12T21:59:18.070450+00:00 app[web.1]: return self.view_functions[rule.endpoint](**req.view_args)
2019-07-12T21:59:18.070452+00:00 app[web.1]: File ""/app/main.py"", line 65, in blog
2019-07-12T21:59:18.070455+00:00 app[web.1]: key=lambda p: p.posted_at, reverse=True):
2019-07-12T21:59:18.070457+00:00 app[web.1]: File ""/app/.heroku/python/lib/python3.6/site-packages/sqlalchemy/orm/query.py"", line 3324, in __iter__
2019-07-12T21:59:18.070459+00:00 app[web.1]: return self._execute_and_instances(context)
2019-07-12T21:59:18.070461+00:00 app[web.1]: File ""/app/.heroku/python/lib/python3.6/site-packages/sqlalchemy/orm/query.py"", line 3349, in _execute_and_instances
2019-07-12T21:59:18.070463+00:00 app[web.1]: result = conn.execute(querycontext.statement, self._params)
2019-07-12T21:59:18.070466+00:00 app[web.1]: File ""/app/.heroku/python/lib/python3.6/site-packages/sqlalchemy/engine/base.py"", line 988, in execute
2019-07-12T21:59:18.070468+00:00 app[web.1]: return meth(self, multiparams, params)
2019-07-12T21:59:18.070470+00:00 app[web.1]: File ""/app/.heroku/python/lib/python3.6/site-packages/sqlalchemy/sql/elements.py"", line 287, in _execute_on_connection
2019-07-12T21:59:18.070472+00:00 app[web.1]: return connection._execute_clauseelement(self, multiparams, params)
2019-07-12T21:59:18.070474+00:00 app[web.1]: File ""/app/.heroku/python/lib/python3.6/site-packages/sqlalchemy/engine/base.py"", line 1107, in _execute_clauseelement
2019-07-12T21:59:18.070477+00:00 app[web.1]: distilled_params,
2019-07-12T21:59:18.070479+00:00 app[web.1]: File ""/app/.heroku/python/lib/python3.6/site-packages/sqlalchemy/engine/base.py"", line 1248, in _execute_context
2019-07-12T21:59:18.070481+00:00 app[web.1]: e, statement, parameters, cursor, context
2019-07-12T21:59:18.070484+00:00 app[web.1]: File ""/app/.heroku/python/lib/python3.6/site-packages/sqlalchemy/engine/base.py"", line 1466, in _handle_dbapi_exception
2019-07-12T21:59:18.070486+00:00 app[web.1]: util.raise_from_cause(sqlalchemy_exception, exc_info)
2019-07-12T21:59:18.070488+00:00 app[web.1]: File ""/app/.heroku/python/lib/python3.6/site-packages/sqlalchemy/util/compat.py"", line 399, in raise_from_cause
2019-07-12T21:59:18.070491+00:00 app[web.1]: reraise(type(exception), exception, tb=exc_tb, cause=cause)
2019-07-12T21:59:18.070493+00:00 app[web.1]: File ""/app/.heroku/python/lib/python3.6/site-packages/sqlalchemy/util/compat.py"", line 153, in reraise
2019-07-12T21:59:18.070495+00:00 app[web.1]: raise value.with_traceback(tb)
2019-07-12T21:59:18.070498+00:00 app[web.1]: File ""/app/.heroku/python/lib/python3.6/site-packages/sqlalchemy/engine/base.py"", line 1244, in _execute_context
2019-07-12T21:59:18.070500+00:00 app[web.1]: cursor, statement, parameters, context
2019-07-12T21:59:18.070502+00:00 app[web.1]: File ""/app/.heroku/python/lib/python3.6/site-packages/sqlalchemy/engine/default.py"", line 550, in do_execute
2019-07-12T21:59:18.070504+00:00 app[web.1]: cursor.execute(statement, parameters)
2019-07-12T21:59:18.070507+00:00 app[web.1]: sqlalchemy.exc.OperationalError: (sqlite3.OperationalError) no such table: user
2019-07-12T21:59:18.070513+00:00 app[web.1]: [SQL: SELECT user.id AS user_id, user.name AS user_name, user.posted_at AS user_posted_at, user.content AS user_content, user.content_type AS user_content_type, user.category AS user_category
2019-07-12T21:59:18.070515+00:00 app[web.1]: FROM user
2019-07-12T21:59:18.070517+00:00 app[web.1]: WHERE user.category = ?]
2019-07-12T21:59:18.070520+00:00 app[web.1]: [parameters: ('blog',)]
2019-07-12T21:59:18.070661+00:00 app[web.1]: (Background on this error at: http://sqlalche.me/e/e3q8)
2019-07-12T21:59:18.072919+00:00 app[web.1]: 10.63.22.102 - - [12/Jul/2019:21:59:18 +0000] ""GET /blog HTTP/1.1"" 500 290 ""https://secure-savannah-20745.herokuapp.com/projects"" ""Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/75.0.3770.100 Safari/537.36""


","You missed that your table is named user in your model:
class Post(db.Model):
    __tablename__ = 'user'
    #               ^^^^^^

The reason it doesn't exist is because Heroku doesn't keep your filesystem changes. You are using a SQLite database:
... app[web.1]: sqlite3.OperationalError: no such table: user
#               ^^^^^^^

but a sqlite database is stored on the filesystem of your Heroku dyno. Heroku cleans up (cycles) dynos all the time and spins up a new dyno for you for new requests. From their documentation on how Heroku works:

Changes to the filesystem on one dyno are not propagated to other dynos and are not persisted across deploys and dyno restarts. A better and more scalable approach is to use a shared resource such as a database or queue.

Each time a new dyno is spun up to handle your requests, you'll have a new, clean filesystem without your sqlite database, and so an empty database is created again without any tables in it.
Bottom line: You can't use sqlite on Heroku.
Use a Postgres database instead, see the Heroku Postgres documentation. Don't worry, there is a free tier you can use. When you provision the database as an add-on, the DATABASE_URL config var is set for you, which you can read from your app environment.
I usually configure Flask apps that are designed to run on Heroku to read the environment variable but to fall back to a SQLite database when the environment variable is not set:
import os
from pathlib import Path

_project_root = Path(__file__).resolve().parent.parent
_default_sqlite_db = _project_root / ""database.db""

SQLALCHEMY_DATABASE_URI = os.environ.get(
    ""DATABASE_URL"", f""sqlite:///{_default_sqlite_db}""
)

This makes it easy to develop the app locally on SQLite, or set up a local Postgres database for integration testing, and then deploy to Heroku for staging or production.
",,,false,
